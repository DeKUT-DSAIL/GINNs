{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6d4f80",
   "metadata": {
    "_cell_guid": "5fbc7d50-bc9d-4e73-aba7-d7ef5fd55ae5",
    "_uuid": "16c6b475-8271-444d-85dd-c0c90ba51a4e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007025,
     "end_time": "2026-02-08T12:56:59.484628",
     "exception": false,
     "start_time": "2026-02-08T12:56:59.477603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook covers three model implementations:\n",
    "\n",
    "1. A physics based model based on the Gompertz function\n",
    "2. To train an LSTM model (data-driven-model) to predict a sequence (e.g., battery capacity over time)\n",
    "3. To train an LSTM model to predict a sequence (e.g., battery capacity over time) while ensuring it respects physical behavior modeled by the Gompertz function.\n",
    "\n",
    "\n",
    "## FIXES \n",
    "\n",
    "### Fix 1 : Added Sanity Check for val loss and val rmse\n",
    "\n",
    "Explanation:\n",
    "Issue\tFix\tWhy\n",
    "torch.Tensor()\t‚Üí torch.tensor()\tThe lowercase version is the recommended constructor for creating a single-value tensor.\n",
    "Missing closing parenthesis\tAdded\tFixes syntax error.\n",
    "avg_val_loss type\tEnsure it‚Äôs a scalar (float or int)\tIf it‚Äôs already a tensor, remove the outer torch.tensor() call.\n",
    "\n",
    "### Fix 2: Use log loss in calculation of metric charts\n",
    "### Fix 3: Update reproducibility\n",
    "### Fix 4: time GPU run\n",
    "### Fix 5: LSTMs that predict RUL given initial x SoH values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91266773",
   "metadata": {
    "_cell_guid": "bc0a48fc-f795-4b2c-a672-5942fa073d9a",
    "_uuid": "d905f68b-0092-4d8e-980e-bd9d93d36ec8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-08T12:56:59.497680Z",
     "iopub.status.busy": "2026-02-08T12:56:59.497403Z",
     "iopub.status.idle": "2026-02-08T12:57:08.891879Z",
     "shell.execute_reply": "2026-02-08T12:57:08.891120Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 9.403236,
     "end_time": "2026-02-08T12:57:08.893531",
     "exception": false,
     "start_time": "2026-02-08T12:56:59.490295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A: Import Libraries and set reproducibility\n",
      "‚úÖ Reproducibility environment set with seed = 42\n",
      "# B: Setup variables and functions\n",
      "## üß† Model Architecture\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"# A: Import Libraries and set reproducibility\")\n",
    "# !git clone https://github.com/Yuri-Njathi/battery-lstm-ML.git\n",
    "# import sys\n",
    "# sys.path.append(\"battery-lstm-ML/\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error , root_mean_squared_error\n",
    "from typing import Callable, Optional\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set all relevant random seeds to ensure full reproducibility.\n",
    "    \"\"\"\n",
    "    # 1. Set basic seeds\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if multiple GPUs\n",
    "    \n",
    "    # 2. Force deterministic behavior in cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # turn off auto-tuning\n",
    "    \n",
    "    # 3. Optional: make dataloaders deterministic\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'  # deterministic cublas (for CUDA >= 10.2)\n",
    "\n",
    "    print(f\"‚úÖ Reproducibility environment set with seed = {seed}\")\n",
    "\n",
    "# Call this once at the start, pull from assess\n",
    "set_seed(42)\n",
    "\n",
    "'''\n",
    "set mode i.e.\n",
    "0 == physics based\n",
    "1 == lstm (data driven) #SoH only \n",
    "2 == lstm (physics constrained)\n",
    "'''\n",
    "mode = 2\n",
    "\n",
    "if mode == 1:\n",
    "    model_columns = ['SoH', 'Cycle number']\n",
    "if mode == 2:\n",
    "    model_columns = ['SoH']\n",
    "\n",
    "\n",
    "\n",
    "print(\"# B: Setup variables and functions\")\n",
    "# # Set variables\n",
    "# WINDOW_SIZE = 35\n",
    "model_type = ['lstm','seq2seq-lstm','pinn'][2]\n",
    "\n",
    "cutoff_soh = 0.70\n",
    "# Set Computing Environment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_SIZE = len(model_columns)\n",
    "OUTPUT_SIZE =3 #RUL #len(model_columns)#1 #controls how many values the lstm outputs\n",
    "num_epochs = 1000 #60\n",
    "batch_size = 4 #32 #because the data is little a small batch size is better when training\n",
    "normalize_soh = False\n",
    "if normalize_soh:\n",
    "    soh_normalization_constant = 115.0 #115.0 may be better as it allows bounding between 0 and 1\n",
    "    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\n",
    "else:\n",
    "    soh_normalization_constant = 1.0\n",
    "    cutoff_soh = cutoff_soh/soh_normalization_constant #set cutoff soh wrt to normalizer\n",
    "\n",
    "\n",
    "\n",
    "# def give_paths_get_loaders(paths,data_type,shuffle=False):\n",
    "#     X_list, y_list, y_target = get_x_y_lists(paths)\n",
    "\n",
    "#     if INPUT_SIZE == 1:\n",
    "#         # Concatenate all X and y\n",
    "#         X_1,y_1,y_2 = torch.cat(X_list, dim=0).squeeze(-1),torch.cat(y_list, dim=0).view(-1,OUTPUT_SIZE).squeeze(-1)\n",
    "#     else:\n",
    "#         X_1,y_1, y_2 = torch.cat(X_list, dim=0), torch.cat(y_list, dim=0), torch.cat(y_target, dim=0).view(-1,OUTPUT_SIZE).squeeze(-1)#.view(-1, OUTPUT_SIZE)\n",
    "    \n",
    "#     print(f\"X_{data_type} , y_{data_type} shapes : \",X_1.shape, y_1.shape)\n",
    "    \n",
    "#     #DataLoader\n",
    "#     print(\"load : \")\n",
    "#     loader = DataLoader(TensorDataset(X_1, y_1), batch_size=32, shuffle=shuffle)\n",
    "#     print(f\"{data_type}loader lengths : \",loader.__len__())\n",
    "#     return loader,X_1,y_1,y2\n",
    "\n",
    "\n",
    "\n",
    "print(\"## üß† Model Architecture\")\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=64, output_size=5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # LSTM: input_size=5 match your features\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Linear Layer: Maps hidden_size (64) -> output_size (5)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [32, 10, 5]\n",
    "        \n",
    "        # Run LSTM\n",
    "        # lstm_out shape: [32, 10, 64]\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take the last time step only\n",
    "        last_time_step = lstm_out[:, -1, :] \n",
    "        # last_time_step shape: [32, 64]\n",
    "        \n",
    "        # Project to 5 output features\n",
    "        prediction = self.fc(last_time_step)\n",
    "        # prediction shape: [32, 5]\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "\n",
    "class PhysicsInformedLSTM(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=128):\n",
    "        super().__init__()\n",
    "        # Direct mapping: Input -> LSTM -> FC -> Output\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 3) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, mid_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(hidden_size, mid_size)\n",
    "        self.fc2 = nn.Linear(mid_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # out: [batch_size, seq_len, hidden_size]\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Apply the linear layers to each timestep\n",
    "        mid = self.relu(self.fc1(out))   # shape: [batch, seq_len, mid_size]\n",
    "        out_seq = self.fc2(mid)          # shape: [batch, seq_len, output_size]\n",
    "\n",
    "        return out_seq\n",
    "\n",
    "\n",
    "    #on initial tensorflow experiments I used 1,64,1,1 for those values.\n",
    "\n",
    "### TEST ON SEQUENTIAL MODEL ###\n",
    "# model(torch.Tensor([[86.4707],[86.4150],[86.3590],[86.3035],[86.2506],[86.2512],[86.1954],[86.1403],[86.1427],[86.0904],[86.0373],[85.9772],[85.9743],[85.9198],[85.8654],[85.8090],[85.8077],[85.7524],[85.6986],[85.6407],[85.5883],[85.5882],[85.6112],[85.4756],[85.4753],[85.4187],[85.3639],[85.3086],[85.3098],[85.3628],[85.1723],[85.1430],[85.1444],[85.0896],[85.0364]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd97df7",
   "metadata": {
    "_cell_guid": "bfed866a-86b9-4ad2-9f29-45f1fac22912",
    "_uuid": "5a5cf86c-22c4-4f30-a2ea-27e58384e19a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005769,
     "end_time": "2026-02-08T12:57:08.905481",
     "exception": false,
     "start_time": "2026-02-08T12:57:08.899712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### ‚ÄúNormalization‚Äù ‚â† ‚Äúscaling to [0,1]‚Äù.\n",
    "\n",
    "#### It simply means rescaling values to a stable, comparable numerical range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe11f9",
   "metadata": {
    "_cell_guid": "ae57eb62-c436-4826-96f2-cc55c36376ee",
    "_uuid": "e5c11278-599b-4fcf-a3a3-4b5cffc4aab6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005543,
     "end_time": "2026-02-08T12:57:08.917410",
     "exception": false,
     "start_time": "2026-02-08T12:57:08.911867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# C: Setup Train, Val and Test Loaders\n",
    "# 0. Data\n",
    "\n",
    "Each `X_train` is of shape `(num_samples, window_size)`\n",
    "\n",
    "Each `y_train` is of shape `(num_samples,)` (usually next value prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc07a5",
   "metadata": {
    "_cell_guid": "e87496f0-d330-454c-82f9-86f0f16e746d",
    "_uuid": "e55a3991-26a1-4a5a-9aa1-a4095a055a67",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005572,
     "end_time": "2026-02-08T12:57:08.928569",
     "exception": false,
     "start_time": "2026-02-08T12:57:08.922997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  üß∞ Convert to Tensors for LSTM\n",
    "LSTM expects input shape: (batch_size, sequence_length, num_features)\n",
    "\n",
    "Let‚Äôs reshape the data and convert it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4179b26c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T12:57:08.941032Z",
     "iopub.status.busy": "2026-02-08T12:57:08.940640Z",
     "iopub.status.idle": "2026-02-08T12:57:08.944369Z",
     "shell.execute_reply": "2026-02-08T12:57:08.943838Z"
    },
    "papermill": {
     "duration": 0.011541,
     "end_time": "2026-02-08T12:57:08.945724",
     "exception": false,
     "start_time": "2026-02-08T12:57:08.934183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6610d3fe",
   "metadata": {
    "_cell_guid": "da093279-8ac7-4672-b25f-7a87931a0801",
    "_uuid": "764511b2-155c-4f27-9abb-b9020665168e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-08T12:57:08.957953Z",
     "iopub.status.busy": "2026-02-08T12:57:08.957697Z",
     "iopub.status.idle": "2026-02-08T12:57:08.975121Z",
     "shell.execute_reply": "2026-02-08T12:57:08.974354Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025006,
     "end_time": "2026-02-08T12:57:08.976405",
     "exception": false,
     "start_time": "2026-02-08T12:57:08.951399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINDOW SIZES TO TEST :  [100] 1\n",
      "['6-6', '8-7', '8-6', '9-1', '10-1', '6-8', '8-8', '10-7', '3-5', '5-1', '5-5', '7-1', '2-7', '10-6', '4-7', '7-7', '7-6', '4-5', '9-2', '10-4', '3-1', '9-7', '8-1', '10-8', '8-4', '4-6', '4-4', '3-8', '5-4', '9-6', '10-5', '7-8', '5-2', '9-8', '1-2', '5-6', '10-2', '2-6', '6-1', '2-4', '1-4', '4-1', '1-6', '6-2', '8-5', '5-7', '1-5', '1-8', '5-3', '6-5', '9-5', '4-8', '7-2', '2-5', '7-3', '9-3', '9-4', '8-2', '10-3', '6-3', '3-2', '7-5', '3-7', '2-3', '1-3', '8-3', '2-8', '7-4', '4-2', '6-4', '1-1', '3-3', '4-3', '3-4', '2-2', '1-7', '3-6']\n",
      "['6-6', '8-6', '10-1', '6-8', '8-8', '10-7', '10-6', '7-6', '4-5', '10-4', '3-1', '8-1', '9-6', '1-2', '6-1', '6-2', '8-5', '5-3', '2-5', '9-4', '7-5', '1-1'] 22\n",
      "55 11 11\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZES = [100] #[i for i in range (5,100,5)]\n",
    "print(\"WINDOW SIZES TO TEST : \",WINDOW_SIZES,len(WINDOW_SIZES))\n",
    "\n",
    "\n",
    "# Make list of CSV paths\n",
    "main_files_path = '/kaggle/input/generate-hust-data-gompertz-k-a-b/'\n",
    "#/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n",
    "csv_files = os.listdir(main_files_path)\n",
    "csv_files = [f for f in csv_files if re.match(r'^\\d', f) and f.endswith('-hust_gompertz_params.csv')]\n",
    "\n",
    "#BatteryML like train-val-test split\n",
    "csv_files = [f.removesuffix('-hust_gompertz_params.csv') for f in csv_files]\n",
    "print(csv_files)\n",
    "\n",
    "train_ids = [\n",
    "    '1-3',  '1-4',  '1-5',  '1-6',  '1-7',  '1-8',  '2-2',  '2-3',\n",
    "    '2-4',  '2-6',  '2-7',  '2-8',  '3-2',  '3-3',  '3-4',  '3-5',\n",
    "    '3-6',  '3-7',  '3-8',  '4-1',  '4-2',  '4-3',  '4-4',  '4-6',\n",
    "    '4-7',  '4-8',  '5-1',  '5-2',  '5-4',  '5-5',  '5-6',  '5-7',\n",
    "    '6-3',  '6-4',  '6-5',  '7-1',  '7-2',  '7-3',  '7-4',  '7-7',\n",
    "    '7-8',  '8-2',  '8-3',  '8-4',  '8-7',  '9-1',  '9-2',  '9-3',\n",
    "    '9-5',  '9-7',  '9-8',  '10-2', '10-3', '10-5', '10-8']\n",
    "\n",
    "test_ids = [f for f in csv_files if f not in train_ids]\n",
    "\n",
    "print(test_ids,len(test_ids))\n",
    "\n",
    "#csv_paths = [os.path.join(main_files_path, file) for file in csv_files]\n",
    "#separate according to train, val and test\n",
    "train_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in train_ids]\n",
    "\n",
    "testing_paths = [os.path.join(main_files_path, file+'-hust_gompertz_params.csv') for file in test_ids]\n",
    "\n",
    "val_paths = testing_paths[:int(len(testing_paths)*0.5)]\n",
    "test_paths = testing_paths[int(len(testing_paths)*0.5):]\n",
    "\n",
    "print(len(train_paths), len(val_paths), len(test_paths))\n",
    "\n",
    "# 1. Helper function to detach any GPU tensors in your lists\n",
    "def to_cpu_numpy(data_list):\n",
    "    clean_list = []\n",
    "    for x in data_list:\n",
    "        if torch.is_tensor(x):\n",
    "            clean_list.append(x.detach().cpu().item()) # Move to CPU, get scalar\n",
    "        else:\n",
    "            clean_list.append(x) # Already a float\n",
    "    return np.array(clean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08bec30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T12:57:08.988808Z",
     "iopub.status.busy": "2026-02-08T12:57:08.988612Z",
     "iopub.status.idle": "2026-02-08T12:57:08.998371Z",
     "shell.execute_reply": "2026-02-08T12:57:08.997567Z"
    },
    "papermill": {
     "duration": 0.017386,
     "end_time": "2026-02-08T12:57:08.999646",
     "exception": false,
     "start_time": "2026-02-08T12:57:08.982260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Cycle Life (25 items): Range 1142 - 1690\n",
      "[1142, 1143, 1295, 1308, 1348, 1380, 1386, 1393, 1400, 1419, 1448, 1481, 1491, 1499, 1500, 1504, 1561, 1572, 1583, 1609, 1649, 1678, 1679, 1685, 1690]\n",
      "--------------------\n",
      "Mid Cycle Life (26 items): Range 1697 - 2030\n",
      "[1697, 1702, 1706, 1717, 1742, 1751, 1766, 1782, 1783, 1804, 1811, 1848, 1858, 1875, 1885, 1905, 1908, 1926, 1938, 1938, 1962, 1971, 1975, 2012, 2030, 2030]\n",
      "--------------------\n",
      "High Cycle Life (26 items): Range 2041 - 2689\n",
      "[2041, 2047, 2057, 2143, 2168, 2178, 2202, 2216, 2217, 2283, 2285, 2285, 2290, 2308, 2342, 2365, 2450, 2460, 2468, 2479, 2491, 2507, 2651, 2657, 2678, 2689]\n"
     ]
    }
   ],
   "source": [
    "# id_cycle_length = {'6-6': 2468, '7-8': 1938, '5-3': 2689, '6-8': 2450, '5-5': 1583, '2-4': 1499, '10-3': 1848, '1-4': 1500, '6-3': 1804, '5-6': 2460, '8-3': 2290, '3-6': 2491, '10-4': 1811, '9-6': 1742, '3-2': 2283, '9-2': 2143, '9-7': 2012, '6-5': 2178, '4-8': 1706, '5-7': 1448, '1-5': 1971, '1-7': 1678, '1-8': 2285, '8-7': 2047, '6-2': 1908, '3-3': 1649, '6-1': 1609, '2-7': 2202, '1-2': 2678, '10-7': 1783, '10-1': 1702, '9-1': 2057, '8-5': 1348, '9-4': 1975, '4-4': 1491, '3-4': 1766, '10-6': 2285, '5-2': 1926, '7-5': 1875, '4-3': 1142, '5-4': 1962, '3-5': 2657, '7-2': 2030, '3-7': 2479, '1-3': 1858, '9-5': 2168, '3-1': 1938, '4-1': 2217, '9-3': 1905, '7-1': 1690, '8-8': 1679, '6-4': 1717, '4-2': 1782, '5-1': 2507, '2-3': 1751, '4-5': 1561, '4-6': 1380, '2-8': 1481, '8-1': 1308, '10-8': 1400, '9-8': 2308, '3-8': 2342, '7-3': 1295, '8-2': 2041, '8-6': 2365, '7-6': 1419, '10-5': 2030, '7-7': 1685, '7-4': 1393, '4-7': 2216, '2-6': 1572, '10-2': 1697, '1-6': 1143, '8-4': 1885, '2-5': 1386, '1-1': 1504, '2-2': 2651}\n",
    "\n",
    "# #3 dictionaries\n",
    "# group_low = {}   # 1142 - 1658\n",
    "# group_mid = {}   # 1659 - 2173\n",
    "# group_high = {}  # 2174 - 2689\n",
    "\n",
    "# for key, value in id_cycle_length.items():\n",
    "#     if 1142 <= value <= 1658:\n",
    "#         group_low[key] = value\n",
    "#     elif 1659 <= value <= 2173:\n",
    "#         group_mid[key] = value\n",
    "#     elif 2174 <= value <= 2689:\n",
    "#         group_high[key] = value\n",
    "\n",
    "# print(f\"Low Cycle Life ({len(group_low)} items):\", group_low)\n",
    "# print(\"-\" * 20)\n",
    "# print(f\"Mid Cycle Life ({len(group_mid)} items):\", group_mid)\n",
    "# print(\"-\" * 20)\n",
    "# print(f\"High Cycle Life ({len(group_high)} items):\", group_high)\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "id_cycle_length = {'6-6': 2468, '7-8': 1938, '5-3': 2689, '6-8': 2450, '5-5': 1583, '2-4': 1499, '10-3': 1848, '1-4': 1500, '6-3': 1804, '5-6': 2460, '8-3': 2290, '3-6': 2491, '10-4': 1811, '9-6': 1742, '3-2': 2283, '9-2': 2143, '9-7': 2012, '6-5': 2178, '4-8': 1706, '5-7': 1448, '1-5': 1971, '1-7': 1678, '1-8': 2285, '8-7': 2047, '6-2': 1908, '3-3': 1649, '6-1': 1609, '2-7': 2202, '1-2': 2678, '10-7': 1783, '10-1': 1702, '9-1': 2057, '8-5': 1348, '9-4': 1975, '4-4': 1491, '3-4': 1766, '10-6': 2285, '5-2': 1926, '7-5': 1875, '4-3': 1142, '5-4': 1962, '3-5': 2657, '7-2': 2030, '3-7': 2479, '1-3': 1858, '9-5': 2168, '3-1': 1938, '4-1': 2217, '9-3': 1905, '7-1': 1690, '8-8': 1679, '6-4': 1717, '4-2': 1782, '5-1': 2507, '2-3': 1751, '4-5': 1561, '4-6': 1380, '2-8': 1481, '8-1': 1308, '10-8': 1400, '9-8': 2308, '3-8': 2342, '7-3': 1295, '8-2': 2041, '8-6': 2365, '7-6': 1419, '10-5': 2030, '7-7': 1685, '7-4': 1393, '4-7': 2216, '2-6': 1572, '10-2': 1697, '1-6': 1143, '8-4': 1885, '2-5': 1386, '1-1': 1504, '2-2': 2651}\n",
    "\n",
    "# 1. Sort the dictionary items by value (Cycle Life)\n",
    "sorted_items = sorted(id_cycle_length.items(), key=lambda x: x[1])\n",
    "\n",
    "# 2. Calculate slice indices\n",
    "total_len = len(sorted_items)\n",
    "split_1 = total_len // 3          # First cut point (approx 33%)\n",
    "split_2 = (total_len * 2) // 3    # Second cut point (approx 66%)\n",
    "\n",
    "# 3. Create the new dictionaries using slicing\n",
    "group_low = dict(sorted_items[:split_1])\n",
    "group_mid = dict(sorted_items[split_1:split_2])\n",
    "group_high = dict(sorted_items[split_2:])\n",
    "\n",
    "# --- Output Results ---\n",
    "print(f\"Low Cycle Life ({len(group_low)} items): Range {min(group_low.values())} - {max(group_low.values())}\")\n",
    "print(list(group_low.values()))\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"Mid Cycle Life ({len(group_mid)} items): Range {min(group_mid.values())} - {max(group_mid.values())}\")\n",
    "print(list(group_mid.values()))\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"High Cycle Life ({len(group_high)} items): Range {min(group_high.values())} - {max(group_high.values())}\")\n",
    "print(list(group_high.values()))\n",
    "\n",
    "# New Dynamic Ranges for reference:\n",
    "# Low:  1142 - 1702\n",
    "# Mid:  1706 - 2178\n",
    "# High: 2202 - 2689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d848dab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T12:57:09.013476Z",
     "iopub.status.busy": "2026-02-08T12:57:09.013263Z",
     "iopub.status.idle": "2026-02-08T12:57:09.069340Z",
     "shell.execute_reply": "2026-02-08T12:57:09.068774Z"
    },
    "papermill": {
     "duration": 0.065146,
     "end_time": "2026-02-08T12:57:09.070662",
     "exception": false,
     "start_time": "2026-02-08T12:57:09.005516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(WINDOW_SIZES,train_paths,val_paths,test_paths,data_range):\n",
    "    print(\"Cutoff SoH : \",cutoff_soh)\n",
    "    \n",
    "    def df_to_X_y_tensor(df, window_size=5,output_size=5):\n",
    "        '''\n",
    "        Converts a time series into (X, y) tensors for LSTM training.\n",
    "        \n",
    "        X shape: (num_samples, window_size, 1)\n",
    "        y shape: (num_samples, 1)\n",
    "        '''\n",
    "        # if isinstance(df, (pd.DataFrame, pd.Series)):\n",
    "        #     df_as_np = df.to_numpy()\n",
    "        # else:\n",
    "        #     df_as_np = df  # Assume already numpy\n",
    "    \n",
    "        X, y , y2 = [], [], []\n",
    "        for i in range(len(df) - window_size):\n",
    "            X.append(list(df['SoH'])[i:window_size+1+i])\n",
    "            #y.append([df_as_np[i + window_size:i + window_size+output_size]])\n",
    "            y.append([list(df['k'])[-1],list(df['a'])[-1],list(df['b'])[-1]])\n",
    "            #append([[val] for val in df_as_np[i + window_size:i + window_size+1]]) #next cycle\n",
    "            y2.append(list(df['rul'])[:1])\n",
    "        X,y,y2 = np.array(X),np.array(y), np.array(y2)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)#.squeeze()\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)#.squeeze()\n",
    "        y_2_tensor = torch.tensor(y2, dtype=torch.float32)  #bug was here written y instead of y2\n",
    "        return X_tensor, y_tensor, y_2_tensor\n",
    "    \n",
    "    def get_x_y_lists(paths):\n",
    "        X_list,y_list,y_target = [],[],[]\n",
    "        for path in paths:\n",
    "            print(path)\n",
    "            df = pd.read_csv(path)\n",
    "            df['Cycle number'] = df['Cycle number']/10000\n",
    "            df['rul'] = df['rul']/10000\n",
    "            #normalize SoH\n",
    "            df['SoH'] =  df['SoH']/soh_normalization_constant\n",
    "            df.index = df['Cycle number']\n",
    "            #SoH = df[model_columns]\n",
    "            X, y , y1 = df_to_X_y_tensor(df, window_size=WINDOW_SIZE,output_size=OUTPUT_SIZE)\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            y_target.append(y1) #RUL\n",
    "        return X_list,y_list, y_target\n",
    "    \n",
    "        \n",
    "    for WINDOW_SIZE in WINDOW_SIZES:\n",
    "        #### PREPARE ALL DATA ####\n",
    "        ### DONT RUN AS LOOP\n",
    "        WINDOW_SIZE = WINDOW_SIZE -1\n",
    "        def give_paths_get_loaders(paths,data_type,shuffle=False):\n",
    "            X_list, y_list, y_target = get_x_y_lists(paths)\n",
    "    \n",
    "            batch_size = torch.cat(X_list, dim=0).shape[0]\n",
    "            \n",
    "            if INPUT_SIZE == 1:\n",
    "                # Concatenate all X and y\n",
    "                X_1,y_1,y_2 = torch.cat(X_list, dim=0).unsqueeze(-1),torch.cat(y_list, dim=0).view(batch_size,-1),torch.cat(y_target, dim=0).view(batch_size,-1)\n",
    "            else:\n",
    "                X_1,y_1,y_2 = torch.cat(X_list, dim=0).squeeze(2),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE),torch.cat(y_target, dim=0).view(-1,INPUT_SIZE)\n",
    "            \n",
    "            print(f\" X_{data_type} shape : {X_1.shape} , y_{data_type} shape : {y_1.shape} Ôºåy_2{data_type} shape: {y_2.shape}\" )\n",
    "            \n",
    "            #DataLoader\n",
    "            print(\"load : \")\n",
    "            loader = DataLoader(TensorDataset(X_1, y_1, y_2), batch_size=3200, shuffle=shuffle)\n",
    "            print(f\"{data_type}loader lengths : \",loader.__len__())\n",
    "            return loader,X_1,y_1, y_2\n",
    "        \n",
    "        data_use = {\n",
    "            0:[\"train\"],1:[\"val\"],2:[\"test\"]\n",
    "        }\n",
    "        #normalize\n",
    "        train_loader,X_train,y_train,y_train_target= give_paths_get_loaders(train_paths,data_use[0],shuffle=True)\n",
    "        val_loader,X_val,y_val,y_val_target= give_paths_get_loaders(val_paths,data_use[1])\n",
    "        test_loader,X_test,y_test,y_test_target = give_paths_get_loaders(test_paths,data_use[2])\n",
    "        #OUTPUT_SIZE = WINDOW_SIZE\n",
    "        print(\"## üß† Model\")\n",
    "        if model_type == 'lstm':\n",
    "            model = LSTMModel(input_size=INPUT_SIZE, output_size=OUTPUT_SIZE).to(device) # values for multioutput model\n",
    "            last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n",
    "            print(\"Last model window : \",last_model_path)\n",
    "        if model_type == 'seq2seq-lstm':\n",
    "            model = Seq2SeqLSTM(INPUT_SIZE,64,32,OUTPUT_SIZE).to(device) # values from previously working tensorflow model # 1, 64, 8, 1 \n",
    "            last_model_path = f'last_model_window_{WINDOW_SIZE}_model_{model_type}.pth'\n",
    "            print(\"Last model window : \",last_model_path)\n",
    "        if model_type == 'pinn':\n",
    "            model = PhysicsInformedLSTM(input_size=INPUT_SIZE).to(device) # values for multioutput model\n",
    "            last_model_path = f'last_model_window_{WINDOW_SIZE+1}_model_{model_type}_data_{data_range}.pth'\n",
    "            print(\"Last model window : \",last_model_path)\n",
    "            # 2. Calculate the \"Mean\" targets from your training data\n",
    "            # Assuming y_train is your tensor of shape [N, 3] (k, a, b)\n",
    "            # We calculate the mean across the batch dimension (dim=0)\n",
    "            # 2. Define your dataset means (approximate values based on your earlier logs)\n",
    "            # You can also calculate these dynamically: k_mean = y_train[:, 0].mean()\n",
    "            k_mean = float(torch.mean(y_train, dim=0)[0])\n",
    "            a_mean = float(torch.mean(y_train, dim=0)[1])\n",
    "            b_mean = float(torch.mean(y_train, dim=0)[2])\n",
    "            \n",
    "            print(f\"üöÄ Initializing model output to: k={k_mean}, a={a_mean}, b={b_mean}\")\n",
    "            \n",
    "            # 3. Manually set the Biases and Weights\n",
    "            with torch.no_grad():\n",
    "                # A. Set the Bias (The \"Starting Value\")\n",
    "                # Since self.fc outputs 3 values [k, a, b], we access them by index\n",
    "                model.fc.bias[0] = k_mean\n",
    "                model.fc.bias[1] = a_mean\n",
    "                model.fc.bias[2] = b_mean\n",
    "                \n",
    "                # B. Shrink the Weights (The \"Noise\")\n",
    "                # We set weights to near-zero so the LSTM input doesn't disturb our nice means yet.\n",
    "                # The model will gradually increase these weights as it learns the patterns.\n",
    "                model.fc.weight.fill_(0.001)\n",
    "                    \n",
    "                print(\"‚úÖ Model Output Parameters Initialized!\")\n",
    "        \n",
    "\n",
    "       \n",
    "    \n",
    "        #break\n",
    "        print('''##\n",
    "        ### üìà Gompertz Function (Physics Law)\n",
    "        \n",
    "        * `x`: Time (or cycle number)\n",
    "        \n",
    "        * `k`: Max value (e.g., max capacity)\n",
    "        \n",
    "        * `a`, `b`: Shape parameters''')\n",
    "        \n",
    "        def gompertz_func(x, k, a, b):\n",
    "            return k * torch.exp(-a * torch.exp(-b * x))\n",
    "        \n",
    "        print(\"## üß† Loss Functions\\n\")\n",
    "        \n",
    "        print('''## ‚öôÔ∏è 1. Data-Informed Loss Function\n",
    "        a data loss (what the LSTM learns from data)\n",
    "        \n",
    "        * Mean Squared Error for Training\n",
    "        * RMSE for autoregressive approximation of compound error\n",
    "        \n",
    "        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n",
    "        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n",
    "        \n",
    "        * `alpha`: controls how strongly physics is enforced.''')\n",
    "            \n",
    "        def pinn_loss(prediction, target, x, k, a, b, alpha=0.5):\n",
    "            data_loss = F.mse_loss(prediction, target)\n",
    "            physics_pred = gompertz_func(x, k, a, b) #this needs to be refined !!!\n",
    "            physics_loss = F.mse_loss(prediction, physics_pred)\n",
    "            return data_loss + alpha * physics_loss, data_loss.item(), physics_loss.item()\n",
    "        \n",
    "        def data_loss_func(prediction, target, x, alpha=1.0):\n",
    "            data_loss = F.mse_loss(prediction, target)\n",
    "            return data_loss, data_loss.item() , None\n",
    "    \n",
    "        def rul_consistency_loss(prediction, true_rul_target, failure_threshold=0.7, lambda_rul=0.1):\n",
    "            \"\"\"\n",
    "            Calculates the error between the True RUL and the RUL calculated \n",
    "            from the predicted Gompertz parameters at the failure threshold.\n",
    "            \n",
    "            Formula: RUL_calc = (a - ln(ln(k / 0.7))) / b\n",
    "            \n",
    "            prediction:      [Batch, 3] (Predicted k, a, b)\n",
    "            true_rul_target: [Batch, 1] (The ground truth RUL for each sample)\n",
    "            failure_threshold: Float (e.g., 0.7 for 70% capacity)\n",
    "            \"\"\"\n",
    "            \n",
    "            # 1. Extract Parameters\n",
    "            # We do NOT unsqueeze to [Batch, 1, 1] because RUL is just one number per battery, not a sequence.\n",
    "            # Shape: [32, 1]\n",
    "            k_pred = prediction[:, 0:1] \n",
    "            a_pred = prediction[:, 1:2]\n",
    "            b_pred = prediction[:, 2:3]\n",
    "            \n",
    "            # 2. Safety Check: k must be > threshold\n",
    "            # If the max capacity (k) is predicted to be LOWER than 0.7, the battery never 'failed' \n",
    "            # (it started broken), which breaks the math. We clamp k to be at least 0.7001.\n",
    "            k_safe = torch.clamp(k_pred, min=failure_threshold + 0.0001)\n",
    "        \n",
    "            # 3. The Inverse Gompertz Formula for RUL\n",
    "            # y = 0.7 (fixed)\n",
    "            # x (RUL) = (a - ln(ln(k / 0.7))) / b\n",
    "            \n",
    "            ratio = k_safe / failure_threshold\n",
    "            \n",
    "            # We create a scalar tensor for the threshold to ensure device compatibility\n",
    "            inner_log = torch.log(ratio)\n",
    "            double_log = torch.log(inner_log + 1e-6)\n",
    "            \n",
    "            # Calculated RUL based on physics parameters\n",
    "            rul_calc = (a_pred - double_log) / (b_pred + 1e-6)\n",
    "        \n",
    "            # 4. Calculate MSE between Physics-RUL and Ground-Truth RUL\n",
    "            # true_rul_target must be shape [32, 1]\n",
    "            rul_loss = F.mse_loss(rul_calc, true_rul_target)\n",
    "            \n",
    "            return lambda_rul * rul_loss\n",
    "        def eol_boundary_loss(prediction, true_failure_cycle, failure_threshold=0.7):\n",
    "            \"\"\"\n",
    "            Enforces that the Gompertz curve passes through the failure_threshold (0.7)\n",
    "            at the exact cycle defined by true_failure_cycle.\n",
    "            \"\"\"\n",
    "            # 1. Extract Parameters\n",
    "            k_pred = prediction[:, 0:1]\n",
    "            a_pred = prediction[:, 1:2]\n",
    "            b_pred = prediction[:, 2:3]\n",
    "            \n",
    "            # 2. Compute Gompertz Value at the True Failure Cycle\n",
    "            # We use the formula: y = k * exp(-a * exp(-b * x))\n",
    "            # true_failure_cycle must be shape [Batch, 1]\n",
    "            \n",
    "            #print(true_failure_cycle)\n",
    "            exponent_term = a_pred-b_pred * true_failure_cycle #*10000\n",
    "            # Clamp exponent to prevent overflow/underflow\n",
    "            exponent_term = torch.clamp(exponent_term, min=-10.0, max=10.0)\n",
    "            \n",
    "            inner_exp = torch.exp(exponent_term)\n",
    "            gompertz_at_failure = k_pred * torch.exp(-inner_exp)\n",
    "            #print(gompertz_at_failure)\n",
    "            \n",
    "            # 3. Calculate Error\n",
    "            # value to be exactly 0.7 \n",
    "            target = torch.full_like(gompertz_at_failure, failure_threshold)\n",
    "            #print(target)\n",
    "            loss = F.mse_loss(gompertz_at_failure, target)\n",
    "            \n",
    "            return loss\n",
    "    \n",
    "        \n",
    "        def physics_informed_loss(prediction, target, target_y, x, alpha=1.0, lambda_ode=0.1, lambda_rul=0.05, weights=None, physics_weights=1.0):\n",
    "            \"\"\"\n",
    "            prediction: (Batch, 3) -> [32, 3] (One set of k,a,b per sequence)\n",
    "            target:     (Batch, 3) -> [32, 3]\n",
    "            x:          (Batch, Window, 1) -> [32, 5, 1] (Time steps)\n",
    "            \"\"\"\n",
    "            boundary_loss = eol_boundary_loss(\n",
    "                    prediction, \n",
    "                    target_y,\n",
    "                    failure_threshold=0.7\n",
    "                )\n",
    "            # # 1. Standard Data Loss (MSE on parameters k, a, b)\n",
    "            # # prediction and target are both [32, 3], so this works directly.\n",
    "            # data_loss = F.mse_loss(prediction, target)\n",
    "            \n",
    "            # 1. Weighted Data Loss\n",
    "            # Define weights: k needs HUGE focus, b needs to be quieted\n",
    "            # Move to same device as your data automatically\n",
    "            if weights is None:\n",
    "                weights = torch.tensor([10000.0, 100.0, 0.1]).to(prediction.device)\n",
    "                \n",
    "            weights =  weights #weig torch.tensor([10000.0, 100.0, 0.1]).to(prediction.device)\n",
    "            \n",
    "            # Calculate squared error manually (Batch, 3)\n",
    "            raw_mse = (prediction - target) ** 2\n",
    "            \n",
    "            # Multiply error by importance weights (Broadcasting [32, 3] * [3])\n",
    "            weighted_mse = raw_mse * weights\n",
    "            \n",
    "            # Now take the mean\n",
    "            data_loss = torch.mean(weighted_mse)\n",
    "        \n",
    "            # 2. Prepare Physics Inputs\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad_(True)\n",
    "        \n",
    "            # --- Slicing & Reshaping (THE FIX) ---\n",
    "            # Prediction is [32, 3]. We need [32, 1, 1] to broadcast over the Window dimension of x (5).\n",
    "            \n",
    "            # Take column 0, keep dims -> [32, 1], then add extra dim -> [32, 1, 1]\n",
    "            k_pred = prediction[:, 0:1].unsqueeze(1) \n",
    "            a_pred = prediction[:, 1:2].unsqueeze(1) \n",
    "            b_pred = prediction[:, 2:3].unsqueeze(1) \n",
    "    \n",
    "            # Inside physics_informed_loss, before math operations:\n",
    "            k_pred = torch.clamp(k_pred, min=0, max=2)\n",
    "            a_pred = torch.clamp(a_pred, min=-5, max=-1)\n",
    "            b_pred = torch.clamp(b_pred, min=-30, max=-3)\n",
    "            # Now:\n",
    "            # b_pred: [32, 1, 1]\n",
    "            # x:      [32, 5, 1]\n",
    "            # Result: [32, 5, 1] (Broadcasting works!)\n",
    "            exponent_term = a_pred - (b_pred * x)\n",
    "            exponent_term = torch.clamp(exponent_term, min=-10.0, max=10.0)\n",
    "    \n",
    "            y_pred_curve = k_pred * torch.exp(-torch.exp(exponent_term))\n",
    "        \n",
    "            # --- Physics Gradients ---\n",
    "            \n",
    "            # Calculate dy/dx (autograd)\n",
    "            dydx_autograd = torch.autograd.grad(\n",
    "                outputs=y_pred_curve,\n",
    "                inputs=x,\n",
    "                grad_outputs=torch.ones_like(y_pred_curve),\n",
    "                create_graph=True, \n",
    "                retain_graph=True,\n",
    "                only_inputs=True\n",
    "            )[0]\n",
    "        \n",
    "            # Calculate dy/dx (ODE Equation)\n",
    "            # dy/dx = b * y * e^(a-bx)\n",
    "            dydx_ode = b_pred * y_pred_curve * torch.exp(exponent_term)\n",
    "        \n",
    "            # Residual\n",
    "            ode_residual = dydx_autograd - dydx_ode\n",
    "            ode_loss = torch.mean(ode_residual ** 2)\n",
    "    \n",
    "            #add RUL loss\n",
    "            # loss_rul = inverse_time_loss(prediction, target_y, x_seq)\n",
    "            loss_rul = rul_consistency_loss(prediction, target_y, failure_threshold=0.7, lambda_rul=lambda_rul)\n",
    "            \n",
    "            total_loss = (alpha * data_loss) + ((lambda_ode * ode_loss) + loss_rul+boundary_loss)*physics_weights\n",
    "        \n",
    "            return total_loss, data_loss.item(), (ode_loss.item()+loss_rul.item()+ boundary_loss.item()) \n",
    "            \n",
    "        epoch = 0\n",
    "        avg_train_loss = 0.0\n",
    "        avg_val_loss = 0.0\n",
    "        val_rmse_total = 0.0\n",
    "        data_loss = 0.0\n",
    "        phys_loss = 0.0 # Only set if you're using physics loss\n",
    "        \n",
    "        if mode == 0:\n",
    "            criterion = physics_loss #??\n",
    "            best_model_path = f'best_pysics_model-window-{WINDOW_SIZE+1}.pth' #??\n",
    "            loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n",
    "        if mode == 1:\n",
    "            criterion = data_loss_func\n",
    "            best_model_path = f'best_lstm_model-window-{WINDOW_SIZE+1}.pth'\n",
    "            loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f}\"\n",
    "        if mode == 2:\n",
    "            # phys_loss = 0.0 # Only set if you're using physics loss\n",
    "            # criterion = pinn_loss\n",
    "            # best_model_path = f'best_pinn_lstm_model-window-{WINDOW_SIZE}.pth'\n",
    "            # loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n",
    "            criterion = physics_informed_loss\n",
    "            best_model_path = f'best_lstm_model-window-{WINDOW_SIZE+1}_model_{model_type}_data_{data_range}.pth'\n",
    "            loss_string = f\"Epoch {epoch+1} | Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Data={data_loss:.4f} | Physics={phys_loss:.4f}\"\n",
    "        \n",
    "        print(\"## üõ†Ô∏è Parameter Strategy\")\n",
    "        \n",
    "        # k = nn.Parameter(torch.tensor(1.0))\n",
    "        # a = nn.Parameter(torch.tensor(0.1))\n",
    "        # b = nn.Parameter(torch.tensor(0.1))\n",
    "        # Include in optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=400)\n",
    "        # Function to set a new LR\n",
    "        def set_learning_rate(optimizer, new_lr):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "            print(f\"‚úÖ Learning Rate updated to {new_lr}\")\n",
    "        \n",
    "        print(\"## üîÅ Training Loop\")\n",
    "        def compute_rmse(pred, target):\n",
    "            return torch.sqrt(F.mse_loss(pred, target))\n",
    "            \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_rmses = []\n",
    "        data_losses = []\n",
    "        phys_losses = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        \n",
    "        # # 2. Create the sequence from 1 to WINDOW_SIZE\n",
    "        # # torch.arange(start, end) excludes the end, so we use WINDOW_SIZE + 1\n",
    "        # x_raw = torch.arange(1, WINDOW_SIZE + 1, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # # 3. Scale it (divide by 10000)\n",
    "        # x_scaled = x_raw / 10000.0\n",
    "        \n",
    "        # # 4. Reshape to (N, 1) and enable gradients\n",
    "        # # .view(-1, 1) makes it a column vector. \n",
    "        # # .requires_grad_(True) is vital for the physics loss (dydx calculation).\n",
    "        # x_seq = x_scaled.view(-1, 1).requires_grad_(True)\n",
    "    \n",
    "        # Provide as a train function\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            total_data_loss = 0\n",
    "            total_phys_loss = 0\n",
    "            for X_batch, y_batch, y_target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                #Set computing environment\n",
    "                X_batch, y_batch, y_target = X_batch.to(device), y_batch.to(device), y_target.to(device)\n",
    "                #Predict\n",
    "                y_pred = model(X_batch)\n",
    "                \n",
    "    \n",
    "                # Create x_seq for physics loss\n",
    "                #x_seq = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_batch.size(0), 1).to(X_batch.device)\n",
    "                x_seq = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_batch.size(0), 1).unsqueeze(-1).to(X_batch.device)\n",
    "    \n",
    "    \n",
    "                # loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.4, lambda_ode=0.2, lambda_rul=0.4)\n",
    "    \n",
    "    \n",
    "                #CURRICULUM LEARNING\n",
    "                if epoch <= 50:\n",
    "                    current_weights = torch.tensor([0.0, 0.0, 100.0]).to(device)\n",
    "                    #loss_weights = torch.tensor([1.0, 0.0, 0.0]).to(device)\n",
    "                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n",
    "                elif epoch <= 100:\n",
    "                    current_weights = torch.tensor([0.0, 10.0, 100.0]).to(device)\n",
    "                    #loss_weights = torch.tensor([1.0, 0.0, 0.0]).to(device)\n",
    "                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n",
    "                    #loss = torch.mean(loss * loss_weights)\n",
    "                elif epoch <= 200:\n",
    "                    current_weights = torch.tensor([1.0, 10.0, 100.0]).to(device)\n",
    "                    #loss_weights = torch.tensor([1.0, 1.0, 0.0]).to(device)\n",
    "                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n",
    "                    #loss = torch.mean(loss * loss_weights)\n",
    "                elif epoch <= 300:\n",
    "                    current_weights = torch.tensor([1.0, 10.0, 100.0]).to(device)\n",
    "                    #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n",
    "                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n",
    "                    #loss = torch.mean(loss * loss_weights)\n",
    "                elif epoch <= 400:\n",
    "                    current_weights = torch.tensor([1.0, 10.0, 100.0]).to(device)\n",
    "                    #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n",
    "                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n",
    "                    #loss = torch.mean(loss * loss_weights)\n",
    "                elif epoch <= 600:\n",
    "                    current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n",
    "                    #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n",
    "                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n",
    "                    #loss = torch.mean(loss * loss_weights)\n",
    "    \n",
    "                elif epoch <= 1800:\n",
    "                    current_weights = torch.tensor([10000.0, 100.0, 10.0]).to(device)\n",
    "                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=100.0, lambda_ode=100.0, lambda_rul=10000.0,weights=current_weights)\n",
    "                    \n",
    "                elif epoch <= 2600:\n",
    "                    current_weights = torch.tensor([0.0, 0.0, 100.0]).to(device)\n",
    "                    #loss_weights = torch.tensor([1.0, 1.0, 0.0]).to(device)\n",
    "                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=10000.0, lambda_ode=100.1, lambda_rul=10000.0,weights=current_weights)\n",
    "                    #loss = torch.mean(loss * loss_weights)\n",
    "                # elif epoch <= 1200:\n",
    "                #     current_weights = torch.tensor([10000.0, 100.0, 0.0]).to(device)\n",
    "                #     #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n",
    "                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=1.0, lambda_ode=1.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n",
    "                #     #loss = torch.mean(loss * loss_weights)\n",
    "                # elif epoch <= 1400:\n",
    "                #     current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n",
    "                #     #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n",
    "                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=1.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n",
    "                #     #loss = torch.mean(loss * loss_weights)\n",
    "                # elif epoch <= 1600:\n",
    "                #     current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n",
    "                #     #loss_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n",
    "                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=1.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n",
    "                # elif epoch <= 1800:\n",
    "                #     current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n",
    "                #     #loss_weights = torch.tensor([10000.0, 100.0, 0.1]).to(device)\n",
    "                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=0.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n",
    "                # elif epoch <= 2000:\n",
    "                #     current_weights = torch.tensor([10000.0, 100.0, 1.0]).to(device)\n",
    "                #     #loss_weights = torch.tensor([10000.0, 100.0, 0.1]).to(device)\n",
    "                #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=0.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n",
    "                else:\n",
    "                    current_weights = torch.tensor([0.0, 0.0, 10000.0]).to(device)\n",
    "                    #loss_weights = torch.tensor([10000.0, 100.0, 0.1]).to(device)\n",
    "                    loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=10000.0, lambda_ode=100.0, lambda_rul=100.0,weights=current_weights,physics_weights=100000)\n",
    "                #     #loss = torch.mean(loss * loss_weights)\n",
    "                # # elif epoch <= 300:\n",
    "                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.4, lambda_ode=0.2, lambda_rul=0.4)\n",
    "                # # elif epoch <= 400:\n",
    "                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.2, lambda_ode=0.2, lambda_rul=0.6)            \n",
    "                # # elif epoch <= 500:\n",
    "                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.4, lambda_ode=0.4, lambda_rul=0.2)\n",
    "                # # elif epoch <= 600:\n",
    "                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.3, lambda_ode=0.4, lambda_rul=0.3)\n",
    "                # # # elif epoch <= 700:\n",
    "                # # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.0, lambda_ode=0.4, lambda_rul=1.0)\n",
    "                # # # elif epoch <= 800:\n",
    "                # # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.2, lambda_ode=0.4, lambda_rul=0.6)\n",
    "                # # else:\n",
    "                # #     loss, data_loss, phys_loss = criterion(y_pred, y_batch,y_target, x_seq,alpha=0.3, lambda_ode=0.4, lambda_rul=0.3)\n",
    "                # # loss.backward()\n",
    "                # # optimizer.step()\n",
    "                total_data_loss += data_loss\n",
    "                total_loss +=loss.item() if torch.is_tensor(loss) else loss\n",
    "                total_phys_loss += phys_loss.item() if torch.is_tensor(phys_loss) else phys_loss\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20.0)\n",
    "                optimizer.step()\n",
    "    \n",
    "            # Adjust learning rate\n",
    "            scheduler.step()\n",
    "            #Get current learning rate\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            #consolidate losses\n",
    "            avg_data_loss = total_data_loss / len(train_loader)\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            avg_phys_loss = total_phys_loss / len(train_loader)\n",
    "            \n",
    "            # ---- Validation Pass ----\n",
    "            model.eval()\n",
    "            val_loss_total = 0.0\n",
    "            val_rmse_total = 0.0\n",
    "    \n",
    "            # if epoch%20 == 0:\n",
    "                # test_rmse = 0 \n",
    "                # for X_batch, y_batch, y_target in test_loader:\n",
    "                #     # Calculate RMSE directly\n",
    "                #     y_pred = model(X_batch.to(device)).cpu().detach().numpy()\n",
    "                #     test_rmse += root_mean_squared_error(y_batch, y_pred)\n",
    "                #     print('\\n Test RMSE : ',test_rmse,'\\n Epoch : ', epoch,'\\n Target : ',y_batch,'\\n Prediction : ',y_pred,'\\n')\n",
    "                    \n",
    "            with torch.no_grad():\n",
    "                for X_val_batch, y_val_batch, y_val_target_batch in val_loader:\n",
    "                    #Set computing environment\n",
    "                    X_val_batch, y_val_batch, y_val_target_batch = X_val_batch.to(device), y_val_batch.to(device), y_val_target_batch.to(device)\n",
    "                    #Predict\n",
    "                    y_val_pred = model(X_val_batch)\n",
    "        \n",
    "                    #ADD A METRIC\n",
    "                    val_rmse_total += compute_rmse(y_val_pred, y_val_batch)\n",
    "                    #temporarily enable gradients just for pinns loss\n",
    "                    with torch.enable_grad():\n",
    "                        x_seq_val = torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_val_batch.size(0), 1).unsqueeze(-1).to(X_val_batch.device)\n",
    "                        x_seq_val.requires_grad_(True) #due to PINN loss\n",
    "                        # = x_seq#torch.arange(WINDOW_SIZE, dtype=torch.float32).unsqueeze(0).repeat(X_val_batch.size(0), 1).to(X_val_batch.device)\n",
    "                        val_loss, _, _ = criterion(y_val_pred, y_val_batch, y_val_target_batch, x_seq_val, alpha=0.3)\n",
    "        \n",
    "                    val_loss_total += val_loss.item()\n",
    "        \n",
    "            avg_val_loss = val_loss_total / len(val_loader)\n",
    "            avg_val_rmse = val_rmse_total / len(val_loader)\n",
    "            # Save model if validation improves\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"‚úÖ Saved best model at epoch {epoch+1} (Val Loss = {best_val_loss:.8f})\")\n",
    "            if epoch+1 == num_epochs:\n",
    "                torch.save(model.state_dict(),last_model_path)\n",
    "                print(f\"‚úÖ Saved last model at epoch {epoch+1} \")\n",
    "    \n",
    "            \n",
    "        \n",
    "            if mode == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n",
    "            if mode == 1:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Val RMSE: {avg_val_rmse:.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f}\")\n",
    "            if mode == 2:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse:.8f} | ‚àö(Val Loss) = {torch.sqrt(torch.tensor(avg_val_loss)):.8f} | Current Learning Rate: {current_lr}\")\n",
    "                #print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss={avg_train_loss:.8f} | Val Loss={avg_val_loss:.8f} | Data={avg_data_loss:.8f} | Physics={phys_loss:.8f} | Val RMSE: {avg_val_rmse.item():.8f}\")\n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_rmses.append(avg_val_rmse)  # assuming avg_val_rmse is a tensor\n",
    "            data_losses.append(avg_data_loss)   # assuming this is the last batch's data loss\n",
    "            phys_losses.append(avg_phys_loss)   # assuming this is the last batch's physics loss\n",
    "    \n",
    "            # Load best model for testing\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "            if epoch == 700 or epoch == 1800 or epoch == 2700:\n",
    "                #Reset LR \n",
    "                # Bump LR back up to 1e-3 to help it learn the new difficult task\n",
    "                set_learning_rate(optimizer, 1e-3)\n",
    "            if epoch%20 == 0:\n",
    "                model.eval()\n",
    "                test_rmse_total = 0 \n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch, y_target in test_loader:\n",
    "                        X_batch = X_batch.to(device)\n",
    "                        y_pred = model(X_batch).cpu().detach().numpy()\n",
    "                        # # Un-normalize to get real values\n",
    "                        # y_pred_real = scaler.inverse_transform(y_pred_norm.cpu().detach().numpy())\n",
    "                        # y_batch_real = scaler.inverse_transform(y_batch.cpu().detach().numpy())\n",
    "    \n",
    "                        # Accumulate\n",
    "                        test_rmse_total += root_mean_squared_error(y_batch, y_pred)\n",
    "                        print('\\n Epoch : ', epoch,'\\n Target : ',y_batch,'\\n Prediction : ',y_pred,'\\n')\n",
    "                \n",
    "                # FIX 2: Calculate Average\n",
    "                final_test_rmse = test_rmse_total / len(test_loader)\n",
    "                print('Final Test RMSE: ', final_test_rmse)\n",
    "            \n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        train_losses=to_cpu_numpy(train_losses)\n",
    "        val_losses=to_cpu_numpy(val_losses)\n",
    "        val_rmses=to_cpu_numpy(val_rmses)\n",
    "        data_losses=to_cpu_numpy(data_losses)\n",
    "        phys_losses=to_cpu_numpy(phys_losses)    \n",
    "        np.savez(f\"training_metrics__window_{WINDOW_SIZE}_model_{model_type}.npz\",\n",
    "                 train_losses=train_losses,\n",
    "                 val_losses=val_losses,\n",
    "                 val_rmses=val_rmses,\n",
    "                 data_losses=data_losses,\n",
    "                 phys_losses=phys_losses)\n",
    "        # # Save using the clean versions\n",
    "        # np.savez(f\"training_metrics__window_{WINDOW_SIZE}_model_{model_type}.npz\",\n",
    "        #          train_losses=to_cpu_numpy(train_losses),\n",
    "        #          val_losses=to_cpu_numpy(val_losses),\n",
    "        #          val_rmses=to_cpu_numpy(val_rmses),\n",
    "        #          data_losses=to_cpu_numpy(data_losses),\n",
    "        #          phys_losses=to_cpu_numpy(phys_losses))\n",
    "    \n",
    "        print(\"‚úÖ Metrics saved successfully!\")\n",
    "        print(\"Plot losses after training 3:\")\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(train_losses[3:], label=\"Train Loss\")\n",
    "        plt.plot(val_losses[3:], label=\"Val Loss\")\n",
    "        plt.plot(val_rmses[3:], label=\"Val RMSE\")\n",
    "        plt.plot(data_losses[3:], label=\"Data Loss\")\n",
    "        plt.plot(phys_losses[3:], label=\"Physics Loss\")\n",
    "        x_line = best_epoch \n",
    "        plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n",
    "        plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Training & Validation Metrics Window Size : {WINDOW_SIZE+1}\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(fname = f\"history-from-3-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(train_losses, label=\"Train Loss\")\n",
    "        plt.plot(val_losses, label=\"Val Loss\")\n",
    "        plt.plot(val_rmses, label=\"Val RMSE\")\n",
    "        plt.plot(data_losses, label=\"Data Loss\")\n",
    "        plt.plot(phys_losses, label=\"Physics Loss\")\n",
    "        plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n",
    "        plt.text(x_line + 0.05, best_val_loss, 'Saved Model', rotation=45, color='red')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Training & Validation Metrics Window Size {WINDOW_SIZE+1}\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(fname = f\"history-full-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(train_losses, label=\"Train Loss\")\n",
    "        plt.plot(val_losses, label=\"Val Loss\")\n",
    "        plt.plot(val_rmses, label=\"Val RMSE\")\n",
    "        plt.plot(data_losses, label=\"Data Loss\")\n",
    "        plt.plot(phys_losses, label=\"Physics Loss\")\n",
    "        plt.axvline(x=x_line, color='red', linestyle='--', linewidth=2)\n",
    "        plt.text(x_line, best_val_loss*10, 'Saved Model', rotation=30, color='red')\n",
    "        plt.yscale('log')  # visualize on log scale\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Training & Validation Metrics Window Size {WINDOW_SIZE+1}\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(fname = f\"history-full-log-window_{WINDOW_SIZE}_model_{model_type}.png\",dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        test_rmse = 0 \n",
    "        \n",
    "        for X_batch, y_batch, y_target in test_loader:\n",
    "            # Calculate RMSE directly\n",
    "            test_rmse += root_mean_squared_error(y_batch, model(X_batch.to(device)).cpu().detach().numpy())\n",
    "        print('Test RMSE : ',test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a6ef08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T12:57:09.083345Z",
     "iopub.status.busy": "2026-02-08T12:57:09.083147Z",
     "iopub.status.idle": "2026-02-08T12:57:09.096204Z",
     "shell.execute_reply": "2026-02-08T12:57:09.095576Z"
    },
    "papermill": {
     "duration": 0.021153,
     "end_time": "2026-02-08T12:57:09.097584",
     "exception": false,
     "start_time": "2026-02-08T12:57:09.076431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset    | Train | Val   | Test \n",
      "-----------------------------------\n",
      "ALL        | 55    | 11    | 11   \n",
      "LOW        | 14    | 2     | 5    \n",
      "MID        | 26    | 5     | 4    \n",
      "HIGH       | 15    | 4     | 2    \n"
     ]
    }
   ],
   "source": [
    "# 1. Setup the Cycle Life Dictionaries (from previous step)\n",
    "id_cycle_length = {\n",
    "    '6-6': 2468, '7-8': 1938, '5-3': 2689, '6-8': 2450, '5-5': 1583, '2-4': 1499, '10-3': 1848, '1-4': 1500, '6-3': 1804, '5-6': 2460, '8-3': 2290, '3-6': 2491, '10-4': 1811, '9-6': 1742, '3-2': 2283, '9-2': 2143, '9-7': 2012, '6-5': 2178, '4-8': 1706, '5-7': 1448, '1-5': 1971, '1-7': 1678, '1-8': 2285, '8-7': 2047, '6-2': 1908, '3-3': 1649, '6-1': 1609, '2-7': 2202, '1-2': 2678, '10-7': 1783, '10-1': 1702, '9-1': 2057, '8-5': 1348, '9-4': 1975, '4-4': 1491, '3-4': 1766, '10-6': 2285, '5-2': 1926, '7-5': 1875, '4-3': 1142, '5-4': 1962, '3-5': 2657, '7-2': 2030, '3-7': 2479, '1-3': 1858, '9-5': 2168, '3-1': 1938, '4-1': 2217, '9-3': 1905, '7-1': 1690, '8-8': 1679, '6-4': 1717, '4-2': 1782, '5-1': 2507, '2-3': 1751, '4-5': 1561, '4-6': 1380, '2-8': 1481, '8-1': 1308, '10-8': 1400, '9-8': 2308, '3-8': 2342, '7-3': 1295, '8-2': 2041, '8-6': 2365, '7-6': 1419, '10-5': 2030, '7-7': 1685, '7-4': 1393, '4-7': 2216, '2-6': 1572, '10-2': 1697, '1-6': 1143, '8-4': 1885, '2-5': 1386, '1-1': 1504, '2-2': 2651\n",
    "}\n",
    "\n",
    "low_ids = {k for k, v in id_cycle_length.items() if 1142 <= v <= 1658}\n",
    "mid_ids = {k for k, v in id_cycle_length.items() if 1659 <= v <= 2173}\n",
    "high_ids = {k for k, v in id_cycle_length.items() if 2174 <= v <= 2689}\n",
    "\n",
    "# 2. Setup Base Paths (Your Original Code)\n",
    "main_files_path = '/kaggle/input/generate-hust-data-gompertz-k-a-b/'\n",
    "\n",
    "# Using dictionary keys as the source of truth for all files since we can't ls directory here\n",
    "all_ids = list(id_cycle_length.keys()) \n",
    "\n",
    "train_ids = [\n",
    "    '1-3', '1-4', '1-5', '1-6', '1-7', '1-8', '2-2', '2-3',\n",
    "    '2-4', '2-6', '2-7', '2-8', '3-2', '3-3', '3-4', '3-5',\n",
    "    '3-6', '3-7', '3-8', '4-1', '4-2', '4-3', '4-4', '4-6',\n",
    "    '4-7', '4-8', '5-1', '5-2', '5-4', '5-5', '5-6', '5-7',\n",
    "    '6-3', '6-4', '6-5', '7-1', '7-2', '7-3', '7-4', '7-7',\n",
    "    '7-8', '8-2', '8-3', '8-4', '8-7', '9-1', '9-2', '9-3',\n",
    "    '9-5', '9-7', '9-8', '10-2', '10-3', '10-5', '10-8'\n",
    "]\n",
    "\n",
    "test_ids_pool = [f for f in all_ids if f not in train_ids]\n",
    "\n",
    "# Generate Full Paths\n",
    "train_paths_all = [os.path.join(main_files_path, f\"{fid}-hust_gompertz_params.csv\") for fid in train_ids]\n",
    "\n",
    "# Split Test Pool into Val and Test\n",
    "mid_point = int(len(test_ids_pool) * 0.5)\n",
    "val_ids = test_ids_pool[:mid_point]\n",
    "test_ids_final = test_ids_pool[mid_point:]\n",
    "\n",
    "val_paths_all = [os.path.join(main_files_path, f\"{fid}-hust_gompertz_params.csv\") for fid in val_ids]\n",
    "test_paths_all = [os.path.join(main_files_path, f\"{fid}-hust_gompertz_params.csv\") for fid in test_ids_final]\n",
    "\n",
    "# 3. Helper Function to Filter Paths\n",
    "def filter_paths(paths, allowed_ids):\n",
    "    \"\"\"Returns paths that contain an ID from the allowed set.\"\"\"\n",
    "    return [p for p in paths if any(f\"/{allowed_id}-\" in p for allowed_id in allowed_ids)]\n",
    "\n",
    "# 4. Create the 4 Splits\n",
    "# Dictionary to hold all datasets for easy access\n",
    "datasets = {\n",
    "    \"all\":  (train_paths_all, val_paths_all, test_paths_all),\n",
    "    \"low\":  (filter_paths(train_paths_all, low_ids), filter_paths(val_paths_all, low_ids), filter_paths(test_paths_all, low_ids)),\n",
    "    \"mid\":  (filter_paths(train_paths_all, mid_ids), filter_paths(val_paths_all, mid_ids), filter_paths(test_paths_all, mid_ids)),\n",
    "    \"high\": (filter_paths(train_paths_all, high_ids), filter_paths(val_paths_all, high_ids), filter_paths(test_paths_all, high_ids)),\n",
    "}\n",
    "\n",
    "# 5. Verify Counts\n",
    "print(f\"{'Dataset':<10} | {'Train':<5} | {'Val':<5} | {'Test':<5}\")\n",
    "print(\"-\" * 35)\n",
    "for name, (tr, va, te) in datasets.items():\n",
    "    print(f\"{name.upper():<10} | {len(tr):<5} | {len(va):<5} | {len(te):<5}\")\n",
    "\n",
    "# Example Usage: Accessing the 'Mid' dataset\n",
    "# train_mid, val_mid, test_mid = datasets['mid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90071b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T12:57:09.110490Z",
     "iopub.status.busy": "2026-02-08T12:57:09.109957Z",
     "iopub.status.idle": "2026-02-08T15:20:26.009305Z",
     "shell.execute_reply": "2026-02-08T15:20:26.008344Z"
    },
    "papermill": {
     "duration": 8596.907404,
     "end_time": "2026-02-08T15:20:26.010852",
     "exception": false,
     "start_time": "2026-02-08T12:57:09.103448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff SoH :  0.7\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\n",
      " X_['train'] shape : torch.Size([99194, 100, 1]) , y_['train'] shape : torch.Size([99194, 3]) Ôºåy_2['train'] shape: torch.Size([99194, 1])\n",
      "load : \n",
      "['train']loader lengths :  31\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n",
      " X_['val'] shape : torch.Size([21099, 100, 1]) , y_['val'] shape : torch.Size([21099, 3]) Ôºåy_2['val'] shape: torch.Size([21099, 1])\n",
      "load : \n",
      "['val']loader lengths :  7\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n",
      " X_['test'] shape : torch.Size([18206, 100, 1]) , y_['test'] shape : torch.Size([18206, 3]) Ôºåy_2['test'] shape: torch.Size([18206, 1])\n",
      "load : \n",
      "['test']loader lengths :  6\n",
      "## üß† Model\n",
      "Last model window :  last_model_window_100_model_pinn_data_all.pth\n",
      "üöÄ Initializing model output to: k=0.9800577163696289, a=-3.6462461948394775, b=-13.37576961517334\n",
      "‚úÖ Model Output Parameters Initialized!\n",
      "##\n",
      "        ### üìà Gompertz Function (Physics Law)\n",
      "        \n",
      "        * `x`: Time (or cycle number)\n",
      "        \n",
      "        * `k`: Max value (e.g., max capacity)\n",
      "        \n",
      "        * `a`, `b`: Shape parameters\n",
      "## üß† Loss Functions\n",
      "\n",
      "## ‚öôÔ∏è 1. Data-Informed Loss Function\n",
      "        a data loss (what the LSTM learns from data)\n",
      "        \n",
      "        * Mean Squared Error for Training\n",
      "        * RMSE for autoregressive approximation of compound error\n",
      "        \n",
      "        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n",
      "        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n",
      "        \n",
      "        * `alpha`: controls how strongly physics is enforced.\n",
      "## üõ†Ô∏è Parameter Strategy\n",
      "## üîÅ Training Loop\n",
      "‚úÖ Saved best model at epoch 1 (Val Loss = 3.51611263)\n",
      "Epoch 1/1000 | Train Loss=55523.39238911 | Val Loss=3.51611263 | Data=555.09867613 | Physics=13.18849265 | Val RMSE: 2.54028988 | ‚àö(Val Loss) = 1.87513006 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0096244  -3.673343  -13.376753 ]\n",
      " [  1.0096209  -3.6733396 -13.376767 ]\n",
      " [  1.0096165  -3.6733353 -13.376785 ]\n",
      " ...\n",
      " [  1.0071034  -3.6708899 -13.388423 ]\n",
      " [  1.0070992  -3.6708856 -13.388444 ]\n",
      " [  1.0070958  -3.6708825 -13.388462 ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  1.0070894  -3.6708763 -13.388488 ]\n",
      " [  1.0070851  -3.670872  -13.38851  ]\n",
      " [  1.0070817  -3.6708689 -13.388528 ]\n",
      " ...\n",
      " [  1.0083891  -3.672142  -13.382455 ]\n",
      " [  1.0083876  -3.6721406 -13.3824625]\n",
      " [  1.0083864  -3.6721394 -13.38247  ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  1.0083816  -3.6721346 -13.382486 ]\n",
      " [  1.0083786  -3.6721318 -13.382501 ]\n",
      " [  1.0083799  -3.6721332 -13.382502 ]\n",
      " ...\n",
      " [  1.0084358  -3.6721876 -13.382244 ]\n",
      " [  1.0084355  -3.672187  -13.382247 ]\n",
      " [  1.008435   -3.6721869 -13.382249 ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  1.0084311  -3.6721828 -13.382261 ]\n",
      " [  1.0084286  -3.6721804 -13.382273 ]\n",
      " [  1.0084231  -3.672175  -13.382292 ]\n",
      " ...\n",
      " [  1.008177   -3.6719356 -13.383437 ]\n",
      " [  1.0081755  -3.6719341 -13.383446 ]\n",
      " [  1.0081743  -3.671933  -13.383452 ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  1.0081731  -3.6719317 -13.383459 ]\n",
      " [  1.0081779  -3.6719365 -13.383448 ]\n",
      " [  1.0081712  -3.6719298 -13.383466 ]\n",
      " ...\n",
      " [  1.0075907  -3.6713645 -13.386157 ]\n",
      " [  1.007588   -3.671362  -13.386171 ]\n",
      " [  1.0075821  -3.6713562 -13.386193 ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  1.0075781  -3.6713524 -13.386212 ]\n",
      " [  1.0075752  -3.6713495 -13.386229 ]\n",
      " [  1.007569   -3.6713433 -13.386253 ]\n",
      " ...\n",
      " [  1.0023     -3.666195  -13.41119  ]\n",
      " [  1.0022912  -3.6661863 -13.41123  ]\n",
      " [  1.0022851  -3.6661804 -13.411263 ]] \n",
      "\n",
      "Final Test RMSE:  1.4048302819331486\n",
      "‚úÖ Saved best model at epoch 2 (Val Loss = 3.37729337)\n",
      "Epoch 2/1000 | Train Loss=55524.30834173 | Val Loss=3.37729337 | Data=555.10774280 | Physics=12.99763117 | Val RMSE: 2.54274988 | ‚àö(Val Loss) = 1.83774137 | Current Learning Rate: 0.002\n",
      "Epoch 3/1000 | Train Loss=55780.22164819 | Val Loss=3.70065205 | Data=557.65001654 | Physics=16.05718466 | Val RMSE: 2.54148769 | ‚àö(Val Loss) = 1.92370796 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 4 (Val Loss = 3.21363866)\n",
      "Epoch 4/1000 | Train Loss=55514.85118448 | Val Loss=3.21363866 | Data=555.01373685 | Physics=13.70873656 | Val RMSE: 2.54197598 | ‚àö(Val Loss) = 1.79266250 | Current Learning Rate: 0.002\n",
      "Epoch 5/1000 | Train Loss=57631.45186492 | Val Loss=4.26493863 | Data=576.15982154 | Physics=15.74194990 | Val RMSE: 2.61401296 | ‚àö(Val Loss) = 2.06517291 | Current Learning Rate: 0.002\n",
      "Epoch 6/1000 | Train Loss=55527.58732359 | Val Loss=3.45873483 | Data=555.14074313 | Physics=14.03979045 | Val RMSE: 2.52995872 | ‚àö(Val Loss) = 1.85976744 | Current Learning Rate: 0.002\n",
      "Epoch 7/1000 | Train Loss=55497.85546875 | Val Loss=3.57737674 | Data=554.84354917 | Physics=13.29094332 | Val RMSE: 2.51619101 | ‚àö(Val Loss) = 1.89139545 | Current Learning Rate: 0.002\n",
      "Epoch 8/1000 | Train Loss=58934.69695060 | Val Loss=3.49848333 | Data=589.20933680 | Physics=13.76257204 | Val RMSE: 2.55822825 | ‚àö(Val Loss) = 1.87042332 | Current Learning Rate: 0.002\n",
      "Epoch 9/1000 | Train Loss=55508.51348286 | Val Loss=3.25901334 | Data=554.94996495 | Physics=13.19277089 | Val RMSE: 2.53299999 | ‚àö(Val Loss) = 1.80527377 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 10 (Val Loss = 3.00135772)\n",
      "Epoch 10/1000 | Train Loss=55499.50151210 | Val Loss=3.00135772 | Data=554.86031612 | Physics=14.10127910 | Val RMSE: 2.55100965 | ‚àö(Val Loss) = 1.73244274 | Current Learning Rate: 0.002\n",
      "Epoch 11/1000 | Train Loss=56892.38180444 | Val Loss=3.99163522 | Data=568.75569596 | Physics=14.85191228 | Val RMSE: 2.52857447 | ‚àö(Val Loss) = 1.99790776 | Current Learning Rate: 0.002\n",
      "Epoch 12/1000 | Train Loss=56632.98185484 | Val Loss=3.18186636 | Data=566.17250799 | Physics=14.74827624 | Val RMSE: 2.49763584 | ‚àö(Val Loss) = 1.78377867 | Current Learning Rate: 0.002\n",
      "Epoch 13/1000 | Train Loss=56778.88621472 | Val Loss=3.79459289 | Data=567.65198246 | Physics=14.37676364 | Val RMSE: 2.53015208 | ‚àö(Val Loss) = 1.94797146 | Current Learning Rate: 0.002\n",
      "Epoch 14/1000 | Train Loss=55742.89591734 | Val Loss=3.88018695 | Data=557.28755139 | Physics=13.14326888 | Val RMSE: 2.56637931 | ‚àö(Val Loss) = 1.96981907 | Current Learning Rate: 0.002\n",
      "Epoch 15/1000 | Train Loss=56323.29876512 | Val Loss=3.81670368 | Data=563.08854626 | Physics=12.98291489 | Val RMSE: 2.56055045 | ‚àö(Val Loss) = 1.95363855 | Current Learning Rate: 0.002\n",
      "Epoch 16/1000 | Train Loss=55499.30355343 | Val Loss=3.99323538 | Data=554.85541165 | Physics=13.62788601 | Val RMSE: 2.54831839 | ‚àö(Val Loss) = 1.99830806 | Current Learning Rate: 0.002\n",
      "Epoch 17/1000 | Train Loss=56916.63432460 | Val Loss=3.37584255 | Data=569.02043496 | Physics=13.56657766 | Val RMSE: 2.52723432 | ‚àö(Val Loss) = 1.83734655 | Current Learning Rate: 0.002\n",
      "Epoch 18/1000 | Train Loss=56612.82094254 | Val Loss=8.93887568 | Data=565.98360812 | Physics=32.42475911 | Val RMSE: 2.86888194 | ‚àö(Val Loss) = 2.98979521 | Current Learning Rate: 0.002\n",
      "Epoch 19/1000 | Train Loss=56359.19279234 | Val Loss=3.25012136 | Data=563.44308964 | Physics=13.13712029 | Val RMSE: 2.53702807 | ‚àö(Val Loss) = 1.80280924 | Current Learning Rate: 0.002\n",
      "Epoch 20/1000 | Train Loss=56429.63041835 | Val Loss=3.16774557 | Data=564.14860289 | Physics=13.47202645 | Val RMSE: 2.53891659 | ‚àö(Val Loss) = 1.77981615 | Current Learning Rate: 0.002\n",
      "Epoch 21/1000 | Train Loss=55689.09690020 | Val Loss=4.21298691 | Data=556.74720616 | Physics=13.31207449 | Val RMSE: 2.59118772 | ‚àö(Val Loss) = 2.05255628 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 22/1000 | Train Loss=55335.73235887 | Val Loss=3.69659293 | Data=553.21822333 | Physics=13.30619953 | Val RMSE: 2.57075357 | ‚àö(Val Loss) = 1.92265260 | Current Learning Rate: 0.002\n",
      "Epoch 23/1000 | Train Loss=55951.17578125 | Val Loss=3.54813919 | Data=559.37040464 | Physics=13.75345715 | Val RMSE: 2.53838706 | ‚àö(Val Loss) = 1.88365042 | Current Learning Rate: 0.002\n",
      "Epoch 24/1000 | Train Loss=56452.56149194 | Val Loss=3.63220033 | Data=564.38919953 | Physics=13.49018719 | Val RMSE: 2.52401400 | ‚àö(Val Loss) = 1.90583324 | Current Learning Rate: 0.002\n",
      "Epoch 25/1000 | Train Loss=55792.01247480 | Val Loss=3.50070896 | Data=557.78186232 | Physics=13.33990694 | Val RMSE: 2.54292297 | ‚àö(Val Loss) = 1.87101817 | Current Learning Rate: 0.002\n",
      "Epoch 26/1000 | Train Loss=57552.63243448 | Val Loss=4.03389284 | Data=575.37065863 | Physics=15.23149218 | Val RMSE: 2.63382483 | ‚àö(Val Loss) = 2.00845528 | Current Learning Rate: 0.002\n",
      "Epoch 27/1000 | Train Loss=55710.27721774 | Val Loss=4.28849655 | Data=556.96271146 | Physics=14.29828773 | Val RMSE: 2.54127455 | ‚àö(Val Loss) = 2.07086849 | Current Learning Rate: 0.002\n",
      "Epoch 28/1000 | Train Loss=55997.11403730 | Val Loss=3.77325358 | Data=559.82251173 | Physics=13.16353142 | Val RMSE: 2.54483390 | ‚àö(Val Loss) = 1.94248652 | Current Learning Rate: 0.002\n",
      "Epoch 29/1000 | Train Loss=55722.34778226 | Val Loss=3.42053580 | Data=557.08720743 | Physics=12.97879514 | Val RMSE: 2.53704739 | ‚àö(Val Loss) = 1.84946907 | Current Learning Rate: 0.002\n",
      "Epoch 30/1000 | Train Loss=55550.27507560 | Val Loss=4.23874576 | Data=555.36629363 | Physics=16.02905468 | Val RMSE: 2.59601879 | ‚àö(Val Loss) = 2.05882144 | Current Learning Rate: 0.002\n",
      "Epoch 31/1000 | Train Loss=56218.42351310 | Val Loss=3.59741174 | Data=562.04130210 | Physics=13.16339392 | Val RMSE: 2.56277442 | ‚àö(Val Loss) = 1.89668441 | Current Learning Rate: 0.002\n",
      "Epoch 32/1000 | Train Loss=56062.52305948 | Val Loss=3.36375636 | Data=560.48704283 | Physics=13.62620070 | Val RMSE: 2.53304672 | ‚àö(Val Loss) = 1.83405459 | Current Learning Rate: 0.002\n",
      "Epoch 33/1000 | Train Loss=55846.30443548 | Val Loss=4.18929715 | Data=558.32568359 | Physics=13.53980841 | Val RMSE: 2.57143879 | ‚àö(Val Loss) = 2.04677725 | Current Learning Rate: 0.002\n",
      "Epoch 34/1000 | Train Loss=56514.70980343 | Val Loss=4.98517544 | Data=564.98010254 | Physics=13.93392054 | Val RMSE: 2.61438441 | ‚àö(Val Loss) = 2.23275065 | Current Learning Rate: 0.002\n",
      "Epoch 35/1000 | Train Loss=57461.68208165 | Val Loss=3.21275178 | Data=574.47400887 | Physics=13.50371269 | Val RMSE: 2.54604578 | ‚àö(Val Loss) = 1.79241514 | Current Learning Rate: 0.002\n",
      "Epoch 36/1000 | Train Loss=57156.23223286 | Val Loss=3.56686677 | Data=571.40318741 | Physics=13.46287293 | Val RMSE: 2.54217601 | ‚àö(Val Loss) = 1.88861501 | Current Learning Rate: 0.002\n",
      "Epoch 37/1000 | Train Loss=55854.75277218 | Val Loss=3.46291711 | Data=558.40737029 | Physics=14.05839560 | Val RMSE: 2.53835750 | ‚àö(Val Loss) = 1.86089146 | Current Learning Rate: 0.002\n",
      "Epoch 38/1000 | Train Loss=56476.10294859 | Val Loss=3.75213059 | Data=564.61177309 | Physics=13.55585293 | Val RMSE: 2.53975821 | ‚àö(Val Loss) = 1.93704164 | Current Learning Rate: 0.002\n",
      "Epoch 39/1000 | Train Loss=56683.26071069 | Val Loss=3.32771295 | Data=566.67210536 | Physics=17.97389050 | Val RMSE: 2.53395247 | ‚àö(Val Loss) = 1.82420206 | Current Learning Rate: 0.002\n",
      "Epoch 40/1000 | Train Loss=55518.42313508 | Val Loss=4.67341375 | Data=555.04891869 | Physics=15.11786837 | Val RMSE: 2.51827002 | ‚àö(Val Loss) = 2.16180801 | Current Learning Rate: 0.002\n",
      "Epoch 41/1000 | Train Loss=55954.19405242 | Val Loss=3.46919589 | Data=559.39664189 | Physics=13.39316127 | Val RMSE: 2.54500508 | ‚àö(Val Loss) = 1.86257780 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 42/1000 | Train Loss=58161.22366431 | Val Loss=4.27673749 | Data=581.42282794 | Physics=14.52895252 | Val RMSE: 2.64064574 | ‚àö(Val Loss) = 2.06802750 | Current Learning Rate: 0.002\n",
      "Epoch 43/1000 | Train Loss=55744.68145161 | Val Loss=3.43908119 | Data=557.29980863 | Physics=13.33729971 | Val RMSE: 2.53782368 | ‚àö(Val Loss) = 1.85447598 | Current Learning Rate: 0.002\n",
      "Epoch 44/1000 | Train Loss=55727.01260081 | Val Loss=5.31441171 | Data=557.13171387 | Physics=13.47301499 | Val RMSE: 2.56649470 | ‚àö(Val Loss) = 2.30530071 | Current Learning Rate: 0.002\n",
      "Epoch 45/1000 | Train Loss=56854.75138609 | Val Loss=3.56138296 | Data=568.40522618 | Physics=13.41282278 | Val RMSE: 2.54776335 | ‚àö(Val Loss) = 1.88716269 | Current Learning Rate: 0.002\n",
      "Epoch 46/1000 | Train Loss=56132.36756552 | Val Loss=4.02598969 | Data=561.18390483 | Physics=14.10405544 | Val RMSE: 2.53573990 | ‚àö(Val Loss) = 2.00648689 | Current Learning Rate: 0.002\n",
      "Epoch 47/1000 | Train Loss=57659.91141633 | Val Loss=3.36678921 | Data=576.45782864 | Physics=13.92101753 | Val RMSE: 2.53236127 | ‚àö(Val Loss) = 1.83488119 | Current Learning Rate: 0.002\n",
      "Epoch 48/1000 | Train Loss=55721.87247984 | Val Loss=3.68377264 | Data=557.08195840 | Physics=14.40986002 | Val RMSE: 2.56150746 | ‚àö(Val Loss) = 1.91931570 | Current Learning Rate: 0.002\n",
      "Epoch 49/1000 | Train Loss=55621.87613407 | Val Loss=4.15960433 | Data=556.07964694 | Physics=13.27692664 | Val RMSE: 2.56392574 | ‚àö(Val Loss) = 2.03951097 | Current Learning Rate: 0.002\n",
      "Epoch 50/1000 | Train Loss=56677.29170867 | Val Loss=3.69298049 | Data=566.62102681 | Physics=13.71197959 | Val RMSE: 2.55374980 | ‚àö(Val Loss) = 1.92171288 | Current Learning Rate: 0.002\n",
      "Epoch 51/1000 | Train Loss=57852.31552419 | Val Loss=3.43141439 | Data=578.36283628 | Physics=13.64335121 | Val RMSE: 2.55575228 | ‚àö(Val Loss) = 1.85240769 | Current Learning Rate: 0.002\n",
      "Epoch 52/1000 | Train Loss=56105.31691028 | Val Loss=5.06566007 | Data=560.91515818 | Physics=15.88576898 | Val RMSE: 2.52353096 | ‚àö(Val Loss) = 2.25070214 | Current Learning Rate: 0.002\n",
      "Epoch 53/1000 | Train Loss=56695.17477319 | Val Loss=5.36243422 | Data=566.80147429 | Physics=13.91845574 | Val RMSE: 2.55265164 | ‚àö(Val Loss) = 2.31569314 | Current Learning Rate: 0.002\n",
      "Epoch 54/1000 | Train Loss=59398.83933972 | Val Loss=24.57598686 | Data=593.74445466 | Physics=29.04902850 | Val RMSE: 2.66361856 | ‚àö(Val Loss) = 4.95741749 | Current Learning Rate: 0.002\n",
      "Epoch 55/1000 | Train Loss=55618.13142641 | Val Loss=3.57324123 | Data=556.04563362 | Physics=12.78500728 | Val RMSE: 2.51922464 | ‚àö(Val Loss) = 1.89030194 | Current Learning Rate: 0.002\n",
      "Epoch 56/1000 | Train Loss=57275.89982359 | Val Loss=3.86145972 | Data=572.61280872 | Physics=13.49607106 | Val RMSE: 2.53680277 | ‚àö(Val Loss) = 1.96505976 | Current Learning Rate: 0.002\n",
      "Epoch 57/1000 | Train Loss=56740.70602319 | Val Loss=3.72774767 | Data=567.26806641 | Physics=13.55549418 | Val RMSE: 2.53358245 | ‚àö(Val Loss) = 1.93073761 | Current Learning Rate: 0.002\n",
      "Epoch 58/1000 | Train Loss=56520.82711694 | Val Loss=5.07538148 | Data=565.05921198 | Physics=14.96535616 | Val RMSE: 2.56219053 | ‚àö(Val Loss) = 2.25286078 | Current Learning Rate: 0.002\n",
      "Epoch 59/1000 | Train Loss=56260.78225806 | Val Loss=8.38549893 | Data=562.46866928 | Physics=13.88029190 | Val RMSE: 2.59083104 | ‚àö(Val Loss) = 2.89577270 | Current Learning Rate: 0.002\n",
      "Epoch 60/1000 | Train Loss=56009.56376008 | Val Loss=5.83517878 | Data=559.95970892 | Physics=13.55914662 | Val RMSE: 2.54124784 | ‚àö(Val Loss) = 2.41561151 | Current Learning Rate: 0.002\n",
      "Epoch 61/1000 | Train Loss=56477.36302923 | Val Loss=5.90966269 | Data=564.62934531 | Physics=13.82951534 | Val RMSE: 2.54648328 | ‚àö(Val Loss) = 2.43097973 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 62/1000 | Train Loss=55865.61844758 | Val Loss=5.71956583 | Data=558.52167339 | Physics=13.39377041 | Val RMSE: 2.58178806 | ‚àö(Val Loss) = 2.39156127 | Current Learning Rate: 0.002\n",
      "Epoch 63/1000 | Train Loss=55948.52570565 | Val Loss=3.92831675 | Data=559.34931207 | Physics=13.74648827 | Val RMSE: 2.59270906 | ‚àö(Val Loss) = 1.98199821 | Current Learning Rate: 0.002\n",
      "Epoch 64/1000 | Train Loss=56283.99987399 | Val Loss=3.73705672 | Data=562.69502504 | Physics=13.71213325 | Val RMSE: 2.51979113 | ‚àö(Val Loss) = 1.93314683 | Current Learning Rate: 0.002\n",
      "Epoch 65/1000 | Train Loss=55975.42704133 | Val Loss=3.47807182 | Data=559.61808925 | Physics=13.22479830 | Val RMSE: 2.54503155 | ‚àö(Val Loss) = 1.86495900 | Current Learning Rate: 0.002\n",
      "Epoch 66/1000 | Train Loss=57032.68951613 | Val Loss=4.07304261 | Data=570.18862226 | Physics=13.54532831 | Val RMSE: 2.54378653 | ‚àö(Val Loss) = 2.01817799 | Current Learning Rate: 0.002\n",
      "Epoch 67/1000 | Train Loss=56817.72353831 | Val Loss=3.35877795 | Data=568.02217643 | Physics=15.76230603 | Val RMSE: 2.52806807 | ‚àö(Val Loss) = 1.83269691 | Current Learning Rate: 0.002\n",
      "Epoch 68/1000 | Train Loss=55779.95917339 | Val Loss=9.27614396 | Data=557.65679538 | Physics=16.17196672 | Val RMSE: 2.53814316 | ‚àö(Val Loss) = 3.04567623 | Current Learning Rate: 0.002\n",
      "Epoch 69/1000 | Train Loss=55670.95438508 | Val Loss=3.87567765 | Data=556.57515790 | Physics=13.70846058 | Val RMSE: 2.53690982 | ‚àö(Val Loss) = 1.96867406 | Current Learning Rate: 0.002\n",
      "Epoch 70/1000 | Train Loss=55539.89289315 | Val Loss=3.87004502 | Data=555.26349074 | Physics=13.20543925 | Val RMSE: 2.55467558 | ‚àö(Val Loss) = 1.96724296 | Current Learning Rate: 0.002\n",
      "Epoch 71/1000 | Train Loss=57312.87260585 | Val Loss=4.14628495 | Data=572.99186657 | Physics=13.78782855 | Val RMSE: 2.54407358 | ‚àö(Val Loss) = 2.03624296 | Current Learning Rate: 0.002\n",
      "Epoch 72/1000 | Train Loss=55312.66267641 | Val Loss=3.99252597 | Data=552.99185476 | Physics=13.70170452 | Val RMSE: 2.53453708 | ‚àö(Val Loss) = 1.99813068 | Current Learning Rate: 0.002\n",
      "Epoch 73/1000 | Train Loss=56355.96875000 | Val Loss=3.97771611 | Data=563.41110328 | Physics=13.67249622 | Val RMSE: 2.53190660 | ‚àö(Val Loss) = 1.99442124 | Current Learning Rate: 0.002\n",
      "Epoch 74/1000 | Train Loss=55805.80153730 | Val Loss=3.87918656 | Data=557.91725995 | Physics=14.60953880 | Val RMSE: 2.51763988 | ‚àö(Val Loss) = 1.96956503 | Current Learning Rate: 0.002\n",
      "Epoch 75/1000 | Train Loss=57374.93371976 | Val Loss=4.75202571 | Data=573.58956417 | Physics=13.75922404 | Val RMSE: 2.54167628 | ‚àö(Val Loss) = 2.17991424 | Current Learning Rate: 0.002\n",
      "Epoch 76/1000 | Train Loss=55549.26814516 | Val Loss=3.83802663 | Data=555.35786684 | Physics=13.13427848 | Val RMSE: 2.53774047 | ‚àö(Val Loss) = 1.95908821 | Current Learning Rate: 0.002\n",
      "Epoch 77/1000 | Train Loss=56796.17956149 | Val Loss=4.49053322 | Data=567.82350602 | Physics=13.34505920 | Val RMSE: 2.55991554 | ‚àö(Val Loss) = 2.11908793 | Current Learning Rate: 0.002\n",
      "Epoch 78/1000 | Train Loss=55787.42830141 | Val Loss=3.92649760 | Data=557.72613427 | Physics=13.27729322 | Val RMSE: 2.53596878 | ‚àö(Val Loss) = 1.98153925 | Current Learning Rate: 0.002\n",
      "Epoch 79/1000 | Train Loss=56090.70488911 | Val Loss=4.22248876 | Data=560.77010813 | Physics=13.56066990 | Val RMSE: 2.54718304 | ‚àö(Val Loss) = 2.05486965 | Current Learning Rate: 0.002\n",
      "Epoch 80/1000 | Train Loss=55273.58933972 | Val Loss=4.53045447 | Data=552.60149064 | Physics=13.50991348 | Val RMSE: 2.51606107 | ‚àö(Val Loss) = 2.12848639 | Current Learning Rate: 0.002\n",
      "Epoch 81/1000 | Train Loss=55391.91481855 | Val Loss=4.11202812 | Data=553.78465025 | Physics=14.28466208 | Val RMSE: 2.55141211 | ‚àö(Val Loss) = 2.02781367 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 82/1000 | Train Loss=56852.97542843 | Val Loss=4.63612444 | Data=568.39150509 | Physics=13.41365489 | Val RMSE: 2.53376174 | ‚àö(Val Loss) = 2.15316629 | Current Learning Rate: 0.002\n",
      "Epoch 83/1000 | Train Loss=55799.37021169 | Val Loss=4.51919360 | Data=557.85517342 | Physics=13.51422142 | Val RMSE: 2.58578658 | ‚àö(Val Loss) = 2.12583947 | Current Learning Rate: 0.002\n",
      "Epoch 84/1000 | Train Loss=55661.94997480 | Val Loss=4.36072080 | Data=556.48380403 | Physics=13.90545368 | Val RMSE: 2.52457047 | ‚àö(Val Loss) = 2.08823395 | Current Learning Rate: 0.002\n",
      "Epoch 85/1000 | Train Loss=55883.55506552 | Val Loss=3.67617008 | Data=558.69834260 | Physics=14.15200884 | Val RMSE: 2.54436970 | ‚àö(Val Loss) = 1.91733408 | Current Learning Rate: 0.002\n",
      "Epoch 86/1000 | Train Loss=56382.51058468 | Val Loss=17.55041810 | Data=563.68905935 | Physics=13.87082406 | Val RMSE: 2.68475747 | ‚àö(Val Loss) = 4.18932199 | Current Learning Rate: 0.002\n",
      "Epoch 87/1000 | Train Loss=56237.31842238 | Val Loss=3.83068711 | Data=562.22607127 | Physics=12.73981953 | Val RMSE: 2.53991127 | ‚àö(Val Loss) = 1.95721412 | Current Learning Rate: 0.002\n",
      "Epoch 88/1000 | Train Loss=62555.66620464 | Val Loss=363.49701800 | Data=625.40118014 | Physics=17.61795972 | Val RMSE: 3.07188821 | ‚àö(Val Loss) = 19.06559753 | Current Learning Rate: 0.002\n",
      "Epoch 89/1000 | Train Loss=56400.60194052 | Val Loss=5.68457140 | Data=563.83050931 | Physics=18.17250513 | Val RMSE: 2.53146267 | ‚àö(Val Loss) = 2.38423395 | Current Learning Rate: 0.002\n",
      "Epoch 90/1000 | Train Loss=55436.47769657 | Val Loss=4.83752654 | Data=554.22764144 | Physics=13.39935568 | Val RMSE: 2.53164959 | ‚àö(Val Loss) = 2.19943762 | Current Learning Rate: 0.002\n",
      "Epoch 91/1000 | Train Loss=56199.32484879 | Val Loss=3.72082882 | Data=561.85424411 | Physics=13.79241057 | Val RMSE: 2.62740493 | ‚àö(Val Loss) = 1.92894495 | Current Learning Rate: 0.002\n",
      "Epoch 92/1000 | Train Loss=56669.58492944 | Val Loss=3.76549359 | Data=566.53181507 | Physics=13.34652704 | Val RMSE: 2.56905866 | ‚àö(Val Loss) = 1.94048798 | Current Learning Rate: 0.002\n",
      "Epoch 93/1000 | Train Loss=55530.64175907 | Val Loss=4.27468869 | Data=555.15744314 | Physics=13.51886642 | Val RMSE: 2.55125666 | ‚àö(Val Loss) = 2.06753206 | Current Learning Rate: 0.002\n",
      "Epoch 94/1000 | Train Loss=56325.02709173 | Val Loss=31.05878265 | Data=563.11377544 | Physics=15.05014094 | Val RMSE: 2.72969460 | ‚àö(Val Loss) = 5.57304049 | Current Learning Rate: 0.002\n",
      "Epoch 95/1000 | Train Loss=56258.16948085 | Val Loss=5.39984376 | Data=562.44276281 | Physics=13.44075017 | Val RMSE: 2.55548835 | ‚àö(Val Loss) = 2.32375646 | Current Learning Rate: 0.002\n",
      "Epoch 96/1000 | Train Loss=56772.52948589 | Val Loss=4.15901509 | Data=567.58716214 | Physics=13.65391426 | Val RMSE: 2.53526735 | ‚àö(Val Loss) = 2.03936648 | Current Learning Rate: 0.002\n",
      "Epoch 97/1000 | Train Loss=55593.77646169 | Val Loss=4.01083560 | Data=555.80167118 | Physics=14.15595727 | Val RMSE: 2.54073715 | ‚àö(Val Loss) = 2.00270700 | Current Learning Rate: 0.002\n",
      "Epoch 98/1000 | Train Loss=55825.83933972 | Val Loss=4.06877443 | Data=558.12070194 | Physics=13.47763037 | Val RMSE: 2.53956318 | ‚àö(Val Loss) = 2.01712036 | Current Learning Rate: 0.002\n",
      "Epoch 99/1000 | Train Loss=56604.33669355 | Val Loss=4.06438906 | Data=565.90041425 | Physics=13.57175489 | Val RMSE: 2.52647471 | ‚àö(Val Loss) = 2.01603293 | Current Learning Rate: 0.002\n",
      "Epoch 100/1000 | Train Loss=56044.01436492 | Val Loss=4.29997948 | Data=560.29919335 | Physics=13.44056439 | Val RMSE: 2.56384659 | ‚àö(Val Loss) = 2.07363915 | Current Learning Rate: 0.002\n",
      "Epoch 101/1000 | Train Loss=55993.60143649 | Val Loss=5.31370854 | Data=559.79467380 | Physics=13.09174839 | Val RMSE: 2.56192064 | ‚àö(Val Loss) = 2.30514812 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 102/1000 | Train Loss=55587.02028730 | Val Loss=6.35845436 | Data=555.73026989 | Physics=13.84380240 | Val RMSE: 2.53799176 | ‚àö(Val Loss) = 2.52159762 | Current Learning Rate: 0.002\n",
      "Epoch 103/1000 | Train Loss=56271.74180948 | Val Loss=6.86112567 | Data=562.54224420 | Physics=14.29828546 | Val RMSE: 2.55030894 | ‚àö(Val Loss) = 2.61937499 | Current Learning Rate: 0.002\n",
      "Epoch 104/1000 | Train Loss=58476.59778226 | Val Loss=3.88617549 | Data=584.62771803 | Physics=13.35856709 | Val RMSE: 2.54522276 | ‚àö(Val Loss) = 1.97133851 | Current Learning Rate: 0.002\n",
      "Epoch 105/1000 | Train Loss=55925.62726815 | Val Loss=4.08043608 | Data=559.11827235 | Physics=13.49568233 | Val RMSE: 2.57214522 | ‚àö(Val Loss) = 2.02000904 | Current Learning Rate: 0.002\n",
      "Epoch 106/1000 | Train Loss=56303.28326613 | Val Loss=4.24932683 | Data=562.89620676 | Physics=13.49150995 | Val RMSE: 2.55312395 | ‚àö(Val Loss) = 2.06138945 | Current Learning Rate: 0.002\n",
      "Epoch 107/1000 | Train Loss=55794.91204637 | Val Loss=3.82696166 | Data=557.80389404 | Physics=13.48029226 | Val RMSE: 2.53653955 | ‚àö(Val Loss) = 1.95626223 | Current Learning Rate: 0.002\n",
      "Epoch 108/1000 | Train Loss=56011.08807964 | Val Loss=36.46799197 | Data=559.96720049 | Physics=13.74298039 | Val RMSE: 2.71739101 | ‚àö(Val Loss) = 6.03887320 | Current Learning Rate: 0.002\n",
      "Epoch 109/1000 | Train Loss=56918.46358367 | Val Loss=3.44113066 | Data=568.99354602 | Physics=15.42677195 | Val RMSE: 2.55577469 | ‚àö(Val Loss) = 1.85502851 | Current Learning Rate: 0.002\n",
      "Epoch 110/1000 | Train Loss=56227.49521169 | Val Loss=5.79225796 | Data=562.05978886 | Physics=14.17732079 | Val RMSE: 2.54159045 | ‚àö(Val Loss) = 2.40671110 | Current Learning Rate: 0.002\n",
      "Epoch 111/1000 | Train Loss=56147.27709173 | Val Loss=3.77081156 | Data=561.33071604 | Physics=13.07443064 | Val RMSE: 2.55246258 | ‚àö(Val Loss) = 1.94185781 | Current Learning Rate: 0.002\n",
      "Epoch 112/1000 | Train Loss=55837.17552923 | Val Loss=3.76784044 | Data=558.23304798 | Physics=13.31113933 | Val RMSE: 2.52303696 | ‚àö(Val Loss) = 1.94109261 | Current Learning Rate: 0.002\n",
      "Epoch 113/1000 | Train Loss=56719.91204637 | Val Loss=6.01059028 | Data=567.03341872 | Physics=13.27845153 | Val RMSE: 2.56988573 | ‚àö(Val Loss) = 2.45165038 | Current Learning Rate: 0.002\n",
      "Epoch 114/1000 | Train Loss=56119.87840222 | Val Loss=4.13004201 | Data=561.05494149 | Physics=13.68551164 | Val RMSE: 2.56235695 | ‚àö(Val Loss) = 2.03225040 | Current Learning Rate: 0.002\n",
      "Epoch 115/1000 | Train Loss=55485.85735887 | Val Loss=4.00467009 | Data=554.72229004 | Physics=13.27578996 | Val RMSE: 2.52791286 | ‚àö(Val Loss) = 2.00116730 | Current Learning Rate: 0.002\n",
      "Epoch 116/1000 | Train Loss=57248.86668347 | Val Loss=6.02941625 | Data=572.34098964 | Physics=14.32066227 | Val RMSE: 2.56276608 | ‚àö(Val Loss) = 2.45548701 | Current Learning Rate: 0.002\n",
      "Epoch 117/1000 | Train Loss=55556.23525706 | Val Loss=13.62931892 | Data=555.40289110 | Physics=13.19010307 | Val RMSE: 2.54646683 | ‚àö(Val Loss) = 3.69179082 | Current Learning Rate: 0.002\n",
      "Epoch 118/1000 | Train Loss=56175.69254032 | Val Loss=4.53549402 | Data=561.59942824 | Physics=13.14574170 | Val RMSE: 2.52317095 | ‚àö(Val Loss) = 2.12966990 | Current Learning Rate: 0.002\n",
      "Epoch 119/1000 | Train Loss=55614.07321069 | Val Loss=3.78020981 | Data=556.00113013 | Physics=13.17420783 | Val RMSE: 2.53409266 | ‚àö(Val Loss) = 1.94427621 | Current Learning Rate: 0.002\n",
      "Epoch 120/1000 | Train Loss=56374.32900706 | Val Loss=4.47320613 | Data=563.59866825 | Physics=13.55074143 | Val RMSE: 2.57669592 | ‚àö(Val Loss) = 2.11499548 | Current Learning Rate: 0.002\n",
      "Epoch 121/1000 | Train Loss=55295.23500504 | Val Loss=4.30652857 | Data=552.81586678 | Physics=13.27513263 | Val RMSE: 2.53227997 | ‚àö(Val Loss) = 2.07521772 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 122/1000 | Train Loss=55596.73601310 | Val Loss=3.87980938 | Data=555.82435460 | Physics=13.48267415 | Val RMSE: 2.53164291 | ‚àö(Val Loss) = 1.96972322 | Current Learning Rate: 0.002\n",
      "Epoch 123/1000 | Train Loss=56828.65524194 | Val Loss=3.67870172 | Data=568.14772280 | Physics=13.67924153 | Val RMSE: 2.52511978 | ‚àö(Val Loss) = 1.91799414 | Current Learning Rate: 0.002\n",
      "Epoch 124/1000 | Train Loss=55882.95236895 | Val Loss=4.28644688 | Data=558.68124094 | Physics=13.02863066 | Val RMSE: 2.61024761 | ‚àö(Val Loss) = 2.07037354 | Current Learning Rate: 0.002\n",
      "Epoch 125/1000 | Train Loss=55776.19581653 | Val Loss=3.45306625 | Data=557.62137924 | Physics=14.24933240 | Val RMSE: 2.53895473 | ‚àö(Val Loss) = 1.85824287 | Current Learning Rate: 0.002\n",
      "Epoch 126/1000 | Train Loss=57050.25982863 | Val Loss=4.29165001 | Data=570.35778612 | Physics=13.51752727 | Val RMSE: 2.56979346 | ‚àö(Val Loss) = 2.07162976 | Current Learning Rate: 0.002\n",
      "Epoch 127/1000 | Train Loss=55307.45262097 | Val Loss=3.62991513 | Data=552.93777564 | Physics=13.23138931 | Val RMSE: 2.54044080 | ‚àö(Val Loss) = 1.90523362 | Current Learning Rate: 0.002\n",
      "Epoch 128/1000 | Train Loss=55889.93334173 | Val Loss=4.25160248 | Data=558.75083087 | Physics=13.14123476 | Val RMSE: 2.57071972 | ‚àö(Val Loss) = 2.06194139 | Current Learning Rate: 0.002\n",
      "Epoch 129/1000 | Train Loss=59769.88369456 | Val Loss=3.80969971 | Data=597.54321880 | Physics=13.74793713 | Val RMSE: 2.51962829 | ‚àö(Val Loss) = 1.95184517 | Current Learning Rate: 0.002\n",
      "Epoch 130/1000 | Train Loss=55470.63986895 | Val Loss=3.41520097 | Data=554.56996204 | Physics=13.62795067 | Val RMSE: 2.55602765 | ‚àö(Val Loss) = 1.84802628 | Current Learning Rate: 0.002\n",
      "Epoch 131/1000 | Train Loss=55594.47316028 | Val Loss=3.39478030 | Data=555.80430751 | Physics=13.19302327 | Val RMSE: 2.54012847 | ‚àö(Val Loss) = 1.84249294 | Current Learning Rate: 0.002\n",
      "Epoch 132/1000 | Train Loss=56406.70425907 | Val Loss=4.08555957 | Data=563.91308988 | Physics=13.68704393 | Val RMSE: 2.54011321 | ‚àö(Val Loss) = 2.02127671 | Current Learning Rate: 0.002\n",
      "Epoch 133/1000 | Train Loss=56698.48059476 | Val Loss=4.28521946 | Data=566.84427274 | Physics=13.60479035 | Val RMSE: 2.52694368 | ‚àö(Val Loss) = 2.07007718 | Current Learning Rate: 0.002\n",
      "Epoch 134/1000 | Train Loss=55920.13004032 | Val Loss=3.80488288 | Data=559.05152942 | Physics=13.73432368 | Val RMSE: 2.55135894 | ‚àö(Val Loss) = 1.95061088 | Current Learning Rate: 0.002\n",
      "Epoch 135/1000 | Train Loss=56222.45085685 | Val Loss=3.81450517 | Data=562.07965679 | Physics=13.29270859 | Val RMSE: 2.54588938 | ‚àö(Val Loss) = 1.95307577 | Current Learning Rate: 0.002\n",
      "Epoch 136/1000 | Train Loss=55793.21849798 | Val Loss=4.17613791 | Data=557.79208472 | Physics=15.00094482 | Val RMSE: 2.52725720 | ‚àö(Val Loss) = 2.04356003 | Current Learning Rate: 0.002\n",
      "Epoch 137/1000 | Train Loss=56725.63230847 | Val Loss=3.63937858 | Data=567.10206063 | Physics=13.26950234 | Val RMSE: 2.53354883 | ‚àö(Val Loss) = 1.90771556 | Current Learning Rate: 0.002\n",
      "Epoch 138/1000 | Train Loss=56454.97794859 | Val Loss=4.18327606 | Data=564.39720301 | Physics=14.26147405 | Val RMSE: 2.54672956 | ‚àö(Val Loss) = 2.04530597 | Current Learning Rate: 0.002\n",
      "Epoch 139/1000 | Train Loss=56442.78540827 | Val Loss=4.07548710 | Data=564.27635537 | Physics=13.24207604 | Val RMSE: 2.53378344 | ‚àö(Val Loss) = 2.01878357 | Current Learning Rate: 0.002\n",
      "Epoch 140/1000 | Train Loss=56443.04498488 | Val Loss=6.54965050 | Data=564.28696958 | Physics=13.84347610 | Val RMSE: 2.54922724 | ‚àö(Val Loss) = 2.55922842 | Current Learning Rate: 0.002\n",
      "Epoch 141/1000 | Train Loss=55985.53112399 | Val Loss=3.75958836 | Data=559.71461733 | Physics=13.55567676 | Val RMSE: 2.53421474 | ‚àö(Val Loss) = 1.93896580 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 142/1000 | Train Loss=56521.66217238 | Val Loss=3.85073095 | Data=565.06405344 | Physics=14.03674167 | Val RMSE: 2.55169773 | ‚àö(Val Loss) = 1.96232796 | Current Learning Rate: 0.002\n",
      "Epoch 143/1000 | Train Loss=56331.68649194 | Val Loss=3.37738581 | Data=563.17364699 | Physics=13.60167047 | Val RMSE: 2.54485583 | ‚àö(Val Loss) = 1.83776653 | Current Learning Rate: 0.002\n",
      "Epoch 144/1000 | Train Loss=55735.84740423 | Val Loss=3.69580689 | Data=557.22022469 | Physics=12.89633927 | Val RMSE: 2.52894545 | ‚àö(Val Loss) = 1.92244816 | Current Learning Rate: 0.002\n",
      "Epoch 145/1000 | Train Loss=55428.38974294 | Val Loss=3.65045600 | Data=554.14531585 | Physics=13.64463420 | Val RMSE: 2.55386710 | ‚àö(Val Loss) = 1.91061664 | Current Learning Rate: 0.002\n",
      "Epoch 146/1000 | Train Loss=56603.29548891 | Val Loss=4.55831701 | Data=565.88862462 | Physics=13.11317036 | Val RMSE: 2.57192183 | ‚àö(Val Loss) = 2.13502169 | Current Learning Rate: 0.002\n",
      "Epoch 147/1000 | Train Loss=57382.37222782 | Val Loss=4.55668535 | Data=573.66841372 | Physics=14.54276001 | Val RMSE: 2.52417517 | ‚àö(Val Loss) = 2.13463950 | Current Learning Rate: 0.002\n",
      "Epoch 148/1000 | Train Loss=57080.80783770 | Val Loss=4.03167653 | Data=570.65193619 | Physics=13.58339915 | Val RMSE: 2.53557587 | ‚àö(Val Loss) = 2.00790334 | Current Learning Rate: 0.002\n",
      "Epoch 149/1000 | Train Loss=56080.84211190 | Val Loss=3.28520226 | Data=560.66441591 | Physics=13.55481234 | Val RMSE: 2.54613566 | ‚àö(Val Loss) = 1.81251264 | Current Learning Rate: 0.002\n",
      "Epoch 150/1000 | Train Loss=56272.43245968 | Val Loss=3.51963260 | Data=562.57526718 | Physics=12.76981407 | Val RMSE: 2.53023767 | ‚àö(Val Loss) = 1.87606835 | Current Learning Rate: 0.002\n",
      "Epoch 151/1000 | Train Loss=55549.06829637 | Val Loss=4.60021870 | Data=555.34921363 | Physics=12.76572433 | Val RMSE: 2.54642725 | ‚àö(Val Loss) = 2.14481211 | Current Learning Rate: 0.002\n",
      "Epoch 152/1000 | Train Loss=55304.14604335 | Val Loss=4.31325626 | Data=552.90572234 | Physics=13.12935120 | Val RMSE: 2.54158568 | ‚àö(Val Loss) = 2.07683802 | Current Learning Rate: 0.002\n",
      "Epoch 153/1000 | Train Loss=57048.56829637 | Val Loss=3.21605702 | Data=570.33684712 | Physics=14.30106199 | Val RMSE: 2.58211637 | ‚àö(Val Loss) = 1.79333687 | Current Learning Rate: 0.002\n",
      "Epoch 154/1000 | Train Loss=56191.97593246 | Val Loss=3.62993836 | Data=561.77955677 | Physics=13.18307486 | Val RMSE: 2.53342700 | ‚àö(Val Loss) = 1.90523970 | Current Learning Rate: 0.002\n",
      "Epoch 155/1000 | Train Loss=55689.77192540 | Val Loss=4.39390709 | Data=556.75999992 | Physics=13.65471659 | Val RMSE: 2.57392454 | ‚àö(Val Loss) = 2.09616494 | Current Learning Rate: 0.002\n",
      "Epoch 156/1000 | Train Loss=55585.90637601 | Val Loss=4.77140859 | Data=555.71845664 | Physics=13.17670402 | Val RMSE: 2.60951233 | ‚àö(Val Loss) = 2.18435550 | Current Learning Rate: 0.002\n",
      "Epoch 157/1000 | Train Loss=56055.20186492 | Val Loss=3.40227742 | Data=560.41245787 | Physics=14.70948401 | Val RMSE: 2.73390102 | ‚àö(Val Loss) = 1.84452641 | Current Learning Rate: 0.002\n",
      "Epoch 158/1000 | Train Loss=56901.44632056 | Val Loss=3.86600610 | Data=568.85754788 | Physics=13.42368900 | Val RMSE: 2.53303885 | ‚àö(Val Loss) = 1.96621621 | Current Learning Rate: 0.002\n",
      "Epoch 159/1000 | Train Loss=56145.69329637 | Val Loss=4.21513362 | Data=561.31116510 | Physics=13.20404573 | Val RMSE: 2.52517986 | ‚àö(Val Loss) = 2.05307913 | Current Learning Rate: 0.002\n",
      "Epoch 160/1000 | Train Loss=57167.33215726 | Val Loss=4.68197060 | Data=571.51115762 | Physics=14.51004035 | Val RMSE: 2.51629877 | ‚àö(Val Loss) = 2.16378617 | Current Learning Rate: 0.002\n",
      "Epoch 161/1000 | Train Loss=56393.53049395 | Val Loss=3.91214555 | Data=563.79964915 | Physics=13.48052035 | Val RMSE: 2.57415867 | ‚àö(Val Loss) = 1.97791445 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 162/1000 | Train Loss=56259.56048387 | Val Loss=4.04721441 | Data=562.44873638 | Physics=13.88549894 | Val RMSE: 2.53543377 | ‚àö(Val Loss) = 2.01176906 | Current Learning Rate: 0.002\n",
      "Epoch 163/1000 | Train Loss=55300.00894657 | Val Loss=3.65716218 | Data=552.86524816 | Physics=13.48106064 | Val RMSE: 2.55960608 | ‚àö(Val Loss) = 1.91237080 | Current Learning Rate: 0.002\n",
      "Epoch 164/1000 | Train Loss=55809.81930444 | Val Loss=4.60511638 | Data=557.96092962 | Physics=14.06271571 | Val RMSE: 2.57547903 | ‚àö(Val Loss) = 2.14595342 | Current Learning Rate: 0.002\n",
      "Epoch 165/1000 | Train Loss=56493.89163306 | Val Loss=5.00992659 | Data=564.79781317 | Physics=13.82611403 | Val RMSE: 2.62862468 | ‚àö(Val Loss) = 2.23828650 | Current Learning Rate: 0.002\n",
      "Epoch 166/1000 | Train Loss=56890.58568548 | Val Loss=3.48679615 | Data=568.75695998 | Physics=13.73296025 | Val RMSE: 2.52331495 | ‚àö(Val Loss) = 1.86729646 | Current Learning Rate: 0.002\n",
      "Epoch 167/1000 | Train Loss=56066.96383569 | Val Loss=16.08541162 | Data=560.52273855 | Physics=38.55968658 | Val RMSE: 2.57208991 | ‚àö(Val Loss) = 4.01066208 | Current Learning Rate: 0.002\n",
      "Epoch 168/1000 | Train Loss=55867.31086190 | Val Loss=4.25255338 | Data=558.53424072 | Physics=13.05977585 | Val RMSE: 2.53802872 | ‚àö(Val Loss) = 2.06217194 | Current Learning Rate: 0.002\n",
      "Epoch 169/1000 | Train Loss=55940.13999496 | Val Loss=4.70191416 | Data=559.26252205 | Physics=13.39154691 | Val RMSE: 2.57042694 | ‚àö(Val Loss) = 2.16838980 | Current Learning Rate: 0.002\n",
      "Epoch 170/1000 | Train Loss=55722.72442036 | Val Loss=3.95684401 | Data=557.09051514 | Physics=13.71772643 | Val RMSE: 2.53834057 | ‚àö(Val Loss) = 1.98918176 | Current Learning Rate: 0.002\n",
      "Epoch 171/1000 | Train Loss=60961.85597278 | Val Loss=4.26160373 | Data=609.47053577 | Physics=13.39364815 | Val RMSE: 2.56249857 | ‚àö(Val Loss) = 2.06436515 | Current Learning Rate: 0.002\n",
      "Epoch 172/1000 | Train Loss=55812.99785786 | Val Loss=3.85454192 | Data=557.98752520 | Physics=13.89501796 | Val RMSE: 2.53807831 | ‚àö(Val Loss) = 1.96329880 | Current Learning Rate: 0.002\n",
      "Epoch 173/1000 | Train Loss=56572.36844758 | Val Loss=4.12755358 | Data=565.57266728 | Physics=13.48139436 | Val RMSE: 2.53453517 | ‚àö(Val Loss) = 2.03163815 | Current Learning Rate: 0.002\n",
      "Epoch 174/1000 | Train Loss=61595.97038810 | Val Loss=4.11788772 | Data=615.76695990 | Physics=13.53993523 | Val RMSE: 2.54842854 | ‚àö(Val Loss) = 2.02925777 | Current Learning Rate: 0.002\n",
      "Epoch 175/1000 | Train Loss=58700.68863407 | Val Loss=5.28202711 | Data=586.85295450 | Physics=13.44448888 | Val RMSE: 2.55343103 | ‚àö(Val Loss) = 2.29826617 | Current Learning Rate: 0.002\n",
      "Epoch 176/1000 | Train Loss=55505.57270665 | Val Loss=4.85292023 | Data=554.91818730 | Physics=15.03507343 | Val RMSE: 2.53322840 | ‚àö(Val Loss) = 2.20293450 | Current Learning Rate: 0.002\n",
      "Epoch 177/1000 | Train Loss=55758.59614415 | Val Loss=3.76480010 | Data=557.44871275 | Physics=13.63459524 | Val RMSE: 2.54095387 | ‚àö(Val Loss) = 1.94030929 | Current Learning Rate: 0.002\n",
      "Epoch 178/1000 | Train Loss=55602.63911290 | Val Loss=4.06868831 | Data=555.88829976 | Physics=13.27493890 | Val RMSE: 2.55211210 | ‚àö(Val Loss) = 2.01709890 | Current Learning Rate: 0.002\n",
      "Epoch 179/1000 | Train Loss=56710.17376512 | Val Loss=3.49769228 | Data=566.95718088 | Physics=13.62028460 | Val RMSE: 2.51411319 | ‚àö(Val Loss) = 1.87021184 | Current Learning Rate: 0.002\n",
      "Epoch 180/1000 | Train Loss=55406.82787298 | Val Loss=3.75500286 | Data=553.93203440 | Physics=12.91567537 | Val RMSE: 2.57130432 | ‚àö(Val Loss) = 1.93778300 | Current Learning Rate: 0.002\n",
      "Epoch 181/1000 | Train Loss=56707.64138105 | Val Loss=5.28255514 | Data=566.93225885 | Physics=13.15149031 | Val RMSE: 2.60186076 | ‚àö(Val Loss) = 2.29838109 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 182/1000 | Train Loss=56483.90410786 | Val Loss=6.59208257 | Data=564.69808074 | Physics=13.45617573 | Val RMSE: 2.54145885 | ‚àö(Val Loss) = 2.56750512 | Current Learning Rate: 0.002\n",
      "Epoch 183/1000 | Train Loss=55727.94657258 | Val Loss=4.41325220 | Data=557.14214103 | Physics=13.35162982 | Val RMSE: 2.53524494 | ‚àö(Val Loss) = 2.10077429 | Current Learning Rate: 0.002\n",
      "Epoch 184/1000 | Train Loss=55760.55531754 | Val Loss=4.21603710 | Data=557.46698195 | Physics=13.33976057 | Val RMSE: 2.55765581 | ‚àö(Val Loss) = 2.05329919 | Current Learning Rate: 0.002\n",
      "Epoch 185/1000 | Train Loss=55904.20425907 | Val Loss=3.81635634 | Data=558.90112108 | Physics=13.77304550 | Val RMSE: 2.51611495 | ‚àö(Val Loss) = 1.95354974 | Current Learning Rate: 0.002\n",
      "Epoch 186/1000 | Train Loss=57162.27545363 | Val Loss=4.48617806 | Data=571.47948530 | Physics=13.73482700 | Val RMSE: 2.61282706 | ‚àö(Val Loss) = 2.11805987 | Current Learning Rate: 0.002\n",
      "Epoch 187/1000 | Train Loss=55893.93321573 | Val Loss=4.57717979 | Data=558.80032053 | Physics=13.77350659 | Val RMSE: 2.55545139 | ‚àö(Val Loss) = 2.13943458 | Current Learning Rate: 0.002\n",
      "Epoch 188/1000 | Train Loss=55325.81174395 | Val Loss=3.96599070 | Data=553.12249165 | Physics=13.07192929 | Val RMSE: 2.54822040 | ‚àö(Val Loss) = 1.99147952 | Current Learning Rate: 0.002\n",
      "Epoch 189/1000 | Train Loss=56210.80947581 | Val Loss=4.86542109 | Data=561.95373732 | Physics=13.81412452 | Val RMSE: 2.56398082 | ‚àö(Val Loss) = 2.20577002 | Current Learning Rate: 0.002\n",
      "Epoch 190/1000 | Train Loss=55625.07837702 | Val Loss=14.78051853 | Data=556.10926671 | Physics=17.44665911 | Val RMSE: 2.62710857 | ‚àö(Val Loss) = 3.84454393 | Current Learning Rate: 0.002\n",
      "Epoch 191/1000 | Train Loss=55904.38583669 | Val Loss=3.92027838 | Data=558.90574006 | Physics=13.40511896 | Val RMSE: 2.54181814 | ‚àö(Val Loss) = 1.97996926 | Current Learning Rate: 0.002\n",
      "Epoch 192/1000 | Train Loss=55801.29271673 | Val Loss=3.51694517 | Data=557.86801837 | Physics=13.46683015 | Val RMSE: 2.51715207 | ‚àö(Val Loss) = 1.87535203 | Current Learning Rate: 0.002\n",
      "Epoch 193/1000 | Train Loss=56031.27507560 | Val Loss=3.79403569 | Data=560.16805735 | Physics=14.81222653 | Val RMSE: 2.54021239 | ‚àö(Val Loss) = 1.94782841 | Current Learning Rate: 0.002\n",
      "Epoch 194/1000 | Train Loss=55669.40221774 | Val Loss=3.59021351 | Data=556.55737305 | Physics=14.14867663 | Val RMSE: 2.62508178 | ‚àö(Val Loss) = 1.89478588 | Current Learning Rate: 0.002\n",
      "Epoch 195/1000 | Train Loss=56099.49773185 | Val Loss=3.79459726 | Data=560.85490959 | Physics=13.89093805 | Val RMSE: 2.53836513 | ‚àö(Val Loss) = 1.94797254 | Current Learning Rate: 0.002\n",
      "Epoch 196/1000 | Train Loss=56218.20476310 | Val Loss=3.27604198 | Data=562.03778864 | Physics=13.68228349 | Val RMSE: 2.53998637 | ‚àö(Val Loss) = 1.80998397 | Current Learning Rate: 0.002\n",
      "Epoch 197/1000 | Train Loss=60232.14919355 | Val Loss=12.90529612 | Data=602.17241841 | Physics=13.33587453 | Val RMSE: 2.87896991 | ‚àö(Val Loss) = 3.59239411 | Current Learning Rate: 0.002\n",
      "Epoch 198/1000 | Train Loss=56438.41053427 | Val Loss=4.17331927 | Data=564.22911219 | Physics=13.43494601 | Val RMSE: 2.54563713 | ‚àö(Val Loss) = 2.04287028 | Current Learning Rate: 0.002\n",
      "Epoch 199/1000 | Train Loss=57426.99218750 | Val Loss=3.58977606 | Data=574.11535448 | Physics=13.53035014 | Val RMSE: 2.53833580 | ‚àö(Val Loss) = 1.89467049 | Current Learning Rate: 0.002\n",
      "Epoch 200/1000 | Train Loss=56885.43586190 | Val Loss=3.80698428 | Data=568.70201455 | Physics=13.30735334 | Val RMSE: 2.53119159 | ‚àö(Val Loss) = 1.95114946 | Current Learning Rate: 0.002\n",
      "Epoch 201/1000 | Train Loss=58199.88256048 | Val Loss=3.86551132 | Data=581.84134994 | Physics=13.46225078 | Val RMSE: 2.53849411 | ‚àö(Val Loss) = 1.96609044 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 202/1000 | Train Loss=55605.15196573 | Val Loss=3.43796988 | Data=555.91651769 | Physics=15.52874631 | Val RMSE: 2.57654548 | ‚àö(Val Loss) = 1.85417640 | Current Learning Rate: 0.002\n",
      "Epoch 203/1000 | Train Loss=55615.96307964 | Val Loss=4.43764664 | Data=556.02340403 | Physics=13.32329233 | Val RMSE: 2.60618949 | ‚àö(Val Loss) = 2.10657239 | Current Learning Rate: 0.002\n",
      "Epoch 204/1000 | Train Loss=57047.26764113 | Val Loss=3.46202093 | Data=570.33726058 | Physics=14.07440405 | Val RMSE: 2.52454877 | ‚àö(Val Loss) = 1.86065066 | Current Learning Rate: 0.002\n",
      "Epoch 205/1000 | Train Loss=55990.70060484 | Val Loss=4.35444524 | Data=559.76530900 | Physics=13.81318960 | Val RMSE: 2.61311626 | ‚àö(Val Loss) = 2.08673072 | Current Learning Rate: 0.002\n",
      "Epoch 206/1000 | Train Loss=56077.50037802 | Val Loss=3.37109567 | Data=560.63639192 | Physics=13.10441041 | Val RMSE: 2.52346921 | ‚àö(Val Loss) = 1.83605433 | Current Learning Rate: 0.002\n",
      "Epoch 207/1000 | Train Loss=55353.81413810 | Val Loss=4.31168752 | Data=553.40167630 | Physics=13.52258289 | Val RMSE: 2.52416778 | ‚àö(Val Loss) = 2.07646036 | Current Learning Rate: 0.002\n",
      "Epoch 208/1000 | Train Loss=56141.41733871 | Val Loss=3.73432450 | Data=561.27462276 | Physics=13.53057828 | Val RMSE: 2.54268932 | ‚àö(Val Loss) = 1.93244004 | Current Learning Rate: 0.002\n",
      "Epoch 209/1000 | Train Loss=55358.45123488 | Val Loss=4.45450544 | Data=553.44975035 | Physics=14.00375730 | Val RMSE: 2.54267502 | ‚àö(Val Loss) = 2.11056995 | Current Learning Rate: 0.002\n",
      "Epoch 210/1000 | Train Loss=55625.62878024 | Val Loss=3.61552840 | Data=556.11936508 | Physics=13.59338520 | Val RMSE: 2.53557181 | ‚àö(Val Loss) = 1.90145433 | Current Learning Rate: 0.002\n",
      "Epoch 211/1000 | Train Loss=55705.79359879 | Val Loss=3.51358723 | Data=556.91423970 | Physics=13.46954589 | Val RMSE: 2.54126048 | ‚àö(Val Loss) = 1.87445652 | Current Learning Rate: 0.002\n",
      "Epoch 212/1000 | Train Loss=60115.68145161 | Val Loss=5.00120837 | Data=600.95623976 | Physics=14.28947025 | Val RMSE: 2.57682943 | ‚àö(Val Loss) = 2.23633814 | Current Learning Rate: 0.002\n",
      "Epoch 213/1000 | Train Loss=55760.52154738 | Val Loss=5.75496452 | Data=557.47036251 | Physics=14.24760154 | Val RMSE: 2.59618330 | ‚àö(Val Loss) = 2.39895058 | Current Learning Rate: 0.002\n",
      "Epoch 214/1000 | Train Loss=56042.86567540 | Val Loss=4.93445771 | Data=560.29204535 | Physics=13.32395025 | Val RMSE: 2.56097078 | ‚àö(Val Loss) = 2.22136402 | Current Learning Rate: 0.002\n",
      "Epoch 215/1000 | Train Loss=58563.89805948 | Val Loss=30.49229649 | Data=585.49962788 | Physics=13.81051587 | Val RMSE: 2.72259021 | ‚àö(Val Loss) = 5.52198315 | Current Learning Rate: 0.002\n",
      "Epoch 216/1000 | Train Loss=56063.32913306 | Val Loss=3.73923341 | Data=560.49294355 | Physics=16.10570217 | Val RMSE: 2.60272741 | ‚àö(Val Loss) = 1.93370974 | Current Learning Rate: 0.002\n",
      "Epoch 217/1000 | Train Loss=55478.12008569 | Val Loss=4.80738437 | Data=554.64548419 | Physics=13.47621223 | Val RMSE: 2.54176211 | ‚àö(Val Loss) = 2.19257474 | Current Learning Rate: 0.002\n",
      "Epoch 218/1000 | Train Loss=56185.77318548 | Val Loss=4.82637739 | Data=561.71360730 | Physics=14.09550665 | Val RMSE: 2.54550052 | ‚àö(Val Loss) = 2.19690180 | Current Learning Rate: 0.002\n",
      "Epoch 219/1000 | Train Loss=55762.65738407 | Val Loss=3.99255441 | Data=557.48827337 | Physics=13.33599367 | Val RMSE: 2.53647876 | ‚àö(Val Loss) = 1.99813771 | Current Learning Rate: 0.002\n",
      "Epoch 220/1000 | Train Loss=56571.17918347 | Val Loss=4.63380669 | Data=565.56071029 | Physics=13.52984526 | Val RMSE: 2.53614950 | ‚àö(Val Loss) = 2.15262794 | Current Learning Rate: 0.002\n",
      "Epoch 221/1000 | Train Loss=57714.40511593 | Val Loss=3.29826897 | Data=576.97896256 | Physics=13.54982497 | Val RMSE: 2.53354025 | ‚àö(Val Loss) = 1.81611371 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 222/1000 | Train Loss=55635.51285282 | Val Loss=13.26826586 | Data=556.21467049 | Physics=15.14758676 | Val RMSE: 2.57041454 | ‚àö(Val Loss) = 3.64256310 | Current Learning Rate: 0.002\n",
      "Epoch 223/1000 | Train Loss=55663.57774698 | Val Loss=5.45191257 | Data=556.48240809 | Physics=13.37121298 | Val RMSE: 2.54708004 | ‚àö(Val Loss) = 2.33493304 | Current Learning Rate: 0.002\n",
      "Epoch 224/1000 | Train Loss=56230.92086694 | Val Loss=5.54428881 | Data=562.17163283 | Physics=13.46416060 | Val RMSE: 2.56340766 | ‚àö(Val Loss) = 2.35463142 | Current Learning Rate: 0.002\n",
      "Epoch 225/1000 | Train Loss=56193.73387097 | Val Loss=4.57821233 | Data=561.79617605 | Physics=13.26055247 | Val RMSE: 2.52639246 | ‚àö(Val Loss) = 2.13967562 | Current Learning Rate: 0.002\n",
      "Epoch 226/1000 | Train Loss=56412.71875000 | Val Loss=7.77343621 | Data=563.98739132 | Physics=14.26794145 | Val RMSE: 2.56290746 | ‚àö(Val Loss) = 2.78808832 | Current Learning Rate: 0.002\n",
      "Epoch 227/1000 | Train Loss=56489.51197077 | Val Loss=4.54435100 | Data=564.75470955 | Physics=13.37505466 | Val RMSE: 2.52455640 | ‚àö(Val Loss) = 2.13174844 | Current Learning Rate: 0.002\n",
      "Epoch 228/1000 | Train Loss=55529.00428427 | Val Loss=3.94492102 | Data=555.15517795 | Physics=13.07732483 | Val RMSE: 2.53268433 | ‚àö(Val Loss) = 1.98618257 | Current Learning Rate: 0.002\n",
      "Epoch 229/1000 | Train Loss=55718.21786794 | Val Loss=5.24200603 | Data=557.04386065 | Physics=16.20726919 | Val RMSE: 2.62602115 | ‚àö(Val Loss) = 2.28954268 | Current Learning Rate: 0.002\n",
      "Epoch 230/1000 | Train Loss=57197.80632560 | Val Loss=87.62096351 | Data=571.83431318 | Physics=13.70157164 | Val RMSE: 2.82721019 | ‚àö(Val Loss) = 9.36060715 | Current Learning Rate: 0.002\n",
      "Epoch 231/1000 | Train Loss=57000.19354839 | Val Loss=5.12249388 | Data=569.86324778 | Physics=13.70426442 | Val RMSE: 2.70897412 | ‚àö(Val Loss) = 2.26329279 | Current Learning Rate: 0.002\n",
      "Epoch 232/1000 | Train Loss=57160.72454637 | Val Loss=4.61067939 | Data=571.46212769 | Physics=13.55325121 | Val RMSE: 2.55762434 | ‚àö(Val Loss) = 2.14724922 | Current Learning Rate: 0.002\n",
      "Epoch 233/1000 | Train Loss=55828.79523690 | Val Loss=4.35813263 | Data=558.15094782 | Physics=13.58284784 | Val RMSE: 2.52710080 | ‚àö(Val Loss) = 2.08761406 | Current Learning Rate: 0.002\n",
      "Epoch 234/1000 | Train Loss=56416.95703125 | Val Loss=4.19311070 | Data=564.02900154 | Physics=13.96678808 | Val RMSE: 2.56176996 | ‚àö(Val Loss) = 2.04770851 | Current Learning Rate: 0.002\n",
      "Epoch 235/1000 | Train Loss=55811.22996472 | Val Loss=3.56986644 | Data=557.97173875 | Physics=14.11160523 | Val RMSE: 2.54695415 | ‚àö(Val Loss) = 1.88940907 | Current Learning Rate: 0.002\n",
      "Epoch 236/1000 | Train Loss=56115.92741935 | Val Loss=4.24137848 | Data=561.01057877 | Physics=13.42055817 | Val RMSE: 2.53287244 | ‚àö(Val Loss) = 2.05946064 | Current Learning Rate: 0.002\n",
      "Epoch 237/1000 | Train Loss=55811.50088206 | Val Loss=3.76499823 | Data=557.97360525 | Physics=13.08527774 | Val RMSE: 2.53273916 | ‚àö(Val Loss) = 1.94036031 | Current Learning Rate: 0.002\n",
      "Epoch 238/1000 | Train Loss=57485.63419859 | Val Loss=3.93503181 | Data=574.69810437 | Physics=13.45358655 | Val RMSE: 2.53381681 | ‚àö(Val Loss) = 1.98369145 | Current Learning Rate: 0.002\n",
      "Epoch 239/1000 | Train Loss=57792.21396169 | Val Loss=4.39622973 | Data=577.78088970 | Physics=13.29041069 | Val RMSE: 2.54422164 | ‚àö(Val Loss) = 2.09671879 | Current Learning Rate: 0.002\n",
      "Epoch 240/1000 | Train Loss=55981.27696573 | Val Loss=4.26047216 | Data=559.67734848 | Physics=13.52707432 | Val RMSE: 2.57684612 | ‚àö(Val Loss) = 2.06409121 | Current Learning Rate: 0.002\n",
      "Epoch 241/1000 | Train Loss=56342.96471774 | Val Loss=4.59839932 | Data=563.28420135 | Physics=13.56033506 | Val RMSE: 2.54387259 | ‚àö(Val Loss) = 2.14438772 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 242/1000 | Train Loss=55548.97517641 | Val Loss=4.67914331 | Data=555.35049340 | Physics=14.38141747 | Val RMSE: 2.53656125 | ‚àö(Val Loss) = 2.16313267 | Current Learning Rate: 0.002\n",
      "Epoch 243/1000 | Train Loss=56603.82270665 | Val Loss=4.58696377 | Data=565.89963261 | Physics=13.26378265 | Val RMSE: 2.55959558 | ‚àö(Val Loss) = 2.14171982 | Current Learning Rate: 0.002\n",
      "Epoch 244/1000 | Train Loss=56282.46875000 | Val Loss=4.63531499 | Data=562.68393043 | Physics=13.87509013 | Val RMSE: 2.54614425 | ‚àö(Val Loss) = 2.15297818 | Current Learning Rate: 0.002\n",
      "Epoch 245/1000 | Train Loss=57147.99773185 | Val Loss=5.23301261 | Data=571.33771146 | Physics=17.48615421 | Val RMSE: 2.64456415 | ‚àö(Val Loss) = 2.28757787 | Current Learning Rate: 0.002\n",
      "Epoch 246/1000 | Train Loss=55472.03125000 | Val Loss=3.48619272 | Data=554.58533699 | Physics=13.42855259 | Val RMSE: 2.61013460 | ‚àö(Val Loss) = 1.86713493 | Current Learning Rate: 0.002\n",
      "Epoch 247/1000 | Train Loss=55917.06691028 | Val Loss=5.08283038 | Data=559.03558940 | Physics=13.83477593 | Val RMSE: 2.56308794 | ‚àö(Val Loss) = 2.25451326 | Current Learning Rate: 0.002\n",
      "Epoch 248/1000 | Train Loss=55867.37827621 | Val Loss=4.03122592 | Data=558.53646949 | Physics=13.21483045 | Val RMSE: 2.55169225 | ‚àö(Val Loss) = 2.00779128 | Current Learning Rate: 0.002\n",
      "Epoch 249/1000 | Train Loss=56396.37915827 | Val Loss=4.54067864 | Data=563.82217112 | Physics=13.78680607 | Val RMSE: 2.53967977 | ‚àö(Val Loss) = 2.13088679 | Current Learning Rate: 0.002\n",
      "Epoch 250/1000 | Train Loss=55691.94606855 | Val Loss=3.96550800 | Data=556.78125788 | Physics=13.54148807 | Val RMSE: 2.53358173 | ‚àö(Val Loss) = 1.99135828 | Current Learning Rate: 0.002\n",
      "Epoch 251/1000 | Train Loss=57725.11151714 | Val Loss=3.96390401 | Data=577.11427947 | Physics=13.29412995 | Val RMSE: 2.51573110 | ‚àö(Val Loss) = 1.99095547 | Current Learning Rate: 0.002\n",
      "Epoch 252/1000 | Train Loss=55714.62449597 | Val Loss=4.06760965 | Data=557.00377434 | Physics=13.23108261 | Val RMSE: 2.55825925 | ‚àö(Val Loss) = 2.01683164 | Current Learning Rate: 0.002\n",
      "Epoch 253/1000 | Train Loss=61341.23097278 | Val Loss=3.03728051 | Data=613.27634947 | Physics=13.68334394 | Val RMSE: 2.49614501 | ‚àö(Val Loss) = 1.74277949 | Current Learning Rate: 0.002\n",
      "Epoch 254/1000 | Train Loss=55702.40940020 | Val Loss=3.88085760 | Data=556.88506588 | Physics=14.98351861 | Val RMSE: 2.53166580 | ‚àö(Val Loss) = 1.96998930 | Current Learning Rate: 0.002\n",
      "Epoch 255/1000 | Train Loss=56482.25302419 | Val Loss=3.84813161 | Data=564.68362919 | Physics=13.43802247 | Val RMSE: 2.50458264 | ‚àö(Val Loss) = 1.96166551 | Current Learning Rate: 0.002\n",
      "Epoch 256/1000 | Train Loss=55902.73576109 | Val Loss=3.81517242 | Data=558.89073919 | Physics=15.27820290 | Val RMSE: 2.50809646 | ‚àö(Val Loss) = 1.95324659 | Current Learning Rate: 0.002\n",
      "Epoch 257/1000 | Train Loss=55463.02494960 | Val Loss=4.15127928 | Data=554.49457378 | Physics=13.39418151 | Val RMSE: 2.51401639 | ‚àö(Val Loss) = 2.03746891 | Current Learning Rate: 0.002\n",
      "Epoch 258/1000 | Train Loss=57617.19695060 | Val Loss=3.81930141 | Data=576.01827904 | Physics=13.91907387 | Val RMSE: 2.53543019 | ‚àö(Val Loss) = 1.95430326 | Current Learning Rate: 0.002\n",
      "Epoch 259/1000 | Train Loss=55480.12500000 | Val Loss=3.63948787 | Data=554.66334583 | Physics=13.98637877 | Val RMSE: 2.58884501 | ‚àö(Val Loss) = 1.90774417 | Current Learning Rate: 0.002\n",
      "Epoch 260/1000 | Train Loss=56292.23928931 | Val Loss=3.92961144 | Data=562.78034727 | Physics=13.71729776 | Val RMSE: 2.56157756 | ‚àö(Val Loss) = 1.98232472 | Current Learning Rate: 0.002\n",
      "Epoch 261/1000 | Train Loss=56018.01953125 | Val Loss=3.75581767 | Data=560.03647933 | Physics=16.12337165 | Val RMSE: 2.54504633 | ‚àö(Val Loss) = 1.93799317 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 262/1000 | Train Loss=58946.70299899 | Val Loss=3.70100765 | Data=589.32324219 | Physics=14.91491477 | Val RMSE: 2.53327560 | ‚àö(Val Loss) = 1.92380035 | Current Learning Rate: 0.002\n",
      "Epoch 263/1000 | Train Loss=57817.80443548 | Val Loss=12.11919526 | Data=578.02261451 | Physics=14.65236599 | Val RMSE: 2.54303503 | ‚àö(Val Loss) = 3.48126340 | Current Learning Rate: 0.002\n",
      "Epoch 264/1000 | Train Loss=55979.37676411 | Val Loss=3.57372186 | Data=559.65637798 | Physics=13.79758854 | Val RMSE: 2.52283335 | ‚àö(Val Loss) = 1.89042902 | Current Learning Rate: 0.002\n",
      "Epoch 265/1000 | Train Loss=57258.32976310 | Val Loss=3.51330094 | Data=572.43761813 | Physics=13.35536460 | Val RMSE: 2.53102565 | ‚àö(Val Loss) = 1.87438011 | Current Learning Rate: 0.002\n",
      "Epoch 266/1000 | Train Loss=56522.33140121 | Val Loss=3.96325745 | Data=565.08346656 | Physics=13.76591053 | Val RMSE: 2.55165911 | ‚àö(Val Loss) = 1.99079323 | Current Learning Rate: 0.002\n",
      "Epoch 267/1000 | Train Loss=57590.82409274 | Val Loss=4.05177786 | Data=575.76066146 | Physics=13.78823110 | Val RMSE: 2.54321671 | ‚àö(Val Loss) = 2.01290274 | Current Learning Rate: 0.002\n",
      "Epoch 268/1000 | Train Loss=55560.93976815 | Val Loss=3.61699117 | Data=555.47452078 | Physics=13.34195063 | Val RMSE: 2.54158854 | ‚àö(Val Loss) = 1.90183890 | Current Learning Rate: 0.002\n",
      "Epoch 269/1000 | Train Loss=56216.06943044 | Val Loss=3.07219072 | Data=562.01576282 | Physics=13.74714737 | Val RMSE: 2.52343225 | ‚àö(Val Loss) = 1.75276661 | Current Learning Rate: 0.002\n",
      "Epoch 270/1000 | Train Loss=56036.41444052 | Val Loss=4.33447589 | Data=560.22605453 | Physics=13.96052540 | Val RMSE: 2.56291485 | ‚àö(Val Loss) = 2.08194041 | Current Learning Rate: 0.002\n",
      "Epoch 271/1000 | Train Loss=56795.03011593 | Val Loss=5.33378632 | Data=567.80640436 | Physics=13.93406145 | Val RMSE: 2.57708406 | ‚àö(Val Loss) = 2.30949926 | Current Learning Rate: 0.002\n",
      "Epoch 272/1000 | Train Loss=56559.25466230 | Val Loss=6.36357995 | Data=565.44496598 | Physics=14.37283738 | Val RMSE: 2.55587745 | ‚àö(Val Loss) = 2.52261376 | Current Learning Rate: 0.002\n",
      "Epoch 273/1000 | Train Loss=55611.18623992 | Val Loss=3.53057506 | Data=555.96990179 | Physics=13.69212756 | Val RMSE: 2.52879953 | ‚àö(Val Loss) = 1.87898242 | Current Learning Rate: 0.002\n",
      "Epoch 274/1000 | Train Loss=56626.64276714 | Val Loss=3.40517047 | Data=566.13005410 | Physics=14.62134907 | Val RMSE: 2.60818934 | ‚àö(Val Loss) = 1.84531045 | Current Learning Rate: 0.002\n",
      "Epoch 275/1000 | Train Loss=56715.86718750 | Val Loss=4.40044488 | Data=567.00463473 | Physics=13.41248492 | Val RMSE: 2.52775097 | ‚àö(Val Loss) = 2.09772372 | Current Learning Rate: 0.002\n",
      "Epoch 276/1000 | Train Loss=55870.71622984 | Val Loss=4.17239646 | Data=558.56798135 | Physics=14.11639224 | Val RMSE: 2.50818539 | ‚àö(Val Loss) = 2.04264450 | Current Learning Rate: 0.002\n",
      "Epoch 277/1000 | Train Loss=58065.19581653 | Val Loss=3.35447301 | Data=580.44898839 | Physics=13.82816545 | Val RMSE: 2.56690168 | ‚àö(Val Loss) = 1.83152211 | Current Learning Rate: 0.002\n",
      "Epoch 278/1000 | Train Loss=55384.38445060 | Val Loss=3.77121997 | Data=553.70896272 | Physics=12.86001146 | Val RMSE: 2.53897285 | ‚àö(Val Loss) = 1.94196296 | Current Learning Rate: 0.002\n",
      "Epoch 279/1000 | Train Loss=55592.19795867 | Val Loss=4.39822461 | Data=555.78589261 | Physics=13.10328634 | Val RMSE: 2.53473520 | ‚àö(Val Loss) = 2.09719443 | Current Learning Rate: 0.002\n",
      "Epoch 280/1000 | Train Loss=58018.56678427 | Val Loss=4.65864539 | Data=580.03940902 | Physics=13.27174274 | Val RMSE: 2.55235910 | ‚àö(Val Loss) = 2.15838957 | Current Learning Rate: 0.002\n",
      "Epoch 281/1000 | Train Loss=55491.16532258 | Val Loss=4.58821489 | Data=554.77611714 | Physics=13.18812731 | Val RMSE: 2.53870082 | ‚àö(Val Loss) = 2.14201188 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 282/1000 | Train Loss=55867.92603327 | Val Loss=4.01110094 | Data=558.54165551 | Physics=13.81847963 | Val RMSE: 2.51211166 | ‚àö(Val Loss) = 2.00277328 | Current Learning Rate: 0.002\n",
      "Epoch 283/1000 | Train Loss=56681.03087198 | Val Loss=4.50773704 | Data=566.67333591 | Physics=12.91963468 | Val RMSE: 2.52826405 | ‚àö(Val Loss) = 2.12314320 | Current Learning Rate: 0.002\n",
      "Epoch 284/1000 | Train Loss=56755.05002520 | Val Loss=6.83965291 | Data=567.40498007 | Physics=13.84134336 | Val RMSE: 2.53682494 | ‚àö(Val Loss) = 2.61527300 | Current Learning Rate: 0.002\n",
      "Epoch 285/1000 | Train Loss=56537.84954637 | Val Loss=4.67876601 | Data=565.23947439 | Physics=13.91397335 | Val RMSE: 2.52670956 | ‚àö(Val Loss) = 2.16304564 | Current Learning Rate: 0.002\n",
      "Epoch 286/1000 | Train Loss=56980.49218750 | Val Loss=12.40800469 | Data=569.66197746 | Physics=13.73582240 | Val RMSE: 2.56561065 | ‚àö(Val Loss) = 3.52249980 | Current Learning Rate: 0.002\n",
      "Epoch 287/1000 | Train Loss=56911.91242440 | Val Loss=3.84296596 | Data=568.95540890 | Physics=13.80674381 | Val RMSE: 2.57535911 | ‚àö(Val Loss) = 1.96034849 | Current Learning Rate: 0.002\n",
      "Epoch 288/1000 | Train Loss=55671.67212702 | Val Loss=3.67992931 | Data=556.57639239 | Physics=13.27371977 | Val RMSE: 2.54541254 | ‚àö(Val Loss) = 1.91831422 | Current Learning Rate: 0.002\n",
      "Epoch 289/1000 | Train Loss=56681.53238407 | Val Loss=4.08688159 | Data=566.67588363 | Physics=13.46774524 | Val RMSE: 2.53801990 | ‚àö(Val Loss) = 2.02160382 | Current Learning Rate: 0.002\n",
      "Epoch 290/1000 | Train Loss=58234.72857863 | Val Loss=23.00369631 | Data=582.20368810 | Physics=13.18231522 | Val RMSE: 2.64075375 | ‚àö(Val Loss) = 4.79621696 | Current Learning Rate: 0.002\n",
      "Epoch 291/1000 | Train Loss=58050.34438004 | Val Loss=4.10934724 | Data=580.33116888 | Physics=13.37453030 | Val RMSE: 2.53984618 | ‚àö(Val Loss) = 2.02715254 | Current Learning Rate: 0.002\n",
      "Epoch 292/1000 | Train Loss=55692.84072581 | Val Loss=4.45507225 | Data=556.79305144 | Physics=13.48366165 | Val RMSE: 2.60226560 | ‚àö(Val Loss) = 2.11070418 | Current Learning Rate: 0.002\n",
      "Epoch 293/1000 | Train Loss=56783.59702621 | Val Loss=3.52505956 | Data=567.67730713 | Physics=14.18804141 | Val RMSE: 2.52783918 | ‚àö(Val Loss) = 1.87751412 | Current Learning Rate: 0.002\n",
      "Epoch 294/1000 | Train Loss=55707.13785282 | Val Loss=4.41949129 | Data=556.93588355 | Physics=13.71623898 | Val RMSE: 2.53384829 | ‚àö(Val Loss) = 2.10225868 | Current Learning Rate: 0.002\n",
      "Epoch 295/1000 | Train Loss=56235.58984375 | Val Loss=3.81025366 | Data=562.22023059 | Physics=13.41994263 | Val RMSE: 2.53681278 | ‚àö(Val Loss) = 1.95198715 | Current Learning Rate: 0.002\n",
      "Epoch 296/1000 | Train Loss=56685.30897177 | Val Loss=4.16493760 | Data=566.70626339 | Physics=13.62208917 | Val RMSE: 2.52490687 | ‚àö(Val Loss) = 2.04081774 | Current Learning Rate: 0.002\n",
      "Epoch 297/1000 | Train Loss=55458.06943044 | Val Loss=3.95279224 | Data=554.44594451 | Physics=13.98795785 | Val RMSE: 2.60008407 | ‚àö(Val Loss) = 1.98816299 | Current Learning Rate: 0.002\n",
      "Epoch 298/1000 | Train Loss=55981.16305444 | Val Loss=4.01949584 | Data=559.67298151 | Physics=13.00270245 | Val RMSE: 2.54971814 | ‚àö(Val Loss) = 2.00486803 | Current Learning Rate: 0.002\n",
      "Epoch 299/1000 | Train Loss=55905.57598286 | Val Loss=5.41282616 | Data=558.91329070 | Physics=13.66583745 | Val RMSE: 2.57621503 | ‚àö(Val Loss) = 2.32654810 | Current Learning Rate: 0.002\n",
      "Epoch 300/1000 | Train Loss=56412.35156250 | Val Loss=4.14691612 | Data=563.98466836 | Physics=13.33026394 | Val RMSE: 2.54959607 | ‚àö(Val Loss) = 2.03639770 | Current Learning Rate: 0.002\n",
      "Epoch 301/1000 | Train Loss=57448.87021169 | Val Loss=3.48469378 | Data=574.33060381 | Physics=13.71777376 | Val RMSE: 2.51960540 | ‚àö(Val Loss) = 1.86673343 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 302/1000 | Train Loss=56740.85824093 | Val Loss=3.88889229 | Data=567.26562106 | Physics=13.60072461 | Val RMSE: 2.53569341 | ‚àö(Val Loss) = 1.97202742 | Current Learning Rate: 0.002\n",
      "Epoch 303/1000 | Train Loss=58363.40713206 | Val Loss=3.96811894 | Data=583.47044914 | Physics=14.07168187 | Val RMSE: 2.53701091 | ‚àö(Val Loss) = 1.99201381 | Current Learning Rate: 0.002\n",
      "Epoch 304/1000 | Train Loss=55542.70614919 | Val Loss=3.06344404 | Data=555.29164567 | Physics=13.87421934 | Val RMSE: 2.53660917 | ‚àö(Val Loss) = 1.75026977 | Current Learning Rate: 0.002\n",
      "Epoch 305/1000 | Train Loss=56052.64125504 | Val Loss=3.10845499 | Data=560.38738620 | Physics=13.60800515 | Val RMSE: 2.52273083 | ‚àö(Val Loss) = 1.76308107 | Current Learning Rate: 0.002\n",
      "Epoch 306/1000 | Train Loss=55662.81917843 | Val Loss=7.81640788 | Data=556.49033479 | Physics=13.66337088 | Val RMSE: 2.56521606 | ‚àö(Val Loss) = 2.79578400 | Current Learning Rate: 0.002\n",
      "Epoch 307/1000 | Train Loss=55734.67641129 | Val Loss=4.25108695 | Data=557.21013026 | Physics=13.49308519 | Val RMSE: 2.55621672 | ‚àö(Val Loss) = 2.06181645 | Current Learning Rate: 0.002\n",
      "Epoch 308/1000 | Train Loss=56645.75050403 | Val Loss=4.28335926 | Data=566.31166520 | Physics=13.33524168 | Val RMSE: 2.53952026 | ‚àö(Val Loss) = 2.06962776 | Current Learning Rate: 0.002\n",
      "Epoch 309/1000 | Train Loss=56902.94443044 | Val Loss=23.54288891 | Data=568.88765987 | Physics=17.82914426 | Val RMSE: 2.58587551 | ‚àö(Val Loss) = 4.85210133 | Current Learning Rate: 0.002\n",
      "Epoch 310/1000 | Train Loss=55440.63356855 | Val Loss=3.67596950 | Data=554.26893862 | Physics=13.18763807 | Val RMSE: 2.57756281 | ‚àö(Val Loss) = 1.91728187 | Current Learning Rate: 0.002\n",
      "Epoch 311/1000 | Train Loss=57081.47983871 | Val Loss=4.60860121 | Data=570.67056274 | Physics=13.32415277 | Val RMSE: 2.52281618 | ‚àö(Val Loss) = 2.14676523 | Current Learning Rate: 0.002\n",
      "Epoch 312/1000 | Train Loss=56881.95992944 | Val Loss=4.56708019 | Data=568.65679144 | Physics=13.92730748 | Val RMSE: 2.53557110 | ‚àö(Val Loss) = 2.13707280 | Current Learning Rate: 0.002\n",
      "Epoch 313/1000 | Train Loss=55564.90007560 | Val Loss=3.27789365 | Data=555.51398099 | Physics=13.52355202 | Val RMSE: 2.54304838 | ‚àö(Val Loss) = 1.81049538 | Current Learning Rate: 0.002\n",
      "Epoch 314/1000 | Train Loss=56743.06691028 | Val Loss=3.36014011 | Data=567.29422489 | Physics=13.66159344 | Val RMSE: 2.55616760 | ‚àö(Val Loss) = 1.83306849 | Current Learning Rate: 0.002\n",
      "Epoch 315/1000 | Train Loss=56408.18535786 | Val Loss=4.13603081 | Data=563.93857894 | Physics=13.21597189 | Val RMSE: 2.52189112 | ‚àö(Val Loss) = 2.03372335 | Current Learning Rate: 0.002\n",
      "Epoch 316/1000 | Train Loss=55458.51940524 | Val Loss=5.11180139 | Data=554.45019728 | Physics=13.39261686 | Val RMSE: 2.55879712 | ‚àö(Val Loss) = 2.26092935 | Current Learning Rate: 0.002\n",
      "Epoch 317/1000 | Train Loss=58174.67678931 | Val Loss=3.98503326 | Data=581.59404632 | Physics=13.38864312 | Val RMSE: 2.55573630 | ‚àö(Val Loss) = 1.99625480 | Current Learning Rate: 0.002\n",
      "Epoch 318/1000 | Train Loss=56364.10471270 | Val Loss=3.36716626 | Data=563.49598152 | Physics=13.27284035 | Val RMSE: 2.53550792 | ‚àö(Val Loss) = 1.83498394 | Current Learning Rate: 0.002\n",
      "Epoch 319/1000 | Train Loss=57015.99206149 | Val Loss=3.94041921 | Data=569.99528454 | Physics=13.90360274 | Val RMSE: 2.53757572 | ‚àö(Val Loss) = 1.98504889 | Current Learning Rate: 0.002\n",
      "Epoch 320/1000 | Train Loss=55977.35320060 | Val Loss=3.61871570 | Data=559.63261167 | Physics=13.68604665 | Val RMSE: 2.53854895 | ‚àö(Val Loss) = 1.90229225 | Current Learning Rate: 0.002\n",
      "Epoch 321/1000 | Train Loss=56374.98072077 | Val Loss=3.45245413 | Data=563.61370259 | Physics=13.51254271 | Val RMSE: 2.54215312 | ‚àö(Val Loss) = 1.85807812 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 322/1000 | Train Loss=55681.61391129 | Val Loss=3.55492606 | Data=556.67993361 | Physics=13.77336394 | Val RMSE: 2.65606380 | ‚àö(Val Loss) = 1.88545120 | Current Learning Rate: 0.002\n",
      "Epoch 323/1000 | Train Loss=55242.13054435 | Val Loss=3.89108925 | Data=552.28607375 | Physics=13.41223468 | Val RMSE: 2.51482892 | ‚àö(Val Loss) = 1.97258437 | Current Learning Rate: 0.002\n",
      "Epoch 324/1000 | Train Loss=55788.33480343 | Val Loss=3.66552995 | Data=557.74496362 | Physics=13.11947494 | Val RMSE: 2.56245708 | ‚àö(Val Loss) = 1.91455734 | Current Learning Rate: 0.002\n",
      "Epoch 325/1000 | Train Loss=58609.22089214 | Val Loss=4.70458456 | Data=585.92659636 | Physics=13.60649176 | Val RMSE: 2.66368461 | ‚àö(Val Loss) = 2.16900539 | Current Learning Rate: 0.002\n",
      "Epoch 326/1000 | Train Loss=61785.09501008 | Val Loss=4.03097212 | Data=617.69557633 | Physics=13.60159060 | Val RMSE: 2.53895807 | ‚àö(Val Loss) = 2.00772810 | Current Learning Rate: 0.002\n",
      "Epoch 327/1000 | Train Loss=58533.46597782 | Val Loss=4.81376614 | Data=585.15427719 | Physics=18.44756309 | Val RMSE: 2.60697055 | ‚àö(Val Loss) = 2.19402957 | Current Learning Rate: 0.002\n",
      "Epoch 328/1000 | Train Loss=55445.15675403 | Val Loss=3.41118945 | Data=554.31014129 | Physics=14.78158275 | Val RMSE: 2.54921031 | ‚àö(Val Loss) = 1.84694064 | Current Learning Rate: 0.002\n",
      "Epoch 329/1000 | Train Loss=56601.26701109 | Val Loss=4.32613955 | Data=565.87161944 | Physics=13.58140481 | Val RMSE: 2.52723932 | ‚àö(Val Loss) = 2.07993746 | Current Learning Rate: 0.002\n",
      "Epoch 330/1000 | Train Loss=55664.87071573 | Val Loss=7.26633481 | Data=556.51056597 | Physics=16.01029638 | Val RMSE: 2.54942703 | ‚àö(Val Loss) = 2.69561410 | Current Learning Rate: 0.002\n",
      "Epoch 331/1000 | Train Loss=55935.64616935 | Val Loss=3.35214257 | Data=559.21995889 | Physics=13.80877092 | Val RMSE: 2.53326488 | ‚àö(Val Loss) = 1.83088577 | Current Learning Rate: 0.002\n",
      "Epoch 332/1000 | Train Loss=56952.12966230 | Val Loss=6.94510872 | Data=569.38175718 | Physics=13.89557801 | Val RMSE: 2.55609703 | ‚àö(Val Loss) = 2.63535738 | Current Learning Rate: 0.002\n",
      "Epoch 333/1000 | Train Loss=55634.01776714 | Val Loss=6.06474614 | Data=556.20421182 | Physics=13.71432184 | Val RMSE: 2.65463209 | ‚àö(Val Loss) = 2.46267056 | Current Learning Rate: 0.002\n",
      "Epoch 334/1000 | Train Loss=55552.73550907 | Val Loss=3.69117320 | Data=555.39211741 | Physics=13.49473455 | Val RMSE: 2.55527663 | ‚àö(Val Loss) = 1.92124259 | Current Learning Rate: 0.002\n",
      "Epoch 335/1000 | Train Loss=56145.62235383 | Val Loss=4.26197960 | Data=561.31833575 | Physics=13.35355951 | Val RMSE: 2.53999782 | ‚àö(Val Loss) = 2.06445622 | Current Learning Rate: 0.002\n",
      "Epoch 336/1000 | Train Loss=55260.63117440 | Val Loss=3.86868880 | Data=552.47212662 | Physics=13.58735217 | Val RMSE: 2.51406145 | ‚àö(Val Loss) = 1.96689832 | Current Learning Rate: 0.002\n",
      "Epoch 337/1000 | Train Loss=55479.00844254 | Val Loss=4.43419441 | Data=554.65389129 | Physics=13.71544824 | Val RMSE: 2.51284289 | ‚àö(Val Loss) = 2.10575271 | Current Learning Rate: 0.002\n",
      "Epoch 338/1000 | Train Loss=56204.47782258 | Val Loss=3.84053276 | Data=561.90197163 | Physics=13.48454141 | Val RMSE: 2.57192731 | ‚àö(Val Loss) = 1.95972776 | Current Learning Rate: 0.002\n",
      "Epoch 339/1000 | Train Loss=55438.29725302 | Val Loss=3.47047823 | Data=554.24725932 | Physics=13.29619738 | Val RMSE: 2.52106142 | ‚àö(Val Loss) = 1.86292195 | Current Learning Rate: 0.002\n",
      "Epoch 340/1000 | Train Loss=56143.87563004 | Val Loss=5.35051060 | Data=561.29732587 | Physics=13.09386914 | Val RMSE: 2.57237768 | ‚àö(Val Loss) = 2.31311703 | Current Learning Rate: 0.002\n",
      "Epoch 341/1000 | Train Loss=55941.34097782 | Val Loss=3.67933388 | Data=559.27795607 | Physics=12.91907044 | Val RMSE: 2.52190495 | ‚àö(Val Loss) = 1.91815901 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0012563   -3.664617   -13.297191  ]\n",
      " [  1.0012484   -3.664613   -13.297352  ]\n",
      " [  1.0012394   -3.6646087  -13.297531  ]\n",
      " ...\n",
      " [  0.99524     -3.6618261  -13.425359  ]\n",
      " [  0.99522984  -3.6618214  -13.425581  ]\n",
      " [  0.9952209   -3.6618176  -13.425784  ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9952085   -3.661811   -13.426028  ]\n",
      " [  0.99519825  -3.6618063  -13.426252  ]\n",
      " [  0.99518925  -3.6618025  -13.426457  ]\n",
      " ...\n",
      " [  0.9982406   -3.66323    -13.36144   ]\n",
      " [  0.9982363   -3.663228   -13.361536  ]\n",
      " [  0.99823254  -3.6632266  -13.361624  ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9982245  -3.663222  -13.361764 ]\n",
      " [  0.9982183  -3.663219  -13.361894 ]\n",
      " [  0.9982173  -3.6632197 -13.361955 ]\n",
      " ...\n",
      " [  0.9983473  -3.6632805 -13.359202 ]\n",
      " [  0.998345   -3.6632795 -13.359258 ]\n",
      " [  0.9983428  -3.6632786 -13.35931  ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9983364  -3.6632748 -13.359416 ]\n",
      " [  0.9983315  -3.6632724 -13.359514 ]\n",
      " [  0.9983228  -3.6632674 -13.359665 ]\n",
      " ...\n",
      " [  0.9977363  -3.6629956 -13.372184 ]\n",
      " [  0.9977318  -3.662994  -13.372287 ]\n",
      " [  0.9977279  -3.6629922 -13.372379 ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.99772406  -3.6629906  -13.372467  ]\n",
      " [  0.9977276   -3.662994   -13.372457  ]\n",
      " [  0.997718    -3.6629877  -13.372592  ]\n",
      " ...\n",
      " [  0.99636686  -3.6623557  -13.4013405 ]\n",
      " [  0.9963594   -3.6623526  -13.401508  ]\n",
      " [  0.9963482   -3.6623466  -13.401721  ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.99633914  -3.6623423  -13.401918  ]\n",
      " [  0.9963311   -3.6623387  -13.4021    ]\n",
      " [  0.99631935  -3.6623328  -13.402325  ]\n",
      " ...\n",
      " [  0.9850711   -3.6568837  -13.642199  ]\n",
      " [  0.98505497  -3.6568754  -13.642531  ]\n",
      " [  0.9850417   -3.6568692  -13.642832  ]] \n",
      "\n",
      "Final Test RMSE:  1.3956319640080135\n",
      "Epoch 342/1000 | Train Loss=56184.14062500 | Val Loss=14.12683828 | Data=561.70117975 | Physics=13.92861659 | Val RMSE: 2.52562332 | ‚àö(Val Loss) = 3.75856876 | Current Learning Rate: 0.002\n",
      "Epoch 343/1000 | Train Loss=58133.79737903 | Val Loss=3.92067577 | Data=581.18407416 | Physics=14.39267556 | Val RMSE: 2.53754997 | ‚àö(Val Loss) = 1.98006964 | Current Learning Rate: 0.002\n",
      "Epoch 344/1000 | Train Loss=57250.10080645 | Val Loss=6.52840057 | Data=572.35466742 | Physics=14.11686973 | Val RMSE: 2.77060866 | ‚àö(Val Loss) = 2.55507350 | Current Learning Rate: 0.002\n",
      "Epoch 345/1000 | Train Loss=57851.55808972 | Val Loss=3.38053460 | Data=578.35956795 | Physics=13.24675228 | Val RMSE: 2.53240442 | ‚àö(Val Loss) = 1.83862305 | Current Learning Rate: 0.002\n",
      "Epoch 346/1000 | Train Loss=57986.06073589 | Val Loss=4.47191129 | Data=579.71829716 | Physics=13.48734048 | Val RMSE: 2.54689097 | ‚àö(Val Loss) = 2.11468935 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 347 (Val Loss = 2.87296553)\n",
      "Epoch 347/1000 | Train Loss=55990.00730847 | Val Loss=2.87296553 | Data=559.75196198 | Physics=20.86446274 | Val RMSE: 2.60712028 | ‚àö(Val Loss) = 1.69498253 | Current Learning Rate: 0.002\n",
      "Epoch 348/1000 | Train Loss=55858.34929435 | Val Loss=2.97337438 | Data=558.44141806 | Physics=13.50971597 | Val RMSE: 2.53847051 | ‚àö(Val Loss) = 1.72434747 | Current Learning Rate: 0.002\n",
      "Epoch 349/1000 | Train Loss=56928.58127520 | Val Loss=3.04851526 | Data=569.12550009 | Physics=13.36151025 | Val RMSE: 2.54951072 | ‚àö(Val Loss) = 1.74599981 | Current Learning Rate: 0.002\n",
      "Epoch 350/1000 | Train Loss=55470.55166331 | Val Loss=3.62690752 | Data=554.55811334 | Physics=13.53461654 | Val RMSE: 2.57146382 | ‚àö(Val Loss) = 1.90444422 | Current Learning Rate: 0.002\n",
      "Epoch 351/1000 | Train Loss=55942.59248992 | Val Loss=3.32155133 | Data=559.27984422 | Physics=13.30213179 | Val RMSE: 2.55285048 | ‚àö(Val Loss) = 1.82251239 | Current Learning Rate: 0.002\n",
      "Epoch 352/1000 | Train Loss=56298.08354335 | Val Loss=3.47061181 | Data=562.82813878 | Physics=12.91120427 | Val RMSE: 2.53767133 | ‚àö(Val Loss) = 1.86295784 | Current Learning Rate: 0.002\n",
      "Epoch 353/1000 | Train Loss=56382.37714214 | Val Loss=3.13122202 | Data=563.67245090 | Physics=13.67095646 | Val RMSE: 2.52722239 | ‚àö(Val Loss) = 1.76952589 | Current Learning Rate: 0.002\n",
      "Epoch 354/1000 | Train Loss=55880.80947581 | Val Loss=3.65341091 | Data=558.66375142 | Physics=13.46441463 | Val RMSE: 2.54733896 | ‚àö(Val Loss) = 1.91138983 | Current Learning Rate: 0.002\n",
      "Epoch 355/1000 | Train Loss=57700.74886593 | Val Loss=4.56377012 | Data=576.83400997 | Physics=13.28073485 | Val RMSE: 2.51194286 | ‚àö(Val Loss) = 2.13629818 | Current Learning Rate: 0.002\n",
      "Epoch 356/1000 | Train Loss=56808.41645665 | Val Loss=3.24522482 | Data=567.90957740 | Physics=13.37836397 | Val RMSE: 2.52619791 | ‚àö(Val Loss) = 1.80145073 | Current Learning Rate: 0.002\n",
      "Epoch 357/1000 | Train Loss=55638.85987903 | Val Loss=3.28972637 | Data=556.24336883 | Physics=13.25221152 | Val RMSE: 2.54055929 | ‚àö(Val Loss) = 1.81376028 | Current Learning Rate: 0.002\n",
      "Epoch 358/1000 | Train Loss=55351.47933468 | Val Loss=3.16847515 | Data=553.37281455 | Physics=13.62185705 | Val RMSE: 2.57130027 | ‚àö(Val Loss) = 1.78002107 | Current Learning Rate: 0.002\n",
      "Epoch 359/1000 | Train Loss=55205.55153730 | Val Loss=3.54716754 | Data=551.91708079 | Physics=13.42217388 | Val RMSE: 2.54557228 | ‚àö(Val Loss) = 1.88339257 | Current Learning Rate: 0.002\n",
      "Epoch 360/1000 | Train Loss=56682.29637097 | Val Loss=3.51595264 | Data=566.67212899 | Physics=13.94936198 | Val RMSE: 2.53238583 | ‚àö(Val Loss) = 1.87508738 | Current Learning Rate: 0.002\n",
      "Epoch 361/1000 | Train Loss=55351.86491935 | Val Loss=3.58125596 | Data=553.37528450 | Physics=13.19356410 | Val RMSE: 2.54081035 | ‚àö(Val Loss) = 1.89242065 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.1180941   -3.6829984  -11.196554  ]\n",
      " [  1.0993893   -3.6778367  -11.566442  ]\n",
      " [  1.118237    -3.6830301  -11.193651  ]\n",
      " ...\n",
      " [  0.9749709   -3.6435103  -13.795204  ]\n",
      " [  0.9749625   -3.6435022  -13.795361  ]\n",
      " [  0.97495484  -3.6434953  -13.7955    ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9749448   -3.6434848  -13.795674  ]\n",
      " [  0.97493625  -3.6434767  -13.795833  ]\n",
      " [  0.9749286   -3.6434696  -13.795972  ]\n",
      " ...\n",
      " [  0.97765696  -3.6460319  -13.746776  ]\n",
      " [  0.9776525   -3.646028   -13.746857  ]\n",
      " [  0.97764844  -3.6460247  -13.746929  ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9776414   -3.6460173  -13.74705   ]\n",
      " [  0.9776355   -3.6460116  -13.747163  ]\n",
      " [  0.9776332   -3.6460109  -13.747211  ]\n",
      " ...\n",
      " [  0.97776175  -3.646128   -13.744884  ]\n",
      " [  0.9777594   -3.6461263  -13.7449255 ]\n",
      " [  0.97775716  -3.6461246  -13.744964  ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9777519   -3.6461189  -13.745053  ]\n",
      " [  0.9777475   -3.6461143  -13.745138  ]\n",
      " [  0.97774005  -3.646106   -13.745272  ]\n",
      " ...\n",
      " [  0.97716755  -3.6455872  -13.755602  ]\n",
      " [  0.977163    -3.6455834  -13.755683  ]\n",
      " [  0.97715896  -3.64558    -13.755754  ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.97715515  -3.645577   -13.755822  ]\n",
      " [  0.9771569   -3.645581   -13.755797  ]\n",
      " [  0.9771496   -3.6455727  -13.755906  ]\n",
      " ...\n",
      " [  0.9759204   -3.6444237  -13.778087  ]\n",
      " [  0.97591394  -3.644418   -13.778205  ]\n",
      " [  0.97590476  -3.6444085  -13.778365  ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9758969   -3.6444008  -13.778512  ]\n",
      " [  0.97588974  -3.6443942  -13.778644  ]\n",
      " [  0.9758799   -3.6443841  -13.778815  ]\n",
      " ...\n",
      " [  0.9678632   -3.6359556  -13.923168  ]\n",
      " [  0.9678529   -3.6359437  -13.923348  ]\n",
      " [  0.9678446   -3.6359346  -13.923502  ]] \n",
      "\n",
      "Final Test RMSE:  1.3545585771401722\n",
      "Epoch 362/1000 | Train Loss=56424.15284778 | Val Loss=3.32521515 | Data=564.07963316 | Physics=15.86777474 | Val RMSE: 2.52352190 | ‚àö(Val Loss) = 1.82351720 | Current Learning Rate: 0.002\n",
      "Epoch 363/1000 | Train Loss=56298.16544859 | Val Loss=3.66259558 | Data=562.84152714 | Physics=12.84354877 | Val RMSE: 2.54364395 | ‚àö(Val Loss) = 1.91379082 | Current Learning Rate: 0.002\n",
      "Epoch 364/1000 | Train Loss=56363.40977823 | Val Loss=3.37778281 | Data=563.48109288 | Physics=13.48025607 | Val RMSE: 2.51913333 | ‚àö(Val Loss) = 1.83787453 | Current Learning Rate: 0.002\n",
      "Epoch 365/1000 | Train Loss=55501.77822581 | Val Loss=4.90924624 | Data=554.87983556 | Physics=17.60605501 | Val RMSE: 2.58845091 | ‚àö(Val Loss) = 2.21568203 | Current Learning Rate: 0.002\n",
      "Epoch 366/1000 | Train Loss=56262.49080141 | Val Loss=3.36390170 | Data=562.46589316 | Physics=12.97471533 | Val RMSE: 2.53302002 | ‚àö(Val Loss) = 1.83409417 | Current Learning Rate: 0.002\n",
      "Epoch 367/1000 | Train Loss=56101.06565020 | Val Loss=3.28856759 | Data=560.84723393 | Physics=13.67817034 | Val RMSE: 2.54400206 | ‚àö(Val Loss) = 1.81344080 | Current Learning Rate: 0.002\n",
      "Epoch 368/1000 | Train Loss=56058.16028226 | Val Loss=3.35292484 | Data=560.44181971 | Physics=13.22201942 | Val RMSE: 2.52428770 | ‚àö(Val Loss) = 1.83109939 | Current Learning Rate: 0.002\n",
      "Epoch 369/1000 | Train Loss=55324.96585181 | Val Loss=4.68577647 | Data=553.10715805 | Physics=13.96093824 | Val RMSE: 2.53502131 | ‚àö(Val Loss) = 2.16466546 | Current Learning Rate: 0.002\n",
      "Epoch 370/1000 | Train Loss=56761.74873992 | Val Loss=3.28522090 | Data=567.46977578 | Physics=13.99314557 | Val RMSE: 2.53922129 | ‚àö(Val Loss) = 1.81251788 | Current Learning Rate: 0.002\n",
      "Epoch 371/1000 | Train Loss=55154.87714214 | Val Loss=3.72779150 | Data=551.40865203 | Physics=13.19904145 | Val RMSE: 2.53231478 | ‚àö(Val Loss) = 1.93074894 | Current Learning Rate: 0.002\n",
      "Epoch 372/1000 | Train Loss=56741.78225806 | Val Loss=3.91533324 | Data=567.25502654 | Physics=13.66670721 | Val RMSE: 2.55026340 | ‚àö(Val Loss) = 1.97872007 | Current Learning Rate: 0.002\n",
      "Epoch 373/1000 | Train Loss=55958.21106351 | Val Loss=3.29613268 | Data=559.43278848 | Physics=13.47073299 | Val RMSE: 2.52505350 | ‚àö(Val Loss) = 1.81552541 | Current Learning Rate: 0.002\n",
      "Epoch 374/1000 | Train Loss=56000.65952621 | Val Loss=3.26424741 | Data=559.86072762 | Physics=13.34975340 | Val RMSE: 2.54919672 | ‚àö(Val Loss) = 1.80672288 | Current Learning Rate: 0.002\n",
      "Epoch 375/1000 | Train Loss=55894.01071069 | Val Loss=3.44200231 | Data=558.80091809 | Physics=13.03801113 | Val RMSE: 2.54198909 | ‚àö(Val Loss) = 1.85526335 | Current Learning Rate: 0.002\n",
      "Epoch 376/1000 | Train Loss=55372.79674899 | Val Loss=3.23642449 | Data=553.58777446 | Physics=13.25945221 | Val RMSE: 2.53068805 | ‚àö(Val Loss) = 1.79900646 | Current Learning Rate: 0.002\n",
      "Epoch 377/1000 | Train Loss=55256.77091734 | Val Loss=3.28832625 | Data=552.43125276 | Physics=12.99406697 | Val RMSE: 2.51226997 | ‚àö(Val Loss) = 1.81337428 | Current Learning Rate: 0.002\n",
      "Epoch 378/1000 | Train Loss=57376.21811996 | Val Loss=3.49289304 | Data=573.59083803 | Physics=14.31405888 | Val RMSE: 2.51383257 | ‚àö(Val Loss) = 1.86892831 | Current Learning Rate: 0.002\n",
      "Epoch 379/1000 | Train Loss=55354.27016129 | Val Loss=3.27654466 | Data=553.40318052 | Physics=13.76362657 | Val RMSE: 2.54140377 | ‚àö(Val Loss) = 1.81012285 | Current Learning Rate: 0.002\n",
      "Epoch 380/1000 | Train Loss=55699.77079133 | Val Loss=3.53320851 | Data=556.85105453 | Physics=13.35554057 | Val RMSE: 2.54816771 | ‚àö(Val Loss) = 1.87968314 | Current Learning Rate: 0.002\n",
      "Epoch 381/1000 | Train Loss=55677.86353327 | Val Loss=3.23634450 | Data=556.63178475 | Physics=13.59794682 | Val RMSE: 2.52151585 | ‚àö(Val Loss) = 1.79898429 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.1180941   -3.6829984  -11.196554  ]\n",
      " [  1.0993893   -3.6778367  -11.566442  ]\n",
      " [  1.118237    -3.6830301  -11.193651  ]\n",
      " ...\n",
      " [  0.9749709   -3.6435103  -13.795204  ]\n",
      " [  0.9749625   -3.6435022  -13.795361  ]\n",
      " [  0.97495484  -3.6434953  -13.7955    ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9749448   -3.6434848  -13.795674  ]\n",
      " [  0.97493625  -3.6434767  -13.795833  ]\n",
      " [  0.9749286   -3.6434696  -13.795972  ]\n",
      " ...\n",
      " [  0.97765696  -3.6460319  -13.746776  ]\n",
      " [  0.9776525   -3.646028   -13.746857  ]\n",
      " [  0.97764844  -3.6460247  -13.746929  ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9776414   -3.6460173  -13.74705   ]\n",
      " [  0.9776355   -3.6460116  -13.747163  ]\n",
      " [  0.9776332   -3.6460109  -13.747211  ]\n",
      " ...\n",
      " [  0.97776175  -3.646128   -13.744884  ]\n",
      " [  0.9777594   -3.6461263  -13.7449255 ]\n",
      " [  0.97775716  -3.6461246  -13.744964  ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9777519   -3.6461189  -13.745053  ]\n",
      " [  0.9777475   -3.6461143  -13.745138  ]\n",
      " [  0.97774005  -3.646106   -13.745272  ]\n",
      " ...\n",
      " [  0.97716755  -3.6455872  -13.755602  ]\n",
      " [  0.977163    -3.6455834  -13.755683  ]\n",
      " [  0.97715896  -3.64558    -13.755754  ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.97715515  -3.645577   -13.755822  ]\n",
      " [  0.9771569   -3.645581   -13.755797  ]\n",
      " [  0.9771496   -3.6455727  -13.755906  ]\n",
      " ...\n",
      " [  0.9759204   -3.6444237  -13.778087  ]\n",
      " [  0.97591394  -3.644418   -13.778205  ]\n",
      " [  0.97590476  -3.6444085  -13.778365  ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9758969   -3.6444008  -13.778512  ]\n",
      " [  0.97588974  -3.6443942  -13.778644  ]\n",
      " [  0.9758799   -3.6443841  -13.778815  ]\n",
      " ...\n",
      " [  0.9678632   -3.6359556  -13.923168  ]\n",
      " [  0.9678529   -3.6359437  -13.923348  ]\n",
      " [  0.9678446   -3.6359346  -13.923502  ]] \n",
      "\n",
      "Final Test RMSE:  1.3545585771401722\n",
      "Epoch 382/1000 | Train Loss=55261.62487399 | Val Loss=3.25938559 | Data=552.47615691 | Physics=13.12805995 | Val RMSE: 2.55005503 | ‚àö(Val Loss) = 1.80537689 | Current Learning Rate: 0.002\n",
      "Epoch 383/1000 | Train Loss=56079.47933468 | Val Loss=3.93255230 | Data=560.64867967 | Physics=16.98314046 | Val RMSE: 2.52808452 | ‚àö(Val Loss) = 1.98306644 | Current Learning Rate: 0.002\n",
      "Epoch 384/1000 | Train Loss=56043.06703629 | Val Loss=3.24233878 | Data=560.28976342 | Physics=13.74315644 | Val RMSE: 2.53360343 | ‚àö(Val Loss) = 1.80064952 | Current Learning Rate: 0.002\n",
      "Epoch 385/1000 | Train Loss=56334.05103327 | Val Loss=4.20819877 | Data=563.19347751 | Physics=13.81134574 | Val RMSE: 2.55962372 | ‚àö(Val Loss) = 2.05138946 | Current Learning Rate: 0.002\n",
      "Epoch 386/1000 | Train Loss=56026.87512601 | Val Loss=3.37857065 | Data=560.11259805 | Physics=13.31211880 | Val RMSE: 2.54630280 | ‚àö(Val Loss) = 1.83808887 | Current Learning Rate: 0.002\n",
      "Epoch 387/1000 | Train Loss=56228.83014113 | Val Loss=3.41148537 | Data=562.14815004 | Physics=13.55113846 | Val RMSE: 2.54811859 | ‚àö(Val Loss) = 1.84702075 | Current Learning Rate: 0.002\n",
      "Epoch 388/1000 | Train Loss=55389.33833165 | Val Loss=3.65795335 | Data=553.75458748 | Physics=13.32436931 | Val RMSE: 2.55033922 | ‚àö(Val Loss) = 1.91257763 | Current Learning Rate: 0.002\n",
      "Epoch 389/1000 | Train Loss=58118.85219254 | Val Loss=79.73696021 | Data=581.01883033 | Physics=12.88887532 | Val RMSE: 2.68574023 | ‚àö(Val Loss) = 8.92955589 | Current Learning Rate: 0.002\n",
      "Epoch 390/1000 | Train Loss=58398.29448085 | Val Loss=3.31476709 | Data=583.79039346 | Physics=13.50290075 | Val RMSE: 2.54455638 | ‚àö(Val Loss) = 1.82065022 | Current Learning Rate: 0.002\n",
      "Epoch 391/1000 | Train Loss=55321.58064516 | Val Loss=3.44229582 | Data=553.07770957 | Physics=13.59488586 | Val RMSE: 2.53827667 | ‚àö(Val Loss) = 1.85534251 | Current Learning Rate: 0.002\n",
      "Epoch 392/1000 | Train Loss=55647.52356351 | Val Loss=3.39112650 | Data=556.33592766 | Physics=14.21382647 | Val RMSE: 2.55333281 | ‚àö(Val Loss) = 1.84150112 | Current Learning Rate: 0.002\n",
      "Epoch 393/1000 | Train Loss=56414.03893649 | Val Loss=3.98791107 | Data=563.99952550 | Physics=17.11499423 | Val RMSE: 2.50426364 | ‚àö(Val Loss) = 1.99697542 | Current Learning Rate: 0.002\n",
      "Epoch 394/1000 | Train Loss=55288.22353831 | Val Loss=3.31748440 | Data=552.74280868 | Physics=13.50439963 | Val RMSE: 2.55433559 | ‚àö(Val Loss) = 1.82139623 | Current Learning Rate: 0.002\n",
      "Epoch 395/1000 | Train Loss=55901.92212702 | Val Loss=3.67328508 | Data=558.87274170 | Physics=13.00187585 | Val RMSE: 2.54566121 | ‚àö(Val Loss) = 1.91658163 | Current Learning Rate: 0.002\n",
      "Epoch 396/1000 | Train Loss=55302.73223286 | Val Loss=3.24645008 | Data=552.88106808 | Physics=12.91462424 | Val RMSE: 2.52163219 | ‚àö(Val Loss) = 1.80179083 | Current Learning Rate: 0.002\n",
      "Epoch 397/1000 | Train Loss=56448.43813004 | Val Loss=3.58430885 | Data=564.33321258 | Physics=13.02821030 | Val RMSE: 2.55653763 | ‚àö(Val Loss) = 1.89322710 | Current Learning Rate: 0.002\n",
      "Epoch 398/1000 | Train Loss=55522.20110887 | Val Loss=4.30902507 | Data=555.08086174 | Physics=13.35945774 | Val RMSE: 2.53650355 | ‚àö(Val Loss) = 2.07581925 | Current Learning Rate: 0.002\n",
      "Epoch 399/1000 | Train Loss=55245.45665323 | Val Loss=3.69944973 | Data=552.31607548 | Physics=13.14543485 | Val RMSE: 2.53793645 | ‚àö(Val Loss) = 1.92339540 | Current Learning Rate: 0.002\n",
      "Epoch 400/1000 | Train Loss=55496.59564012 | Val Loss=3.46710121 | Data=554.82283660 | Physics=13.75137206 | Val RMSE: 2.52922010 | ‚àö(Val Loss) = 1.86201537 | Current Learning Rate: 0.0002\n",
      "Epoch 401/1000 | Train Loss=55248.03364415 | Val Loss=3.15041453 | Data=552.33188949 | Physics=13.59725976 | Val RMSE: 2.56176949 | ‚àö(Val Loss) = 1.77494073 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.1180941   -3.6829984  -11.196554  ]\n",
      " [  1.0993893   -3.6778367  -11.566442  ]\n",
      " [  1.118237    -3.6830301  -11.193651  ]\n",
      " ...\n",
      " [  0.9749709   -3.6435103  -13.795204  ]\n",
      " [  0.9749625   -3.6435022  -13.795361  ]\n",
      " [  0.97495484  -3.6434953  -13.7955    ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9749448   -3.6434848  -13.795674  ]\n",
      " [  0.97493625  -3.6434767  -13.795833  ]\n",
      " [  0.9749286   -3.6434696  -13.795972  ]\n",
      " ...\n",
      " [  0.97765696  -3.6460319  -13.746776  ]\n",
      " [  0.9776525   -3.646028   -13.746857  ]\n",
      " [  0.97764844  -3.6460247  -13.746929  ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9776414   -3.6460173  -13.74705   ]\n",
      " [  0.9776355   -3.6460116  -13.747163  ]\n",
      " [  0.9776332   -3.6460109  -13.747211  ]\n",
      " ...\n",
      " [  0.97776175  -3.646128   -13.744884  ]\n",
      " [  0.9777594   -3.6461263  -13.7449255 ]\n",
      " [  0.97775716  -3.6461246  -13.744964  ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9777519   -3.6461189  -13.745053  ]\n",
      " [  0.9777475   -3.6461143  -13.745138  ]\n",
      " [  0.97774005  -3.646106   -13.745272  ]\n",
      " ...\n",
      " [  0.97716755  -3.6455872  -13.755602  ]\n",
      " [  0.977163    -3.6455834  -13.755683  ]\n",
      " [  0.97715896  -3.64558    -13.755754  ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.97715515  -3.645577   -13.755822  ]\n",
      " [  0.9771569   -3.645581   -13.755797  ]\n",
      " [  0.9771496   -3.6455727  -13.755906  ]\n",
      " ...\n",
      " [  0.9759204   -3.6444237  -13.778087  ]\n",
      " [  0.97591394  -3.644418   -13.778205  ]\n",
      " [  0.97590476  -3.6444085  -13.778365  ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9758969   -3.6444008  -13.778512  ]\n",
      " [  0.97588974  -3.6443942  -13.778644  ]\n",
      " [  0.9758799   -3.6443841  -13.778815  ]\n",
      " ...\n",
      " [  0.9678632   -3.6359556  -13.923168  ]\n",
      " [  0.9678529   -3.6359437  -13.923348  ]\n",
      " [  0.9678446   -3.6359346  -13.923502  ]] \n",
      "\n",
      "Final Test RMSE:  1.3545585771401722\n",
      "‚úÖ Saved best model at epoch 402 (Val Loss = 2.77160141)\n",
      "Epoch 402/1000 | Train Loss=1601.40050482 | Val Loss=2.77160141 | Data=15.86870941 | Physics=14.50475759 | Val RMSE: 2.58288670 | ‚àö(Val Loss) = 1.66481268 | Current Learning Rate: 0.0002\n",
      "Epoch 403/1000 | Train Loss=1589.57078503 | Val Loss=2.77405179 | Data=15.75229839 | Physics=14.38134407 | Val RMSE: 2.57044029 | ‚àö(Val Loss) = 1.66554856 | Current Learning Rate: 0.0002\n",
      "Epoch 404/1000 | Train Loss=1589.96026021 | Val Loss=2.77812115 | Data=15.75632643 | Physics=14.42036918 | Val RMSE: 2.57134390 | ‚àö(Val Loss) = 1.66676974 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 405 (Val Loss = 2.76828981)\n",
      "Epoch 405/1000 | Train Loss=1586.50130340 | Val Loss=2.76828981 | Data=15.72173817 | Physics=14.11112143 | Val RMSE: 2.56899905 | ‚àö(Val Loss) = 1.66381788 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 406 (Val Loss = 2.74813120)\n",
      "Epoch 406/1000 | Train Loss=1577.89774839 | Val Loss=2.74813120 | Data=15.63610551 | Physics=14.76236710 | Val RMSE: 2.56088209 | ‚àö(Val Loss) = 1.65774882 | Current Learning Rate: 0.0002\n",
      "Epoch 407/1000 | Train Loss=1577.08699676 | Val Loss=2.75819372 | Data=15.62784103 | Physics=14.36760127 | Val RMSE: 2.55713010 | ‚àö(Val Loss) = 1.66078103 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 408 (Val Loss = 2.74265395)\n",
      "Epoch 408/1000 | Train Loss=1576.54276399 | Val Loss=2.74265395 | Data=15.62243283 | Physics=14.39989611 | Val RMSE: 2.55723810 | ‚àö(Val Loss) = 1.65609598 | Current Learning Rate: 0.0002\n",
      "Epoch 409/1000 | Train Loss=1577.10134592 | Val Loss=2.77323689 | Data=15.62820361 | Physics=14.25990823 | Val RMSE: 2.55802488 | ‚àö(Val Loss) = 1.66530383 | Current Learning Rate: 0.0002\n",
      "Epoch 410/1000 | Train Loss=1574.90662409 | Val Loss=2.75901703 | Data=15.60639366 | Physics=14.27607943 | Val RMSE: 2.55665135 | ‚àö(Val Loss) = 1.66102886 | Current Learning Rate: 0.0002\n",
      "Epoch 411/1000 | Train Loss=1574.82747133 | Val Loss=2.76006148 | Data=15.60575070 | Physics=14.27095931 | Val RMSE: 2.55590487 | ‚àö(Val Loss) = 1.66134334 | Current Learning Rate: 0.0002\n",
      "Epoch 412/1000 | Train Loss=1575.01901147 | Val Loss=2.75039625 | Data=15.60749094 | Physics=14.33919534 | Val RMSE: 2.55484557 | ‚àö(Val Loss) = 1.65843189 | Current Learning Rate: 0.0002\n",
      "Epoch 413/1000 | Train Loss=1575.03568785 | Val Loss=2.75935136 | Data=15.60754733 | Physics=14.17840055 | Val RMSE: 2.55593777 | ‚àö(Val Loss) = 1.66112947 | Current Learning Rate: 0.0002\n",
      "Epoch 414/1000 | Train Loss=1574.96498157 | Val Loss=2.75222052 | Data=15.60692089 | Physics=14.49119033 | Val RMSE: 2.55618143 | ‚àö(Val Loss) = 1.65898180 | Current Learning Rate: 0.0002\n",
      "Epoch 415/1000 | Train Loss=1574.60212560 | Val Loss=2.75001832 | Data=15.60336414 | Physics=14.37522240 | Val RMSE: 2.55519724 | ‚àö(Val Loss) = 1.65831792 | Current Learning Rate: 0.0002\n",
      "Epoch 416/1000 | Train Loss=1574.61871929 | Val Loss=2.75680413 | Data=15.60346203 | Physics=13.94726401 | Val RMSE: 2.55595350 | ‚àö(Val Loss) = 1.66036272 | Current Learning Rate: 0.0002\n",
      "Epoch 417/1000 | Train Loss=1574.58523264 | Val Loss=2.76274220 | Data=15.60312951 | Physics=14.07374536 | Val RMSE: 2.55659389 | ‚àö(Val Loss) = 1.66214991 | Current Learning Rate: 0.0002\n",
      "Epoch 418/1000 | Train Loss=1574.60226736 | Val Loss=2.74866813 | Data=15.60328920 | Physics=14.52032317 | Val RMSE: 2.55526900 | ‚àö(Val Loss) = 1.65791082 | Current Learning Rate: 0.0002\n",
      "Epoch 419/1000 | Train Loss=1574.92562374 | Val Loss=2.75422235 | Data=15.60655102 | Physics=14.27624554 | Val RMSE: 2.55578089 | ‚àö(Val Loss) = 1.65958500 | Current Learning Rate: 0.0002\n",
      "Epoch 420/1000 | Train Loss=1575.64606304 | Val Loss=2.75147571 | Data=15.61381205 | Physics=14.41684633 | Val RMSE: 2.55577636 | ‚àö(Val Loss) = 1.65875733 | Current Learning Rate: 0.0002\n",
      "Epoch 421/1000 | Train Loss=1575.65245007 | Val Loss=2.75007199 | Data=15.61390889 | Physics=14.34178366 | Val RMSE: 2.55533361 | ‚àö(Val Loss) = 1.65833414 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.9872616   -3.5763807  -13.227556  ]\n",
      " [  0.98719203  -3.5768306  -13.229032  ]\n",
      " [  0.9872188   -3.5766625  -13.228516  ]\n",
      " ...\n",
      " [  0.97930884  -3.6438274  -13.500626  ]\n",
      " [  0.97930753  -3.6438625  -13.500823  ]\n",
      " [  0.9793063   -3.6438944  -13.501     ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.97930485  -3.6439319  -13.50122   ]\n",
      " [  0.9793036   -3.6439672  -13.501419  ]\n",
      " [  0.97930235  -3.643999   -13.501596  ]\n",
      " ...\n",
      " [  0.9801424   -3.6309493  -13.434217  ]\n",
      " [  0.98014003  -3.6309762  -13.434342  ]\n",
      " [  0.98013777  -3.6310015  -13.434457  ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9801351   -3.6310346  -13.4346285 ]\n",
      " [  0.9801324   -3.6310678  -13.43479   ]\n",
      " [  0.98013014  -3.6310904  -13.434877  ]\n",
      " ...\n",
      " [  0.98019475  -3.6303446  -13.431353  ]\n",
      " [  0.9801931   -3.6303625  -13.431432  ]\n",
      " [  0.98019147  -3.6303794  -13.431507  ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9801895   -3.6304045  -13.431641  ]\n",
      " [  0.9801874   -3.6304302  -13.4317665 ]\n",
      " [  0.98018473  -3.630465   -13.431949  ]\n",
      " ...\n",
      " [  0.97991663  -3.6336844  -13.447344  ]\n",
      " [  0.97991455  -3.63371    -13.447464  ]\n",
      " [  0.97991264  -3.6337335  -13.447574  ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9799108   -3.6337554  -13.447678  ]\n",
      " [  0.9799099   -3.6337616  -13.447675  ]\n",
      " [  0.9799081   -3.633787   -13.447827  ]\n",
      " ...\n",
      " [  0.97950166  -3.639784   -13.478407  ]\n",
      " [  0.9794999   -3.639815   -13.478568  ]\n",
      " [  0.97949785  -3.6398528  -13.478778  ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9794959   -3.6398888  -13.478971  ]\n",
      " [  0.9794939   -3.6399226  -13.479147  ]\n",
      " [  0.9794917   -3.6399627  -13.479369  ]\n",
      " ...\n",
      " [  0.98035336  -3.6639466  -13.649312  ]\n",
      " [  0.98035735  -3.663966   -13.649512  ]\n",
      " [  0.98036134  -3.6639807  -13.649679  ]] \n",
      "\n",
      "Final Test RMSE:  1.3891885826985042\n",
      "Epoch 422/1000 | Train Loss=1576.62382655 | Val Loss=2.75609461 | Data=15.62334731 | Physics=14.35866791 | Val RMSE: 2.55606341 | ‚àö(Val Loss) = 1.66014898 | Current Learning Rate: 0.0002\n",
      "Epoch 423/1000 | Train Loss=1574.98234312 | Val Loss=2.75954619 | Data=15.60686127 | Physics=14.34838265 | Val RMSE: 2.55618739 | ‚àö(Val Loss) = 1.66118824 | Current Learning Rate: 0.0002\n",
      "Epoch 424/1000 | Train Loss=1575.16495637 | Val Loss=2.75378894 | Data=15.60902306 | Physics=14.48633781 | Val RMSE: 2.55527735 | ‚àö(Val Loss) = 1.65945446 | Current Learning Rate: 0.0002\n",
      "Epoch 425/1000 | Train Loss=1574.94498173 | Val Loss=2.74358187 | Data=15.60691252 | Physics=14.05209735 | Val RMSE: 2.55434179 | ‚àö(Val Loss) = 1.65637612 | Current Learning Rate: 0.0002\n",
      "Epoch 426/1000 | Train Loss=1576.03999575 | Val Loss=2.75056146 | Data=15.61739759 | Physics=14.42611308 | Val RMSE: 2.55487823 | ‚àö(Val Loss) = 1.65848172 | Current Learning Rate: 0.0002\n",
      "Epoch 427/1000 | Train Loss=1574.66978012 | Val Loss=2.76271588 | Data=15.60403095 | Physics=14.73523865 | Val RMSE: 2.55648208 | ‚àö(Val Loss) = 1.66214192 | Current Learning Rate: 0.0002\n",
      "Epoch 428/1000 | Train Loss=1574.43703535 | Val Loss=2.75137752 | Data=15.60150777 | Physics=14.05120845 | Val RMSE: 2.55533957 | ‚àö(Val Loss) = 1.65872765 | Current Learning Rate: 0.0002\n",
      "Epoch 429/1000 | Train Loss=1575.53521138 | Val Loss=2.76184278 | Data=15.61250545 | Physics=14.48757949 | Val RMSE: 2.55632186 | ‚àö(Val Loss) = 1.66187930 | Current Learning Rate: 0.0002\n",
      "Epoch 430/1000 | Train Loss=1574.77858020 | Val Loss=2.75664651 | Data=15.60520898 | Physics=14.31203835 | Val RMSE: 2.55649352 | ‚àö(Val Loss) = 1.66031528 | Current Learning Rate: 0.0002\n",
      "Epoch 431/1000 | Train Loss=1575.63074518 | Val Loss=2.75557527 | Data=15.61363469 | Physics=14.25174541 | Val RMSE: 2.55630398 | ‚àö(Val Loss) = 1.65999258 | Current Learning Rate: 0.0002\n",
      "Epoch 432/1000 | Train Loss=1575.85338174 | Val Loss=2.75226254 | Data=15.61563726 | Physics=14.35274239 | Val RMSE: 2.55557537 | ‚àö(Val Loss) = 1.65899444 | Current Learning Rate: 0.0002\n",
      "Epoch 433/1000 | Train Loss=1574.60148768 | Val Loss=2.75337361 | Data=15.60332600 | Physics=14.44911194 | Val RMSE: 2.55573988 | ‚àö(Val Loss) = 1.65932930 | Current Learning Rate: 0.0002\n",
      "Epoch 434/1000 | Train Loss=1577.04780431 | Val Loss=2.75463961 | Data=15.62765069 | Physics=13.98308207 | Val RMSE: 2.55623007 | ‚àö(Val Loss) = 1.65971065 | Current Learning Rate: 0.0002\n",
      "Epoch 435/1000 | Train Loss=1575.50035834 | Val Loss=2.75136719 | Data=15.61229051 | Physics=14.35653006 | Val RMSE: 2.55536079 | ‚àö(Val Loss) = 1.65872455 | Current Learning Rate: 0.0002\n",
      "Epoch 436/1000 | Train Loss=1575.33126733 | Val Loss=2.74982965 | Data=15.61050421 | Physics=14.16931037 | Val RMSE: 2.55495000 | ‚àö(Val Loss) = 1.65826106 | Current Learning Rate: 0.0002\n",
      "Epoch 437/1000 | Train Loss=1574.63456480 | Val Loss=2.75190284 | Data=15.60349864 | Physics=14.69377191 | Val RMSE: 2.55516815 | ‚àö(Val Loss) = 1.65888608 | Current Learning Rate: 0.0002\n",
      "Epoch 438/1000 | Train Loss=1575.20431420 | Val Loss=2.75116890 | Data=15.60917328 | Physics=14.32476854 | Val RMSE: 2.55561328 | ‚àö(Val Loss) = 1.65866482 | Current Learning Rate: 0.0002\n",
      "Epoch 439/1000 | Train Loss=1574.66147146 | Val Loss=2.76100724 | Data=15.60394798 | Physics=14.37071013 | Val RMSE: 2.55601764 | ‚àö(Val Loss) = 1.66162789 | Current Learning Rate: 0.0002\n",
      "Epoch 440/1000 | Train Loss=1574.38942792 | Val Loss=2.75531321 | Data=15.60120370 | Physics=14.29833330 | Val RMSE: 2.55586314 | ‚àö(Val Loss) = 1.65991354 | Current Learning Rate: 0.0002\n",
      "Epoch 441/1000 | Train Loss=1575.50531990 | Val Loss=2.75763405 | Data=15.61235219 | Physics=14.11807895 | Val RMSE: 2.55660987 | ‚àö(Val Loss) = 1.66061258 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.9872616   -3.5763807  -13.227556  ]\n",
      " [  0.98719203  -3.5768306  -13.229032  ]\n",
      " [  0.9872188   -3.5766625  -13.228516  ]\n",
      " ...\n",
      " [  0.97930884  -3.6438274  -13.500626  ]\n",
      " [  0.97930753  -3.6438625  -13.500823  ]\n",
      " [  0.9793063   -3.6438944  -13.501     ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.97930485  -3.6439319  -13.50122   ]\n",
      " [  0.9793036   -3.6439672  -13.501419  ]\n",
      " [  0.97930235  -3.643999   -13.501596  ]\n",
      " ...\n",
      " [  0.9801424   -3.6309493  -13.434217  ]\n",
      " [  0.98014003  -3.6309762  -13.434342  ]\n",
      " [  0.98013777  -3.6310015  -13.434457  ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9801351   -3.6310346  -13.4346285 ]\n",
      " [  0.9801324   -3.6310678  -13.43479   ]\n",
      " [  0.98013014  -3.6310904  -13.434877  ]\n",
      " ...\n",
      " [  0.98019475  -3.6303446  -13.431353  ]\n",
      " [  0.9801931   -3.6303625  -13.431432  ]\n",
      " [  0.98019147  -3.6303794  -13.431507  ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9801895   -3.6304045  -13.431641  ]\n",
      " [  0.9801874   -3.6304302  -13.4317665 ]\n",
      " [  0.98018473  -3.630465   -13.431949  ]\n",
      " ...\n",
      " [  0.97991663  -3.6336844  -13.447344  ]\n",
      " [  0.97991455  -3.63371    -13.447464  ]\n",
      " [  0.97991264  -3.6337335  -13.447574  ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9799108   -3.6337554  -13.447678  ]\n",
      " [  0.9799099   -3.6337616  -13.447675  ]\n",
      " [  0.9799081   -3.633787   -13.447827  ]\n",
      " ...\n",
      " [  0.97950166  -3.639784   -13.478407  ]\n",
      " [  0.9794999   -3.639815   -13.478568  ]\n",
      " [  0.97949785  -3.6398528  -13.478778  ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9794959   -3.6398888  -13.478971  ]\n",
      " [  0.9794939   -3.6399226  -13.479147  ]\n",
      " [  0.9794917   -3.6399627  -13.479369  ]\n",
      " ...\n",
      " [  0.98035336  -3.6639466  -13.649312  ]\n",
      " [  0.98035735  -3.663966   -13.649512  ]\n",
      " [  0.98036134  -3.6639807  -13.649679  ]] \n",
      "\n",
      "Final Test RMSE:  1.3891885826985042\n",
      "Epoch 442/1000 | Train Loss=1576.21105169 | Val Loss=2.76190987 | Data=15.61923852 | Physics=13.69377014 | Val RMSE: 2.55697250 | ‚àö(Val Loss) = 1.66189945 | Current Learning Rate: 0.0002\n",
      "Epoch 443/1000 | Train Loss=1575.09147004 | Val Loss=2.75117241 | Data=15.60828590 | Physics=14.32437102 | Val RMSE: 2.55499363 | ‚àö(Val Loss) = 1.65866578 | Current Learning Rate: 0.0002\n",
      "Epoch 444/1000 | Train Loss=1575.14433830 | Val Loss=2.75577853 | Data=15.60882824 | Physics=14.26432666 | Val RMSE: 2.55565357 | ‚àö(Val Loss) = 1.66005373 | Current Learning Rate: 0.0002\n",
      "Epoch 445/1000 | Train Loss=1577.40120180 | Val Loss=2.76196614 | Data=15.63092773 | Physics=13.94166657 | Val RMSE: 2.55774093 | ‚àö(Val Loss) = 1.66191638 | Current Learning Rate: 0.0002\n",
      "Epoch 446/1000 | Train Loss=1575.13684476 | Val Loss=2.75911772 | Data=15.60855773 | Physics=14.25306302 | Val RMSE: 2.55615735 | ‚àö(Val Loss) = 1.66105914 | Current Learning Rate: 0.0002\n",
      "Epoch 447/1000 | Train Loss=1576.07693186 | Val Loss=2.75099111 | Data=15.61807171 | Physics=14.00653883 | Val RMSE: 2.55451107 | ‚àö(Val Loss) = 1.65861118 | Current Learning Rate: 0.0002\n",
      "Epoch 448/1000 | Train Loss=1575.01712922 | Val Loss=2.75178473 | Data=15.60739259 | Physics=13.81319815 | Val RMSE: 2.55501318 | ‚àö(Val Loss) = 1.65885043 | Current Learning Rate: 0.0002\n",
      "Epoch 449/1000 | Train Loss=1574.63196195 | Val Loss=2.76128574 | Data=15.60366840 | Physics=14.42360111 | Val RMSE: 2.55648327 | ‚àö(Val Loss) = 1.66171169 | Current Learning Rate: 0.0002\n",
      "Epoch 450/1000 | Train Loss=1575.51650706 | Val Loss=2.76435212 | Data=15.61245361 | Physics=14.09942513 | Val RMSE: 2.55650330 | ‚àö(Val Loss) = 1.66263413 | Current Learning Rate: 0.0002\n",
      "Epoch 451/1000 | Train Loss=1577.72293583 | Val Loss=2.74488462 | Data=15.63433103 | Physics=14.25388281 | Val RMSE: 2.55410552 | ‚àö(Val Loss) = 1.65676939 | Current Learning Rate: 0.0002\n",
      "Epoch 452/1000 | Train Loss=1575.00226421 | Val Loss=2.75336149 | Data=15.60731002 | Physics=14.56107228 | Val RMSE: 2.55499434 | ‚àö(Val Loss) = 1.65932560 | Current Learning Rate: 0.0002\n",
      "Epoch 453/1000 | Train Loss=1576.56831606 | Val Loss=2.74721541 | Data=15.62287315 | Physics=14.25225966 | Val RMSE: 2.55413675 | ‚àö(Val Loss) = 1.65747261 | Current Learning Rate: 0.0002\n",
      "Epoch 454/1000 | Train Loss=1575.39132198 | Val Loss=2.75669172 | Data=15.61120661 | Physics=14.37474353 | Val RMSE: 2.55588317 | ‚àö(Val Loss) = 1.66032875 | Current Learning Rate: 0.0002\n",
      "Epoch 455/1000 | Train Loss=1574.49751528 | Val Loss=2.75359903 | Data=15.60236217 | Physics=14.52039874 | Val RMSE: 2.55580878 | ‚àö(Val Loss) = 1.65939713 | Current Learning Rate: 0.0002\n",
      "Epoch 456/1000 | Train Loss=1577.77100791 | Val Loss=2.76986078 | Data=15.63473520 | Physics=14.41855796 | Val RMSE: 2.55686903 | ‚àö(Val Loss) = 1.66428983 | Current Learning Rate: 0.0002\n",
      "Epoch 457/1000 | Train Loss=1577.64762632 | Val Loss=2.75352609 | Data=15.63370123 | Physics=14.34209957 | Val RMSE: 2.55452681 | ‚àö(Val Loss) = 1.65937519 | Current Learning Rate: 0.0002\n",
      "Epoch 458/1000 | Train Loss=1574.65224137 | Val Loss=2.75563048 | Data=15.60378315 | Physics=13.97806639 | Val RMSE: 2.55560493 | ‚àö(Val Loss) = 1.66000915 | Current Learning Rate: 0.0002\n",
      "Epoch 459/1000 | Train Loss=1574.48771421 | Val Loss=2.75143863 | Data=15.60212311 | Physics=14.19795734 | Val RMSE: 2.55484843 | ‚àö(Val Loss) = 1.65874612 | Current Learning Rate: 0.0002\n",
      "Epoch 460/1000 | Train Loss=1574.53836158 | Val Loss=2.75242925 | Data=15.60256521 | Physics=14.37360923 | Val RMSE: 2.55496716 | ‚àö(Val Loss) = 1.65904462 | Current Learning Rate: 0.0002\n",
      "Epoch 461/1000 | Train Loss=1574.50133490 | Val Loss=2.75512905 | Data=15.60219660 | Physics=14.53931913 | Val RMSE: 2.55538321 | ‚àö(Val Loss) = 1.65985811 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.9872616   -3.5763807  -13.227556  ]\n",
      " [  0.98719203  -3.5768306  -13.229032  ]\n",
      " [  0.9872188   -3.5766625  -13.228516  ]\n",
      " ...\n",
      " [  0.97930884  -3.6438274  -13.500626  ]\n",
      " [  0.97930753  -3.6438625  -13.500823  ]\n",
      " [  0.9793063   -3.6438944  -13.501     ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.97930485  -3.6439319  -13.50122   ]\n",
      " [  0.9793036   -3.6439672  -13.501419  ]\n",
      " [  0.97930235  -3.643999   -13.501596  ]\n",
      " ...\n",
      " [  0.9801424   -3.6309493  -13.434217  ]\n",
      " [  0.98014003  -3.6309762  -13.434342  ]\n",
      " [  0.98013777  -3.6310015  -13.434457  ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9801351   -3.6310346  -13.4346285 ]\n",
      " [  0.9801324   -3.6310678  -13.43479   ]\n",
      " [  0.98013014  -3.6310904  -13.434877  ]\n",
      " ...\n",
      " [  0.98019475  -3.6303446  -13.431353  ]\n",
      " [  0.9801931   -3.6303625  -13.431432  ]\n",
      " [  0.98019147  -3.6303794  -13.431507  ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9801895   -3.6304045  -13.431641  ]\n",
      " [  0.9801874   -3.6304302  -13.4317665 ]\n",
      " [  0.98018473  -3.630465   -13.431949  ]\n",
      " ...\n",
      " [  0.97991663  -3.6336844  -13.447344  ]\n",
      " [  0.97991455  -3.63371    -13.447464  ]\n",
      " [  0.97991264  -3.6337335  -13.447574  ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9799108   -3.6337554  -13.447678  ]\n",
      " [  0.9799099   -3.6337616  -13.447675  ]\n",
      " [  0.9799081   -3.633787   -13.447827  ]\n",
      " ...\n",
      " [  0.97950166  -3.639784   -13.478407  ]\n",
      " [  0.9794999   -3.639815   -13.478568  ]\n",
      " [  0.97949785  -3.6398528  -13.478778  ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9794959   -3.6398888  -13.478971  ]\n",
      " [  0.9794939   -3.6399226  -13.479147  ]\n",
      " [  0.9794917   -3.6399627  -13.479369  ]\n",
      " ...\n",
      " [  0.98035336  -3.6639466  -13.649312  ]\n",
      " [  0.98035735  -3.663966   -13.649512  ]\n",
      " [  0.98036134  -3.6639807  -13.649679  ]] \n",
      "\n",
      "Final Test RMSE:  1.3891885826985042\n",
      "Epoch 462/1000 | Train Loss=1575.45753528 | Val Loss=2.74889715 | Data=15.61183410 | Physics=14.32967621 | Val RMSE: 2.55466580 | ‚àö(Val Loss) = 1.65797985 | Current Learning Rate: 0.0002\n",
      "Epoch 463/1000 | Train Loss=1574.33081448 | Val Loss=2.75824804 | Data=15.60070284 | Physics=14.27407288 | Val RMSE: 2.55589271 | ‚àö(Val Loss) = 1.66079748 | Current Learning Rate: 0.0002\n",
      "Epoch 464/1000 | Train Loss=1575.75699345 | Val Loss=2.75037467 | Data=15.61477172 | Physics=14.06904024 | Val RMSE: 2.55474710 | ‚àö(Val Loss) = 1.65842533 | Current Learning Rate: 0.0002\n",
      "Epoch 465/1000 | Train Loss=1574.17797064 | Val Loss=2.75594131 | Data=15.59914490 | Physics=14.10325274 | Val RMSE: 2.55556941 | ‚àö(Val Loss) = 1.66010284 | Current Learning Rate: 0.0002\n",
      "Epoch 466/1000 | Train Loss=1579.25562311 | Val Loss=2.75540411 | Data=15.64953561 | Physics=14.42392192 | Val RMSE: 2.55586171 | ‚àö(Val Loss) = 1.65994096 | Current Learning Rate: 0.0002\n",
      "Epoch 467/1000 | Train Loss=1575.88179262 | Val Loss=2.74398158 | Data=15.61621869 | Physics=14.10004246 | Val RMSE: 2.55341840 | ‚àö(Val Loss) = 1.65649676 | Current Learning Rate: 0.0002\n",
      "Epoch 468/1000 | Train Loss=1576.32076140 | Val Loss=2.74449129 | Data=15.62030832 | Physics=13.94837684 | Val RMSE: 2.55368662 | ‚àö(Val Loss) = 1.65665066 | Current Learning Rate: 0.0002\n",
      "Epoch 469/1000 | Train Loss=1576.23486722 | Val Loss=2.74982838 | Data=15.61950265 | Physics=14.75457847 | Val RMSE: 2.55644107 | ‚àö(Val Loss) = 1.65826058 | Current Learning Rate: 0.0002\n",
      "Epoch 470/1000 | Train Loss=1574.17015814 | Val Loss=2.75008660 | Data=15.59894556 | Physics=14.30111254 | Val RMSE: 2.55506659 | ‚àö(Val Loss) = 1.65833855 | Current Learning Rate: 0.0002\n",
      "Epoch 471/1000 | Train Loss=1574.10005828 | Val Loss=2.75345816 | Data=15.59821941 | Physics=14.82900307 | Val RMSE: 2.55537271 | ‚àö(Val Loss) = 1.65935481 | Current Learning Rate: 0.0002\n",
      "Epoch 472/1000 | Train Loss=1575.35637443 | Val Loss=2.75585020 | Data=15.61059463 | Physics=14.30700773 | Val RMSE: 2.55595851 | ‚àö(Val Loss) = 1.66007543 | Current Learning Rate: 0.0002\n",
      "Epoch 473/1000 | Train Loss=1577.92048891 | Val Loss=2.75284217 | Data=15.63636906 | Physics=14.33255439 | Val RMSE: 2.55582166 | ‚àö(Val Loss) = 1.65916908 | Current Learning Rate: 0.0002\n",
      "Epoch 474/1000 | Train Loss=1574.99693643 | Val Loss=2.74561046 | Data=15.60730525 | Physics=14.09616682 | Val RMSE: 2.55419898 | ‚àö(Val Loss) = 1.65698838 | Current Learning Rate: 0.0002\n",
      "Epoch 475/1000 | Train Loss=1574.68102634 | Val Loss=2.75009652 | Data=15.60384784 | Physics=14.13339250 | Val RMSE: 2.55555582 | ‚àö(Val Loss) = 1.65834153 | Current Learning Rate: 0.0002\n",
      "Epoch 476/1000 | Train Loss=1578.32559696 | Val Loss=2.76279222 | Data=15.64043839 | Physics=14.39070182 | Val RMSE: 2.55718756 | ‚àö(Val Loss) = 1.66216493 | Current Learning Rate: 0.0002\n",
      "Epoch 477/1000 | Train Loss=1574.80860556 | Val Loss=2.76743854 | Data=15.60520609 | Physics=14.14819859 | Val RMSE: 2.55672359 | ‚àö(Val Loss) = 1.66356206 | Current Learning Rate: 0.0002\n",
      "Epoch 478/1000 | Train Loss=1574.30878670 | Val Loss=2.75829763 | Data=15.60045928 | Physics=14.48013587 | Val RMSE: 2.55471873 | ‚àö(Val Loss) = 1.66081238 | Current Learning Rate: 0.0002\n",
      "Epoch 479/1000 | Train Loss=1574.59499433 | Val Loss=2.75734018 | Data=15.60348603 | Physics=14.01231615 | Val RMSE: 2.55600023 | ‚àö(Val Loss) = 1.66052413 | Current Learning Rate: 0.0002\n",
      "Epoch 480/1000 | Train Loss=1574.74109674 | Val Loss=2.74985537 | Data=15.60470809 | Physics=14.49207562 | Val RMSE: 2.55413914 | ‚àö(Val Loss) = 1.65826881 | Current Learning Rate: 0.0002\n",
      "Epoch 481/1000 | Train Loss=1574.32486060 | Val Loss=2.76384837 | Data=15.60044876 | Physics=14.45087525 | Val RMSE: 2.55624557 | ‚àö(Val Loss) = 1.66248262 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.9872616   -3.5763807  -13.227556  ]\n",
      " [  0.98719203  -3.5768306  -13.229032  ]\n",
      " [  0.9872188   -3.5766625  -13.228516  ]\n",
      " ...\n",
      " [  0.97930884  -3.6438274  -13.500626  ]\n",
      " [  0.97930753  -3.6438625  -13.500823  ]\n",
      " [  0.9793063   -3.6438944  -13.501     ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.97930485  -3.6439319  -13.50122   ]\n",
      " [  0.9793036   -3.6439672  -13.501419  ]\n",
      " [  0.97930235  -3.643999   -13.501596  ]\n",
      " ...\n",
      " [  0.9801424   -3.6309493  -13.434217  ]\n",
      " [  0.98014003  -3.6309762  -13.434342  ]\n",
      " [  0.98013777  -3.6310015  -13.434457  ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9801351   -3.6310346  -13.4346285 ]\n",
      " [  0.9801324   -3.6310678  -13.43479   ]\n",
      " [  0.98013014  -3.6310904  -13.434877  ]\n",
      " ...\n",
      " [  0.98019475  -3.6303446  -13.431353  ]\n",
      " [  0.9801931   -3.6303625  -13.431432  ]\n",
      " [  0.98019147  -3.6303794  -13.431507  ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9801895   -3.6304045  -13.431641  ]\n",
      " [  0.9801874   -3.6304302  -13.4317665 ]\n",
      " [  0.98018473  -3.630465   -13.431949  ]\n",
      " ...\n",
      " [  0.97991663  -3.6336844  -13.447344  ]\n",
      " [  0.97991455  -3.63371    -13.447464  ]\n",
      " [  0.97991264  -3.6337335  -13.447574  ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9799108   -3.6337554  -13.447678  ]\n",
      " [  0.9799099   -3.6337616  -13.447675  ]\n",
      " [  0.9799081   -3.633787   -13.447827  ]\n",
      " ...\n",
      " [  0.97950166  -3.639784   -13.478407  ]\n",
      " [  0.9794999   -3.639815   -13.478568  ]\n",
      " [  0.97949785  -3.6398528  -13.478778  ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9794959   -3.6398888  -13.478971  ]\n",
      " [  0.9794939   -3.6399226  -13.479147  ]\n",
      " [  0.9794917   -3.6399627  -13.479369  ]\n",
      " ...\n",
      " [  0.98035336  -3.6639466  -13.649312  ]\n",
      " [  0.98035735  -3.663966   -13.649512  ]\n",
      " [  0.98036134  -3.6639807  -13.649679  ]] \n",
      "\n",
      "Final Test RMSE:  1.3891885826985042\n",
      "Epoch 482/1000 | Train Loss=1577.39663007 | Val Loss=2.75025220 | Data=15.63113603 | Physics=14.10274983 | Val RMSE: 2.55429626 | ‚àö(Val Loss) = 1.65838850 | Current Learning Rate: 0.0002\n",
      "Epoch 483/1000 | Train Loss=1575.36408455 | Val Loss=2.74838103 | Data=15.61082643 | Physics=14.15956870 | Val RMSE: 2.55393219 | ‚àö(Val Loss) = 1.65782416 | Current Learning Rate: 0.0002\n",
      "Epoch 484/1000 | Train Loss=1575.35993416 | Val Loss=2.75841490 | Data=15.61073740 | Physics=14.31613897 | Val RMSE: 2.55612946 | ‚àö(Val Loss) = 1.66084766 | Current Learning Rate: 0.0002\n",
      "Epoch 485/1000 | Train Loss=1575.05635711 | Val Loss=2.76960785 | Data=15.60784041 | Physics=14.26516142 | Val RMSE: 2.55727386 | ‚àö(Val Loss) = 1.66421390 | Current Learning Rate: 0.0002\n",
      "Epoch 486/1000 | Train Loss=1575.11419481 | Val Loss=2.74988175 | Data=15.60842849 | Physics=14.93986136 | Val RMSE: 2.55468678 | ‚àö(Val Loss) = 1.65827680 | Current Learning Rate: 0.0002\n",
      "Epoch 487/1000 | Train Loss=1574.96093750 | Val Loss=2.74720537 | Data=15.60692252 | Physics=14.26073251 | Val RMSE: 2.55404449 | ‚àö(Val Loss) = 1.65746951 | Current Learning Rate: 0.0002\n",
      "Epoch 488/1000 | Train Loss=1578.54489037 | Val Loss=2.75613804 | Data=15.64252835 | Physics=13.65880499 | Val RMSE: 2.55615497 | ‚àö(Val Loss) = 1.66016209 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 489 (Val Loss = 2.73531222)\n",
      "Epoch 489/1000 | Train Loss=1576.07310437 | Val Loss=2.73531222 | Data=15.61820953 | Physics=14.20852491 | Val RMSE: 2.55181360 | ‚àö(Val Loss) = 1.65387797 | Current Learning Rate: 0.0002\n",
      "Epoch 490/1000 | Train Loss=1574.34675057 | Val Loss=2.75520686 | Data=15.60073379 | Physics=14.44978181 | Val RMSE: 2.55230117 | ‚àö(Val Loss) = 1.65988159 | Current Learning Rate: 0.0002\n",
      "Epoch 491/1000 | Train Loss=1572.97091183 | Val Loss=2.75173200 | Data=15.58729292 | Physics=14.13279930 | Val RMSE: 2.55183363 | ‚àö(Val Loss) = 1.65883458 | Current Learning Rate: 0.0002\n",
      "Epoch 492/1000 | Train Loss=1574.10017247 | Val Loss=2.75941268 | Data=15.59860922 | Physics=14.18969849 | Val RMSE: 2.55345416 | ‚àö(Val Loss) = 1.66114807 | Current Learning Rate: 0.0002\n",
      "Epoch 493/1000 | Train Loss=1573.58898532 | Val Loss=2.75438817 | Data=15.59328067 | Physics=14.26272674 | Val RMSE: 2.55218196 | ‚àö(Val Loss) = 1.65963495 | Current Learning Rate: 0.0002\n",
      "Epoch 494/1000 | Train Loss=1573.79030289 | Val Loss=2.75082463 | Data=15.59549039 | Physics=13.91857992 | Val RMSE: 2.55186009 | ‚àö(Val Loss) = 1.65856099 | Current Learning Rate: 0.0002\n",
      "Epoch 495/1000 | Train Loss=1574.33255497 | Val Loss=2.74924741 | Data=15.60081260 | Physics=13.52186266 | Val RMSE: 2.55236292 | ‚àö(Val Loss) = 1.65808547 | Current Learning Rate: 0.0002\n",
      "Epoch 496/1000 | Train Loss=1573.97387302 | Val Loss=2.76155139 | Data=15.59705030 | Physics=14.26998996 | Val RMSE: 2.55395579 | ‚àö(Val Loss) = 1.66179156 | Current Learning Rate: 0.0002\n",
      "Epoch 497/1000 | Train Loss=1575.29892263 | Val Loss=2.75208110 | Data=15.61036987 | Physics=14.57969450 | Val RMSE: 2.55247474 | ‚àö(Val Loss) = 1.65893972 | Current Learning Rate: 0.0002\n",
      "Epoch 498/1000 | Train Loss=1573.56622118 | Val Loss=2.75054611 | Data=15.59315122 | Physics=13.80307429 | Val RMSE: 2.55196929 | ‚àö(Val Loss) = 1.65847707 | Current Learning Rate: 0.0002\n",
      "Epoch 499/1000 | Train Loss=1575.56019642 | Val Loss=2.74797201 | Data=15.61302656 | Physics=14.20875870 | Val RMSE: 2.55189967 | ‚àö(Val Loss) = 1.65770078 | Current Learning Rate: 0.0002\n",
      "Epoch 500/1000 | Train Loss=1573.81547300 | Val Loss=2.74850013 | Data=15.59575447 | Physics=14.23154334 | Val RMSE: 2.55191278 | ‚àö(Val Loss) = 1.65786004 | Current Learning Rate: 0.0002\n",
      "Epoch 501/1000 | Train Loss=1572.87713820 | Val Loss=2.75147748 | Data=15.58621200 | Physics=14.10132170 | Val RMSE: 2.55187130 | ‚àö(Val Loss) = 1.65875781 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99160266  -3.5620732  -13.1507635 ]\n",
      " [  0.99150854  -3.5626504  -13.152554  ]\n",
      " [  0.99154377  -3.5624392  -13.1519375 ]\n",
      " ...\n",
      " [  0.98063016  -3.6406848  -13.464569  ]\n",
      " [  0.9806281   -3.6407166  -13.464769  ]\n",
      " [  0.98062605  -3.6407454  -13.464949  ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9806237   -3.6407795  -13.465172  ]\n",
      " [  0.98062164  -3.6408117  -13.465373  ]\n",
      " [  0.98061967  -3.6408405  -13.465553  ]\n",
      " ...\n",
      " [  0.9817578   -3.628535   -13.396777  ]\n",
      " [  0.98175484  -3.6285613  -13.396906  ]\n",
      " [  0.981752    -3.628586   -13.397024  ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9817486  -3.6286182 -13.397201 ]\n",
      " [  0.9817451  -3.628651  -13.397366 ]\n",
      " [  0.9817424  -3.628673  -13.397457 ]\n",
      " ...\n",
      " [  0.9818238  -3.627943  -13.393821 ]\n",
      " [  0.9818217  -3.6279607 -13.393903 ]\n",
      " [  0.9818197  -3.6279771 -13.393981 ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9818171  -3.628002  -13.394118 ]\n",
      " [  0.9818145  -3.6280272 -13.394248 ]\n",
      " [  0.9818111  -3.6280613 -13.394435 ]\n",
      " ...\n",
      " [  0.98147    -3.6311882 -13.410276 ]\n",
      " [  0.9814673  -3.6312127 -13.4104   ]\n",
      " [  0.9814648  -3.6312351 -13.410513 ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9814625   -3.6312563  -13.41062   ]\n",
      " [  0.98146135  -3.6312618  -13.410618  ]\n",
      " [  0.9814589   -3.6312861  -13.410773  ]\n",
      " ...\n",
      " [  0.98091674  -3.6369667  -13.442009  ]\n",
      " [  0.9809143   -3.6369956  -13.442173  ]\n",
      " [  0.9809114   -3.6370306  -13.442387  ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.98090863  -3.6370645  -13.442583  ]\n",
      " [  0.9809059   -3.637096   -13.442762  ]\n",
      " [  0.9809028   -3.6371334  -13.442989  ]\n",
      " ...\n",
      " [  0.9811178   -3.656939   -13.614251  ]\n",
      " [  0.9811211   -3.6569517  -13.614452  ]\n",
      " [  0.9811246   -3.6569598  -13.614619  ]] \n",
      "\n",
      "Final Test RMSE:  1.395874798297882\n",
      "Epoch 502/1000 | Train Loss=1574.94352476 | Val Loss=2.75459910 | Data=15.60685462 | Physics=14.15431679 | Val RMSE: 2.55355644 | ‚àö(Val Loss) = 1.65969849 | Current Learning Rate: 0.0002\n",
      "Epoch 503/1000 | Train Loss=1574.07185216 | Val Loss=2.75188267 | Data=15.59818059 | Physics=14.23190764 | Val RMSE: 2.55250931 | ‚àö(Val Loss) = 1.65887988 | Current Learning Rate: 0.0002\n",
      "Epoch 504/1000 | Train Loss=1581.58350397 | Val Loss=2.75915459 | Data=15.67292478 | Physics=14.01060315 | Val RMSE: 2.55342412 | ‚àö(Val Loss) = 1.66107035 | Current Learning Rate: 0.0002\n",
      "Epoch 505/1000 | Train Loss=1576.08560673 | Val Loss=2.75279519 | Data=15.61825654 | Physics=13.91555345 | Val RMSE: 2.55202818 | ‚àö(Val Loss) = 1.65915501 | Current Learning Rate: 0.0002\n",
      "Epoch 506/1000 | Train Loss=1580.50573730 | Val Loss=2.74961075 | Data=15.66208310 | Physics=14.45494169 | Val RMSE: 2.55232763 | ‚àö(Val Loss) = 1.65819502 | Current Learning Rate: 0.0002\n",
      "Epoch 507/1000 | Train Loss=1577.92661212 | Val Loss=2.75057039 | Data=15.63639155 | Physics=14.77882910 | Val RMSE: 2.55216265 | ‚àö(Val Loss) = 1.65848434 | Current Learning Rate: 0.0002\n",
      "Epoch 508/1000 | Train Loss=1576.13397020 | Val Loss=2.75474076 | Data=15.61870080 | Physics=14.13230687 | Val RMSE: 2.55355835 | ‚àö(Val Loss) = 1.65974116 | Current Learning Rate: 0.0002\n",
      "Epoch 509/1000 | Train Loss=1573.92838804 | Val Loss=2.74915371 | Data=15.59668762 | Physics=13.94097817 | Val RMSE: 2.55219817 | ‚àö(Val Loss) = 1.65805721 | Current Learning Rate: 0.0002\n",
      "Epoch 510/1000 | Train Loss=1575.56055869 | Val Loss=2.75783990 | Data=15.61296358 | Physics=14.25693576 | Val RMSE: 2.55348778 | ‚àö(Val Loss) = 1.66067457 | Current Learning Rate: 0.0002\n",
      "Epoch 511/1000 | Train Loss=1578.59255686 | Val Loss=2.75194562 | Data=15.64305333 | Physics=13.66555314 | Val RMSE: 2.55236053 | ‚àö(Val Loss) = 1.65889895 | Current Learning Rate: 0.0002\n",
      "Epoch 512/1000 | Train Loss=1575.87959929 | Val Loss=2.74950268 | Data=15.61608250 | Physics=14.44889708 | Val RMSE: 2.55177784 | ‚àö(Val Loss) = 1.65816247 | Current Learning Rate: 0.0002\n",
      "Epoch 513/1000 | Train Loss=1573.79502426 | Val Loss=2.75542869 | Data=15.59555112 | Physics=14.30760384 | Val RMSE: 2.55327797 | ‚àö(Val Loss) = 1.65994847 | Current Learning Rate: 0.0002\n",
      "Epoch 514/1000 | Train Loss=1577.75709189 | Val Loss=2.75412673 | Data=15.63497063 | Physics=14.14528881 | Val RMSE: 2.55272079 | ‚àö(Val Loss) = 1.65955615 | Current Learning Rate: 0.0002\n",
      "Epoch 515/1000 | Train Loss=1576.22835811 | Val Loss=2.74019597 | Data=15.61961728 | Physics=14.28250411 | Val RMSE: 2.55035901 | ‚àö(Val Loss) = 1.65535378 | Current Learning Rate: 0.0002\n",
      "Epoch 516/1000 | Train Loss=1574.88803396 | Val Loss=2.75407275 | Data=15.60630020 | Physics=14.50842457 | Val RMSE: 2.55274582 | ‚àö(Val Loss) = 1.65953994 | Current Learning Rate: 0.0002\n",
      "Epoch 517/1000 | Train Loss=1574.30039929 | Val Loss=2.75108933 | Data=15.60041283 | Physics=14.36517619 | Val RMSE: 2.55205226 | ‚àö(Val Loss) = 1.65864086 | Current Learning Rate: 0.0002\n",
      "Epoch 518/1000 | Train Loss=1575.28108461 | Val Loss=2.75727454 | Data=15.61031449 | Physics=14.05223874 | Val RMSE: 2.55362797 | ‚àö(Val Loss) = 1.66050434 | Current Learning Rate: 0.0002\n",
      "Epoch 519/1000 | Train Loss=1573.79416189 | Val Loss=2.74921948 | Data=15.59546415 | Physics=14.41814226 | Val RMSE: 2.55136013 | ‚àö(Val Loss) = 1.65807700 | Current Learning Rate: 0.0002\n",
      "Epoch 520/1000 | Train Loss=1575.25839923 | Val Loss=2.75030260 | Data=15.61006728 | Physics=13.73163063 | Val RMSE: 2.55175900 | ‚àö(Val Loss) = 1.65840364 | Current Learning Rate: 0.0002\n",
      "Epoch 521/1000 | Train Loss=1575.87346821 | Val Loss=2.74606559 | Data=15.61609763 | Physics=14.00571144 | Val RMSE: 2.55136991 | ‚àö(Val Loss) = 1.65712571 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99160266  -3.5620732  -13.1507635 ]\n",
      " [  0.99150854  -3.5626504  -13.152554  ]\n",
      " [  0.99154377  -3.5624392  -13.1519375 ]\n",
      " ...\n",
      " [  0.98063016  -3.6406848  -13.464569  ]\n",
      " [  0.9806281   -3.6407166  -13.464769  ]\n",
      " [  0.98062605  -3.6407454  -13.464949  ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9806237   -3.6407795  -13.465172  ]\n",
      " [  0.98062164  -3.6408117  -13.465373  ]\n",
      " [  0.98061967  -3.6408405  -13.465553  ]\n",
      " ...\n",
      " [  0.9817578   -3.628535   -13.396777  ]\n",
      " [  0.98175484  -3.6285613  -13.396906  ]\n",
      " [  0.981752    -3.628586   -13.397024  ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9817486  -3.6286182 -13.397201 ]\n",
      " [  0.9817451  -3.628651  -13.397366 ]\n",
      " [  0.9817424  -3.628673  -13.397457 ]\n",
      " ...\n",
      " [  0.9818238  -3.627943  -13.393821 ]\n",
      " [  0.9818217  -3.6279607 -13.393903 ]\n",
      " [  0.9818197  -3.6279771 -13.393981 ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9818171  -3.628002  -13.394118 ]\n",
      " [  0.9818145  -3.6280272 -13.394248 ]\n",
      " [  0.9818111  -3.6280613 -13.394435 ]\n",
      " ...\n",
      " [  0.98147    -3.6311882 -13.410276 ]\n",
      " [  0.9814673  -3.6312127 -13.4104   ]\n",
      " [  0.9814648  -3.6312351 -13.410513 ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9814625   -3.6312563  -13.41062   ]\n",
      " [  0.98146135  -3.6312618  -13.410618  ]\n",
      " [  0.9814589   -3.6312861  -13.410773  ]\n",
      " ...\n",
      " [  0.98091674  -3.6369667  -13.442009  ]\n",
      " [  0.9809143   -3.6369956  -13.442173  ]\n",
      " [  0.9809114   -3.6370306  -13.442387  ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.98090863  -3.6370645  -13.442583  ]\n",
      " [  0.9809059   -3.637096   -13.442762  ]\n",
      " [  0.9809028   -3.6371334  -13.442989  ]\n",
      " ...\n",
      " [  0.9811178   -3.656939   -13.614251  ]\n",
      " [  0.9811211   -3.6569517  -13.614452  ]\n",
      " [  0.9811246   -3.6569598  -13.614619  ]] \n",
      "\n",
      "Final Test RMSE:  1.395874798297882\n",
      "Epoch 522/1000 | Train Loss=1573.81654013 | Val Loss=2.75327740 | Data=15.59563268 | Physics=14.18355010 | Val RMSE: 2.55321693 | ‚àö(Val Loss) = 1.65930021 | Current Learning Rate: 0.0002\n",
      "Epoch 523/1000 | Train Loss=1573.82480548 | Val Loss=2.75072071 | Data=15.59577622 | Physics=13.70021929 | Val RMSE: 2.55147409 | ‚àö(Val Loss) = 1.65852964 | Current Learning Rate: 0.0002\n",
      "Epoch 524/1000 | Train Loss=1574.07669560 | Val Loss=2.74558270 | Data=15.59827119 | Physics=13.89684909 | Val RMSE: 2.55115795 | ‚àö(Val Loss) = 1.65697992 | Current Learning Rate: 0.0002\n",
      "Epoch 525/1000 | Train Loss=1573.70507025 | Val Loss=2.75408397 | Data=15.59450137 | Physics=14.02582991 | Val RMSE: 2.55228877 | ‚àö(Val Loss) = 1.65954328 | Current Learning Rate: 0.0002\n",
      "Epoch 526/1000 | Train Loss=1575.50633191 | Val Loss=2.74707047 | Data=15.61243251 | Physics=14.07228461 | Val RMSE: 2.55132675 | ‚àö(Val Loss) = 1.65742886 | Current Learning Rate: 0.0002\n",
      "Epoch 527/1000 | Train Loss=1576.09290338 | Val Loss=2.75271684 | Data=15.61841297 | Physics=13.88086264 | Val RMSE: 2.55272388 | ‚àö(Val Loss) = 1.65913129 | Current Learning Rate: 0.0002\n",
      "Epoch 528/1000 | Train Loss=1576.07791630 | Val Loss=2.75941025 | Data=15.61825029 | Physics=14.40295547 | Val RMSE: 2.55327988 | ‚àö(Val Loss) = 1.66114724 | Current Learning Rate: 0.0002\n",
      "Epoch 529/1000 | Train Loss=1574.89624811 | Val Loss=2.75726433 | Data=15.60643713 | Physics=14.61383678 | Val RMSE: 2.55322695 | ‚àö(Val Loss) = 1.66050124 | Current Learning Rate: 0.0002\n",
      "Epoch 530/1000 | Train Loss=1574.66694887 | Val Loss=2.75152653 | Data=15.60405371 | Physics=14.06618512 | Val RMSE: 2.55315733 | ‚àö(Val Loss) = 1.65877259 | Current Learning Rate: 0.0002\n",
      "Epoch 531/1000 | Train Loss=1575.01334110 | Val Loss=2.75240983 | Data=15.60757348 | Physics=14.48771195 | Val RMSE: 2.55197144 | ‚àö(Val Loss) = 1.65903890 | Current Learning Rate: 0.0002\n",
      "Epoch 532/1000 | Train Loss=1576.10683909 | Val Loss=2.74875785 | Data=15.61843623 | Physics=14.19768558 | Val RMSE: 2.55178070 | ‚àö(Val Loss) = 1.65793788 | Current Learning Rate: 0.0002\n",
      "Epoch 533/1000 | Train Loss=1574.59346254 | Val Loss=2.74936428 | Data=15.60352719 | Physics=14.35912170 | Val RMSE: 2.55150890 | ‚àö(Val Loss) = 1.65812075 | Current Learning Rate: 0.0002\n",
      "Epoch 534/1000 | Train Loss=1575.02213017 | Val Loss=2.74753461 | Data=15.60744962 | Physics=14.26410825 | Val RMSE: 2.55152059 | ‚àö(Val Loss) = 1.65756881 | Current Learning Rate: 0.0002\n",
      "Epoch 535/1000 | Train Loss=1577.31739463 | Val Loss=2.75761038 | Data=15.63057340 | Physics=14.53184286 | Val RMSE: 2.55151153 | ‚àö(Val Loss) = 1.66060543 | Current Learning Rate: 0.0002\n",
      "Epoch 536/1000 | Train Loss=1574.07758947 | Val Loss=2.76030498 | Data=15.59818591 | Physics=13.93466921 | Val RMSE: 2.55385709 | ‚àö(Val Loss) = 1.66141653 | Current Learning Rate: 0.0002\n",
      "Epoch 537/1000 | Train Loss=1573.89927624 | Val Loss=2.74578329 | Data=15.59653116 | Physics=14.83777334 | Val RMSE: 2.55104971 | ‚àö(Val Loss) = 1.65704048 | Current Learning Rate: 0.0002\n",
      "Epoch 538/1000 | Train Loss=1574.97624748 | Val Loss=2.74728617 | Data=15.60715131 | Physics=14.20556729 | Val RMSE: 2.55162215 | ‚àö(Val Loss) = 1.65749395 | Current Learning Rate: 0.0002\n",
      "Epoch 539/1000 | Train Loss=1574.10824487 | Val Loss=2.74505136 | Data=15.59855261 | Physics=14.51721323 | Val RMSE: 2.55057526 | ‚àö(Val Loss) = 1.65681970 | Current Learning Rate: 0.0002\n",
      "Epoch 540/1000 | Train Loss=1575.02713505 | Val Loss=2.75011175 | Data=15.60777329 | Physics=13.94098223 | Val RMSE: 2.55218005 | ‚àö(Val Loss) = 1.65834606 | Current Learning Rate: 0.0002\n",
      "Epoch 541/1000 | Train Loss=1574.38554924 | Val Loss=2.74860600 | Data=15.60129612 | Physics=14.03228777 | Val RMSE: 2.55164719 | ‚àö(Val Loss) = 1.65789199 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99160266  -3.5620732  -13.1507635 ]\n",
      " [  0.99150854  -3.5626504  -13.152554  ]\n",
      " [  0.99154377  -3.5624392  -13.1519375 ]\n",
      " ...\n",
      " [  0.98063016  -3.6406848  -13.464569  ]\n",
      " [  0.9806281   -3.6407166  -13.464769  ]\n",
      " [  0.98062605  -3.6407454  -13.464949  ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9806237   -3.6407795  -13.465172  ]\n",
      " [  0.98062164  -3.6408117  -13.465373  ]\n",
      " [  0.98061967  -3.6408405  -13.465553  ]\n",
      " ...\n",
      " [  0.9817578   -3.628535   -13.396777  ]\n",
      " [  0.98175484  -3.6285613  -13.396906  ]\n",
      " [  0.981752    -3.628586   -13.397024  ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9817486  -3.6286182 -13.397201 ]\n",
      " [  0.9817451  -3.628651  -13.397366 ]\n",
      " [  0.9817424  -3.628673  -13.397457 ]\n",
      " ...\n",
      " [  0.9818238  -3.627943  -13.393821 ]\n",
      " [  0.9818217  -3.6279607 -13.393903 ]\n",
      " [  0.9818197  -3.6279771 -13.393981 ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9818171  -3.628002  -13.394118 ]\n",
      " [  0.9818145  -3.6280272 -13.394248 ]\n",
      " [  0.9818111  -3.6280613 -13.394435 ]\n",
      " ...\n",
      " [  0.98147    -3.6311882 -13.410276 ]\n",
      " [  0.9814673  -3.6312127 -13.4104   ]\n",
      " [  0.9814648  -3.6312351 -13.410513 ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9814625   -3.6312563  -13.41062   ]\n",
      " [  0.98146135  -3.6312618  -13.410618  ]\n",
      " [  0.9814589   -3.6312861  -13.410773  ]\n",
      " ...\n",
      " [  0.98091674  -3.6369667  -13.442009  ]\n",
      " [  0.9809143   -3.6369956  -13.442173  ]\n",
      " [  0.9809114   -3.6370306  -13.442387  ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.98090863  -3.6370645  -13.442583  ]\n",
      " [  0.9809059   -3.637096   -13.442762  ]\n",
      " [  0.9809028   -3.6371334  -13.442989  ]\n",
      " ...\n",
      " [  0.9811178   -3.656939   -13.614251  ]\n",
      " [  0.9811211   -3.6569517  -13.614452  ]\n",
      " [  0.9811246   -3.6569598  -13.614619  ]] \n",
      "\n",
      "Final Test RMSE:  1.395874798297882\n",
      "Epoch 542/1000 | Train Loss=1573.77370920 | Val Loss=2.74824709 | Data=15.59523287 | Physics=14.40370091 | Val RMSE: 2.55147028 | ‚àö(Val Loss) = 1.65778375 | Current Learning Rate: 0.0002\n",
      "Epoch 543/1000 | Train Loss=1573.93360163 | Val Loss=2.75116892 | Data=15.59686695 | Physics=13.33255560 | Val RMSE: 2.55236053 | ‚àö(Val Loss) = 1.65866482 | Current Learning Rate: 0.0002\n",
      "Epoch 544/1000 | Train Loss=1573.45284148 | Val Loss=2.75985757 | Data=15.59196312 | Physics=14.06726878 | Val RMSE: 2.55288625 | ‚àö(Val Loss) = 1.66128194 | Current Learning Rate: 0.0002\n",
      "Epoch 545/1000 | Train Loss=1576.91479098 | Val Loss=2.75215676 | Data=15.62655470 | Physics=14.16301690 | Val RMSE: 2.55298734 | ‚àö(Val Loss) = 1.65896249 | Current Learning Rate: 0.0002\n",
      "Epoch 546/1000 | Train Loss=1573.77687516 | Val Loss=2.74753089 | Data=15.59529351 | Physics=14.27833281 | Val RMSE: 2.55195427 | ‚àö(Val Loss) = 1.65756774 | Current Learning Rate: 0.0002\n",
      "Epoch 547/1000 | Train Loss=1574.83609107 | Val Loss=2.76036439 | Data=15.60594100 | Physics=14.16574624 | Val RMSE: 2.55342221 | ‚àö(Val Loss) = 1.66143441 | Current Learning Rate: 0.0002\n",
      "Epoch 548/1000 | Train Loss=1573.85953251 | Val Loss=2.75119821 | Data=15.59600713 | Physics=14.07093257 | Val RMSE: 2.55210185 | ‚àö(Val Loss) = 1.65867364 | Current Learning Rate: 0.0002\n",
      "Epoch 549/1000 | Train Loss=1575.54578424 | Val Loss=2.75415719 | Data=15.61281952 | Physics=14.52757661 | Val RMSE: 2.55285120 | ‚àö(Val Loss) = 1.65956533 | Current Learning Rate: 0.0002\n",
      "Epoch 550/1000 | Train Loss=1577.41105012 | Val Loss=2.74616984 | Data=15.63160010 | Physics=14.68531404 | Val RMSE: 2.54953480 | ‚àö(Val Loss) = 1.65715718 | Current Learning Rate: 0.0002\n",
      "Epoch 551/1000 | Train Loss=1574.86766791 | Val Loss=2.75900911 | Data=15.60610184 | Physics=14.19657279 | Val RMSE: 2.55307555 | ‚àö(Val Loss) = 1.66102648 | Current Learning Rate: 0.0002\n",
      "Epoch 552/1000 | Train Loss=1573.41640546 | Val Loss=2.75309641 | Data=15.59165450 | Physics=14.76192176 | Val RMSE: 2.55253649 | ‚àö(Val Loss) = 1.65924573 | Current Learning Rate: 0.0002\n",
      "Epoch 553/1000 | Train Loss=1573.96235903 | Val Loss=2.74503653 | Data=15.59711136 | Physics=13.92149319 | Val RMSE: 2.54929328 | ‚àö(Val Loss) = 1.65681517 | Current Learning Rate: 0.0002\n",
      "Epoch 554/1000 | Train Loss=1576.05645949 | Val Loss=2.75725578 | Data=15.61803756 | Physics=14.42826564 | Val RMSE: 2.55345488 | ‚àö(Val Loss) = 1.66049862 | Current Learning Rate: 0.0002\n",
      "Epoch 555/1000 | Train Loss=1581.56195265 | Val Loss=2.75888585 | Data=15.67244201 | Physics=13.81176999 | Val RMSE: 2.55308580 | ‚àö(Val Loss) = 1.66098940 | Current Learning Rate: 0.0002\n",
      "Epoch 556/1000 | Train Loss=1573.94437925 | Val Loss=2.75150698 | Data=15.59684950 | Physics=14.19102967 | Val RMSE: 2.55236244 | ‚àö(Val Loss) = 1.65876675 | Current Learning Rate: 0.0002\n",
      "Epoch 557/1000 | Train Loss=1573.96621015 | Val Loss=2.74861268 | Data=15.59723731 | Physics=13.93565806 | Val RMSE: 2.55109358 | ‚àö(Val Loss) = 1.65789402 | Current Learning Rate: 0.0002\n",
      "Epoch 558/1000 | Train Loss=1574.49567241 | Val Loss=2.74989262 | Data=15.60244720 | Physics=14.62527572 | Val RMSE: 2.55225062 | ‚àö(Val Loss) = 1.65828001 | Current Learning Rate: 0.0002\n",
      "Epoch 559/1000 | Train Loss=1573.60757939 | Val Loss=2.75940579 | Data=15.59354945 | Physics=14.27692452 | Val RMSE: 2.55352783 | ‚àö(Val Loss) = 1.66114593 | Current Learning Rate: 0.0002\n",
      "Epoch 560/1000 | Train Loss=1574.98099247 | Val Loss=2.76115478 | Data=15.60715211 | Physics=13.88207708 | Val RMSE: 2.55326629 | ‚àö(Val Loss) = 1.66167235 | Current Learning Rate: 0.0002\n",
      "Epoch 561/1000 | Train Loss=1574.08926096 | Val Loss=2.74998794 | Data=15.59858479 | Physics=14.19431809 | Val RMSE: 2.55213714 | ‚àö(Val Loss) = 1.65830874 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99160266  -3.5620732  -13.1507635 ]\n",
      " [  0.99150854  -3.5626504  -13.152554  ]\n",
      " [  0.99154377  -3.5624392  -13.1519375 ]\n",
      " ...\n",
      " [  0.98063016  -3.6406848  -13.464569  ]\n",
      " [  0.9806281   -3.6407166  -13.464769  ]\n",
      " [  0.98062605  -3.6407454  -13.464949  ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9806237   -3.6407795  -13.465172  ]\n",
      " [  0.98062164  -3.6408117  -13.465373  ]\n",
      " [  0.98061967  -3.6408405  -13.465553  ]\n",
      " ...\n",
      " [  0.9817578   -3.628535   -13.396777  ]\n",
      " [  0.98175484  -3.6285613  -13.396906  ]\n",
      " [  0.981752    -3.628586   -13.397024  ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9817486  -3.6286182 -13.397201 ]\n",
      " [  0.9817451  -3.628651  -13.397366 ]\n",
      " [  0.9817424  -3.628673  -13.397457 ]\n",
      " ...\n",
      " [  0.9818238  -3.627943  -13.393821 ]\n",
      " [  0.9818217  -3.6279607 -13.393903 ]\n",
      " [  0.9818197  -3.6279771 -13.393981 ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9818171  -3.628002  -13.394118 ]\n",
      " [  0.9818145  -3.6280272 -13.394248 ]\n",
      " [  0.9818111  -3.6280613 -13.394435 ]\n",
      " ...\n",
      " [  0.98147    -3.6311882 -13.410276 ]\n",
      " [  0.9814673  -3.6312127 -13.4104   ]\n",
      " [  0.9814648  -3.6312351 -13.410513 ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9814625   -3.6312563  -13.41062   ]\n",
      " [  0.98146135  -3.6312618  -13.410618  ]\n",
      " [  0.9814589   -3.6312861  -13.410773  ]\n",
      " ...\n",
      " [  0.98091674  -3.6369667  -13.442009  ]\n",
      " [  0.9809143   -3.6369956  -13.442173  ]\n",
      " [  0.9809114   -3.6370306  -13.442387  ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.98090863  -3.6370645  -13.442583  ]\n",
      " [  0.9809059   -3.637096   -13.442762  ]\n",
      " [  0.9809028   -3.6371334  -13.442989  ]\n",
      " ...\n",
      " [  0.9811178   -3.656939   -13.614251  ]\n",
      " [  0.9811211   -3.6569517  -13.614452  ]\n",
      " [  0.9811246   -3.6569598  -13.614619  ]] \n",
      "\n",
      "Final Test RMSE:  1.395874798297882\n",
      "Epoch 562/1000 | Train Loss=1579.03679042 | Val Loss=2.75049352 | Data=15.64737274 | Physics=14.35583508 | Val RMSE: 2.55138850 | ‚àö(Val Loss) = 1.65846121 | Current Learning Rate: 0.0002\n",
      "Epoch 563/1000 | Train Loss=1574.78224231 | Val Loss=2.76068300 | Data=15.60528321 | Physics=13.63653330 | Val RMSE: 2.55458164 | ‚àö(Val Loss) = 1.66153038 | Current Learning Rate: 0.0002\n",
      "Epoch 564/1000 | Train Loss=1573.64481083 | Val Loss=2.76009524 | Data=15.59388773 | Physics=14.37615189 | Val RMSE: 2.55366755 | ‚àö(Val Loss) = 1.66135335 | Current Learning Rate: 0.0002\n",
      "Epoch 565/1000 | Train Loss=1575.65098523 | Val Loss=2.75085531 | Data=15.61391698 | Physics=13.66992868 | Val RMSE: 2.55204391 | ‚àö(Val Loss) = 1.65857017 | Current Learning Rate: 0.0002\n",
      "Epoch 566/1000 | Train Loss=1578.80870401 | Val Loss=2.74887065 | Data=15.64525995 | Physics=14.34966092 | Val RMSE: 2.55138874 | ‚àö(Val Loss) = 1.65797186 | Current Learning Rate: 0.0002\n",
      "Epoch 567/1000 | Train Loss=1574.67952999 | Val Loss=2.74636139 | Data=15.60433631 | Physics=14.60733124 | Val RMSE: 2.55042529 | ‚àö(Val Loss) = 1.65721500 | Current Learning Rate: 0.0002\n",
      "Epoch 568/1000 | Train Loss=1574.44714749 | Val Loss=2.75386930 | Data=15.60208601 | Physics=14.42481722 | Val RMSE: 2.55194187 | ‚àö(Val Loss) = 1.65947866 | Current Learning Rate: 0.0002\n",
      "Epoch 569/1000 | Train Loss=1573.64601184 | Val Loss=2.74352677 | Data=15.59386475 | Physics=14.31780088 | Val RMSE: 2.55129027 | ‚àö(Val Loss) = 1.65635943 | Current Learning Rate: 0.0002\n",
      "Epoch 570/1000 | Train Loss=1576.65409605 | Val Loss=2.76529526 | Data=15.62373174 | Physics=14.22604079 | Val RMSE: 2.55457306 | ‚àö(Val Loss) = 1.66291773 | Current Learning Rate: 0.0002\n",
      "Epoch 571/1000 | Train Loss=1574.09899902 | Val Loss=2.75309906 | Data=15.59858073 | Physics=14.27927970 | Val RMSE: 2.55316496 | ‚àö(Val Loss) = 1.65924656 | Current Learning Rate: 0.0002\n",
      "Epoch 572/1000 | Train Loss=1575.63119408 | Val Loss=2.75375229 | Data=15.61364045 | Physics=14.43117796 | Val RMSE: 2.55078483 | ‚àö(Val Loss) = 1.65944338 | Current Learning Rate: 0.0002\n",
      "Epoch 573/1000 | Train Loss=1573.72875189 | Val Loss=2.75630412 | Data=15.59482150 | Physics=14.42376672 | Val RMSE: 2.55359507 | ‚àö(Val Loss) = 1.66021204 | Current Learning Rate: 0.0002\n",
      "Epoch 574/1000 | Train Loss=1574.76105327 | Val Loss=2.75328818 | Data=15.60498678 | Physics=14.48898193 | Val RMSE: 2.55280757 | ‚àö(Val Loss) = 1.65930355 | Current Learning Rate: 0.0002\n",
      "Epoch 575/1000 | Train Loss=1574.24682617 | Val Loss=2.75255440 | Data=15.59995673 | Physics=14.23066272 | Val RMSE: 2.55231929 | ‚àö(Val Loss) = 1.65908241 | Current Learning Rate: 0.0002\n",
      "Epoch 576/1000 | Train Loss=1573.90039062 | Val Loss=2.75552181 | Data=15.59651526 | Physics=14.84457278 | Val RMSE: 2.55230284 | ‚àö(Val Loss) = 1.65997648 | Current Learning Rate: 0.0002\n",
      "Epoch 577/1000 | Train Loss=1574.05059618 | Val Loss=2.75092184 | Data=15.59795712 | Physics=14.12506679 | Val RMSE: 2.55259085 | ‚àö(Val Loss) = 1.65859032 | Current Learning Rate: 0.0002\n",
      "Epoch 578/1000 | Train Loss=1574.73948620 | Val Loss=2.75838361 | Data=15.60485729 | Physics=14.10676091 | Val RMSE: 2.55342650 | ‚àö(Val Loss) = 1.66083825 | Current Learning Rate: 0.0002\n",
      "Epoch 579/1000 | Train Loss=1573.48330787 | Val Loss=2.75817885 | Data=15.59238837 | Physics=14.39572775 | Val RMSE: 2.55389905 | ‚àö(Val Loss) = 1.66077662 | Current Learning Rate: 0.0002\n",
      "Epoch 580/1000 | Train Loss=1575.30995621 | Val Loss=2.75981092 | Data=15.61044284 | Physics=13.75215716 | Val RMSE: 2.55463982 | ‚àö(Val Loss) = 1.66126788 | Current Learning Rate: 0.0002\n",
      "Epoch 581/1000 | Train Loss=1573.54440997 | Val Loss=2.74518092 | Data=15.59289339 | Physics=14.31436529 | Val RMSE: 2.55071759 | ‚àö(Val Loss) = 1.65685868 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99160266  -3.5620732  -13.1507635 ]\n",
      " [  0.99150854  -3.5626504  -13.152554  ]\n",
      " [  0.99154377  -3.5624392  -13.1519375 ]\n",
      " ...\n",
      " [  0.98063016  -3.6406848  -13.464569  ]\n",
      " [  0.9806281   -3.6407166  -13.464769  ]\n",
      " [  0.98062605  -3.6407454  -13.464949  ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9806237   -3.6407795  -13.465172  ]\n",
      " [  0.98062164  -3.6408117  -13.465373  ]\n",
      " [  0.98061967  -3.6408405  -13.465553  ]\n",
      " ...\n",
      " [  0.9817578   -3.628535   -13.396777  ]\n",
      " [  0.98175484  -3.6285613  -13.396906  ]\n",
      " [  0.981752    -3.628586   -13.397024  ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9817486  -3.6286182 -13.397201 ]\n",
      " [  0.9817451  -3.628651  -13.397366 ]\n",
      " [  0.9817424  -3.628673  -13.397457 ]\n",
      " ...\n",
      " [  0.9818238  -3.627943  -13.393821 ]\n",
      " [  0.9818217  -3.6279607 -13.393903 ]\n",
      " [  0.9818197  -3.6279771 -13.393981 ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9818171  -3.628002  -13.394118 ]\n",
      " [  0.9818145  -3.6280272 -13.394248 ]\n",
      " [  0.9818111  -3.6280613 -13.394435 ]\n",
      " ...\n",
      " [  0.98147    -3.6311882 -13.410276 ]\n",
      " [  0.9814673  -3.6312127 -13.4104   ]\n",
      " [  0.9814648  -3.6312351 -13.410513 ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9814625   -3.6312563  -13.41062   ]\n",
      " [  0.98146135  -3.6312618  -13.410618  ]\n",
      " [  0.9814589   -3.6312861  -13.410773  ]\n",
      " ...\n",
      " [  0.98091674  -3.6369667  -13.442009  ]\n",
      " [  0.9809143   -3.6369956  -13.442173  ]\n",
      " [  0.9809114   -3.6370306  -13.442387  ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.98090863  -3.6370645  -13.442583  ]\n",
      " [  0.9809059   -3.637096   -13.442762  ]\n",
      " [  0.9809028   -3.6371334  -13.442989  ]\n",
      " ...\n",
      " [  0.9811178   -3.656939   -13.614251  ]\n",
      " [  0.9811211   -3.6569517  -13.614452  ]\n",
      " [  0.9811246   -3.6569598  -13.614619  ]] \n",
      "\n",
      "Final Test RMSE:  1.395874798297882\n",
      "Epoch 582/1000 | Train Loss=1574.15647839 | Val Loss=2.74775474 | Data=15.59889907 | Physics=14.03345518 | Val RMSE: 2.55152965 | ‚àö(Val Loss) = 1.65763533 | Current Learning Rate: 0.0002\n",
      "Epoch 583/1000 | Train Loss=1573.86407668 | Val Loss=2.75147031 | Data=15.59620208 | Physics=14.51101249 | Val RMSE: 2.55226326 | ‚àö(Val Loss) = 1.65875566 | Current Learning Rate: 0.0002\n",
      "Epoch 584/1000 | Train Loss=1574.19330031 | Val Loss=2.75847818 | Data=15.59933382 | Physics=13.98170877 | Val RMSE: 2.55389714 | ‚àö(Val Loss) = 1.66086674 | Current Learning Rate: 0.0002\n",
      "Epoch 585/1000 | Train Loss=1574.01184476 | Val Loss=2.75025768 | Data=15.59771722 | Physics=14.49455899 | Val RMSE: 2.55131984 | ‚àö(Val Loss) = 1.65839005 | Current Learning Rate: 0.0002\n",
      "Epoch 586/1000 | Train Loss=1573.69468640 | Val Loss=2.75626323 | Data=15.59432297 | Physics=13.89947477 | Val RMSE: 2.55206251 | ‚àö(Val Loss) = 1.66019976 | Current Learning Rate: 0.0002\n",
      "Epoch 587/1000 | Train Loss=1576.33103500 | Val Loss=2.75761063 | Data=15.62063199 | Physics=14.44549711 | Val RMSE: 2.55359411 | ‚àö(Val Loss) = 1.66060543 | Current Learning Rate: 0.0002\n",
      "Epoch 588/1000 | Train Loss=1573.01327810 | Val Loss=2.75036553 | Data=15.58769648 | Physics=14.16899894 | Val RMSE: 2.55202174 | ‚àö(Val Loss) = 1.65842259 | Current Learning Rate: 0.0002\n",
      "Epoch 589/1000 | Train Loss=1573.52770996 | Val Loss=2.75200394 | Data=15.59276968 | Physics=14.15555478 | Val RMSE: 2.55129743 | ‚àö(Val Loss) = 1.65891647 | Current Learning Rate: 0.0002\n",
      "Epoch 590/1000 | Train Loss=1575.70342821 | Val Loss=2.75017068 | Data=15.61463011 | Physics=14.24838038 | Val RMSE: 2.55126357 | ‚àö(Val Loss) = 1.65836382 | Current Learning Rate: 0.0002\n",
      "Epoch 591/1000 | Train Loss=1573.90183184 | Val Loss=2.75210334 | Data=15.59654654 | Physics=14.57947554 | Val RMSE: 2.55284786 | ‚àö(Val Loss) = 1.65894639 | Current Learning Rate: 0.0002\n",
      "Epoch 592/1000 | Train Loss=1574.18534211 | Val Loss=2.74658446 | Data=15.59922046 | Physics=14.10665816 | Val RMSE: 2.55199385 | ‚àö(Val Loss) = 1.65728223 | Current Learning Rate: 0.0002\n",
      "Epoch 593/1000 | Train Loss=1574.27014948 | Val Loss=2.75355033 | Data=15.60009363 | Physics=14.44796774 | Val RMSE: 2.55290127 | ‚àö(Val Loss) = 1.65938246 | Current Learning Rate: 0.0002\n",
      "Epoch 594/1000 | Train Loss=1573.73448132 | Val Loss=2.75164913 | Data=15.59482384 | Physics=14.02624214 | Val RMSE: 2.55197024 | ‚àö(Val Loss) = 1.65880954 | Current Learning Rate: 0.0002\n",
      "Epoch 595/1000 | Train Loss=1574.29967868 | Val Loss=2.75437464 | Data=15.60064411 | Physics=13.91310212 | Val RMSE: 2.55243611 | ‚àö(Val Loss) = 1.65963089 | Current Learning Rate: 0.0002\n",
      "Epoch 596/1000 | Train Loss=1573.44514711 | Val Loss=2.75339708 | Data=15.59187348 | Physics=14.19426103 | Val RMSE: 2.55271864 | ‚àö(Val Loss) = 1.65933633 | Current Learning Rate: 0.0002\n",
      "Epoch 597/1000 | Train Loss=1574.06631568 | Val Loss=2.76300589 | Data=15.59819652 | Physics=14.52447568 | Val RMSE: 2.55318999 | ‚àö(Val Loss) = 1.66222918 | Current Learning Rate: 0.0002\n",
      "Epoch 598/1000 | Train Loss=1575.81774902 | Val Loss=2.75581548 | Data=15.61557167 | Physics=13.91435928 | Val RMSE: 2.55382919 | ‚àö(Val Loss) = 1.66006494 | Current Learning Rate: 0.0002\n",
      "Epoch 599/1000 | Train Loss=1576.18004977 | Val Loss=2.74731859 | Data=15.61925959 | Physics=14.16921790 | Val RMSE: 2.55010366 | ‚àö(Val Loss) = 1.65750372 | Current Learning Rate: 0.0002\n",
      "Epoch 600/1000 | Train Loss=1575.12272004 | Val Loss=2.75068200 | Data=15.60865282 | Physics=14.26772852 | Val RMSE: 2.55160761 | ‚àö(Val Loss) = 1.65851796 | Current Learning Rate: 0.0002\n",
      "Epoch 601/1000 | Train Loss=1574.79421308 | Val Loss=2.74779056 | Data=15.60547921 | Physics=14.21810735 | Val RMSE: 2.55176449 | ‚àö(Val Loss) = 1.65764606 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99160266  -3.5620732  -13.1507635 ]\n",
      " [  0.99150854  -3.5626504  -13.152554  ]\n",
      " [  0.99154377  -3.5624392  -13.1519375 ]\n",
      " ...\n",
      " [  0.98063016  -3.6406848  -13.464569  ]\n",
      " [  0.9806281   -3.6407166  -13.464769  ]\n",
      " [  0.98062605  -3.6407454  -13.464949  ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9806237   -3.6407795  -13.465172  ]\n",
      " [  0.98062164  -3.6408117  -13.465373  ]\n",
      " [  0.98061967  -3.6408405  -13.465553  ]\n",
      " ...\n",
      " [  0.9817578   -3.628535   -13.396777  ]\n",
      " [  0.98175484  -3.6285613  -13.396906  ]\n",
      " [  0.981752    -3.628586   -13.397024  ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9817486  -3.6286182 -13.397201 ]\n",
      " [  0.9817451  -3.628651  -13.397366 ]\n",
      " [  0.9817424  -3.628673  -13.397457 ]\n",
      " ...\n",
      " [  0.9818238  -3.627943  -13.393821 ]\n",
      " [  0.9818217  -3.6279607 -13.393903 ]\n",
      " [  0.9818197  -3.6279771 -13.393981 ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9818171  -3.628002  -13.394118 ]\n",
      " [  0.9818145  -3.6280272 -13.394248 ]\n",
      " [  0.9818111  -3.6280613 -13.394435 ]\n",
      " ...\n",
      " [  0.98147    -3.6311882 -13.410276 ]\n",
      " [  0.9814673  -3.6312127 -13.4104   ]\n",
      " [  0.9814648  -3.6312351 -13.410513 ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9814625   -3.6312563  -13.41062   ]\n",
      " [  0.98146135  -3.6312618  -13.410618  ]\n",
      " [  0.9814589   -3.6312861  -13.410773  ]\n",
      " ...\n",
      " [  0.98091674  -3.6369667  -13.442009  ]\n",
      " [  0.9809143   -3.6369956  -13.442173  ]\n",
      " [  0.9809114   -3.6370306  -13.442387  ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.98090863  -3.6370645  -13.442583  ]\n",
      " [  0.9809059   -3.637096   -13.442762  ]\n",
      " [  0.9809028   -3.6371334  -13.442989  ]\n",
      " ...\n",
      " [  0.9811178   -3.656939   -13.614251  ]\n",
      " [  0.9811211   -3.6569517  -13.614452  ]\n",
      " [  0.9811246   -3.6569598  -13.614619  ]] \n",
      "\n",
      "Final Test RMSE:  1.395874798297882\n",
      "‚úÖ Saved best model at epoch 602 (Val Loss = 2.72486540)\n",
      "Epoch 602/1000 | Train Loss=6547.81938319 | Val Loss=2.72486540 | Data=65.33505741 | Physics=14.18862734 | Val RMSE: 2.54560590 | ‚àö(Val Loss) = 1.65071666 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 603 (Val Loss = 2.72211031)\n",
      "Epoch 603/1000 | Train Loss=6543.87112525 | Val Loss=2.72211031 | Data=65.29700581 | Physics=14.01322346 | Val RMSE: 2.54116583 | ‚àö(Val Loss) = 1.64988184 | Current Learning Rate: 0.0002\n",
      "Epoch 604/1000 | Train Loss=6512.67515121 | Val Loss=2.74561741 | Data=64.98843593 | Physics=13.98222743 | Val RMSE: 2.55010462 | ‚àö(Val Loss) = 1.65699041 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 605 (Val Loss = 2.70566313)\n",
      "Epoch 605/1000 | Train Loss=6531.46057523 | Val Loss=2.70566313 | Data=65.17645731 | Physics=14.03917433 | Val RMSE: 2.53646898 | ‚àö(Val Loss) = 1.64489007 | Current Learning Rate: 0.0002\n",
      "Epoch 606/1000 | Train Loss=6503.47612147 | Val Loss=2.73713865 | Data=64.89729998 | Physics=14.36643023 | Val RMSE: 2.55140257 | ‚àö(Val Loss) = 1.65443003 | Current Learning Rate: 0.0002\n",
      "Epoch 607/1000 | Train Loss=6516.13202495 | Val Loss=2.71978930 | Data=65.02414347 | Physics=13.02152177 | Val RMSE: 2.54409862 | ‚àö(Val Loss) = 1.64917839 | Current Learning Rate: 0.0002\n",
      "Epoch 608/1000 | Train Loss=6578.13152092 | Val Loss=2.74109851 | Data=65.64451205 | Physics=13.97581373 | Val RMSE: 2.54613781 | ‚àö(Val Loss) = 1.65562630 | Current Learning Rate: 0.0002\n",
      "Epoch 609/1000 | Train Loss=6499.21610383 | Val Loss=2.72118868 | Data=64.85489778 | Physics=13.57640101 | Val RMSE: 2.54554653 | ‚àö(Val Loss) = 1.64960265 | Current Learning Rate: 0.0002\n",
      "Epoch 610/1000 | Train Loss=6513.40391885 | Val Loss=2.74539118 | Data=64.99662202 | Physics=13.74377330 | Val RMSE: 2.54816055 | ‚àö(Val Loss) = 1.65692222 | Current Learning Rate: 0.0002\n",
      "Epoch 611/1000 | Train Loss=6500.64640562 | Val Loss=2.73489694 | Data=64.86937824 | Physics=13.86075033 | Val RMSE: 2.54963827 | ‚àö(Val Loss) = 1.65375233 | Current Learning Rate: 0.0002\n",
      "Epoch 612/1000 | Train Loss=6501.01119897 | Val Loss=2.72241652 | Data=64.87342625 | Physics=13.61789598 | Val RMSE: 2.54594493 | ‚àö(Val Loss) = 1.64997470 | Current Learning Rate: 0.0002\n",
      "Epoch 613/1000 | Train Loss=6499.67094569 | Val Loss=2.72899210 | Data=64.85968067 | Physics=13.66124991 | Val RMSE: 2.54235721 | ‚àö(Val Loss) = 1.65196609 | Current Learning Rate: 0.0002\n",
      "Epoch 614/1000 | Train Loss=6505.43664945 | Val Loss=2.71535467 | Data=64.91735409 | Physics=13.95570217 | Val RMSE: 2.54236603 | ‚àö(Val Loss) = 1.64783335 | Current Learning Rate: 0.0002\n",
      "Epoch 615/1000 | Train Loss=6512.34879032 | Val Loss=2.70824311 | Data=64.98678675 | Physics=13.59483517 | Val RMSE: 2.54191065 | ‚àö(Val Loss) = 1.64567411 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 616 (Val Loss = 2.69930403)\n",
      "Epoch 616/1000 | Train Loss=6502.60677608 | Val Loss=2.69930403 | Data=64.88895133 | Physics=13.88773317 | Val RMSE: 2.54662848 | ‚àö(Val Loss) = 1.64295590 | Current Learning Rate: 0.0002\n",
      "Epoch 617/1000 | Train Loss=6492.45038432 | Val Loss=2.71285467 | Data=64.78671302 | Physics=14.12752186 | Val RMSE: 2.54302692 | ‚àö(Val Loss) = 1.64707458 | Current Learning Rate: 0.0002\n",
      "Epoch 618/1000 | Train Loss=6495.60175151 | Val Loss=2.71908833 | Data=64.81853707 | Physics=14.13542207 | Val RMSE: 2.53933668 | ‚àö(Val Loss) = 1.64896584 | Current Learning Rate: 0.0002\n",
      "Epoch 619/1000 | Train Loss=6493.04763105 | Val Loss=2.74477392 | Data=64.79326113 | Physics=13.77835406 | Val RMSE: 2.55084705 | ‚àö(Val Loss) = 1.65673590 | Current Learning Rate: 0.0002\n",
      "Epoch 620/1000 | Train Loss=6495.50126008 | Val Loss=2.73695298 | Data=64.81703814 | Physics=13.74568943 | Val RMSE: 2.55185223 | ‚àö(Val Loss) = 1.65437388 | Current Learning Rate: 0.0002\n",
      "Epoch 621/1000 | Train Loss=6489.01208102 | Val Loss=2.73405331 | Data=64.75271963 | Physics=14.04498839 | Val RMSE: 2.54391718 | ‚àö(Val Loss) = 1.65349734 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  1.0109742   -3.347274   -10.243309  ]\n",
      " [  1.0108926   -3.3480191  -10.247104  ]\n",
      " [  1.0110841   -3.3462272  -10.238023  ]\n",
      " ...\n",
      " [  0.9778874   -3.6480947  -13.490935  ]\n",
      " [  0.97789127  -3.6480722  -13.491392  ]\n",
      " [  0.97789526  -3.6480494  -13.491822  ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9778987   -3.648027   -13.49231   ]\n",
      " [  0.97790265  -3.648004   -13.49277   ]\n",
      " [  0.97790676  -3.647981   -13.493202  ]\n",
      " ...\n",
      " [  0.97744626  -3.654092   -13.220072  ]\n",
      " [  0.97744095  -3.654096   -13.221392  ]\n",
      " [  0.9774363   -3.6540985  -13.222573  ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.9774308   -3.654102   -13.223832  ]\n",
      " [  0.97742575  -3.6541061  -13.225078  ]\n",
      " [  0.9774219   -3.6541083  -13.22622   ]\n",
      " ...\n",
      " [  0.97755384  -3.6539721  -13.196455  ]\n",
      " [  0.9775488   -3.6539783  -13.197522  ]\n",
      " [  0.9775432   -3.653985   -13.198698  ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.97753674  -3.6539927  -13.199953  ]\n",
      " [  0.97753066  -3.6540015  -13.201195  ]\n",
      " [  0.9775241   -3.6540105  -13.202458  ]\n",
      " ...\n",
      " [  0.97722584  -3.6536238  -13.313126  ]\n",
      " [  0.9772257   -3.6536143  -13.313806  ]\n",
      " [  0.97722566  -3.653604   -13.31447   ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9772256   -3.6535938  -13.3151245 ]\n",
      " [  0.97722685  -3.6535819  -13.315644  ]\n",
      " [  0.9772255   -3.6535716  -13.316351  ]\n",
      " ...\n",
      " [  0.9774884   -3.6506715  -13.431717  ]\n",
      " [  0.9774913   -3.6506515  -13.432237  ]\n",
      " [  0.97749364  -3.650632   -13.432818  ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9774965   -3.650612   -13.433378  ]\n",
      " [  0.97749954  -3.6505916  -13.433914  ]\n",
      " [  0.977502    -3.6505713  -13.434512  ]\n",
      " ...\n",
      " [  0.98257464  -3.625193   -13.765372  ]\n",
      " [  0.98258257  -3.625157   -13.765685  ]\n",
      " [  0.9825908   -3.625121   -13.765955  ]] \n",
      "\n",
      "Final Test RMSE:  1.4439084430535634\n",
      "Epoch 622/1000 | Train Loss=6490.34727823 | Val Loss=2.73047696 | Data=64.76591652 | Physics=13.54527226 | Val RMSE: 2.55183148 | ‚àö(Val Loss) = 1.65241551 | Current Learning Rate: 0.0002\n",
      "Epoch 623/1000 | Train Loss=6506.48007497 | Val Loss=2.74819899 | Data=64.92736152 | Physics=13.77037008 | Val RMSE: 2.55310822 | ‚àö(Val Loss) = 1.65776932 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 624 (Val Loss = 2.68808082)\n",
      "Epoch 624/1000 | Train Loss=6493.73987210 | Val Loss=2.68808082 | Data=64.79982167 | Physics=13.05649610 | Val RMSE: 2.53600812 | ‚àö(Val Loss) = 1.63953674 | Current Learning Rate: 0.0002\n",
      "Epoch 625/1000 | Train Loss=6490.45263672 | Val Loss=2.72242255 | Data=64.76688730 | Physics=13.84731556 | Val RMSE: 2.53413296 | ‚àö(Val Loss) = 1.64997649 | Current Learning Rate: 0.0002\n",
      "Epoch 626/1000 | Train Loss=6488.78677860 | Val Loss=2.70336566 | Data=64.75049431 | Physics=13.77580806 | Val RMSE: 2.53978419 | ‚àö(Val Loss) = 1.64419150 | Current Learning Rate: 0.0002\n",
      "Epoch 627/1000 | Train Loss=6488.29295300 | Val Loss=2.73142211 | Data=64.74548918 | Physics=13.79895733 | Val RMSE: 2.54726863 | ‚àö(Val Loss) = 1.65270150 | Current Learning Rate: 0.0002\n",
      "Epoch 628/1000 | Train Loss=6483.08193674 | Val Loss=2.71586460 | Data=64.69352525 | Physics=13.47887666 | Val RMSE: 2.53818965 | ‚àö(Val Loss) = 1.64798808 | Current Learning Rate: 0.0002\n",
      "Epoch 629/1000 | Train Loss=6487.94800592 | Val Loss=2.71301265 | Data=64.74206764 | Physics=13.42354108 | Val RMSE: 2.53916264 | ‚àö(Val Loss) = 1.64712250 | Current Learning Rate: 0.0002\n",
      "Epoch 630/1000 | Train Loss=6487.45559791 | Val Loss=2.71964528 | Data=64.73738541 | Physics=13.89631313 | Val RMSE: 2.54299641 | ‚àö(Val Loss) = 1.64913476 | Current Learning Rate: 0.0002\n",
      "Epoch 631/1000 | Train Loss=6485.35433468 | Val Loss=2.72650565 | Data=64.71612352 | Physics=14.40533269 | Val RMSE: 2.54729414 | ‚àö(Val Loss) = 1.65121341 | Current Learning Rate: 0.0002\n",
      "Epoch 632/1000 | Train Loss=6491.86241494 | Val Loss=2.74560234 | Data=64.78104302 | Physics=12.80826295 | Val RMSE: 2.55025172 | ‚àö(Val Loss) = 1.65698588 | Current Learning Rate: 0.0002\n",
      "Epoch 633/1000 | Train Loss=6498.07291142 | Val Loss=2.72009612 | Data=64.84302742 | Physics=13.94626668 | Val RMSE: 2.54593945 | ‚àö(Val Loss) = 1.64927137 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 634 (Val Loss = 2.67937789)\n",
      "Epoch 634/1000 | Train Loss=6503.99710181 | Val Loss=2.67937789 | Data=64.90216803 | Physics=13.81542182 | Val RMSE: 2.53831291 | ‚àö(Val Loss) = 1.63688052 | Current Learning Rate: 0.0002\n",
      "Epoch 635/1000 | Train Loss=6488.56698904 | Val Loss=2.72783040 | Data=64.74806423 | Physics=14.13970930 | Val RMSE: 2.54626751 | ‚àö(Val Loss) = 1.65161443 | Current Learning Rate: 0.0002\n",
      "Epoch 636/1000 | Train Loss=6487.23796623 | Val Loss=2.73500698 | Data=64.73463058 | Physics=13.79441396 | Val RMSE: 2.54906392 | ‚àö(Val Loss) = 1.65378571 | Current Learning Rate: 0.0002\n",
      "Epoch 637/1000 | Train Loss=6498.38818359 | Val Loss=2.68532560 | Data=64.84567938 | Physics=14.05057534 | Val RMSE: 2.53930712 | ‚àö(Val Loss) = 1.63869631 | Current Learning Rate: 0.0002\n",
      "Epoch 638/1000 | Train Loss=6491.45991368 | Val Loss=2.73591939 | Data=64.77679948 | Physics=14.00756324 | Val RMSE: 2.54486918 | ‚àö(Val Loss) = 1.65406156 | Current Learning Rate: 0.0002\n",
      "Epoch 639/1000 | Train Loss=6495.91264491 | Val Loss=2.72393301 | Data=64.82094968 | Physics=13.99217783 | Val RMSE: 2.54025173 | ‚àö(Val Loss) = 1.65043414 | Current Learning Rate: 0.0002\n",
      "Epoch 640/1000 | Train Loss=6487.31594947 | Val Loss=2.73999847 | Data=64.73565895 | Physics=13.92925432 | Val RMSE: 2.54929423 | ‚àö(Val Loss) = 1.65529406 | Current Learning Rate: 0.0002\n",
      "Epoch 641/1000 | Train Loss=6485.73861202 | Val Loss=2.73603172 | Data=64.71989835 | Physics=13.22036035 | Val RMSE: 2.54748869 | ‚àö(Val Loss) = 1.65409541 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 642/1000 | Train Loss=6485.88807334 | Val Loss=2.71874550 | Data=64.72099021 | Physics=13.47760629 | Val RMSE: 2.53937244 | ‚àö(Val Loss) = 1.64886189 | Current Learning Rate: 0.0002\n",
      "Epoch 643/1000 | Train Loss=6494.97615297 | Val Loss=2.71998979 | Data=64.81216381 | Physics=13.87134621 | Val RMSE: 2.54088831 | ‚àö(Val Loss) = 1.64923918 | Current Learning Rate: 0.0002\n",
      "Epoch 644/1000 | Train Loss=6493.25474105 | Val Loss=2.70966999 | Data=64.79440788 | Physics=14.02896125 | Val RMSE: 2.53952265 | ‚àö(Val Loss) = 1.64610755 | Current Learning Rate: 0.0002\n",
      "Epoch 645/1000 | Train Loss=6489.66122732 | Val Loss=2.75276994 | Data=64.75890793 | Physics=14.01039870 | Val RMSE: 2.55381584 | ‚àö(Val Loss) = 1.65914738 | Current Learning Rate: 0.0002\n",
      "Epoch 646/1000 | Train Loss=6493.54789882 | Val Loss=2.73273986 | Data=64.79796231 | Physics=13.43914723 | Val RMSE: 2.54957867 | ‚àö(Val Loss) = 1.65310013 | Current Learning Rate: 0.0002\n",
      "Epoch 647/1000 | Train Loss=6484.06369708 | Val Loss=2.70251470 | Data=64.70312734 | Physics=13.10676344 | Val RMSE: 2.54052258 | ‚àö(Val Loss) = 1.64393270 | Current Learning Rate: 0.0002\n",
      "Epoch 648/1000 | Train Loss=6486.44592679 | Val Loss=2.73610236 | Data=64.72688010 | Physics=13.77458562 | Val RMSE: 2.54674006 | ‚àö(Val Loss) = 1.65411675 | Current Learning Rate: 0.0002\n",
      "Epoch 649/1000 | Train Loss=6485.51490045 | Val Loss=2.73133277 | Data=64.71775301 | Physics=13.34082899 | Val RMSE: 2.54061627 | ‚àö(Val Loss) = 1.65267444 | Current Learning Rate: 0.0002\n",
      "Epoch 650/1000 | Train Loss=6495.07785723 | Val Loss=2.73133205 | Data=64.81291605 | Physics=13.76243091 | Val RMSE: 2.54242826 | ‚àö(Val Loss) = 1.65267420 | Current Learning Rate: 0.0002\n",
      "Epoch 651/1000 | Train Loss=6487.39097152 | Val Loss=2.74500919 | Data=64.73616655 | Physics=13.68803055 | Val RMSE: 2.55137372 | ‚àö(Val Loss) = 1.65680695 | Current Learning Rate: 0.0002\n",
      "Epoch 652/1000 | Train Loss=6484.72435736 | Val Loss=2.73378038 | Data=64.70976147 | Physics=13.84065412 | Val RMSE: 2.54078293 | ‚àö(Val Loss) = 1.65341473 | Current Learning Rate: 0.0002\n",
      "Epoch 653/1000 | Train Loss=6488.51130922 | Val Loss=2.74130419 | Data=64.74733500 | Physics=13.58008903 | Val RMSE: 2.55534697 | ‚àö(Val Loss) = 1.65568841 | Current Learning Rate: 0.0002\n",
      "Epoch 654/1000 | Train Loss=6483.27756426 | Val Loss=2.73890011 | Data=64.69534093 | Physics=13.21319681 | Val RMSE: 2.54919410 | ‚àö(Val Loss) = 1.65496230 | Current Learning Rate: 0.0002\n",
      "Epoch 655/1000 | Train Loss=6482.49310106 | Val Loss=2.73815703 | Data=64.68761272 | Physics=13.97470013 | Val RMSE: 2.54432011 | ‚àö(Val Loss) = 1.65473771 | Current Learning Rate: 0.0002\n",
      "Epoch 656/1000 | Train Loss=6487.22048261 | Val Loss=2.75375001 | Data=64.73449892 | Physics=13.75081199 | Val RMSE: 2.55009174 | ‚àö(Val Loss) = 1.65944266 | Current Learning Rate: 0.0002\n",
      "Epoch 657/1000 | Train Loss=6490.57073778 | Val Loss=2.74623112 | Data=64.76810037 | Physics=13.56253530 | Val RMSE: 2.55036545 | ‚àö(Val Loss) = 1.65717566 | Current Learning Rate: 0.0002\n",
      "Epoch 658/1000 | Train Loss=6490.02079133 | Val Loss=2.71870808 | Data=64.76224715 | Physics=13.43026565 | Val RMSE: 2.53672004 | ‚àö(Val Loss) = 1.64885056 | Current Learning Rate: 0.0002\n",
      "Epoch 659/1000 | Train Loss=6486.42792339 | Val Loss=2.73044831 | Data=64.72647808 | Physics=13.43837116 | Val RMSE: 2.54118872 | ‚àö(Val Loss) = 1.65240681 | Current Learning Rate: 0.0002\n",
      "Epoch 660/1000 | Train Loss=6489.74521169 | Val Loss=2.71347595 | Data=64.76006920 | Physics=13.66623661 | Val RMSE: 2.53863764 | ‚àö(Val Loss) = 1.64726317 | Current Learning Rate: 0.0002\n",
      "Epoch 661/1000 | Train Loss=6488.17636404 | Val Loss=2.75190906 | Data=64.74415010 | Physics=13.84173696 | Val RMSE: 2.54948163 | ‚àö(Val Loss) = 1.65888786 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 662/1000 | Train Loss=6488.71961631 | Val Loss=2.73280781 | Data=64.74986353 | Physics=14.13399053 | Val RMSE: 2.54227877 | ‚àö(Val Loss) = 1.65312064 | Current Learning Rate: 0.0002\n",
      "Epoch 663/1000 | Train Loss=6488.83933972 | Val Loss=2.70644586 | Data=64.75067299 | Physics=13.93009022 | Val RMSE: 2.53726697 | ‚àö(Val Loss) = 1.64512789 | Current Learning Rate: 0.0002\n",
      "Epoch 664/1000 | Train Loss=6490.17713584 | Val Loss=2.72426156 | Data=64.76409691 | Physics=13.92434721 | Val RMSE: 2.54422832 | ‚àö(Val Loss) = 1.65053368 | Current Learning Rate: 0.0002\n",
      "Epoch 665/1000 | Train Loss=6497.60066469 | Val Loss=2.72540504 | Data=64.83833350 | Physics=13.80789617 | Val RMSE: 2.54440355 | ‚àö(Val Loss) = 1.65088010 | Current Learning Rate: 0.0002\n",
      "Epoch 666/1000 | Train Loss=6487.84507308 | Val Loss=2.74402239 | Data=64.74099584 | Physics=13.89935129 | Val RMSE: 2.54761219 | ‚àö(Val Loss) = 1.65650904 | Current Learning Rate: 0.0002\n",
      "Epoch 667/1000 | Train Loss=6483.85720136 | Val Loss=2.73452241 | Data=64.70099726 | Physics=13.95213736 | Val RMSE: 2.54509139 | ‚àö(Val Loss) = 1.65363908 | Current Learning Rate: 0.0002\n",
      "Epoch 668/1000 | Train Loss=6499.01756237 | Val Loss=2.69143384 | Data=64.85228422 | Physics=13.75680162 | Val RMSE: 2.53476071 | ‚àö(Val Loss) = 1.64055908 | Current Learning Rate: 0.0002\n",
      "Epoch 669/1000 | Train Loss=6486.18405053 | Val Loss=2.73289496 | Data=64.72433644 | Physics=14.04155099 | Val RMSE: 2.54327083 | ‚àö(Val Loss) = 1.65314698 | Current Learning Rate: 0.0002\n",
      "Epoch 670/1000 | Train Loss=6489.40013861 | Val Loss=2.71317734 | Data=64.75644745 | Physics=13.97917010 | Val RMSE: 2.53902125 | ‚àö(Val Loss) = 1.64717257 | Current Learning Rate: 0.0002\n",
      "Epoch 671/1000 | Train Loss=6486.53142326 | Val Loss=2.72809058 | Data=64.72782640 | Physics=14.16791597 | Val RMSE: 2.54384184 | ‚àö(Val Loss) = 1.65169322 | Current Learning Rate: 0.0002\n",
      "Epoch 672/1000 | Train Loss=6496.80586883 | Val Loss=2.68886712 | Data=64.82999039 | Physics=14.10122399 | Val RMSE: 2.53782725 | ‚àö(Val Loss) = 1.63977659 | Current Learning Rate: 0.0002\n",
      "Epoch 673/1000 | Train Loss=6489.38923891 | Val Loss=2.72729680 | Data=64.75653728 | Physics=13.28884707 | Val RMSE: 2.54667664 | ‚àö(Val Loss) = 1.65145290 | Current Learning Rate: 0.0002\n",
      "Epoch 674/1000 | Train Loss=6488.53651084 | Val Loss=2.74385138 | Data=64.74763563 | Physics=13.46783317 | Val RMSE: 2.54864502 | ‚àö(Val Loss) = 1.65645754 | Current Learning Rate: 0.0002\n",
      "Epoch 675/1000 | Train Loss=6492.51061618 | Val Loss=2.74181051 | Data=64.78730959 | Physics=14.25742519 | Val RMSE: 2.54805732 | ‚àö(Val Loss) = 1.65584135 | Current Learning Rate: 0.0002\n",
      "Epoch 676/1000 | Train Loss=6487.42871094 | Val Loss=2.70943011 | Data=64.73646336 | Physics=13.79097396 | Val RMSE: 2.53833508 | ‚àö(Val Loss) = 1.64603472 | Current Learning Rate: 0.0002\n",
      "Epoch 677/1000 | Train Loss=6484.67130796 | Val Loss=2.72341482 | Data=64.70946257 | Physics=13.76096701 | Val RMSE: 2.54124784 | ‚àö(Val Loss) = 1.65027726 | Current Learning Rate: 0.0002\n",
      "Epoch 678/1000 | Train Loss=6487.03791268 | Val Loss=2.75090200 | Data=64.73274982 | Physics=13.57340814 | Val RMSE: 2.55511618 | ‚àö(Val Loss) = 1.65858436 | Current Learning Rate: 0.0002\n",
      "Epoch 679/1000 | Train Loss=6485.03055696 | Val Loss=2.71249583 | Data=64.71277508 | Physics=13.97737695 | Val RMSE: 2.53832126 | ‚àö(Val Loss) = 1.64696562 | Current Learning Rate: 0.0002\n",
      "Epoch 680/1000 | Train Loss=6487.53268334 | Val Loss=2.72849658 | Data=64.73780244 | Physics=13.34223058 | Val RMSE: 2.54568720 | ‚àö(Val Loss) = 1.65181613 | Current Learning Rate: 0.0002\n",
      "Epoch 681/1000 | Train Loss=6491.59335622 | Val Loss=2.73247738 | Data=64.77820070 | Physics=13.43021818 | Val RMSE: 2.54355931 | ‚àö(Val Loss) = 1.65302074 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 682/1000 | Train Loss=6485.24226626 | Val Loss=2.72237220 | Data=64.71484350 | Physics=13.62725418 | Val RMSE: 2.54290390 | ‚àö(Val Loss) = 1.64996135 | Current Learning Rate: 0.0002\n",
      "Epoch 683/1000 | Train Loss=6487.40278478 | Val Loss=2.72270167 | Data=64.73667612 | Physics=13.01910617 | Val RMSE: 2.54071999 | ‚àö(Val Loss) = 1.65006113 | Current Learning Rate: 0.0002\n",
      "Epoch 684/1000 | Train Loss=6493.24763735 | Val Loss=2.71209727 | Data=64.79513328 | Physics=13.40417288 | Val RMSE: 2.53725958 | ‚àö(Val Loss) = 1.64684463 | Current Learning Rate: 0.0002\n",
      "Epoch 685/1000 | Train Loss=6488.56698904 | Val Loss=2.72816988 | Data=64.74794942 | Physics=14.50674142 | Val RMSE: 2.54537463 | ‚àö(Val Loss) = 1.65171731 | Current Learning Rate: 0.0002\n",
      "Epoch 686/1000 | Train Loss=6487.91042402 | Val Loss=2.72534416 | Data=64.74168138 | Physics=14.42421192 | Val RMSE: 2.54430771 | ‚àö(Val Loss) = 1.65086162 | Current Learning Rate: 0.0002\n",
      "Epoch 687/1000 | Train Loss=6487.55843624 | Val Loss=2.72864253 | Data=64.73800831 | Physics=14.06593168 | Val RMSE: 2.54455566 | ‚àö(Val Loss) = 1.65186036 | Current Learning Rate: 0.0002\n",
      "Epoch 688/1000 | Train Loss=6482.66043977 | Val Loss=2.71838232 | Data=64.68921108 | Physics=13.56499217 | Val RMSE: 2.54076290 | ‚àö(Val Loss) = 1.64875174 | Current Learning Rate: 0.0002\n",
      "Epoch 689/1000 | Train Loss=6486.89697266 | Val Loss=2.71892834 | Data=64.73152013 | Physics=13.80951022 | Val RMSE: 2.54386210 | ‚àö(Val Loss) = 1.64891732 | Current Learning Rate: 0.0002\n",
      "Epoch 690/1000 | Train Loss=6496.02167339 | Val Loss=2.70565598 | Data=64.82230193 | Physics=13.68932723 | Val RMSE: 2.53847599 | ‚àö(Val Loss) = 1.64488780 | Current Learning Rate: 0.0002\n",
      "Epoch 691/1000 | Train Loss=6485.91121157 | Val Loss=2.73539501 | Data=64.72166603 | Physics=13.63835046 | Val RMSE: 2.53976941 | ‚àö(Val Loss) = 1.65390289 | Current Learning Rate: 0.0002\n",
      "Epoch 692/1000 | Train Loss=6490.15613974 | Val Loss=2.72653070 | Data=64.76380059 | Physics=13.53579801 | Val RMSE: 2.55128384 | ‚àö(Val Loss) = 1.65122104 | Current Learning Rate: 0.0002\n",
      "Epoch 693/1000 | Train Loss=6494.85877646 | Val Loss=2.72936646 | Data=64.81059524 | Physics=13.79315844 | Val RMSE: 2.54964948 | ‚àö(Val Loss) = 1.65207946 | Current Learning Rate: 0.0002\n",
      "Epoch 694/1000 | Train Loss=6483.51675907 | Val Loss=2.71000960 | Data=64.69784743 | Physics=14.21768373 | Val RMSE: 2.53881574 | ‚àö(Val Loss) = 1.64621067 | Current Learning Rate: 0.0002\n",
      "Epoch 695/1000 | Train Loss=6495.60957976 | Val Loss=2.69926700 | Data=64.81852168 | Physics=13.82740363 | Val RMSE: 2.53595805 | ‚àö(Val Loss) = 1.64294457 | Current Learning Rate: 0.0002\n",
      "Epoch 696/1000 | Train Loss=6489.97812185 | Val Loss=2.73879836 | Data=64.76205383 | Physics=13.76135307 | Val RMSE: 2.55050564 | ‚àö(Val Loss) = 1.65493155 | Current Learning Rate: 0.0002\n",
      "Epoch 697/1000 | Train Loss=6484.18044355 | Val Loss=2.70904030 | Data=64.70438397 | Physics=13.70505947 | Val RMSE: 2.53881788 | ‚àö(Val Loss) = 1.64591622 | Current Learning Rate: 0.0002\n",
      "Epoch 698/1000 | Train Loss=6492.37854398 | Val Loss=2.72966394 | Data=64.78606255 | Physics=13.83269840 | Val RMSE: 2.54857135 | ‚àö(Val Loss) = 1.65216947 | Current Learning Rate: 0.0002\n",
      "Epoch 699/1000 | Train Loss=6495.05208858 | Val Loss=2.72192665 | Data=64.81287372 | Physics=13.30795479 | Val RMSE: 2.54295588 | ‚àö(Val Loss) = 1.64982629 | Current Learning Rate: 0.0002\n",
      "Epoch 700/1000 | Train Loss=6486.62564579 | Val Loss=2.74663681 | Data=64.72863548 | Physics=13.81455298 | Val RMSE: 2.55498600 | ‚àö(Val Loss) = 1.65729809 | Current Learning Rate: 0.0002\n",
      "Epoch 701/1000 | Train Loss=6486.69953377 | Val Loss=2.74101916 | Data=64.72957205 | Physics=13.88096751 | Val RMSE: 2.54559088 | ‚àö(Val Loss) = 1.65560234 | Current Learning Rate: 0.0002\n",
      "‚úÖ Learning Rate updated to 0.001\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 702/1000 | Train Loss=6498.79978894 | Val Loss=2.73800450 | Data=64.84981426 | Physics=13.73073895 | Val RMSE: 2.54161429 | ‚àö(Val Loss) = 1.65469170 | Current Learning Rate: 0.001\n",
      "Epoch 703/1000 | Train Loss=6511.67658455 | Val Loss=2.72664302 | Data=64.97788226 | Physics=14.21413765 | Val RMSE: 2.53548503 | ‚àö(Val Loss) = 1.65125501 | Current Learning Rate: 0.001\n",
      "Epoch 704/1000 | Train Loss=6492.31623299 | Val Loss=2.74830656 | Data=64.78523365 | Physics=14.01450782 | Val RMSE: 2.54753780 | ‚àö(Val Loss) = 1.65780175 | Current Learning Rate: 0.001\n",
      "Epoch 705/1000 | Train Loss=6521.94517074 | Val Loss=2.71752942 | Data=65.08083959 | Physics=13.89910471 | Val RMSE: 2.53494596 | ‚àö(Val Loss) = 1.64849305 | Current Learning Rate: 0.001\n",
      "Epoch 706/1000 | Train Loss=6543.22319178 | Val Loss=2.74253818 | Data=65.29274368 | Physics=14.03697801 | Val RMSE: 2.55019188 | ‚àö(Val Loss) = 1.65606105 | Current Learning Rate: 0.001\n",
      "Epoch 707/1000 | Train Loss=6496.51998803 | Val Loss=2.74881110 | Data=64.82706808 | Physics=14.14584637 | Val RMSE: 2.55360651 | ‚àö(Val Loss) = 1.65795386 | Current Learning Rate: 0.001\n",
      "Epoch 708/1000 | Train Loss=6490.09411227 | Val Loss=2.76272615 | Data=64.76300270 | Physics=13.66015232 | Val RMSE: 2.54671979 | ‚àö(Val Loss) = 1.66214502 | Current Learning Rate: 0.001\n",
      "Epoch 709/1000 | Train Loss=6504.52030305 | Val Loss=2.70054965 | Data=64.90686527 | Physics=14.00186956 | Val RMSE: 2.53453517 | ‚àö(Val Loss) = 1.64333487 | Current Learning Rate: 0.001\n",
      "Epoch 710/1000 | Train Loss=6523.60063319 | Val Loss=2.74670058 | Data=65.09623546 | Physics=14.75888328 | Val RMSE: 2.55504227 | ‚àö(Val Loss) = 1.65731728 | Current Learning Rate: 0.001\n",
      "Epoch 711/1000 | Train Loss=6491.40818737 | Val Loss=2.70208749 | Data=64.77645849 | Physics=14.22488919 | Val RMSE: 2.53275251 | ‚àö(Val Loss) = 1.64380276 | Current Learning Rate: 0.001\n",
      "Epoch 712/1000 | Train Loss=6528.55454574 | Val Loss=2.74520258 | Data=65.14691199 | Physics=13.92982309 | Val RMSE: 2.54671383 | ‚àö(Val Loss) = 1.65686524 | Current Learning Rate: 0.001\n",
      "Epoch 713/1000 | Train Loss=6491.55864100 | Val Loss=2.74643039 | Data=64.77812835 | Physics=13.53151584 | Val RMSE: 2.55044174 | ‚àö(Val Loss) = 1.65723574 | Current Learning Rate: 0.001\n",
      "Epoch 714/1000 | Train Loss=6517.09586064 | Val Loss=2.73421196 | Data=65.03279975 | Physics=13.77732330 | Val RMSE: 2.53024411 | ‚àö(Val Loss) = 1.65354526 | Current Learning Rate: 0.001\n",
      "Epoch 715/1000 | Train Loss=6503.89905179 | Val Loss=2.74738214 | Data=64.90073985 | Physics=13.69651528 | Val RMSE: 2.55204463 | ‚àö(Val Loss) = 1.65752292 | Current Learning Rate: 0.001\n",
      "Epoch 716/1000 | Train Loss=6503.48235887 | Val Loss=2.73900794 | Data=64.89651908 | Physics=14.02384038 | Val RMSE: 2.54080653 | ‚àö(Val Loss) = 1.65499485 | Current Learning Rate: 0.001\n",
      "Epoch 717/1000 | Train Loss=6518.75061429 | Val Loss=2.77956011 | Data=65.04904064 | Physics=13.80484458 | Val RMSE: 2.54111528 | ‚àö(Val Loss) = 1.66720128 | Current Learning Rate: 0.001\n",
      "Epoch 718/1000 | Train Loss=6547.38692351 | Val Loss=2.69702365 | Data=65.33492697 | Physics=13.73857138 | Val RMSE: 2.53496337 | ‚àö(Val Loss) = 1.64226174 | Current Learning Rate: 0.001\n",
      "Epoch 719/1000 | Train Loss=6490.20925214 | Val Loss=2.70590899 | Data=64.76465274 | Physics=13.38044372 | Val RMSE: 2.52779675 | ‚àö(Val Loss) = 1.64496469 | Current Learning Rate: 0.001\n",
      "Epoch 720/1000 | Train Loss=6503.78709362 | Val Loss=2.68981357 | Data=64.89936066 | Physics=14.31035224 | Val RMSE: 2.53227568 | ‚àö(Val Loss) = 1.64006507 | Current Learning Rate: 0.001\n",
      "Epoch 721/1000 | Train Loss=6495.94753339 | Val Loss=2.75540599 | Data=64.82117979 | Physics=13.57376627 | Val RMSE: 2.55251598 | ‚àö(Val Loss) = 1.65994155 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 722/1000 | Train Loss=6491.08470892 | Val Loss=2.74363764 | Data=64.77335136 | Physics=13.85227871 | Val RMSE: 2.54764104 | ‚àö(Val Loss) = 1.65639293 | Current Learning Rate: 0.001\n",
      "Epoch 723/1000 | Train Loss=6496.31155494 | Val Loss=2.74173187 | Data=64.82527222 | Physics=13.70415396 | Val RMSE: 2.54594660 | ‚àö(Val Loss) = 1.65581763 | Current Learning Rate: 0.001\n",
      "Epoch 724/1000 | Train Loss=6491.49762160 | Val Loss=2.72781405 | Data=64.77708349 | Physics=14.31816253 | Val RMSE: 2.54508519 | ‚àö(Val Loss) = 1.65160954 | Current Learning Rate: 0.001\n",
      "Epoch 725/1000 | Train Loss=6505.39758695 | Val Loss=2.69605857 | Data=64.91596296 | Physics=14.08816055 | Val RMSE: 2.53859639 | ‚àö(Val Loss) = 1.64196789 | Current Learning Rate: 0.001\n",
      "Epoch 726/1000 | Train Loss=6501.61293473 | Val Loss=2.73818199 | Data=64.87793252 | Physics=13.88669314 | Val RMSE: 2.54612660 | ‚àö(Val Loss) = 1.65474534 | Current Learning Rate: 0.001\n",
      "Epoch 727/1000 | Train Loss=6490.61827432 | Val Loss=2.72154719 | Data=64.76811969 | Physics=13.71021605 | Val RMSE: 2.53812599 | ‚àö(Val Loss) = 1.64971125 | Current Learning Rate: 0.001\n",
      "Epoch 728/1000 | Train Loss=6505.05273438 | Val Loss=2.70600571 | Data=64.91259950 | Physics=13.48882232 | Val RMSE: 2.55026627 | ‚àö(Val Loss) = 1.64499414 | Current Learning Rate: 0.001\n",
      "Epoch 729/1000 | Train Loss=6512.33746535 | Val Loss=2.71461442 | Data=64.98449387 | Physics=13.66462868 | Val RMSE: 2.53523874 | ‚àö(Val Loss) = 1.64760864 | Current Learning Rate: 0.001\n",
      "Epoch 730/1000 | Train Loss=6516.88854587 | Val Loss=2.71046518 | Data=65.03121973 | Physics=14.19061317 | Val RMSE: 2.54118729 | ‚àö(Val Loss) = 1.64634907 | Current Learning Rate: 0.001\n",
      "Epoch 731/1000 | Train Loss=6501.41817351 | Val Loss=2.73247474 | Data=64.87542823 | Physics=13.59643250 | Val RMSE: 2.53511047 | ‚àö(Val Loss) = 1.65301991 | Current Learning Rate: 0.001\n",
      "Epoch 732/1000 | Train Loss=6502.39610635 | Val Loss=2.70320393 | Data=64.88565900 | Physics=13.67788258 | Val RMSE: 2.54104090 | ‚àö(Val Loss) = 1.64414227 | Current Learning Rate: 0.001\n",
      "Epoch 733/1000 | Train Loss=6493.52005103 | Val Loss=2.73632224 | Data=64.79730803 | Physics=13.73298055 | Val RMSE: 2.54361057 | ‚àö(Val Loss) = 1.65418327 | Current Learning Rate: 0.001\n",
      "Epoch 734/1000 | Train Loss=6497.17748236 | Val Loss=2.70442126 | Data=64.83359971 | Physics=13.54563317 | Val RMSE: 2.53104663 | ‚àö(Val Loss) = 1.64451253 | Current Learning Rate: 0.001\n",
      "Epoch 735/1000 | Train Loss=7099.42300907 | Val Loss=2.74112799 | Data=70.85491316 | Physics=13.09986706 | Val RMSE: 2.53150368 | ‚àö(Val Loss) = 1.65563524 | Current Learning Rate: 0.001\n",
      "Epoch 736/1000 | Train Loss=6499.78992881 | Val Loss=2.75318233 | Data=64.85977862 | Physics=13.89074826 | Val RMSE: 2.53664327 | ‚àö(Val Loss) = 1.65927160 | Current Learning Rate: 0.001\n",
      "Epoch 737/1000 | Train Loss=6501.20257371 | Val Loss=2.72650742 | Data=64.87368897 | Physics=13.77232481 | Val RMSE: 2.53338790 | ‚àö(Val Loss) = 1.65121388 | Current Learning Rate: 0.001\n",
      "Epoch 738/1000 | Train Loss=6490.30287613 | Val Loss=2.73526343 | Data=64.76505464 | Physics=13.41394527 | Val RMSE: 2.53661275 | ‚àö(Val Loss) = 1.65386319 | Current Learning Rate: 0.001\n",
      "Epoch 739/1000 | Train Loss=6496.01286857 | Val Loss=2.76364079 | Data=64.82156409 | Physics=13.34105247 | Val RMSE: 2.54195023 | ‚àö(Val Loss) = 1.66242015 | Current Learning Rate: 0.001\n",
      "Epoch 740/1000 | Train Loss=6514.30901903 | Val Loss=2.77720314 | Data=65.00518811 | Physics=13.73862291 | Val RMSE: 2.56681252 | ‚àö(Val Loss) = 1.66649425 | Current Learning Rate: 0.001\n",
      "Epoch 741/1000 | Train Loss=6537.79585118 | Val Loss=2.75959573 | Data=65.23849081 | Physics=14.09239804 | Val RMSE: 2.54988790 | ‚àö(Val Loss) = 1.66120303 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 742/1000 | Train Loss=6497.87232233 | Val Loss=2.75305422 | Data=64.84079201 | Physics=13.19998284 | Val RMSE: 2.55012107 | ‚àö(Val Loss) = 1.65923297 | Current Learning Rate: 0.001\n",
      "Epoch 743/1000 | Train Loss=6510.85468120 | Val Loss=2.73482553 | Data=64.96958542 | Physics=14.10570166 | Val RMSE: 2.54302454 | ‚àö(Val Loss) = 1.65373087 | Current Learning Rate: 0.001\n",
      "Epoch 744/1000 | Train Loss=6494.75508758 | Val Loss=2.74814079 | Data=64.80995363 | Physics=13.28216343 | Val RMSE: 2.55567980 | ‚àö(Val Loss) = 1.65775168 | Current Learning Rate: 0.001\n",
      "Epoch 745/1000 | Train Loss=6551.82116305 | Val Loss=2.75241312 | Data=65.37845858 | Physics=13.58661893 | Val RMSE: 2.56004715 | ‚àö(Val Loss) = 1.65903974 | Current Learning Rate: 0.001\n",
      "Epoch 746/1000 | Train Loss=6517.45419607 | Val Loss=2.73382464 | Data=65.03557205 | Physics=14.03392912 | Val RMSE: 2.54471159 | ‚àö(Val Loss) = 1.65342820 | Current Learning Rate: 0.001\n",
      "Epoch 747/1000 | Train Loss=6503.57314768 | Val Loss=2.72055561 | Data=64.89772809 | Physics=13.88106907 | Val RMSE: 2.54455066 | ‚àö(Val Loss) = 1.64941061 | Current Learning Rate: 0.001\n",
      "Epoch 748/1000 | Train Loss=6493.14303490 | Val Loss=2.77324672 | Data=64.79303188 | Physics=13.68139023 | Val RMSE: 2.56357455 | ‚àö(Val Loss) = 1.66530681 | Current Learning Rate: 0.001\n",
      "Epoch 749/1000 | Train Loss=6498.39158581 | Val Loss=2.71189744 | Data=64.84561982 | Physics=13.20264843 | Val RMSE: 2.53199959 | ‚àö(Val Loss) = 1.64678395 | Current Learning Rate: 0.001\n",
      "Epoch 750/1000 | Train Loss=6497.36050907 | Val Loss=2.71479057 | Data=64.83581543 | Physics=13.93479881 | Val RMSE: 2.53203487 | ‚àö(Val Loss) = 1.64766216 | Current Learning Rate: 0.001\n",
      "Epoch 751/1000 | Train Loss=6491.01469569 | Val Loss=2.76638045 | Data=64.77202397 | Physics=13.53224970 | Val RMSE: 2.55029368 | ‚àö(Val Loss) = 1.66324401 | Current Learning Rate: 0.001\n",
      "Epoch 752/1000 | Train Loss=6501.16390499 | Val Loss=2.72844507 | Data=64.87347769 | Physics=13.89646408 | Val RMSE: 2.53429270 | ‚àö(Val Loss) = 1.65180051 | Current Learning Rate: 0.001\n",
      "Epoch 753/1000 | Train Loss=6512.80838899 | Val Loss=2.74683838 | Data=64.98918213 | Physics=13.77451708 | Val RMSE: 2.53651786 | ‚àö(Val Loss) = 1.65735888 | Current Learning Rate: 0.001\n",
      "Epoch 754/1000 | Train Loss=6496.17132371 | Val Loss=2.74506474 | Data=64.82363116 | Physics=14.10495370 | Val RMSE: 2.55628657 | ‚àö(Val Loss) = 1.65682364 | Current Learning Rate: 0.001\n",
      "Epoch 755/1000 | Train Loss=6545.45759829 | Val Loss=2.71746279 | Data=65.31508021 | Physics=13.71644459 | Val RMSE: 2.53740096 | ‚àö(Val Loss) = 1.64847291 | Current Learning Rate: 0.001\n",
      "Epoch 756/1000 | Train Loss=6525.10351562 | Val Loss=2.74739973 | Data=65.11141352 | Physics=13.72111715 | Val RMSE: 2.54623747 | ‚àö(Val Loss) = 1.65752816 | Current Learning Rate: 0.001\n",
      "Epoch 757/1000 | Train Loss=6513.29446510 | Val Loss=2.75348972 | Data=64.99475012 | Physics=13.72596012 | Val RMSE: 2.55183983 | ‚àö(Val Loss) = 1.65936422 | Current Learning Rate: 0.001\n",
      "Epoch 758/1000 | Train Loss=6551.39577558 | Val Loss=2.69861636 | Data=65.37383086 | Physics=14.17844726 | Val RMSE: 2.53485227 | ‚àö(Val Loss) = 1.64274657 | Current Learning Rate: 0.001\n",
      "Epoch 759/1000 | Train Loss=6548.32168284 | Val Loss=2.70284820 | Data=65.34431261 | Physics=13.84777135 | Val RMSE: 2.53617787 | ‚àö(Val Loss) = 1.64403415 | Current Learning Rate: 0.001\n",
      "Epoch 760/1000 | Train Loss=6502.37492124 | Val Loss=2.70343957 | Data=64.88556413 | Physics=13.77879035 | Val RMSE: 2.54501653 | ‚àö(Val Loss) = 1.64421391 | Current Learning Rate: 0.001\n",
      "Epoch 761/1000 | Train Loss=6521.84499433 | Val Loss=2.75998238 | Data=65.07975006 | Physics=13.79661185 | Val RMSE: 2.55536628 | ‚àö(Val Loss) = 1.66131949 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 762/1000 | Train Loss=6507.92587576 | Val Loss=2.73913739 | Data=64.94051472 | Physics=13.73017280 | Val RMSE: 2.54723406 | ‚àö(Val Loss) = 1.65503395 | Current Learning Rate: 0.001\n",
      "Epoch 763/1000 | Train Loss=6492.94124874 | Val Loss=2.74710619 | Data=64.79160014 | Physics=13.91235076 | Val RMSE: 2.55281353 | ‚àö(Val Loss) = 1.65743959 | Current Learning Rate: 0.001\n",
      "Epoch 764/1000 | Train Loss=6496.92244204 | Val Loss=2.74710477 | Data=64.83065021 | Physics=13.80141973 | Val RMSE: 2.54913831 | ‚àö(Val Loss) = 1.65743923 | Current Learning Rate: 0.001\n",
      "Epoch 765/1000 | Train Loss=6518.28128150 | Val Loss=2.81165625 | Data=65.04395060 | Physics=13.81646930 | Val RMSE: 2.54260135 | ‚àö(Val Loss) = 1.67679942 | Current Learning Rate: 0.001\n",
      "Epoch 766/1000 | Train Loss=6523.32536857 | Val Loss=2.76641463 | Data=65.09442459 | Physics=13.59297007 | Val RMSE: 2.55198836 | ‚àö(Val Loss) = 1.66325426 | Current Learning Rate: 0.001\n",
      "Epoch 767/1000 | Train Loss=6518.64105028 | Val Loss=2.73753318 | Data=65.04770217 | Physics=13.50399360 | Val RMSE: 2.53322673 | ‚àö(Val Loss) = 1.65454924 | Current Learning Rate: 0.001\n",
      "Epoch 768/1000 | Train Loss=6507.21703314 | Val Loss=2.71586546 | Data=64.93387481 | Physics=13.91380257 | Val RMSE: 2.53472257 | ‚àö(Val Loss) = 1.64798832 | Current Learning Rate: 0.001\n",
      "Epoch 769/1000 | Train Loss=6502.31527218 | Val Loss=2.69575080 | Data=64.88487945 | Physics=13.97182578 | Val RMSE: 2.53798151 | ‚àö(Val Loss) = 1.64187419 | Current Learning Rate: 0.001\n",
      "Epoch 770/1000 | Train Loss=6503.56527218 | Val Loss=2.79178059 | Data=64.89716438 | Physics=14.36122851 | Val RMSE: 2.56398940 | ‚àö(Val Loss) = 1.67086220 | Current Learning Rate: 0.001\n",
      "Epoch 771/1000 | Train Loss=6535.96085874 | Val Loss=2.76468018 | Data=65.22044754 | Physics=13.54557677 | Val RMSE: 2.54759121 | ‚àö(Val Loss) = 1.66273272 | Current Learning Rate: 0.001\n",
      "Epoch 772/1000 | Train Loss=6492.94540701 | Val Loss=2.74445781 | Data=64.79165994 | Physics=13.86390307 | Val RMSE: 2.55212855 | ‚àö(Val Loss) = 1.65664053 | Current Learning Rate: 0.001\n",
      "Epoch 773/1000 | Train Loss=6548.26285282 | Val Loss=2.71425737 | Data=65.34266367 | Physics=13.91836520 | Val RMSE: 2.53881812 | ‚àö(Val Loss) = 1.64750040 | Current Learning Rate: 0.001\n",
      "Epoch 774/1000 | Train Loss=6508.18502709 | Val Loss=2.73801905 | Data=64.94358690 | Physics=14.35138917 | Val RMSE: 2.53564739 | ‚àö(Val Loss) = 1.65469599 | Current Learning Rate: 0.001\n",
      "Epoch 775/1000 | Train Loss=6760.22761782 | Val Loss=2.76675189 | Data=67.46301823 | Physics=13.57814382 | Val RMSE: 2.53531003 | ‚àö(Val Loss) = 1.66335559 | Current Learning Rate: 0.001\n",
      "Epoch 776/1000 | Train Loss=6540.24029738 | Val Loss=2.75500992 | Data=65.26326481 | Physics=13.37026273 | Val RMSE: 2.56323409 | ‚àö(Val Loss) = 1.65982223 | Current Learning Rate: 0.001\n",
      "Epoch 777/1000 | Train Loss=6502.87931578 | Val Loss=2.73753421 | Data=64.88993528 | Physics=13.59893302 | Val RMSE: 2.53965688 | ‚àö(Val Loss) = 1.65454960 | Current Learning Rate: 0.001\n",
      "Epoch 778/1000 | Train Loss=6489.30705015 | Val Loss=2.71194441 | Data=64.75566889 | Physics=13.94974560 | Val RMSE: 2.53995681 | ‚àö(Val Loss) = 1.64679825 | Current Learning Rate: 0.001\n",
      "Epoch 779/1000 | Train Loss=6493.15412361 | Val Loss=2.70640370 | Data=64.79383764 | Physics=13.55461179 | Val RMSE: 2.53545642 | ‚àö(Val Loss) = 1.64511514 | Current Learning Rate: 0.001\n",
      "Epoch 780/1000 | Train Loss=6509.42408014 | Val Loss=2.69816623 | Data=64.95581055 | Physics=13.58903955 | Val RMSE: 2.53200579 | ‚àö(Val Loss) = 1.64260960 | Current Learning Rate: 0.001\n",
      "Epoch 781/1000 | Train Loss=6495.75327621 | Val Loss=2.76145371 | Data=64.81927490 | Physics=13.82201351 | Val RMSE: 2.54407811 | ‚àö(Val Loss) = 1.66176224 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 782/1000 | Train Loss=6492.76995653 | Val Loss=2.72949611 | Data=64.78981215 | Physics=13.65239228 | Val RMSE: 2.53592682 | ‚àö(Val Loss) = 1.65211868 | Current Learning Rate: 0.001\n",
      "Epoch 783/1000 | Train Loss=6492.20649572 | Val Loss=2.77793591 | Data=64.78443380 | Physics=13.92594588 | Val RMSE: 2.53874922 | ‚àö(Val Loss) = 1.66671407 | Current Learning Rate: 0.001\n",
      "Epoch 784/1000 | Train Loss=6489.04386656 | Val Loss=2.72726904 | Data=64.75260039 | Physics=13.91795464 | Val RMSE: 2.54443598 | ‚àö(Val Loss) = 1.65144455 | Current Learning Rate: 0.001\n",
      "Epoch 785/1000 | Train Loss=6492.06930444 | Val Loss=2.72528042 | Data=64.78244031 | Physics=14.06088710 | Val RMSE: 2.53782797 | ‚àö(Val Loss) = 1.65084231 | Current Learning Rate: 0.001\n",
      "Epoch 786/1000 | Train Loss=6501.43423954 | Val Loss=2.68516669 | Data=64.87597816 | Physics=13.19502614 | Val RMSE: 2.52882552 | ‚àö(Val Loss) = 1.63864779 | Current Learning Rate: 0.001\n",
      "Epoch 787/1000 | Train Loss=6508.67713584 | Val Loss=2.72819703 | Data=64.94856656 | Physics=14.06012971 | Val RMSE: 2.54657936 | ‚àö(Val Loss) = 1.65172553 | Current Learning Rate: 0.001\n",
      "Epoch 788/1000 | Train Loss=6498.40269027 | Val Loss=2.71755173 | Data=64.84615769 | Physics=13.70224567 | Val RMSE: 2.53898168 | ‚àö(Val Loss) = 1.64849985 | Current Learning Rate: 0.001\n",
      "Epoch 789/1000 | Train Loss=6510.21166205 | Val Loss=2.70614644 | Data=64.96386251 | Physics=13.79258071 | Val RMSE: 2.53412056 | ‚àö(Val Loss) = 1.64503694 | Current Learning Rate: 0.001\n",
      "Epoch 790/1000 | Train Loss=6527.32733745 | Val Loss=2.75219211 | Data=65.13432091 | Physics=13.93596215 | Val RMSE: 2.55586481 | ‚àö(Val Loss) = 1.65897322 | Current Learning Rate: 0.001\n",
      "Epoch 791/1000 | Train Loss=6500.44310736 | Val Loss=2.74467382 | Data=64.86621364 | Physics=13.86025886 | Val RMSE: 2.53324032 | ‚àö(Val Loss) = 1.65670574 | Current Learning Rate: 0.001\n",
      "Epoch 792/1000 | Train Loss=6500.48908455 | Val Loss=2.73495058 | Data=64.86713508 | Physics=13.74944654 | Val RMSE: 2.53997874 | ‚àö(Val Loss) = 1.65376854 | Current Learning Rate: 0.001\n",
      "Epoch 793/1000 | Train Loss=6508.20014806 | Val Loss=2.72747568 | Data=64.94366061 | Physics=13.51059956 | Val RMSE: 2.53234816 | ‚àö(Val Loss) = 1.65150714 | Current Learning Rate: 0.001\n",
      "Epoch 794/1000 | Train Loss=6490.10680759 | Val Loss=2.75428349 | Data=64.76346564 | Physics=14.00472282 | Val RMSE: 2.54305577 | ‚àö(Val Loss) = 1.65960336 | Current Learning Rate: 0.001\n",
      "Epoch 795/1000 | Train Loss=6493.29151966 | Val Loss=2.71976225 | Data=64.79466211 | Physics=13.76420904 | Val RMSE: 2.53509474 | ‚àö(Val Loss) = 1.64917016 | Current Learning Rate: 0.001\n",
      "Epoch 796/1000 | Train Loss=6492.98941532 | Val Loss=2.75854919 | Data=64.79130222 | Physics=13.59994022 | Val RMSE: 2.54835367 | ‚àö(Val Loss) = 1.66088808 | Current Learning Rate: 0.001\n",
      "Epoch 797/1000 | Train Loss=6497.81569745 | Val Loss=2.72685340 | Data=64.84024946 | Physics=13.85440240 | Val RMSE: 2.54155588 | ‚àö(Val Loss) = 1.65131867 | Current Learning Rate: 0.001\n",
      "Epoch 798/1000 | Train Loss=6499.22879914 | Val Loss=2.73827961 | Data=64.85325438 | Physics=13.70396471 | Val RMSE: 2.53932667 | ‚àö(Val Loss) = 1.65477479 | Current Learning Rate: 0.001\n",
      "Epoch 799/1000 | Train Loss=6490.88000882 | Val Loss=2.70480435 | Data=64.77100840 | Physics=13.40772351 | Val RMSE: 2.53348541 | ‚àö(Val Loss) = 1.64462900 | Current Learning Rate: 0.001\n",
      "Epoch 800/1000 | Train Loss=6517.48828125 | Val Loss=2.69257752 | Data=65.03637498 | Physics=13.56159324 | Val RMSE: 2.53735638 | ‚àö(Val Loss) = 1.64090753 | Current Learning Rate: 0.0001\n",
      "Epoch 801/1000 | Train Loss=6489.30193107 | Val Loss=2.73466588 | Data=64.75535214 | Physics=13.73838606 | Val RMSE: 2.54524136 | ‚àö(Val Loss) = 1.65368247 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 802/1000 | Train Loss=6487.78605406 | Val Loss=2.72128353 | Data=64.74050485 | Physics=13.10893136 | Val RMSE: 2.54314518 | ‚àö(Val Loss) = 1.64963126 | Current Learning Rate: 0.0001\n",
      "Epoch 803/1000 | Train Loss=6484.93529486 | Val Loss=2.73432961 | Data=64.71185844 | Physics=14.01129750 | Val RMSE: 2.54929805 | ‚àö(Val Loss) = 1.65358090 | Current Learning Rate: 0.0001\n",
      "Epoch 804/1000 | Train Loss=6486.23771421 | Val Loss=2.71949811 | Data=64.72510504 | Physics=13.55678601 | Val RMSE: 2.54061985 | ‚àö(Val Loss) = 1.64909005 | Current Learning Rate: 0.0001\n",
      "Epoch 805/1000 | Train Loss=6486.49355784 | Val Loss=2.70606858 | Data=64.72761905 | Physics=14.00358636 | Val RMSE: 2.53833938 | ‚àö(Val Loss) = 1.64501321 | Current Learning Rate: 0.0001\n",
      "Epoch 806/1000 | Train Loss=6484.16207787 | Val Loss=2.72375046 | Data=64.70433020 | Physics=13.81578122 | Val RMSE: 2.53888512 | ‚àö(Val Loss) = 1.65037882 | Current Learning Rate: 0.0001\n",
      "Epoch 807/1000 | Train Loss=6497.26734186 | Val Loss=2.73526674 | Data=64.83523867 | Physics=13.94410648 | Val RMSE: 2.54710102 | ‚àö(Val Loss) = 1.65386415 | Current Learning Rate: 0.0001\n",
      "Epoch 808/1000 | Train Loss=6488.53619582 | Val Loss=2.72648499 | Data=64.74786377 | Physics=14.03671325 | Val RMSE: 2.54103756 | ‚àö(Val Loss) = 1.65120709 | Current Learning Rate: 0.0001\n",
      "Epoch 809/1000 | Train Loss=6491.63402533 | Val Loss=2.72811497 | Data=64.77868111 | Physics=13.67654518 | Val RMSE: 2.54728842 | ‚àö(Val Loss) = 1.65170062 | Current Learning Rate: 0.0001\n",
      "Epoch 810/1000 | Train Loss=6491.03093498 | Val Loss=2.70194386 | Data=64.77303400 | Physics=13.74890649 | Val RMSE: 2.54014969 | ‚àö(Val Loss) = 1.64375901 | Current Learning Rate: 0.0001\n",
      "Epoch 811/1000 | Train Loss=6486.78173828 | Val Loss=2.72143097 | Data=64.73027137 | Physics=13.61564759 | Val RMSE: 2.54216623 | ‚àö(Val Loss) = 1.64967608 | Current Learning Rate: 0.0001\n",
      "Epoch 812/1000 | Train Loss=6487.87208606 | Val Loss=2.72693906 | Data=64.74145114 | Physics=13.70477284 | Val RMSE: 2.54105806 | ‚àö(Val Loss) = 1.65134454 | Current Learning Rate: 0.0001\n",
      "Epoch 813/1000 | Train Loss=6489.23324093 | Val Loss=2.72390164 | Data=64.75483470 | Physics=13.67152259 | Val RMSE: 2.54124188 | ‚àö(Val Loss) = 1.65042472 | Current Learning Rate: 0.0001\n",
      "Epoch 814/1000 | Train Loss=6487.84663243 | Val Loss=2.73529885 | Data=64.74108198 | Physics=14.30658858 | Val RMSE: 2.54729986 | ‚àö(Val Loss) = 1.65387392 | Current Learning Rate: 0.0001\n",
      "Epoch 815/1000 | Train Loss=6483.61156439 | Val Loss=2.71048794 | Data=64.69872912 | Physics=14.15890430 | Val RMSE: 2.53880596 | ‚àö(Val Loss) = 1.64635599 | Current Learning Rate: 0.0001\n",
      "Epoch 816/1000 | Train Loss=6485.24708606 | Val Loss=2.72014767 | Data=64.71511262 | Physics=13.97546839 | Val RMSE: 2.54218149 | ‚àö(Val Loss) = 1.64928699 | Current Learning Rate: 0.0001\n",
      "Epoch 817/1000 | Train Loss=6487.79807208 | Val Loss=2.72666365 | Data=64.74056995 | Physics=13.68153657 | Val RMSE: 2.54356575 | ‚àö(Val Loss) = 1.65126121 | Current Learning Rate: 0.0001\n",
      "Epoch 818/1000 | Train Loss=6490.91124307 | Val Loss=2.71880462 | Data=64.77168569 | Physics=13.76573560 | Val RMSE: 2.54367304 | ‚àö(Val Loss) = 1.64887977 | Current Learning Rate: 0.0001\n",
      "Epoch 819/1000 | Train Loss=6489.76395539 | Val Loss=2.72860272 | Data=64.76011178 | Physics=13.27002849 | Val RMSE: 2.54800653 | ‚àö(Val Loss) = 1.65184820 | Current Learning Rate: 0.0001\n",
      "Epoch 820/1000 | Train Loss=6487.24623551 | Val Loss=2.72498575 | Data=64.73496283 | Physics=13.19803719 | Val RMSE: 2.54248810 | ‚àö(Val Loss) = 1.65075314 | Current Learning Rate: 0.0001\n",
      "Epoch 821/1000 | Train Loss=6494.00177986 | Val Loss=2.74467470 | Data=64.80262818 | Physics=13.48811538 | Val RMSE: 2.55048752 | ‚àö(Val Loss) = 1.65670598 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 822/1000 | Train Loss=6492.18123110 | Val Loss=2.74865995 | Data=64.78427259 | Physics=13.82371598 | Val RMSE: 2.55405092 | ‚àö(Val Loss) = 1.65790832 | Current Learning Rate: 0.0001\n",
      "Epoch 823/1000 | Train Loss=6488.72547568 | Val Loss=2.72454793 | Data=64.74983314 | Physics=13.80910192 | Val RMSE: 2.54459929 | ‚àö(Val Loss) = 1.65062046 | Current Learning Rate: 0.0001\n",
      "Epoch 824/1000 | Train Loss=6486.26406565 | Val Loss=2.73093779 | Data=64.72504634 | Physics=13.54087477 | Val RMSE: 2.54604030 | ‚àö(Val Loss) = 1.65255487 | Current Learning Rate: 0.0001\n",
      "Epoch 825/1000 | Train Loss=6487.25989163 | Val Loss=2.71022789 | Data=64.73480348 | Physics=14.22931866 | Val RMSE: 2.54193878 | ‚àö(Val Loss) = 1.64627695 | Current Learning Rate: 0.0001\n",
      "Epoch 826/1000 | Train Loss=6489.82818800 | Val Loss=2.74221878 | Data=64.76077824 | Physics=13.73454706 | Val RMSE: 2.55080795 | ‚àö(Val Loss) = 1.65596461 | Current Learning Rate: 0.0001\n",
      "Epoch 827/1000 | Train Loss=6489.54975743 | Val Loss=2.72382862 | Data=64.75813220 | Physics=13.45266707 | Val RMSE: 2.54256344 | ‚àö(Val Loss) = 1.65040255 | Current Learning Rate: 0.0001\n",
      "Epoch 828/1000 | Train Loss=6490.15591923 | Val Loss=2.74088425 | Data=64.76379899 | Physics=13.85844134 | Val RMSE: 2.54884195 | ‚àö(Val Loss) = 1.65556169 | Current Learning Rate: 0.0001\n",
      "Epoch 829/1000 | Train Loss=6489.71408770 | Val Loss=2.72892731 | Data=64.75955028 | Physics=13.94462726 | Val RMSE: 2.54750681 | ‚àö(Val Loss) = 1.65194654 | Current Learning Rate: 0.0001\n",
      "Epoch 830/1000 | Train Loss=6494.54529990 | Val Loss=2.75305408 | Data=64.80796383 | Physics=13.84670091 | Val RMSE: 2.55334044 | ‚àö(Val Loss) = 1.65923297 | Current Learning Rate: 0.0001\n",
      "Epoch 831/1000 | Train Loss=6484.22467238 | Val Loss=2.71638911 | Data=64.70491065 | Physics=13.80083893 | Val RMSE: 2.54122305 | ‚àö(Val Loss) = 1.64814723 | Current Learning Rate: 0.0001\n",
      "Epoch 832/1000 | Train Loss=6485.99207724 | Val Loss=2.73037950 | Data=64.72236006 | Physics=13.83295910 | Val RMSE: 2.54652929 | ‚àö(Val Loss) = 1.65238607 | Current Learning Rate: 0.0001\n",
      "Epoch 833/1000 | Train Loss=6486.41084929 | Val Loss=2.72981647 | Data=64.72681156 | Physics=14.14190436 | Val RMSE: 2.54492426 | ‚àö(Val Loss) = 1.65221560 | Current Learning Rate: 0.0001\n",
      "Epoch 834/1000 | Train Loss=6487.66195186 | Val Loss=2.71065422 | Data=64.73895424 | Physics=13.92456836 | Val RMSE: 2.54196119 | ‚àö(Val Loss) = 1.64640641 | Current Learning Rate: 0.0001\n",
      "Epoch 835/1000 | Train Loss=6486.14358619 | Val Loss=2.73954839 | Data=64.72410694 | Physics=14.33639970 | Val RMSE: 2.54977107 | ‚àö(Val Loss) = 1.65515816 | Current Learning Rate: 0.0001\n",
      "Epoch 836/1000 | Train Loss=6489.01371913 | Val Loss=2.72350198 | Data=64.75254034 | Physics=13.87182645 | Val RMSE: 2.54282618 | ‚àö(Val Loss) = 1.65030360 | Current Learning Rate: 0.0001\n",
      "Epoch 837/1000 | Train Loss=6489.64284589 | Val Loss=2.73771982 | Data=64.75898275 | Physics=13.47285355 | Val RMSE: 2.54991579 | ‚àö(Val Loss) = 1.65460563 | Current Learning Rate: 0.0001\n",
      "Epoch 838/1000 | Train Loss=6488.27005103 | Val Loss=2.72302597 | Data=64.74520591 | Physics=14.06814465 | Val RMSE: 2.54584289 | ‚àö(Val Loss) = 1.65015936 | Current Learning Rate: 0.0001\n",
      "Epoch 839/1000 | Train Loss=6485.87677986 | Val Loss=2.73045356 | Data=64.72142976 | Physics=13.77087537 | Val RMSE: 2.54657054 | ‚àö(Val Loss) = 1.65240836 | Current Learning Rate: 0.0001\n",
      "Epoch 840/1000 | Train Loss=6490.22408959 | Val Loss=2.73400235 | Data=64.76460611 | Physics=13.69401003 | Val RMSE: 2.54970622 | ‚àö(Val Loss) = 1.65348184 | Current Learning Rate: 0.0001\n",
      "Epoch 841/1000 | Train Loss=6494.60211379 | Val Loss=2.73797901 | Data=64.80836278 | Physics=14.00889395 | Val RMSE: 2.54687285 | ‚àö(Val Loss) = 1.65468395 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 842/1000 | Train Loss=6491.62419670 | Val Loss=2.73866573 | Data=64.77869538 | Physics=13.92147554 | Val RMSE: 2.54829741 | ‚àö(Val Loss) = 1.65489149 | Current Learning Rate: 0.0001\n",
      "Epoch 843/1000 | Train Loss=6488.88065461 | Val Loss=2.70495329 | Data=64.75121923 | Physics=13.81697031 | Val RMSE: 2.53901768 | ‚àö(Val Loss) = 1.64467418 | Current Learning Rate: 0.0001\n",
      "Epoch 844/1000 | Train Loss=6489.48365045 | Val Loss=2.74836329 | Data=64.75714751 | Physics=13.92008077 | Val RMSE: 2.55158472 | ‚àö(Val Loss) = 1.65781879 | Current Learning Rate: 0.0001\n",
      "Epoch 845/1000 | Train Loss=6491.41196762 | Val Loss=2.73794552 | Data=64.77632769 | Physics=14.03606857 | Val RMSE: 2.55222058 | ‚àö(Val Loss) = 1.65467381 | Current Learning Rate: 0.0001\n",
      "Epoch 846/1000 | Train Loss=6492.95402281 | Val Loss=2.72181491 | Data=64.79206024 | Physics=13.82737646 | Val RMSE: 2.54376721 | ‚àö(Val Loss) = 1.64979231 | Current Learning Rate: 0.0001\n",
      "Epoch 847/1000 | Train Loss=6493.31136593 | Val Loss=2.73535513 | Data=64.79564581 | Physics=13.97148742 | Val RMSE: 2.54875422 | ‚àö(Val Loss) = 1.65389097 | Current Learning Rate: 0.0001\n",
      "Epoch 848/1000 | Train Loss=6486.84474231 | Val Loss=2.72619069 | Data=64.73081429 | Physics=13.32899237 | Val RMSE: 2.53829575 | ‚àö(Val Loss) = 1.65111804 | Current Learning Rate: 0.0001\n",
      "Epoch 849/1000 | Train Loss=6487.82609312 | Val Loss=2.70204956 | Data=64.74087980 | Physics=13.56422367 | Val RMSE: 2.53652835 | ‚àö(Val Loss) = 1.64379120 | Current Learning Rate: 0.0001\n",
      "Epoch 850/1000 | Train Loss=6486.85020791 | Val Loss=2.75017092 | Data=64.73099949 | Physics=14.09814046 | Val RMSE: 2.55274248 | ‚àö(Val Loss) = 1.65836394 | Current Learning Rate: 0.0001\n",
      "Epoch 851/1000 | Train Loss=6485.88536416 | Val Loss=2.71546462 | Data=64.72137980 | Physics=13.72362527 | Val RMSE: 2.54303074 | ‚àö(Val Loss) = 1.64786673 | Current Learning Rate: 0.0001\n",
      "Epoch 852/1000 | Train Loss=6488.04977319 | Val Loss=2.72365940 | Data=64.74311472 | Physics=13.26867193 | Val RMSE: 2.54124427 | ‚àö(Val Loss) = 1.65035129 | Current Learning Rate: 0.0001\n",
      "Epoch 853/1000 | Train Loss=6487.39530305 | Val Loss=2.73410249 | Data=64.73654716 | Physics=13.36115235 | Val RMSE: 2.54603052 | ‚àö(Val Loss) = 1.65351212 | Current Learning Rate: 0.0001\n",
      "Epoch 854/1000 | Train Loss=6486.11339151 | Val Loss=2.72625239 | Data=64.72353966 | Physics=13.26502422 | Val RMSE: 2.54511929 | ‚àö(Val Loss) = 1.65113664 | Current Learning Rate: 0.0001\n",
      "Epoch 855/1000 | Train Loss=6485.49377835 | Val Loss=2.72040811 | Data=64.71752807 | Physics=13.19651482 | Val RMSE: 2.54363179 | ‚àö(Val Loss) = 1.64936602 | Current Learning Rate: 0.0001\n",
      "Epoch 856/1000 | Train Loss=6490.21032321 | Val Loss=2.73479175 | Data=64.76453757 | Physics=13.24193026 | Val RMSE: 2.54503417 | ‚àö(Val Loss) = 1.65372062 | Current Learning Rate: 0.0001\n",
      "Epoch 857/1000 | Train Loss=6487.64683090 | Val Loss=2.70833773 | Data=64.73928144 | Physics=13.65955182 | Val RMSE: 2.54243422 | ‚àö(Val Loss) = 1.64570284 | Current Learning Rate: 0.0001\n",
      "Epoch 858/1000 | Train Loss=6491.49569997 | Val Loss=2.73110264 | Data=64.77745204 | Physics=14.00838067 | Val RMSE: 2.54631543 | ‚àö(Val Loss) = 1.65260482 | Current Learning Rate: 0.0001\n",
      "Epoch 859/1000 | Train Loss=6494.80248236 | Val Loss=2.74240818 | Data=64.81028329 | Physics=13.75130578 | Val RMSE: 2.54802704 | ‚àö(Val Loss) = 1.65602183 | Current Learning Rate: 0.0001\n",
      "Epoch 860/1000 | Train Loss=6487.50576487 | Val Loss=2.71166458 | Data=64.73782484 | Physics=13.63275031 | Val RMSE: 2.54333186 | ‚àö(Val Loss) = 1.64671326 | Current Learning Rate: 0.0001\n",
      "Epoch 861/1000 | Train Loss=6489.00815902 | Val Loss=2.72133582 | Data=64.75264137 | Physics=13.20166780 | Val RMSE: 2.54356503 | ‚àö(Val Loss) = 1.64964724 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 862/1000 | Train Loss=6498.62859123 | Val Loss=2.74062485 | Data=64.84871169 | Physics=13.66481005 | Val RMSE: 2.54929352 | ‚àö(Val Loss) = 1.65548325 | Current Learning Rate: 0.0001\n",
      "Epoch 863/1000 | Train Loss=6487.38122165 | Val Loss=2.72765935 | Data=64.73623436 | Physics=13.97170613 | Val RMSE: 2.54524708 | ‚àö(Val Loss) = 1.65156269 | Current Learning Rate: 0.0001\n",
      "Epoch 864/1000 | Train Loss=6491.58040890 | Val Loss=2.72159725 | Data=64.77856618 | Physics=13.77327829 | Val RMSE: 2.54190135 | ‚àö(Val Loss) = 1.64972639 | Current Learning Rate: 0.0001\n",
      "Epoch 865/1000 | Train Loss=6485.84877457 | Val Loss=2.72358503 | Data=64.72122771 | Physics=13.27403881 | Val RMSE: 2.54485130 | ‚àö(Val Loss) = 1.65032876 | Current Learning Rate: 0.0001\n",
      "Epoch 866/1000 | Train Loss=6486.38641948 | Val Loss=2.71460894 | Data=64.72652140 | Physics=13.70844893 | Val RMSE: 2.54162908 | ‚àö(Val Loss) = 1.64760697 | Current Learning Rate: 0.0001\n",
      "Epoch 867/1000 | Train Loss=6484.78840096 | Val Loss=2.71259085 | Data=64.71048638 | Physics=13.66838157 | Val RMSE: 2.53984952 | ‚àö(Val Loss) = 1.64699447 | Current Learning Rate: 0.0001\n",
      "Epoch 868/1000 | Train Loss=6488.78321888 | Val Loss=2.71626019 | Data=64.75039931 | Physics=13.64100070 | Val RMSE: 2.54246259 | ‚àö(Val Loss) = 1.64810801 | Current Learning Rate: 0.0001\n",
      "Epoch 869/1000 | Train Loss=6488.84672694 | Val Loss=2.71882576 | Data=64.75072885 | Physics=14.05438925 | Val RMSE: 2.54433584 | ‚àö(Val Loss) = 1.64888620 | Current Learning Rate: 0.0001\n",
      "Epoch 870/1000 | Train Loss=6492.81116116 | Val Loss=2.74439626 | Data=64.79071599 | Physics=13.53288338 | Val RMSE: 2.54939747 | ‚àö(Val Loss) = 1.65662193 | Current Learning Rate: 0.0001\n",
      "Epoch 871/1000 | Train Loss=6487.57363596 | Val Loss=2.73435546 | Data=64.73819080 | Physics=14.04052104 | Val RMSE: 2.54847646 | ‚àö(Val Loss) = 1.65358865 | Current Learning Rate: 0.0001\n",
      "Epoch 872/1000 | Train Loss=6486.94113848 | Val Loss=2.70705958 | Data=64.73194504 | Physics=13.69532567 | Val RMSE: 2.54300094 | ‚àö(Val Loss) = 1.64531446 | Current Learning Rate: 0.0001\n",
      "Epoch 873/1000 | Train Loss=6492.07064327 | Val Loss=2.73888765 | Data=64.78326244 | Physics=13.59373780 | Val RMSE: 2.54846191 | ‚àö(Val Loss) = 1.65495849 | Current Learning Rate: 0.0001\n",
      "Epoch 874/1000 | Train Loss=6487.13259199 | Val Loss=2.70335002 | Data=64.73388672 | Physics=13.96965723 | Val RMSE: 2.53710985 | ‚àö(Val Loss) = 1.64418674 | Current Learning Rate: 0.0001\n",
      "Epoch 875/1000 | Train Loss=6487.51015940 | Val Loss=2.72059677 | Data=64.73755141 | Physics=13.59157008 | Val RMSE: 2.54279065 | ‚àö(Val Loss) = 1.64942312 | Current Learning Rate: 0.0001\n",
      "Epoch 876/1000 | Train Loss=6490.80719191 | Val Loss=2.72386744 | Data=64.77069547 | Physics=13.81392100 | Val RMSE: 2.54466796 | ‚àö(Val Loss) = 1.65041435 | Current Learning Rate: 0.0001\n",
      "Epoch 877/1000 | Train Loss=6487.15437563 | Val Loss=2.72418712 | Data=64.73413861 | Physics=13.82373160 | Val RMSE: 2.54380584 | ‚àö(Val Loss) = 1.65051115 | Current Learning Rate: 0.0001\n",
      "Epoch 878/1000 | Train Loss=6488.94400517 | Val Loss=2.72889753 | Data=64.75215715 | Physics=13.96028258 | Val RMSE: 2.54486918 | ‚àö(Val Loss) = 1.65193748 | Current Learning Rate: 0.0001\n",
      "Epoch 879/1000 | Train Loss=6485.39631111 | Val Loss=2.74066491 | Data=64.71651988 | Physics=13.84755344 | Val RMSE: 2.55101252 | ‚àö(Val Loss) = 1.65549541 | Current Learning Rate: 0.0001\n",
      "Epoch 880/1000 | Train Loss=6484.01209677 | Val Loss=2.70388176 | Data=64.70257445 | Physics=13.37492413 | Val RMSE: 2.53729701 | ‚àö(Val Loss) = 1.64434838 | Current Learning Rate: 0.0001\n",
      "Epoch 881/1000 | Train Loss=6486.32525832 | Val Loss=2.73062004 | Data=64.72595436 | Physics=13.46955787 | Val RMSE: 2.54548240 | ‚àö(Val Loss) = 1.65245879 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 882/1000 | Train Loss=6487.90582472 | Val Loss=2.71540904 | Data=64.74171866 | Physics=13.59454944 | Val RMSE: 2.54347801 | ‚àö(Val Loss) = 1.64784980 | Current Learning Rate: 0.0001\n",
      "Epoch 883/1000 | Train Loss=6491.07530557 | Val Loss=2.74265311 | Data=64.77292953 | Physics=13.83462242 | Val RMSE: 2.55013442 | ‚àö(Val Loss) = 1.65609574 | Current Learning Rate: 0.0001\n",
      "Epoch 884/1000 | Train Loss=6487.24633002 | Val Loss=2.72881504 | Data=64.73504368 | Physics=13.63790516 | Val RMSE: 2.54747748 | ‚àö(Val Loss) = 1.65191257 | Current Learning Rate: 0.0001\n",
      "Epoch 885/1000 | Train Loss=6487.35882371 | Val Loss=2.72750018 | Data=64.73608226 | Physics=13.58877633 | Val RMSE: 2.54422355 | ‚àö(Val Loss) = 1.65151453 | Current Learning Rate: 0.0001\n",
      "Epoch 886/1000 | Train Loss=6491.33697707 | Val Loss=2.73974654 | Data=64.77580249 | Physics=13.51847446 | Val RMSE: 2.54865217 | ‚àö(Val Loss) = 1.65521801 | Current Learning Rate: 0.0001\n",
      "Epoch 887/1000 | Train Loss=6489.00256741 | Val Loss=2.72303387 | Data=64.75243132 | Physics=13.61271070 | Val RMSE: 2.54236531 | ‚àö(Val Loss) = 1.65016174 | Current Learning Rate: 0.0001\n",
      "Epoch 888/1000 | Train Loss=6491.32977886 | Val Loss=2.70661975 | Data=64.77585048 | Physics=13.51211870 | Val RMSE: 2.53816295 | ‚àö(Val Loss) = 1.64518070 | Current Learning Rate: 0.0001\n",
      "Epoch 889/1000 | Train Loss=6486.77304372 | Val Loss=2.71637919 | Data=64.73027814 | Physics=13.42698773 | Val RMSE: 2.54106998 | ‚àö(Val Loss) = 1.64814413 | Current Learning Rate: 0.0001\n",
      "Epoch 890/1000 | Train Loss=6490.18899635 | Val Loss=2.73556589 | Data=64.76442497 | Physics=13.69910591 | Val RMSE: 2.54632640 | ‚àö(Val Loss) = 1.65395463 | Current Learning Rate: 0.0001\n",
      "Epoch 891/1000 | Train Loss=6486.76683783 | Val Loss=2.73160697 | Data=64.73022522 | Physics=13.25638403 | Val RMSE: 2.54826593 | ‚àö(Val Loss) = 1.65275741 | Current Learning Rate: 0.0001\n",
      "Epoch 892/1000 | Train Loss=6487.34140310 | Val Loss=2.71134633 | Data=64.73598517 | Physics=13.60626165 | Val RMSE: 2.54102206 | ‚àö(Val Loss) = 1.64661670 | Current Learning Rate: 0.0001\n",
      "Epoch 893/1000 | Train Loss=6487.62588206 | Val Loss=2.71620578 | Data=64.73866900 | Physics=13.64890525 | Val RMSE: 2.54178905 | ‚àö(Val Loss) = 1.64809155 | Current Learning Rate: 0.0001\n",
      "Epoch 894/1000 | Train Loss=6504.76198652 | Val Loss=2.72776637 | Data=64.90999296 | Physics=13.68452591 | Val RMSE: 2.54246140 | ‚àö(Val Loss) = 1.65159512 | Current Learning Rate: 0.0001\n",
      "Epoch 895/1000 | Train Loss=6490.12435421 | Val Loss=2.74604009 | Data=64.76353713 | Physics=14.59857344 | Val RMSE: 2.54861021 | ‚àö(Val Loss) = 1.65711796 | Current Learning Rate: 0.0001\n",
      "Epoch 896/1000 | Train Loss=6491.42151273 | Val Loss=2.74891979 | Data=64.77658536 | Physics=13.74021529 | Val RMSE: 2.55464458 | ‚àö(Val Loss) = 1.65798664 | Current Learning Rate: 0.0001\n",
      "Epoch 897/1000 | Train Loss=6493.99341608 | Val Loss=2.74751897 | Data=64.80230651 | Physics=14.00551414 | Val RMSE: 2.55284286 | ‚àö(Val Loss) = 1.65756416 | Current Learning Rate: 0.0001\n",
      "Epoch 898/1000 | Train Loss=6488.34489982 | Val Loss=2.73438157 | Data=64.74591274 | Physics=14.23331555 | Val RMSE: 2.54871631 | ‚àö(Val Loss) = 1.65359664 | Current Learning Rate: 0.0001\n",
      "Epoch 899/1000 | Train Loss=6490.33911920 | Val Loss=2.71314629 | Data=64.76604966 | Physics=13.79974711 | Val RMSE: 2.53946590 | ‚àö(Val Loss) = 1.64716303 | Current Learning Rate: 0.0001\n",
      "Epoch 900/1000 | Train Loss=6490.96972656 | Val Loss=2.72462065 | Data=64.77213952 | Physics=13.48871169 | Val RMSE: 2.54134512 | ‚àö(Val Loss) = 1.65064251 | Current Learning Rate: 0.0001\n",
      "Epoch 901/1000 | Train Loss=6494.74873992 | Val Loss=2.72956970 | Data=64.81001626 | Physics=13.75075400 | Val RMSE: 2.54671144 | ‚àö(Val Loss) = 1.65214097 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 902/1000 | Train Loss=6486.18472782 | Val Loss=2.72301372 | Data=64.72440141 | Physics=13.58399244 | Val RMSE: 2.54116392 | ‚àö(Val Loss) = 1.65015566 | Current Learning Rate: 0.0001\n",
      "Epoch 903/1000 | Train Loss=6487.43307397 | Val Loss=2.71501229 | Data=64.73691104 | Physics=13.81391433 | Val RMSE: 2.54290247 | ‚àö(Val Loss) = 1.64772940 | Current Learning Rate: 0.0001\n",
      "Epoch 904/1000 | Train Loss=6503.26359312 | Val Loss=2.73479814 | Data=64.89454454 | Physics=14.02324294 | Val RMSE: 2.54395962 | ‚àö(Val Loss) = 1.65372252 | Current Learning Rate: 0.0001\n",
      "Epoch 905/1000 | Train Loss=6483.46210307 | Val Loss=2.73241839 | Data=64.69728470 | Physics=13.42134855 | Val RMSE: 2.54599094 | ‚àö(Val Loss) = 1.65300286 | Current Learning Rate: 0.0001\n",
      "Epoch 906/1000 | Train Loss=6489.43757876 | Val Loss=2.72786425 | Data=64.75706445 | Physics=13.33868875 | Val RMSE: 2.54390907 | ‚àö(Val Loss) = 1.65162468 | Current Learning Rate: 0.0001\n",
      "Epoch 907/1000 | Train Loss=6492.60828818 | Val Loss=2.74297221 | Data=64.78805936 | Physics=13.55413145 | Val RMSE: 2.54775929 | ‚àö(Val Loss) = 1.65619206 | Current Learning Rate: 0.0001\n",
      "Epoch 908/1000 | Train Loss=6485.28451046 | Val Loss=2.71366572 | Data=64.71555845 | Physics=13.30857893 | Val RMSE: 2.53719759 | ‚àö(Val Loss) = 1.64732075 | Current Learning Rate: 0.0001\n",
      "Epoch 909/1000 | Train Loss=6486.48617061 | Val Loss=2.72320400 | Data=64.72756010 | Physics=13.73017058 | Val RMSE: 2.54433823 | ‚àö(Val Loss) = 1.65021324 | Current Learning Rate: 0.0001\n",
      "Epoch 910/1000 | Train Loss=6489.58702432 | Val Loss=2.72102220 | Data=64.75838064 | Physics=13.53303640 | Val RMSE: 2.54036188 | ‚àö(Val Loss) = 1.64955211 | Current Learning Rate: 0.0001\n",
      "Epoch 911/1000 | Train Loss=6488.49760585 | Val Loss=2.73986260 | Data=64.74771241 | Physics=13.58093508 | Val RMSE: 2.54788065 | ‚àö(Val Loss) = 1.65525305 | Current Learning Rate: 0.0001\n",
      "Epoch 912/1000 | Train Loss=6490.71500126 | Val Loss=2.74973261 | Data=64.76934211 | Physics=13.79757281 | Val RMSE: 2.55153942 | ‚àö(Val Loss) = 1.65823174 | Current Learning Rate: 0.0001\n",
      "Epoch 913/1000 | Train Loss=6489.99930696 | Val Loss=2.73260378 | Data=64.76239420 | Physics=13.15347842 | Val RMSE: 2.54440236 | ‚àö(Val Loss) = 1.65305889 | Current Learning Rate: 0.0001\n",
      "Epoch 914/1000 | Train Loss=6494.61580141 | Val Loss=2.74187026 | Data=64.80851869 | Physics=14.04113542 | Val RMSE: 2.55214381 | ‚àö(Val Loss) = 1.65585935 | Current Learning Rate: 0.0001\n",
      "Epoch 915/1000 | Train Loss=6485.22538117 | Val Loss=2.72657001 | Data=64.71481557 | Physics=13.62066759 | Val RMSE: 2.54291272 | ‚àö(Val Loss) = 1.65123296 | Current Learning Rate: 0.0001\n",
      "Epoch 916/1000 | Train Loss=6493.55794796 | Val Loss=2.73117734 | Data=64.79797056 | Physics=13.97159620 | Val RMSE: 2.54685330 | ‚àö(Val Loss) = 1.65262735 | Current Learning Rate: 0.0001\n",
      "Epoch 917/1000 | Train Loss=6491.55196258 | Val Loss=2.72912518 | Data=64.77766738 | Physics=13.30082633 | Val RMSE: 2.54238009 | ‚àö(Val Loss) = 1.65200639 | Current Learning Rate: 0.0001\n",
      "Epoch 918/1000 | Train Loss=6492.11726626 | Val Loss=2.73033234 | Data=64.78368661 | Physics=13.75339655 | Val RMSE: 2.54671979 | ‚àö(Val Loss) = 1.65237176 | Current Learning Rate: 0.0001\n",
      "Epoch 919/1000 | Train Loss=6485.55487651 | Val Loss=2.73221368 | Data=64.71807492 | Physics=13.71987585 | Val RMSE: 2.54633093 | ‚àö(Val Loss) = 1.65294099 | Current Learning Rate: 0.0001\n",
      "Epoch 920/1000 | Train Loss=6491.57270665 | Val Loss=2.74869181 | Data=64.77819763 | Physics=13.81103562 | Val RMSE: 2.55308437 | ‚àö(Val Loss) = 1.65791786 | Current Learning Rate: 0.0001\n",
      "Epoch 921/1000 | Train Loss=6489.03903100 | Val Loss=2.72348709 | Data=64.75283727 | Physics=13.90620648 | Val RMSE: 2.54141498 | ‚àö(Val Loss) = 1.65029907 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 922/1000 | Train Loss=6495.56747732 | Val Loss=2.74186113 | Data=64.81798037 | Physics=13.79211143 | Val RMSE: 2.55033779 | ‚àö(Val Loss) = 1.65585661 | Current Learning Rate: 0.0001\n",
      "Epoch 923/1000 | Train Loss=6486.98910030 | Val Loss=2.72182615 | Data=64.73249522 | Physics=13.97030850 | Val RMSE: 2.54245853 | ‚àö(Val Loss) = 1.64979577 | Current Learning Rate: 0.0001\n",
      "Epoch 924/1000 | Train Loss=6488.90480091 | Val Loss=2.72599318 | Data=64.75131484 | Physics=13.86545593 | Val RMSE: 2.54388690 | ‚àö(Val Loss) = 1.65105820 | Current Learning Rate: 0.0001\n",
      "Epoch 925/1000 | Train Loss=6496.42148122 | Val Loss=2.72863391 | Data=64.82661598 | Physics=13.62398017 | Val RMSE: 2.54432464 | ‚àö(Val Loss) = 1.65185773 | Current Learning Rate: 0.0001\n",
      "Epoch 926/1000 | Train Loss=6488.12994582 | Val Loss=2.71860902 | Data=64.74386867 | Physics=13.95295958 | Val RMSE: 2.54430389 | ‚àö(Val Loss) = 1.64882052 | Current Learning Rate: 0.0001\n",
      "Epoch 927/1000 | Train Loss=6488.49829889 | Val Loss=2.74683554 | Data=64.74726905 | Physics=13.34447973 | Val RMSE: 2.55097485 | ‚àö(Val Loss) = 1.65735793 | Current Learning Rate: 0.0001\n",
      "Epoch 928/1000 | Train Loss=6489.37262160 | Val Loss=2.73376380 | Data=64.75631640 | Physics=13.57196255 | Val RMSE: 2.54802036 | ‚àö(Val Loss) = 1.65340972 | Current Learning Rate: 0.0001\n",
      "Epoch 929/1000 | Train Loss=6489.41596837 | Val Loss=2.71575479 | Data=64.75670907 | Physics=13.66558261 | Val RMSE: 2.54138637 | ‚àö(Val Loss) = 1.64795470 | Current Learning Rate: 0.0001\n",
      "Epoch 930/1000 | Train Loss=6493.18471207 | Val Loss=2.73083303 | Data=64.79433933 | Physics=13.47485120 | Val RMSE: 2.54283881 | ‚àö(Val Loss) = 1.65252328 | Current Learning Rate: 0.0001\n",
      "Epoch 931/1000 | Train Loss=6488.96197707 | Val Loss=2.73574896 | Data=64.75218176 | Physics=14.38068307 | Val RMSE: 2.54912853 | ‚àö(Val Loss) = 1.65400994 | Current Learning Rate: 0.0001\n",
      "Epoch 932/1000 | Train Loss=6484.38363155 | Val Loss=2.74477685 | Data=64.70644982 | Physics=13.78499252 | Val RMSE: 2.54519010 | ‚àö(Val Loss) = 1.65673685 | Current Learning Rate: 0.0001\n",
      "Epoch 933/1000 | Train Loss=6489.28375441 | Val Loss=2.71343562 | Data=64.75533799 | Physics=13.38236912 | Val RMSE: 2.54081464 | ‚àö(Val Loss) = 1.64725089 | Current Learning Rate: 0.0001\n",
      "Epoch 934/1000 | Train Loss=6490.39527155 | Val Loss=2.75396662 | Data=64.76637883 | Physics=13.89446532 | Val RMSE: 2.55430722 | ‚àö(Val Loss) = 1.65950787 | Current Learning Rate: 0.0001\n",
      "Epoch 935/1000 | Train Loss=6486.50455204 | Val Loss=2.72151423 | Data=64.72762422 | Physics=13.94228678 | Val RMSE: 2.54296160 | ‚àö(Val Loss) = 1.64970124 | Current Learning Rate: 0.0001\n",
      "Epoch 936/1000 | Train Loss=6486.22032510 | Val Loss=2.73190849 | Data=64.72485745 | Physics=13.83417944 | Val RMSE: 2.54733133 | ‚àö(Val Loss) = 1.65284860 | Current Learning Rate: 0.0001\n",
      "Epoch 937/1000 | Train Loss=6488.90790386 | Val Loss=2.73213645 | Data=64.75140209 | Physics=13.69623425 | Val RMSE: 2.54742289 | ‚àö(Val Loss) = 1.65291762 | Current Learning Rate: 0.0001\n",
      "Epoch 938/1000 | Train Loss=6487.97134892 | Val Loss=2.71132457 | Data=64.74219205 | Physics=14.18164018 | Val RMSE: 2.54196024 | ‚àö(Val Loss) = 1.64661002 | Current Learning Rate: 0.0001\n",
      "Epoch 939/1000 | Train Loss=6493.52482359 | Val Loss=2.73646304 | Data=64.79771485 | Physics=14.10128614 | Val RMSE: 2.54850650 | ‚àö(Val Loss) = 1.65422583 | Current Learning Rate: 0.0001\n",
      "Epoch 940/1000 | Train Loss=6487.73894279 | Val Loss=2.72686377 | Data=64.73992563 | Physics=13.43991659 | Val RMSE: 2.54547071 | ‚àö(Val Loss) = 1.65132189 | Current Learning Rate: 0.0001\n",
      "Epoch 941/1000 | Train Loss=6487.00751323 | Val Loss=2.72746538 | Data=64.73248586 | Physics=13.38846610 | Val RMSE: 2.54554367 | ‚àö(Val Loss) = 1.65150392 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 942/1000 | Train Loss=6493.37610257 | Val Loss=2.72565860 | Data=64.79608265 | Physics=13.66157574 | Val RMSE: 2.54583788 | ‚àö(Val Loss) = 1.65095687 | Current Learning Rate: 0.0001\n",
      "Epoch 943/1000 | Train Loss=6486.65596648 | Val Loss=2.73006413 | Data=64.72917741 | Physics=13.91998956 | Val RMSE: 2.54742384 | ‚àö(Val Loss) = 1.65229058 | Current Learning Rate: 0.0001\n",
      "Epoch 944/1000 | Train Loss=6488.29885963 | Val Loss=2.73290835 | Data=64.74523495 | Physics=13.44599532 | Val RMSE: 2.54740191 | ‚àö(Val Loss) = 1.65315104 | Current Learning Rate: 0.0001\n",
      "Epoch 945/1000 | Train Loss=6496.40818737 | Val Loss=2.73563315 | Data=64.82639916 | Physics=13.98623617 | Val RMSE: 2.54681706 | ‚àö(Val Loss) = 1.65397489 | Current Learning Rate: 0.0001\n",
      "Epoch 946/1000 | Train Loss=6486.04583543 | Val Loss=2.72293938 | Data=64.72316594 | Physics=13.64866001 | Val RMSE: 2.54160857 | ‚àö(Val Loss) = 1.65013313 | Current Learning Rate: 0.0001\n",
      "Epoch 947/1000 | Train Loss=6485.92356036 | Val Loss=2.73527254 | Data=64.72172608 | Physics=12.95453137 | Val RMSE: 2.55120158 | ‚àö(Val Loss) = 1.65386593 | Current Learning Rate: 0.0001\n",
      "Epoch 948/1000 | Train Loss=6494.62589781 | Val Loss=2.74932554 | Data=64.80879002 | Physics=13.93317428 | Val RMSE: 2.54969954 | ‚àö(Val Loss) = 1.65810907 | Current Learning Rate: 0.0001\n",
      "Epoch 949/1000 | Train Loss=6485.01513672 | Val Loss=2.72559680 | Data=64.71271256 | Physics=13.73254095 | Val RMSE: 2.54437971 | ‚àö(Val Loss) = 1.65093815 | Current Learning Rate: 0.0001\n",
      "Epoch 950/1000 | Train Loss=6487.75045678 | Val Loss=2.73290087 | Data=64.74013409 | Physics=13.96506176 | Val RMSE: 2.54399848 | ‚àö(Val Loss) = 1.65314877 | Current Learning Rate: 0.0001\n",
      "Epoch 951/1000 | Train Loss=6488.44052419 | Val Loss=2.72260628 | Data=64.74693483 | Physics=13.99462719 | Val RMSE: 2.54388571 | ‚àö(Val Loss) = 1.65003216 | Current Learning Rate: 0.0001\n",
      "Epoch 952/1000 | Train Loss=6489.95978768 | Val Loss=2.71998342 | Data=64.76225330 | Physics=13.55398877 | Val RMSE: 2.54443860 | ‚àö(Val Loss) = 1.64923716 | Current Learning Rate: 0.0001\n",
      "Epoch 953/1000 | Train Loss=6487.98492629 | Val Loss=2.72713893 | Data=64.74218012 | Physics=13.34378638 | Val RMSE: 2.54167509 | ‚àö(Val Loss) = 1.65140522 | Current Learning Rate: 0.0001\n",
      "Epoch 954/1000 | Train Loss=6499.16162109 | Val Loss=2.72556783 | Data=64.85396244 | Physics=14.01567268 | Val RMSE: 2.54528427 | ‚àö(Val Loss) = 1.65092933 | Current Learning Rate: 0.0001\n",
      "Epoch 955/1000 | Train Loss=6491.51296308 | Val Loss=2.72589122 | Data=64.77771845 | Physics=13.82692099 | Val RMSE: 2.54524803 | ‚àö(Val Loss) = 1.65102732 | Current Learning Rate: 0.0001\n",
      "Epoch 956/1000 | Train Loss=6488.01154549 | Val Loss=2.72371845 | Data=64.74270039 | Physics=13.79274137 | Val RMSE: 2.54075575 | ‚àö(Val Loss) = 1.65036917 | Current Learning Rate: 0.0001\n",
      "Epoch 957/1000 | Train Loss=6502.05386845 | Val Loss=2.74185488 | Data=64.88255667 | Physics=13.56508487 | Val RMSE: 2.55232096 | ‚àö(Val Loss) = 1.65585470 | Current Learning Rate: 0.0001\n",
      "Epoch 958/1000 | Train Loss=6484.97356981 | Val Loss=2.72250506 | Data=64.71238733 | Physics=13.87434291 | Val RMSE: 2.54354858 | ‚àö(Val Loss) = 1.65000153 | Current Learning Rate: 0.0001\n",
      "Epoch 959/1000 | Train Loss=6487.72519216 | Val Loss=2.72589683 | Data=64.73978572 | Physics=13.96265649 | Val RMSE: 2.54610062 | ‚àö(Val Loss) = 1.65102899 | Current Learning Rate: 0.0001\n",
      "Epoch 960/1000 | Train Loss=6490.17006363 | Val Loss=2.72062767 | Data=64.76430659 | Physics=13.37729318 | Val RMSE: 2.54086542 | ‚àö(Val Loss) = 1.64943254 | Current Learning Rate: 0.0001\n",
      "Epoch 961/1000 | Train Loss=6490.25935610 | Val Loss=2.72407608 | Data=64.76508097 | Physics=13.40064599 | Val RMSE: 2.54636550 | ‚àö(Val Loss) = 1.65047753 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 962/1000 | Train Loss=6494.01746787 | Val Loss=2.71961203 | Data=64.80272453 | Physics=14.13613196 | Val RMSE: 2.54114127 | ‚àö(Val Loss) = 1.64912462 | Current Learning Rate: 0.0001\n",
      "Epoch 963/1000 | Train Loss=6491.33069241 | Val Loss=2.74062883 | Data=64.77563243 | Physics=14.13353779 | Val RMSE: 2.54674697 | ‚àö(Val Loss) = 1.65548444 | Current Learning Rate: 0.0001\n",
      "Epoch 964/1000 | Train Loss=6489.82245464 | Val Loss=2.74855708 | Data=64.76089318 | Physics=14.16145136 | Val RMSE: 2.55239725 | ‚àö(Val Loss) = 1.65787733 | Current Learning Rate: 0.0001\n",
      "Epoch 965/1000 | Train Loss=6492.22210496 | Val Loss=2.73906079 | Data=64.78459500 | Physics=13.26965882 | Val RMSE: 2.54772806 | ‚àö(Val Loss) = 1.65501082 | Current Learning Rate: 0.0001\n",
      "Epoch 966/1000 | Train Loss=6487.59302545 | Val Loss=2.72686330 | Data=64.73861362 | Physics=13.70949835 | Val RMSE: 2.54388714 | ‚àö(Val Loss) = 1.65132165 | Current Learning Rate: 0.0001\n",
      "Epoch 967/1000 | Train Loss=6488.46504851 | Val Loss=2.70767099 | Data=64.74724603 | Physics=13.67540030 | Val RMSE: 2.53760982 | ‚àö(Val Loss) = 1.64550018 | Current Learning Rate: 0.0001\n",
      "Epoch 968/1000 | Train Loss=6487.27001953 | Val Loss=2.72888827 | Data=64.73521288 | Physics=13.82015223 | Val RMSE: 2.54300594 | ‚àö(Val Loss) = 1.65193474 | Current Learning Rate: 0.0001\n",
      "Epoch 969/1000 | Train Loss=6492.76771988 | Val Loss=2.74860888 | Data=64.78995440 | Physics=13.63662385 | Val RMSE: 2.55521894 | ‚àö(Val Loss) = 1.65789294 | Current Learning Rate: 0.0001\n",
      "Epoch 970/1000 | Train Loss=6488.19351689 | Val Loss=2.72992933 | Data=64.74457710 | Physics=13.35904843 | Val RMSE: 2.54572177 | ‚àö(Val Loss) = 1.65224981 | Current Learning Rate: 0.0001\n",
      "Epoch 971/1000 | Train Loss=6489.37504725 | Val Loss=2.71768779 | Data=64.75637350 | Physics=13.52186186 | Val RMSE: 2.54507565 | ‚àö(Val Loss) = 1.64854109 | Current Learning Rate: 0.0001\n",
      "Epoch 972/1000 | Train Loss=6495.21265436 | Val Loss=2.72160361 | Data=64.81432195 | Physics=13.21856908 | Val RMSE: 2.54243708 | ‚àö(Val Loss) = 1.64972830 | Current Learning Rate: 0.0001\n",
      "Epoch 973/1000 | Train Loss=6493.08807964 | Val Loss=2.72705367 | Data=64.79349887 | Physics=14.17875975 | Val RMSE: 2.54233170 | ‚àö(Val Loss) = 1.65137935 | Current Learning Rate: 0.0001\n",
      "Epoch 974/1000 | Train Loss=6495.51879095 | Val Loss=2.73999827 | Data=64.81744680 | Physics=14.37245081 | Val RMSE: 2.54920149 | ‚àö(Val Loss) = 1.65529406 | Current Learning Rate: 0.0001\n",
      "Epoch 975/1000 | Train Loss=6486.46876575 | Val Loss=2.72204901 | Data=64.72735694 | Physics=13.70412368 | Val RMSE: 2.54355478 | ‚àö(Val Loss) = 1.64986336 | Current Learning Rate: 0.0001\n",
      "Epoch 976/1000 | Train Loss=6489.28521925 | Val Loss=2.74366690 | Data=64.75529554 | Physics=13.53367195 | Val RMSE: 2.55038619 | ‚àö(Val Loss) = 1.65640175 | Current Learning Rate: 0.0001\n",
      "Epoch 977/1000 | Train Loss=6493.19446195 | Val Loss=2.72932870 | Data=64.79413912 | Physics=13.74718152 | Val RMSE: 2.54424214 | ‚àö(Val Loss) = 1.65206802 | Current Learning Rate: 0.0001\n",
      "Epoch 978/1000 | Train Loss=6491.26677482 | Val Loss=2.72719097 | Data=64.77515178 | Physics=13.56648461 | Val RMSE: 2.54436779 | ‚àö(Val Loss) = 1.65142095 | Current Learning Rate: 0.0001\n",
      "Epoch 979/1000 | Train Loss=6487.38693926 | Val Loss=2.72898418 | Data=64.73645659 | Physics=13.72550918 | Val RMSE: 2.54541636 | ‚àö(Val Loss) = 1.65196371 | Current Learning Rate: 0.0001\n",
      "Epoch 980/1000 | Train Loss=6489.86485635 | Val Loss=2.74113784 | Data=64.76097636 | Physics=13.68483795 | Val RMSE: 2.55147505 | ‚àö(Val Loss) = 1.65563822 | Current Learning Rate: 0.0001\n",
      "Epoch 981/1000 | Train Loss=6483.90843939 | Val Loss=2.70982503 | Data=64.70176426 | Physics=13.67886969 | Val RMSE: 2.53773069 | ‚àö(Val Loss) = 1.64615464 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073]]) \n",
      " Prediction :  [[  0.99287224  -3.456695   -10.95133   ]\n",
      " [  0.99285024  -3.4570212  -10.953124  ]\n",
      " [  0.99286866  -3.4566557  -10.951083  ]\n",
      " ...\n",
      " [  0.9783393   -3.6526299  -13.517331  ]\n",
      " [  0.9783428   -3.6526048  -13.517766  ]\n",
      " [  0.9783464   -3.6525788  -13.518176  ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703]]) \n",
      " Prediction :  [[  0.9783493   -3.652554   -13.5186405 ]\n",
      " [  0.9783529   -3.6525283  -13.519079  ]\n",
      " [  0.97835666  -3.6525018  -13.519491  ]\n",
      " ...\n",
      " [  0.9791203   -3.6578615  -13.114739  ]\n",
      " [  0.97908837  -3.6579475  -13.118374  ]\n",
      " [  0.97906214  -3.6580174  -13.121388  ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        [  0.9822,  -3.6727, -13.0703],\n",
      "        ...,\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159]]) \n",
      " Prediction :  [[  0.97903484  -3.658088   -13.124477  ]\n",
      " [  0.9790082   -3.6581583  -13.127548  ]\n",
      " [  0.97898346  -3.6582236  -13.130515  ]\n",
      " ...\n",
      " [  0.979671    -3.656235   -13.056457  ]\n",
      " [  0.9796441   -3.6563191  -13.05918   ]\n",
      " [  0.9796113   -3.6564212  -13.062527  ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[  0.9795772   -3.656525   -13.06596   ]\n",
      " [  0.9795437   -3.6566288  -13.069374  ]\n",
      " [  0.9795121   -3.6567252  -13.07256   ]\n",
      " ...\n",
      " [  0.97791314  -3.6595373  -13.30752   ]\n",
      " [  0.9779101   -3.6595268  -13.308614  ]\n",
      " [  0.9779072   -3.6595156  -13.309692  ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542]]) \n",
      " Prediction :  [[  0.9779043   -3.659504   -13.310762  ]\n",
      " [  0.97790325  -3.6594877  -13.311739  ]\n",
      " [  0.9778988   -3.6594768  -13.312859  ]\n",
      " ...\n",
      " [  0.9779802   -3.6556723  -13.458173  ]\n",
      " [  0.9779829   -3.655647   -13.458723  ]\n",
      " [  0.9779847   -3.6556232  -13.459329  ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        [  0.9919,  -3.1226, -14.7542],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.97798723  -3.6555989  -13.459913  ]\n",
      " [  0.97799     -3.6555734  -13.460475  ]\n",
      " [  0.97799194  -3.655549   -13.461096  ]\n",
      " ...\n",
      " [  0.9824444   -3.6289124  -13.765697  ]\n",
      " [  0.9824512   -3.6288767  -13.765974  ]\n",
      " [  0.9824585   -3.6288402  -13.76621   ]] \n",
      "\n",
      "Final Test RMSE:  1.4480867038170497\n",
      "Epoch 982/1000 | Train Loss=6487.02142137 | Val Loss=2.71867681 | Data=64.73278747 | Physics=14.01411171 | Val RMSE: 2.54057217 | ‚àö(Val Loss) = 1.64884102 | Current Learning Rate: 0.0001\n",
      "Epoch 983/1000 | Train Loss=6486.24173072 | Val Loss=2.70756028 | Data=64.72497817 | Physics=14.36181990 | Val RMSE: 2.54034615 | ‚àö(Val Loss) = 1.64546657 | Current Learning Rate: 0.0001\n",
      "Epoch 984/1000 | Train Loss=6490.89684665 | Val Loss=2.72632955 | Data=64.77131641 | Physics=13.74706320 | Val RMSE: 2.54536891 | ‚àö(Val Loss) = 1.65116012 | Current Learning Rate: 0.0001\n",
      "Epoch 985/1000 | Train Loss=6499.28747165 | Val Loss=2.74580697 | Data=64.85533622 | Physics=13.95295828 | Val RMSE: 2.55278015 | ‚àö(Val Loss) = 1.65704763 | Current Learning Rate: 0.0001\n",
      "Epoch 986/1000 | Train Loss=6488.01253780 | Val Loss=2.72092404 | Data=64.74261955 | Physics=13.77784234 | Val RMSE: 2.54268217 | ‚àö(Val Loss) = 1.64952242 | Current Learning Rate: 0.0001\n",
      "Epoch 987/1000 | Train Loss=6487.67217427 | Val Loss=2.72795750 | Data=64.73904973 | Physics=13.86335144 | Val RMSE: 2.54134893 | ‚àö(Val Loss) = 1.65165293 | Current Learning Rate: 0.0001\n",
      "Epoch 988/1000 | Train Loss=6487.96301663 | Val Loss=2.71439617 | Data=64.74235264 | Physics=13.62336742 | Val RMSE: 2.54238510 | ‚àö(Val Loss) = 1.64754248 | Current Learning Rate: 0.0001\n",
      "Epoch 989/1000 | Train Loss=6495.19816343 | Val Loss=2.73919349 | Data=64.81426473 | Physics=13.82993869 | Val RMSE: 2.54826045 | ‚àö(Val Loss) = 1.65505087 | Current Learning Rate: 0.0001\n",
      "Epoch 990/1000 | Train Loss=6491.21796245 | Val Loss=2.72668033 | Data=64.77450451 | Physics=13.75083438 | Val RMSE: 2.54612684 | ‚àö(Val Loss) = 1.65126622 | Current Learning Rate: 0.0001\n",
      "Epoch 991/1000 | Train Loss=6491.06497291 | Val Loss=2.71713622 | Data=64.77339554 | Physics=14.18340276 | Val RMSE: 2.54259276 | ‚àö(Val Loss) = 1.64837384 | Current Learning Rate: 0.0001\n",
      "Epoch 992/1000 | Train Loss=6484.83622102 | Val Loss=2.75318393 | Data=64.71085690 | Physics=13.66284637 | Val RMSE: 2.55517864 | ‚àö(Val Loss) = 1.65927207 | Current Learning Rate: 0.0001\n",
      "Epoch 993/1000 | Train Loss=6487.85490171 | Val Loss=2.72648484 | Data=64.74112960 | Physics=13.86644761 | Val RMSE: 2.54376459 | ‚àö(Val Loss) = 1.65120709 | Current Learning Rate: 0.0001\n",
      "Epoch 994/1000 | Train Loss=6489.87191280 | Val Loss=2.72499316 | Data=64.76128387 | Physics=14.13470486 | Val RMSE: 2.54591417 | ‚àö(Val Loss) = 1.65075541 | Current Learning Rate: 0.0001\n",
      "Epoch 995/1000 | Train Loss=6492.20862210 | Val Loss=2.70980929 | Data=64.78441005 | Physics=13.82573792 | Val RMSE: 2.53674436 | ‚àö(Val Loss) = 1.64614987 | Current Learning Rate: 0.0001\n",
      "Epoch 996/1000 | Train Loss=6492.23768271 | Val Loss=2.74066588 | Data=64.78483348 | Physics=13.60552570 | Val RMSE: 2.55096889 | ‚àö(Val Loss) = 1.65549564 | Current Learning Rate: 0.0001\n",
      "Epoch 997/1000 | Train Loss=6486.50285093 | Val Loss=2.71604521 | Data=64.72769387 | Physics=13.54414061 | Val RMSE: 2.54238129 | ‚àö(Val Loss) = 1.64804280 | Current Learning Rate: 0.0001\n",
      "Epoch 998/1000 | Train Loss=6486.57826676 | Val Loss=2.72165702 | Data=64.72828847 | Physics=13.57890809 | Val RMSE: 2.54137015 | ‚àö(Val Loss) = 1.64974451 | Current Learning Rate: 0.0001\n",
      "Epoch 999/1000 | Train Loss=6487.24760585 | Val Loss=2.71951463 | Data=64.73514545 | Physics=13.42229814 | Val RMSE: 2.54115057 | ‚àö(Val Loss) = 1.64909506 | Current Learning Rate: 0.0001\n",
      "‚úÖ Saved last model at epoch 1000 \n",
      "Epoch 1000/1000 | Train Loss=6485.97312878 | Val Loss=2.72730853 | Data=64.72224045 | Physics=13.87218092 | Val RMSE: 2.54491425 | ‚àö(Val Loss) = 1.65145648 | Current Learning Rate: 0.0001\n",
      "‚úÖ Metrics saved successfully!\n",
      "Plot losses after training 3:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE :  8.688520222902298\n",
      "Cutoff SoH :  0.7\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\n",
      " X_['train'] shape : torch.Size([18590, 100, 1]) , y_['train'] shape : torch.Size([18590, 3]) Ôºåy_2['train'] shape: torch.Size([18590, 1])\n",
      "load : \n",
      "['train']loader lengths :  6\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n",
      " X_['val'] shape : torch.Size([2759, 100, 1]) , y_['val'] shape : torch.Size([2759, 3]) Ôºåy_2['val'] shape: torch.Size([2759, 1])\n",
      "load : \n",
      "['val']loader lengths :  1\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n",
      " X_['test'] shape : torch.Size([6683, 100, 1]) , y_['test'] shape : torch.Size([6683, 3]) Ôºåy_2['test'] shape: torch.Size([6683, 1])\n",
      "load : \n",
      "['test']loader lengths :  3\n",
      "## üß† Model\n",
      "Last model window :  last_model_window_100_model_pinn_data_low.pth\n",
      "üöÄ Initializing model output to: k=0.9609278440475464, a=-3.9271509647369385, b=-19.298616409301758\n",
      "‚úÖ Model Output Parameters Initialized!\n",
      "##\n",
      "        ### üìà Gompertz Function (Physics Law)\n",
      "        \n",
      "        * `x`: Time (or cycle number)\n",
      "        \n",
      "        * `k`: Max value (e.g., max capacity)\n",
      "        \n",
      "        * `a`, `b`: Shape parameters\n",
      "## üß† Loss Functions\n",
      "\n",
      "## ‚öôÔ∏è 1. Data-Informed Loss Function\n",
      "        a data loss (what the LSTM learns from data)\n",
      "        \n",
      "        * Mean Squared Error for Training\n",
      "        * RMSE for autoregressive approximation of compound error\n",
      "        \n",
      "        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n",
      "        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n",
      "        \n",
      "        * `alpha`: controls how strongly physics is enforced.\n",
      "## üõ†Ô∏è Parameter Strategy\n",
      "## üîÅ Training Loop\n",
      "‚úÖ Saved best model at epoch 1 (Val Loss = 1.18832064)\n",
      "Epoch 1/1000 | Train Loss=38082.00065104 | Val Loss=1.18832064 | Data=380.80053202 | Physics=1.90073197 | Val RMSE: 2.12546158 | ‚àö(Val Loss) = 1.09010124 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9616637   -3.928686   -19.294453  ]\n",
      " [  0.96166354  -3.928686   -19.294453  ]\n",
      " [  0.9616634   -3.9286857  -19.294453  ]\n",
      " ...\n",
      " [  0.96136564  -3.928378   -19.294924  ]\n",
      " [  0.9613648   -3.9283772  -19.294924  ]\n",
      " [  0.9613642   -3.9283764  -19.294926  ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9613638   -3.9283762  -19.294926  ]\n",
      " [  0.96136355  -3.9283757  -19.294926  ]\n",
      " [  0.96136224  -3.9283745  -19.29493   ]\n",
      " ...\n",
      " [  0.9606791   -3.9276671  -19.296011  ]\n",
      " [  0.96067655  -3.9276645  -19.296017  ]\n",
      " [  0.96067494  -3.9276628  -19.296019  ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.96067274  -3.9276605  -19.296022  ]\n",
      " [  0.96067023  -3.9276578  -19.296026  ]\n",
      " [  0.9606687   -3.9276564  -19.296028  ]\n",
      " [  0.9606666   -3.9276543  -19.296032  ]\n",
      " [  0.9606642   -3.9276516  -19.296036  ]\n",
      " [  0.96066153  -3.927649   -19.29604   ]\n",
      " [  0.96065974  -3.927647   -19.296043  ]\n",
      " [  0.9606586   -3.927646   -19.296045  ]\n",
      " [  0.96065784  -3.9276452  -19.296045  ]\n",
      " [  0.9606562   -3.9276435  -19.29605   ]\n",
      " [  0.960654    -3.9276412  -19.296053  ]\n",
      " [  0.96065265  -3.9276397  -19.296055  ]\n",
      " [  0.9606506   -3.9276376  -19.296057  ]\n",
      " [  0.9606493   -3.9276364  -19.296059  ]\n",
      " [  0.96064734  -3.9276342  -19.296062  ]\n",
      " [  0.96064675  -3.9276335  -19.296062  ]\n",
      " [  0.9606446   -3.9276314  -19.296066  ]\n",
      " [  0.96064323  -3.92763    -19.296068  ]\n",
      " [  0.9606412   -3.9276278  -19.296072  ]\n",
      " [  0.9606399   -3.9276266  -19.296074  ]\n",
      " [  0.9606379   -3.9276245  -19.296078  ]\n",
      " [  0.96063554  -3.927622   -19.296082  ]\n",
      " [  0.96063405  -3.9276204  -19.296083  ]\n",
      " [  0.9606349   -3.9276214  -19.296082  ]\n",
      " [  0.96063143  -3.9276178  -19.296087  ]\n",
      " [  0.96062917  -3.9276154  -19.296091  ]\n",
      " [  0.96062654  -3.9276128  -19.296097  ]\n",
      " [  0.96062493  -3.927611   -19.296099  ]\n",
      " [  0.9606228   -3.9276087  -19.296103  ]\n",
      " [  0.9606209   -3.9276068  -19.296104  ]\n",
      " [  0.9606191   -3.927605   -19.296108  ]\n",
      " [  0.9606168   -3.9276025  -19.296112  ]\n",
      " [  0.96061534  -3.927601   -19.296114  ]\n",
      " [  0.9606139   -3.9275997  -19.296116  ]\n",
      " [  0.9606124   -3.927598   -19.296118  ]\n",
      " [  0.9606103   -3.9275959  -19.296122  ]\n",
      " [  0.96060896  -3.9275944  -19.296124  ]\n",
      " [  0.96060693  -3.9275923  -19.296127  ]\n",
      " [  0.9606045   -3.92759    -19.296131  ]\n",
      " [  0.960603    -3.9275882  -19.296133  ]\n",
      " [  0.96060085  -3.927586   -19.296137  ]\n",
      " [  0.96059954  -3.9275846  -19.296139  ]\n",
      " [  0.9605976   -3.9275827  -19.296143  ]\n",
      " [  0.96059626  -3.9275813  -19.296143  ]\n",
      " [  0.96059436  -3.9275794  -19.296146  ]\n",
      " [  0.960592    -3.9275768  -19.29615   ]\n",
      " [  0.9605905   -3.9275753  -19.296152  ]\n",
      " [  0.9605884   -3.9275732  -19.296156  ]\n",
      " [  0.96058595  -3.9275706  -19.29616   ]\n",
      " [  0.9605844   -3.927569   -19.296162  ]\n",
      " [  0.9605823   -3.9275668  -19.296165  ]\n",
      " [  0.9605799   -3.9275644  -19.29617   ]\n",
      " [  0.96057713  -3.9275615  -19.296175  ]\n",
      " [  0.9605784   -3.9275627  -19.296171  ]\n",
      " [  0.9605752   -3.9275596  -19.296177  ]\n",
      " [  0.96057194  -3.927556   -19.296183  ]\n",
      " [  0.96056926  -3.9275532  -19.296186  ]\n",
      " [  0.960567    -3.9275508  -19.29619   ]\n",
      " [  0.96056616  -3.92755    -19.296192  ]\n",
      " [  0.96056396  -3.9275477  -19.296196  ]\n",
      " [  0.9605625   -3.9275463  -19.296198  ]\n",
      " [  0.9605605   -3.927544   -19.2962    ]\n",
      " [  0.9605592   -3.927543   -19.296202  ]\n",
      " [  0.9605572   -3.9275408  -19.296206  ]\n",
      " [  0.96055484  -3.9275384  -19.29621   ]\n",
      " [  0.9605539   -3.9275374  -19.296211  ]\n",
      " [  0.9605516   -3.927535   -19.296215  ]\n",
      " [  0.9605502   -3.9275334  -19.296217  ]\n",
      " [  0.9605481   -3.9275315  -19.29622   ]\n",
      " [  0.96054566  -3.9275289  -19.296225  ]\n",
      " [  0.96054417  -3.9275272  -19.296227  ]\n",
      " [  0.9605414   -3.9275246  -19.29623   ]\n",
      " [  0.9605415   -3.9275246  -19.29623   ]\n",
      " [  0.9605392   -3.9275222  -19.296234  ]\n",
      " [  0.96053666  -3.9275196  -19.296238  ]\n",
      " [  0.960535    -3.9275177  -19.296242  ]\n",
      " [  0.9605328   -3.9275155  -19.296244  ]\n",
      " [  0.9605314   -3.927514   -19.296247  ]\n",
      " [  0.96052945  -3.927512   -19.29625   ]\n",
      " [  0.960527    -3.9275095  -19.296253  ]\n",
      " [  0.96052545  -3.9275079  -19.296257  ]\n",
      " [  0.96052337  -3.9275057  -19.296259  ]\n",
      " [  0.9605208   -3.927503   -19.296265  ]\n",
      " [  0.96051925  -3.9275014  -19.296267  ]\n",
      " [  0.9605171   -3.9274993  -19.29627   ]\n",
      " [  0.96051574  -3.9274979  -19.296272  ]\n",
      " [  0.9605138   -3.9274957  -19.296274  ]\n",
      " [  0.9605113   -3.9274933  -19.296278  ]\n",
      " [  0.96050984  -3.9274917  -19.296282  ]\n",
      " [  0.9605077   -3.9274895  -19.296284  ]\n",
      " [  0.9605064   -3.927488   -19.296286  ]\n",
      " [  0.9605044   -3.9274862  -19.29629   ]\n",
      " [  0.960502    -3.9274836  -19.296293  ]\n",
      " [  0.9605005   -3.927482   -19.296295  ]\n",
      " [  0.960499    -3.9274805  -19.296299  ]\n",
      " [  0.96049684  -3.9274783  -19.2963    ]\n",
      " [  0.9604948   -3.9274762  -19.296305  ]\n",
      " [  0.9604925   -3.9274735  -19.296309  ]\n",
      " [  0.960491    -3.927472   -19.29631   ]\n",
      " [  0.96048903  -3.92747    -19.296314  ]\n",
      " [  0.96048766  -3.9274688  -19.296316  ]\n",
      " [  0.96048564  -3.9274666  -19.29632   ]\n",
      " [  0.9604833   -3.9274642  -19.296324  ]\n",
      " [  0.9604818   -3.9274626  -19.296326  ]\n",
      " [  0.96047854  -3.9274592  -19.296331  ]\n",
      " [  0.9604765   -3.927457   -19.296333  ]\n",
      " [  0.9604752   -3.9274557  -19.296335  ]\n",
      " [  0.9604762   -3.9274569  -19.296333  ]\n",
      " [  0.9604728   -3.9274533  -19.296339  ]\n",
      " [  0.9604706   -3.927451   -19.296343  ]\n",
      " [  0.96046805  -3.9274483  -19.296347  ]\n",
      " [  0.96046525  -3.9274454  -19.296352  ]\n",
      " [  0.9604635   -3.9274435  -19.296354  ]\n",
      " [  0.9604613   -3.9274414  -19.296358  ]\n",
      " [  0.96045995  -3.92744    -19.29636   ]\n",
      " [  0.9604579   -3.9274378  -19.296364  ]\n",
      " [  0.9604555   -3.9274354  -19.296368  ]\n",
      " [  0.9604539   -3.9274337  -19.29637   ]\n",
      " [  0.96045184  -3.9274316  -19.296373  ]\n",
      " [  0.96044934  -3.927429   -19.296377  ]\n",
      " [  0.9604478   -3.9274273  -19.29638   ]\n",
      " [  0.96044564  -3.9274251  -19.296383  ]\n",
      " [  0.96044314  -3.9274225  -19.296387  ]\n",
      " [  0.9604416   -3.9274209  -19.296389  ]\n",
      " [  0.9604395   -3.9274187  -19.296392  ]\n",
      " [  0.960437    -3.927416   -19.296396  ]\n",
      " [  0.96043545  -3.9274144  -19.296398  ]\n",
      " [  0.9604333   -3.9274123  -19.296402  ]\n",
      " [  0.9604308   -3.9274096  -19.296406  ]\n",
      " [  0.96042925  -3.927408   -19.29641   ]\n",
      " [  0.9604271   -3.9274058  -19.296412  ]\n",
      " [  0.96042573  -3.9274044  -19.296415  ]\n",
      " [  0.96042377  -3.9274025  -19.296417  ]\n",
      " [  0.9604213   -3.9273999  -19.296421  ]\n",
      " [  0.96041983  -3.9273982  -19.296425  ]\n",
      " [  0.9604206   -3.9273992  -19.296421  ]\n",
      " [  0.9604183   -3.9273968  -19.296427  ]\n",
      " [  0.9604156   -3.927394   -19.29643   ]\n",
      " [  0.96041274  -3.927391   -19.296436  ]\n",
      " [  0.96041095  -3.9273891  -19.296438  ]\n",
      " [  0.9604086   -3.9273868  -19.296442  ]\n",
      " [  0.96040606  -3.927384   -19.296446  ]\n",
      " [  0.96040446  -3.9273822  -19.296448  ]\n",
      " [  0.96040225  -3.92738    -19.296452  ]\n",
      " [  0.96040034  -3.9273782  -19.296455  ]\n",
      " [  0.96039915  -3.927377   -19.296457  ]\n",
      " [  0.9603967   -3.9273744  -19.296461  ]\n",
      " [  0.9603951   -3.9273727  -19.296463  ]\n",
      " [  0.96039295  -3.9273705  -19.296467  ]\n",
      " [  0.96039045  -3.927368   -19.29647   ]\n",
      " [  0.9603889   -3.9273663  -19.296473  ]\n",
      " [  0.9603868   -3.927364   -19.296476  ]\n",
      " [  0.96038425  -3.9273615  -19.29648   ]\n",
      " [  0.9603821   -3.927359   -19.296484  ]\n",
      " [  0.9603802   -3.9273572  -19.296488  ]\n",
      " [  0.9603778   -3.9273548  -19.296492  ]\n",
      " [  0.9603763   -3.9273531  -19.296494  ]\n",
      " [  0.9603743   -3.927351   -19.296495  ]\n",
      " [  0.960373    -3.9273498  -19.296497  ]\n",
      " [  0.9603722   -3.9273489  -19.2965    ]\n",
      " [  0.9603694   -3.927346   -19.296505  ]\n",
      " [  0.96036756  -3.927344   -19.296507  ]\n",
      " [  0.9603653   -3.9273417  -19.29651   ]\n",
      " [  0.9603627   -3.927339   -19.296515  ]\n",
      " [  0.96036106  -3.9273374  -19.296516  ]\n",
      " [  0.9603589   -3.927335   -19.29652   ]\n",
      " [  0.96035755  -3.9273338  -19.296522  ]\n",
      " [  0.9603556   -3.9273317  -19.296526  ]\n",
      " [  0.96035314  -3.927329   -19.29653   ]\n",
      " [  0.96035165  -3.9273276  -19.296532  ]\n",
      " [  0.9603495   -3.9273255  -19.296535  ]\n",
      " [  0.96034706  -3.9273229  -19.29654   ]\n",
      " [  0.96034545  -3.9273212  -19.296541  ]\n",
      " [  0.9603433   -3.927319   -19.296545  ]\n",
      " [  0.96034193  -3.9273176  -19.296547  ]\n",
      " [  0.9603429   -3.9273186  -19.296545  ]\n",
      " [  0.96034056  -3.9273162  -19.296549  ]\n",
      " [  0.9603379   -3.9273133  -19.296555  ]\n",
      " [  0.9603344   -3.9273098  -19.29656   ]\n",
      " [  0.9603328   -3.927308   -19.296562  ]\n",
      " [  0.96033126  -3.9273064  -19.296564  ]\n",
      " [  0.9603286   -3.9273036  -19.29657   ]\n",
      " [  0.9603269   -3.927302   -19.296572  ]\n",
      " [  0.96032465  -3.9272995  -19.296576  ]\n",
      " [  0.9603233   -3.927298   -19.296577  ]\n",
      " [  0.96032125  -3.9272962  -19.296581  ]\n",
      " [  0.9603188   -3.9272935  -19.296585  ]\n",
      " [  0.96031725  -3.9272919  -19.296587  ]\n",
      " [  0.96031517  -3.9272897  -19.29659   ]\n",
      " [  0.96031266  -3.927287   -19.296595  ]\n",
      " [  0.9603111   -3.9272854  -19.296597  ]\n",
      " [  0.960309    -3.9272833  -19.2966    ]\n",
      " [  0.9603077   -3.927282   -19.296602  ]\n",
      " [  0.96030575  -3.92728    -19.296606  ]\n",
      " [  0.96030426  -3.9272783  -19.296608  ]\n",
      " [  0.9603022   -3.9272761  -19.296612  ]\n",
      " [  0.96029973  -3.9272738  -19.296616  ]\n",
      " [  0.9602982   -3.927272   -19.296618  ]\n",
      " [  0.96029615  -3.92727    -19.296621  ]\n",
      " [  0.9602939   -3.9272676  -19.296625  ]\n",
      " [  0.9602924   -3.9272661  -19.296627  ]\n",
      " [  0.9602903   -3.927264   -19.296629  ]\n",
      " [  0.9602878   -3.9272614  -19.296635  ]\n",
      " [  0.9602857   -3.9272592  -19.296637  ]\n",
      " [  0.96028376  -3.927257   -19.29664   ]\n",
      " [  0.96028143  -3.9272547  -19.296644  ]\n",
      " [  0.96027994  -3.9272532  -19.296646  ]\n",
      " [  0.9602779   -3.927251   -19.29665   ]\n",
      " [  0.9602754   -3.9272485  -19.296654  ]\n",
      " [  0.96027386  -3.9272468  -19.296656  ]\n",
      " [  0.9602717   -3.9272447  -19.29666   ]\n",
      " [  0.9602704   -3.9272432  -19.296661  ]\n",
      " [  0.96026844  -3.9272413  -19.296665  ]\n",
      " [  0.9602677   -3.9272406  -19.296665  ]\n",
      " [  0.96026564  -3.9272382  -19.296669  ]\n",
      " [  0.9602631   -3.9272356  -19.296673  ]\n",
      " [  0.96026146  -3.927234   -19.296675  ]\n",
      " [  0.9602593   -3.9272318  -19.296679  ]\n",
      " [  0.960258    -3.9272304  -19.29668   ]\n",
      " [  0.960256    -3.9272282  -19.296684  ]\n",
      " [  0.9602536   -3.9272258  -19.296688  ]\n",
      " [  0.96025205  -3.9272242  -19.29669   ]\n",
      " [  0.96024996  -3.927222   -19.296694  ]\n",
      " [  0.9602475   -3.9272194  -19.296698  ]\n",
      " [  0.9602459   -3.927218   -19.2967    ]\n",
      " [  0.9602438   -3.9272156  -19.296703  ]\n",
      " [  0.9602413   -3.9272132  -19.296707  ]\n",
      " [  0.96023977  -3.9272115  -19.296711  ]\n",
      " [  0.9602376   -3.9272091  -19.296713  ]\n",
      " [  0.9602351   -3.9272068  -19.296719  ]\n",
      " [  0.96023357  -3.927205   -19.29672   ]\n",
      " [  0.9602314   -3.9272027  -19.296724  ]\n",
      " [  0.9602301   -3.9272015  -19.296726  ]\n",
      " [  0.96022815  -3.9271994  -19.296728  ]\n",
      " [  0.9602257   -3.927197   -19.296732  ]\n",
      " [  0.9602242   -3.9271953  -19.296736  ]\n",
      " [  0.96022207  -3.9271932  -19.296738  ]\n",
      " [  0.96022075  -3.9271917  -19.29674   ]\n",
      " [  0.9602188   -3.9271898  -19.296743  ]\n",
      " [  0.96021634  -3.9271872  -19.296747  ]\n",
      " [  0.9602178   -3.9271886  -19.296743  ]\n",
      " [  0.9602147   -3.9271855  -19.29675   ]\n",
      " [  0.96021265  -3.9271834  -19.296753  ]\n",
      " [  0.9602102   -3.9271808  -19.296757  ]\n",
      " [  0.96020746  -3.927178   -19.296762  ]\n",
      " [  0.9602058   -3.9271762  -19.296764  ]\n",
      " [  0.96020657  -3.927177   -19.296762  ]\n",
      " [  0.960203    -3.9271734  -19.296768  ]\n",
      " [  0.96020067  -3.927171   -19.296772  ]\n",
      " [  0.9601981   -3.9271681  -19.296776  ]\n",
      " [  0.96019536  -3.9271653  -19.296782  ]\n",
      " [  0.9601936   -3.9271636  -19.296783  ]\n",
      " [  0.96019137  -3.9271612  -19.296787  ]\n",
      " [  0.9601888   -3.9271586  -19.296791  ]\n",
      " [  0.9601872   -3.927157   -19.296793  ]\n",
      " [  0.96018505  -3.9271548  -19.296797  ]\n",
      " [  0.96018374  -3.9271533  -19.296799  ]\n",
      " [  0.9601817   -3.9271512  -19.296803  ]\n",
      " [  0.96018046  -3.92715    -19.296804  ]\n",
      " [  0.96017855  -3.9271479  -19.296808  ]\n",
      " [  0.9601773   -3.9271467  -19.29681   ]\n",
      " [  0.96017534  -3.9271445  -19.296812  ]\n",
      " [  0.9601747   -3.927144   -19.296814  ]\n",
      " [  0.9601726   -3.9271417  -19.296818  ]\n",
      " [  0.96017003  -3.927139   -19.296822  ]\n",
      " [  0.9601684   -3.9271374  -19.296824  ]\n",
      " [  0.9601663   -3.9271352  -19.296827  ]\n",
      " [  0.9601649   -3.9271338  -19.29683   ]\n",
      " [  0.96016294  -3.9271317  -19.296833  ]\n",
      " [  0.9601605   -3.9271293  -19.296837  ]\n",
      " [  0.960159    -3.9271276  -19.296839  ]\n",
      " [  0.9601569   -3.9271255  -19.296843  ]\n",
      " [  0.96015555  -3.927124   -19.296844  ]\n",
      " [  0.9601542   -3.9271226  -19.296846  ]\n",
      " [  0.9601516   -3.92712    -19.29685   ]\n",
      " [  0.96014994  -3.9271183  -19.296852  ]\n",
      " [  0.9601478   -3.927116   -19.296856  ]\n",
      " [  0.9601464   -3.9271145  -19.296858  ]\n",
      " [  0.9601444   -3.9271126  -19.296862  ]\n",
      " [  0.960142    -3.92711    -19.296865  ]\n",
      " [  0.96014047  -3.9271083  -19.296867  ]\n",
      " [  0.9601384   -3.9271061  -19.296871  ]\n",
      " [  0.96013707  -3.9271047  -19.296873  ]] \n",
      "\n",
      "Final Test RMSE:  0.907160018881162\n",
      "‚úÖ Saved best model at epoch 2 (Val Loss = 1.18027091)\n",
      "Epoch 2/1000 | Train Loss=38054.86002604 | Val Loss=1.18027091 | Data=380.52928162 | Physics=1.87318859 | Val RMSE: 2.12639904 | ‚àö(Val Loss) = 1.08640277 | Current Learning Rate: 0.002\n",
      "Epoch 3/1000 | Train Loss=38056.58723958 | Val Loss=1.29662359 | Data=380.54659526 | Physics=1.87731388 | Val RMSE: 2.12691760 | ‚àö(Val Loss) = 1.13869381 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 4 (Val Loss = 1.16811800)\n",
      "Epoch 4/1000 | Train Loss=38097.94726562 | Val Loss=1.16811800 | Data=380.96014404 | Physics=1.98671625 | Val RMSE: 2.12653685 | ‚àö(Val Loss) = 1.08079505 | Current Learning Rate: 0.002\n",
      "Epoch 5/1000 | Train Loss=38231.22200521 | Val Loss=24.01822853 | Data=382.27554321 | Physics=12.17411630 | Val RMSE: 2.10618114 | ‚àö(Val Loss) = 4.90083981 | Current Learning Rate: 0.002\n",
      "Epoch 6/1000 | Train Loss=38083.17513021 | Val Loss=1.18260670 | Data=380.81246948 | Physics=1.91145716 | Val RMSE: 2.14272070 | ‚àö(Val Loss) = 1.08747721 | Current Learning Rate: 0.002\n",
      "Epoch 7/1000 | Train Loss=38067.14648438 | Val Loss=1.43863368 | Data=380.65177917 | Physics=1.96675594 | Val RMSE: 2.12975144 | ‚àö(Val Loss) = 1.19943058 | Current Learning Rate: 0.002\n",
      "Epoch 8/1000 | Train Loss=38065.12955729 | Val Loss=1.31487703 | Data=380.63186137 | Physics=2.00529598 | Val RMSE: 2.11977243 | ‚àö(Val Loss) = 1.14668083 | Current Learning Rate: 0.002\n",
      "Epoch 9/1000 | Train Loss=38042.31119792 | Val Loss=1.22189355 | Data=380.40384420 | Physics=1.90556092 | Val RMSE: 2.12439966 | ‚àö(Val Loss) = 1.10539293 | Current Learning Rate: 0.002\n",
      "Epoch 10/1000 | Train Loss=38055.38541667 | Val Loss=1.31668925 | Data=380.53457642 | Physics=1.88145043 | Val RMSE: 2.12674999 | ‚àö(Val Loss) = 1.14747083 | Current Learning Rate: 0.002\n",
      "Epoch 11/1000 | Train Loss=38049.31966146 | Val Loss=2.06511068 | Data=380.47358704 | Physics=2.17078433 | Val RMSE: 2.12641859 | ‚àö(Val Loss) = 1.43704927 | Current Learning Rate: 0.002\n",
      "Epoch 12/1000 | Train Loss=38030.73763021 | Val Loss=1.18446434 | Data=380.28781128 | Physics=1.94672864 | Val RMSE: 2.13518929 | ‚àö(Val Loss) = 1.08833098 | Current Learning Rate: 0.002\n",
      "Epoch 13/1000 | Train Loss=37999.14518229 | Val Loss=1.77044308 | Data=379.97200012 | Physics=1.89907160 | Val RMSE: 2.12020540 | ‚àö(Val Loss) = 1.33058000 | Current Learning Rate: 0.002\n",
      "Epoch 14/1000 | Train Loss=38032.07421875 | Val Loss=1.69406903 | Data=380.30020142 | Physics=2.39677277 | Val RMSE: 2.11560893 | ‚àö(Val Loss) = 1.30156410 | Current Learning Rate: 0.002\n",
      "Epoch 15/1000 | Train Loss=37991.00976562 | Val Loss=1.42571104 | Data=379.89082845 | Physics=1.84378395 | Val RMSE: 2.12856293 | ‚àö(Val Loss) = 1.19403148 | Current Learning Rate: 0.002\n",
      "Epoch 16/1000 | Train Loss=38062.65364583 | Val Loss=1.26898658 | Data=380.60721334 | Physics=1.95386141 | Val RMSE: 2.12414551 | ‚àö(Val Loss) = 1.12649310 | Current Learning Rate: 0.002\n",
      "Epoch 17/1000 | Train Loss=38039.50000000 | Val Loss=1.25932348 | Data=380.37557983 | Physics=1.95813330 | Val RMSE: 2.13446832 | ‚àö(Val Loss) = 1.12219584 | Current Learning Rate: 0.002\n",
      "Epoch 18/1000 | Train Loss=38016.44791667 | Val Loss=1.55634189 | Data=380.14496867 | Physics=1.97322485 | Val RMSE: 2.12307930 | ‚àö(Val Loss) = 1.24753428 | Current Learning Rate: 0.002\n",
      "Epoch 19/1000 | Train Loss=38054.38476562 | Val Loss=1.52784443 | Data=380.52448527 | Physics=1.93838791 | Val RMSE: 2.12080908 | ‚àö(Val Loss) = 1.23606002 | Current Learning Rate: 0.002\n",
      "Epoch 20/1000 | Train Loss=38048.35286458 | Val Loss=5.26016331 | Data=380.46384176 | Physics=2.10347115 | Val RMSE: 2.14189935 | ‚àö(Val Loss) = 2.29350448 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 21 (Val Loss = 1.11587656)\n",
      "Epoch 21/1000 | Train Loss=37944.65820312 | Val Loss=1.11587656 | Data=379.42380269 | Physics=2.54130529 | Val RMSE: 2.15925765 | ‚àö(Val Loss) = 1.05635059 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 22/1000 | Train Loss=38231.18424479 | Val Loss=1.57157969 | Data=382.29185994 | Physics=1.99752845 | Val RMSE: 2.13244987 | ‚àö(Val Loss) = 1.25362659 | Current Learning Rate: 0.002\n",
      "Epoch 23/1000 | Train Loss=38022.97070312 | Val Loss=1.56286478 | Data=380.20298767 | Physics=4.08623218 | Val RMSE: 2.12197232 | ‚àö(Val Loss) = 1.25014591 | Current Learning Rate: 0.002\n",
      "Epoch 24/1000 | Train Loss=38173.99283854 | Val Loss=1.34583902 | Data=381.71582540 | Physics=1.98557924 | Val RMSE: 2.12300014 | ‚àö(Val Loss) = 1.16010296 | Current Learning Rate: 0.002\n",
      "Epoch 25/1000 | Train Loss=38264.04427083 | Val Loss=1.23650312 | Data=382.61725362 | Physics=1.87696073 | Val RMSE: 2.11829257 | ‚àö(Val Loss) = 1.11198163 | Current Learning Rate: 0.002\n",
      "Epoch 26/1000 | Train Loss=38820.36718750 | Val Loss=1.49328744 | Data=388.17728170 | Physics=2.67770137 | Val RMSE: 2.11462212 | ‚àö(Val Loss) = 1.22200143 | Current Learning Rate: 0.002\n",
      "Epoch 27/1000 | Train Loss=38143.71093750 | Val Loss=1.41453934 | Data=381.41650899 | Physics=2.09860689 | Val RMSE: 2.12273002 | ‚àö(Val Loss) = 1.18934405 | Current Learning Rate: 0.002\n",
      "Epoch 28/1000 | Train Loss=38138.30143229 | Val Loss=1.55188262 | Data=381.36307780 | Physics=1.92675695 | Val RMSE: 2.10758018 | ‚àö(Val Loss) = 1.24574578 | Current Learning Rate: 0.002\n",
      "Epoch 29/1000 | Train Loss=38224.08789062 | Val Loss=1.34471917 | Data=382.21868388 | Physics=1.96686367 | Val RMSE: 2.12605834 | ‚àö(Val Loss) = 1.15962029 | Current Learning Rate: 0.002\n",
      "Epoch 30/1000 | Train Loss=38048.19791667 | Val Loss=1.96276915 | Data=380.46223450 | Physics=2.03832967 | Val RMSE: 2.11383128 | ‚àö(Val Loss) = 1.40098858 | Current Learning Rate: 0.002\n",
      "Epoch 31/1000 | Train Loss=39001.19596354 | Val Loss=3.37088394 | Data=389.98442586 | Physics=3.00734440 | Val RMSE: 2.10111284 | ‚àö(Val Loss) = 1.83599675 | Current Learning Rate: 0.002\n",
      "Epoch 32/1000 | Train Loss=37950.71679688 | Val Loss=1.43063533 | Data=379.48654683 | Physics=1.95303924 | Val RMSE: 2.12399888 | ‚àö(Val Loss) = 1.19609165 | Current Learning Rate: 0.002\n",
      "Epoch 33/1000 | Train Loss=38679.83593750 | Val Loss=1.35260653 | Data=386.77256266 | Physics=1.92452413 | Val RMSE: 2.12519598 | ‚àö(Val Loss) = 1.16301608 | Current Learning Rate: 0.002\n",
      "Epoch 34/1000 | Train Loss=38147.53059896 | Val Loss=1.33259213 | Data=381.45553080 | Physics=2.05211163 | Val RMSE: 2.12583947 | ‚àö(Val Loss) = 1.15437949 | Current Learning Rate: 0.002\n",
      "Epoch 35/1000 | Train Loss=38065.95898438 | Val Loss=1.17911422 | Data=380.63947042 | Physics=2.06828253 | Val RMSE: 2.11175370 | ‚àö(Val Loss) = 1.08587027 | Current Learning Rate: 0.002\n",
      "Epoch 36/1000 | Train Loss=38342.08919271 | Val Loss=1.27501011 | Data=383.39791361 | Physics=1.83717118 | Val RMSE: 2.12499619 | ‚àö(Val Loss) = 1.12916350 | Current Learning Rate: 0.002\n",
      "Epoch 37/1000 | Train Loss=38046.33789062 | Val Loss=1.18882763 | Data=380.44316101 | Physics=2.25892601 | Val RMSE: 2.12185597 | ‚àö(Val Loss) = 1.09033370 | Current Learning Rate: 0.002\n",
      "Epoch 38/1000 | Train Loss=38212.86523438 | Val Loss=1.45507205 | Data=382.10892741 | Physics=1.94844932 | Val RMSE: 2.11094403 | ‚àö(Val Loss) = 1.20626366 | Current Learning Rate: 0.002\n",
      "Epoch 39/1000 | Train Loss=38339.32747396 | Val Loss=1.18174148 | Data=383.37056478 | Physics=1.94667436 | Val RMSE: 2.12283635 | ‚àö(Val Loss) = 1.08707929 | Current Learning Rate: 0.002\n",
      "Epoch 40/1000 | Train Loss=38084.33789062 | Val Loss=1.32376838 | Data=380.82353719 | Physics=2.04476317 | Val RMSE: 2.11838365 | ‚àö(Val Loss) = 1.15055132 | Current Learning Rate: 0.002\n",
      "Epoch 41/1000 | Train Loss=38192.96093750 | Val Loss=1.25321591 | Data=381.90880839 | Physics=2.01166470 | Val RMSE: 2.12206244 | ‚àö(Val Loss) = 1.11947131 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 42/1000 | Train Loss=37921.02734375 | Val Loss=1.29705322 | Data=379.18947347 | Physics=1.95614225 | Val RMSE: 2.11843729 | ‚àö(Val Loss) = 1.13888240 | Current Learning Rate: 0.002\n",
      "Epoch 43/1000 | Train Loss=38902.78320312 | Val Loss=1.31557393 | Data=389.00299072 | Physics=2.22038466 | Val RMSE: 2.12403440 | ‚àö(Val Loss) = 1.14698470 | Current Learning Rate: 0.002\n",
      "Epoch 44/1000 | Train Loss=38075.78580729 | Val Loss=1.28639066 | Data=380.73813375 | Physics=1.89573601 | Val RMSE: 2.12567329 | ‚àö(Val Loss) = 1.13419163 | Current Learning Rate: 0.002\n",
      "Epoch 45/1000 | Train Loss=38290.95703125 | Val Loss=1.18984199 | Data=382.88454692 | Physics=1.87146210 | Val RMSE: 2.12410116 | ‚àö(Val Loss) = 1.09079874 | Current Learning Rate: 0.002\n",
      "Epoch 46/1000 | Train Loss=38321.85221354 | Val Loss=2.76073861 | Data=383.19855245 | Physics=2.13561885 | Val RMSE: 2.11262894 | ‚àö(Val Loss) = 1.66154706 | Current Learning Rate: 0.002\n",
      "Epoch 47/1000 | Train Loss=38074.93098958 | Val Loss=1.29407537 | Data=380.72882080 | Physics=2.00755627 | Val RMSE: 2.12410450 | ‚àö(Val Loss) = 1.13757432 | Current Learning Rate: 0.002\n",
      "Epoch 48/1000 | Train Loss=38552.06315104 | Val Loss=1.25936937 | Data=385.49936422 | Physics=2.07991350 | Val RMSE: 2.12111211 | ‚àö(Val Loss) = 1.12221622 | Current Learning Rate: 0.002\n",
      "Epoch 49/1000 | Train Loss=38062.29361979 | Val Loss=1.26778853 | Data=380.60322062 | Physics=1.80983418 | Val RMSE: 2.12335086 | ‚àö(Val Loss) = 1.12596118 | Current Learning Rate: 0.002\n",
      "Epoch 50/1000 | Train Loss=38780.59700521 | Val Loss=1.82677603 | Data=387.78107707 | Physics=2.44021390 | Val RMSE: 2.11386800 | ‚àö(Val Loss) = 1.35158277 | Current Learning Rate: 0.002\n",
      "Epoch 51/1000 | Train Loss=38175.53255208 | Val Loss=1.31854188 | Data=381.73549906 | Physics=2.04782966 | Val RMSE: 2.12452579 | ‚àö(Val Loss) = 1.14827776 | Current Learning Rate: 0.002\n",
      "Epoch 52/1000 | Train Loss=38505.13736979 | Val Loss=1.28953969 | Data=385.02857971 | Physics=2.02146942 | Val RMSE: 2.12674069 | ‚àö(Val Loss) = 1.13557899 | Current Learning Rate: 0.002\n",
      "Epoch 53/1000 | Train Loss=38093.91796875 | Val Loss=1.28138602 | Data=380.91949463 | Physics=1.91076020 | Val RMSE: 2.12558603 | ‚àö(Val Loss) = 1.13198328 | Current Learning Rate: 0.002\n",
      "Epoch 54/1000 | Train Loss=38323.12695312 | Val Loss=1.22692788 | Data=383.20795186 | Physics=1.95939637 | Val RMSE: 2.11829543 | ‚àö(Val Loss) = 1.10766780 | Current Learning Rate: 0.002\n",
      "Epoch 55/1000 | Train Loss=39060.23307292 | Val Loss=2.16137338 | Data=390.57962545 | Physics=2.25292235 | Val RMSE: 2.10490131 | ‚àö(Val Loss) = 1.47016096 | Current Learning Rate: 0.002\n",
      "Epoch 56/1000 | Train Loss=38199.31184896 | Val Loss=1.28755784 | Data=381.97197978 | Physics=2.17754221 | Val RMSE: 2.12360501 | ‚àö(Val Loss) = 1.13470602 | Current Learning Rate: 0.002\n",
      "Epoch 57/1000 | Train Loss=38098.85221354 | Val Loss=1.37659574 | Data=380.96808370 | Physics=2.27377753 | Val RMSE: 2.10721350 | ‚àö(Val Loss) = 1.17328417 | Current Learning Rate: 0.002\n",
      "Epoch 58/1000 | Train Loss=38589.65039062 | Val Loss=1.19568944 | Data=385.86594645 | Physics=2.09445130 | Val RMSE: 2.12201357 | ‚àö(Val Loss) = 1.09347582 | Current Learning Rate: 0.002\n",
      "Epoch 59/1000 | Train Loss=38176.53125000 | Val Loss=1.25021672 | Data=381.74548340 | Physics=1.94759245 | Val RMSE: 2.12523198 | ‚àö(Val Loss) = 1.11813092 | Current Learning Rate: 0.002\n",
      "Epoch 60/1000 | Train Loss=38440.71679688 | Val Loss=1.20628393 | Data=384.38448079 | Physics=1.94811244 | Val RMSE: 2.12465310 | ‚àö(Val Loss) = 1.09830952 | Current Learning Rate: 0.002\n",
      "Epoch 61/1000 | Train Loss=38151.58984375 | Val Loss=1.26802802 | Data=381.49601746 | Physics=1.93242081 | Val RMSE: 2.12519908 | ‚àö(Val Loss) = 1.12606752 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 62/1000 | Train Loss=38561.49023438 | Val Loss=1.20209324 | Data=385.59238688 | Physics=1.89532462 | Val RMSE: 2.12310672 | ‚àö(Val Loss) = 1.09640014 | Current Learning Rate: 0.002\n",
      "Epoch 63/1000 | Train Loss=38044.76497396 | Val Loss=1.27802432 | Data=380.42774963 | Physics=2.04084866 | Val RMSE: 2.12434244 | ‚àö(Val Loss) = 1.13049734 | Current Learning Rate: 0.002\n",
      "Epoch 64/1000 | Train Loss=38214.02604167 | Val Loss=1.22393048 | Data=382.11999512 | Physics=2.01146088 | Val RMSE: 2.12074113 | ‚àö(Val Loss) = 1.10631394 | Current Learning Rate: 0.002\n",
      "Epoch 65/1000 | Train Loss=38044.58854167 | Val Loss=1.38156748 | Data=380.42521159 | Physics=2.00025246 | Val RMSE: 2.11861014 | ‚àö(Val Loss) = 1.17540097 | Current Learning Rate: 0.002\n",
      "Epoch 66/1000 | Train Loss=38960.57161458 | Val Loss=1.34848583 | Data=389.58147176 | Physics=2.12856596 | Val RMSE: 2.12321520 | ‚àö(Val Loss) = 1.16124320 | Current Learning Rate: 0.002\n",
      "Epoch 67/1000 | Train Loss=38229.10742188 | Val Loss=1.32523239 | Data=382.27125041 | Physics=1.97628090 | Val RMSE: 2.12560654 | ‚àö(Val Loss) = 1.15118742 | Current Learning Rate: 0.002\n",
      "Epoch 68/1000 | Train Loss=38180.11914062 | Val Loss=1.25836647 | Data=381.78015645 | Physics=2.08027924 | Val RMSE: 2.12075782 | ‚àö(Val Loss) = 1.12176931 | Current Learning Rate: 0.002\n",
      "Epoch 69/1000 | Train Loss=38934.31575521 | Val Loss=1.51676595 | Data=389.31802877 | Physics=1.85596562 | Val RMSE: 2.12164068 | ‚àö(Val Loss) = 1.23157048 | Current Learning Rate: 0.002\n",
      "Epoch 70/1000 | Train Loss=38178.92382812 | Val Loss=1.26778245 | Data=381.76947530 | Physics=1.96433073 | Val RMSE: 2.12551928 | ‚àö(Val Loss) = 1.12595844 | Current Learning Rate: 0.002\n",
      "Epoch 71/1000 | Train Loss=38535.69791667 | Val Loss=1.14491975 | Data=385.33130900 | Physics=1.98935802 | Val RMSE: 2.12440372 | ‚àö(Val Loss) = 1.07000923 | Current Learning Rate: 0.002\n",
      "Epoch 72/1000 | Train Loss=38404.76041667 | Val Loss=2.90079165 | Data=384.02721151 | Physics=2.44422264 | Val RMSE: 2.11988521 | ‚àö(Val Loss) = 1.70317101 | Current Learning Rate: 0.002\n",
      "Epoch 73/1000 | Train Loss=37962.15039062 | Val Loss=1.31515682 | Data=379.60173543 | Physics=1.99390499 | Val RMSE: 2.12478352 | ‚àö(Val Loss) = 1.14680290 | Current Learning Rate: 0.002\n",
      "Epoch 74/1000 | Train Loss=38154.83723958 | Val Loss=1.39843380 | Data=381.52843221 | Physics=2.05206996 | Val RMSE: 2.11679769 | ‚àö(Val Loss) = 1.18255389 | Current Learning Rate: 0.002\n",
      "Epoch 75/1000 | Train Loss=38555.53971354 | Val Loss=1.31226766 | Data=385.53163147 | Physics=2.04979732 | Val RMSE: 2.12416124 | ‚àö(Val Loss) = 1.14554250 | Current Learning Rate: 0.002\n",
      "Epoch 76/1000 | Train Loss=38386.55013021 | Val Loss=1.60427451 | Data=383.84557597 | Physics=2.09471451 | Val RMSE: 2.11754370 | ‚àö(Val Loss) = 1.26659954 | Current Learning Rate: 0.002\n",
      "Epoch 77/1000 | Train Loss=38202.37369792 | Val Loss=1.27383637 | Data=382.00380452 | Physics=1.98188305 | Val RMSE: 2.12231565 | ‚àö(Val Loss) = 1.12864363 | Current Learning Rate: 0.002\n",
      "Epoch 78/1000 | Train Loss=38032.01953125 | Val Loss=1.57170939 | Data=380.30046590 | Physics=1.93967113 | Val RMSE: 2.11290145 | ‚àö(Val Loss) = 1.25367832 | Current Learning Rate: 0.002\n",
      "Epoch 79/1000 | Train Loss=39364.58789062 | Val Loss=3.14341354 | Data=393.61865743 | Physics=4.10913731 | Val RMSE: 2.10356641 | ‚àö(Val Loss) = 1.77296746 | Current Learning Rate: 0.002\n",
      "Epoch 80/1000 | Train Loss=38056.67057292 | Val Loss=1.56488299 | Data=380.54406230 | Physics=2.50442221 | Val RMSE: 2.12470675 | ‚àö(Val Loss) = 1.25095284 | Current Learning Rate: 0.002\n",
      "Epoch 81/1000 | Train Loss=38123.48697917 | Val Loss=1.27981043 | Data=381.21510824 | Physics=1.95566005 | Val RMSE: 2.11806870 | ‚àö(Val Loss) = 1.13128710 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 82/1000 | Train Loss=38389.23958333 | Val Loss=1.32767534 | Data=383.86841329 | Physics=1.95306070 | Val RMSE: 2.12403154 | ‚àö(Val Loss) = 1.15224791 | Current Learning Rate: 0.002\n",
      "Epoch 83/1000 | Train Loss=38098.53320312 | Val Loss=1.49929452 | Data=380.96556600 | Physics=2.01732234 | Val RMSE: 2.11823821 | ‚àö(Val Loss) = 1.22445679 | Current Learning Rate: 0.002\n",
      "Epoch 84/1000 | Train Loss=38164.96679688 | Val Loss=1.32177854 | Data=381.62947591 | Physics=2.03630142 | Val RMSE: 2.12339306 | ‚àö(Val Loss) = 1.14968634 | Current Learning Rate: 0.002\n",
      "Epoch 85/1000 | Train Loss=38437.84309896 | Val Loss=1.33503234 | Data=384.35760498 | Physics=2.12432679 | Val RMSE: 2.12414312 | ‚àö(Val Loss) = 1.15543604 | Current Learning Rate: 0.002\n",
      "Epoch 86/1000 | Train Loss=38170.83463542 | Val Loss=1.24654257 | Data=381.68861898 | Physics=1.94122070 | Val RMSE: 2.12324095 | ‚àö(Val Loss) = 1.11648667 | Current Learning Rate: 0.002\n",
      "Epoch 87/1000 | Train Loss=38495.31380208 | Val Loss=1.22147131 | Data=384.92845154 | Physics=1.91220098 | Val RMSE: 2.12550306 | ‚àö(Val Loss) = 1.10520196 | Current Learning Rate: 0.002\n",
      "Epoch 88/1000 | Train Loss=38056.00260417 | Val Loss=2.44973016 | Data=380.53941345 | Physics=2.36490323 | Val RMSE: 2.11600709 | ‚àö(Val Loss) = 1.56516135 | Current Learning Rate: 0.002\n",
      "Epoch 89/1000 | Train Loss=38417.16927083 | Val Loss=1.32250750 | Data=384.14820862 | Physics=1.95191902 | Val RMSE: 2.12456083 | ‚àö(Val Loss) = 1.15000331 | Current Learning Rate: 0.002\n",
      "Epoch 90/1000 | Train Loss=38199.61979167 | Val Loss=1.24069858 | Data=381.97634379 | Physics=2.02402936 | Val RMSE: 2.12175870 | ‚àö(Val Loss) = 1.11386645 | Current Learning Rate: 0.002\n",
      "Epoch 91/1000 | Train Loss=38181.25976562 | Val Loss=1.45510530 | Data=381.79283142 | Physics=1.99441796 | Val RMSE: 2.11278868 | ‚àö(Val Loss) = 1.20627749 | Current Learning Rate: 0.002\n",
      "Epoch 92/1000 | Train Loss=38552.05794271 | Val Loss=1.27278733 | Data=385.49731954 | Physics=1.97703833 | Val RMSE: 2.11972761 | ‚àö(Val Loss) = 1.12817872 | Current Learning Rate: 0.002\n",
      "Epoch 93/1000 | Train Loss=38332.58072917 | Val Loss=2.50573778 | Data=383.30486552 | Physics=2.59918702 | Val RMSE: 2.12319398 | ‚àö(Val Loss) = 1.58295226 | Current Learning Rate: 0.002\n",
      "Epoch 94/1000 | Train Loss=38021.61718750 | Val Loss=1.32911074 | Data=380.19628906 | Physics=1.97320695 | Val RMSE: 2.12295771 | ‚àö(Val Loss) = 1.15287066 | Current Learning Rate: 0.002\n",
      "Epoch 95/1000 | Train Loss=38078.11132812 | Val Loss=1.37314904 | Data=380.76105245 | Physics=2.04826490 | Val RMSE: 2.11554313 | ‚àö(Val Loss) = 1.17181444 | Current Learning Rate: 0.002\n",
      "Epoch 96/1000 | Train Loss=38576.49218750 | Val Loss=1.28316557 | Data=385.74078369 | Physics=1.92337681 | Val RMSE: 2.12355375 | ‚àö(Val Loss) = 1.13276899 | Current Learning Rate: 0.002\n",
      "Epoch 97/1000 | Train Loss=37926.93554688 | Val Loss=1.22768974 | Data=379.24950663 | Physics=1.99091012 | Val RMSE: 2.12105060 | ‚àö(Val Loss) = 1.10801160 | Current Learning Rate: 0.002\n",
      "Epoch 98/1000 | Train Loss=37988.19986979 | Val Loss=1.32905233 | Data=379.86212158 | Physics=1.99918671 | Val RMSE: 2.12386632 | ‚àö(Val Loss) = 1.15284526 | Current Learning Rate: 0.002\n",
      "Epoch 99/1000 | Train Loss=38346.59049479 | Val Loss=1.29207778 | Data=383.44551086 | Physics=1.97690811 | Val RMSE: 2.11804938 | ‚àö(Val Loss) = 1.13669598 | Current Learning Rate: 0.002\n",
      "Epoch 100/1000 | Train Loss=38030.59114583 | Val Loss=1.47757840 | Data=380.28475444 | Physics=1.95697072 | Val RMSE: 2.12678933 | ‚àö(Val Loss) = 1.21555686 | Current Learning Rate: 0.002\n",
      "Epoch 101/1000 | Train Loss=38007.98502604 | Val Loss=1.57397664 | Data=380.06010437 | Physics=1.98955960 | Val RMSE: 2.11919093 | ‚àö(Val Loss) = 1.25458229 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 102/1000 | Train Loss=38958.42187500 | Val Loss=1.34123087 | Data=389.55996704 | Physics=2.16393411 | Val RMSE: 2.12006736 | ‚àö(Val Loss) = 1.15811527 | Current Learning Rate: 0.002\n",
      "Epoch 103/1000 | Train Loss=38167.29101562 | Val Loss=1.26770270 | Data=381.65314738 | Physics=2.03192571 | Val RMSE: 2.12523580 | ‚àö(Val Loss) = 1.12592304 | Current Learning Rate: 0.002\n",
      "Epoch 104/1000 | Train Loss=38420.92187500 | Val Loss=1.11982715 | Data=384.18480428 | Physics=1.91747978 | Val RMSE: 2.12585449 | ‚àö(Val Loss) = 1.05821884 | Current Learning Rate: 0.002\n",
      "Epoch 105/1000 | Train Loss=38158.51692708 | Val Loss=1.19876540 | Data=381.56547038 | Physics=1.91866722 | Val RMSE: 2.12638116 | ‚àö(Val Loss) = 1.09488142 | Current Learning Rate: 0.002\n",
      "Epoch 106/1000 | Train Loss=38234.28710938 | Val Loss=1.24383390 | Data=382.32157389 | Physics=1.93917526 | Val RMSE: 2.12479758 | ‚àö(Val Loss) = 1.11527300 | Current Learning Rate: 0.002\n",
      "Epoch 107/1000 | Train Loss=38037.34765625 | Val Loss=1.22491467 | Data=380.35368856 | Physics=2.05072570 | Val RMSE: 2.12256622 | ‚àö(Val Loss) = 1.10675859 | Current Learning Rate: 0.002\n",
      "Epoch 108/1000 | Train Loss=38131.99479167 | Val Loss=1.21383834 | Data=381.30008952 | Physics=2.00837676 | Val RMSE: 2.11673474 | ‚àö(Val Loss) = 1.10174334 | Current Learning Rate: 0.002\n",
      "Epoch 109/1000 | Train Loss=38498.24674479 | Val Loss=1.27699316 | Data=384.96041870 | Physics=1.94225454 | Val RMSE: 2.12373924 | ‚àö(Val Loss) = 1.13004124 | Current Learning Rate: 0.002\n",
      "Epoch 110/1000 | Train Loss=38057.18164062 | Val Loss=1.27207851 | Data=380.55207316 | Physics=2.00242896 | Val RMSE: 2.12124991 | ‚àö(Val Loss) = 1.12786460 | Current Learning Rate: 0.002\n",
      "Epoch 111/1000 | Train Loss=37772.58138021 | Val Loss=1.45010495 | Data=377.70529175 | Physics=2.20005297 | Val RMSE: 2.12596560 | ‚àö(Val Loss) = 1.20420301 | Current Learning Rate: 0.002\n",
      "Epoch 112/1000 | Train Loss=38059.24544271 | Val Loss=1.48420131 | Data=380.57260640 | Physics=2.00633380 | Val RMSE: 2.11873078 | ‚àö(Val Loss) = 1.21827805 | Current Learning Rate: 0.002\n",
      "Epoch 113/1000 | Train Loss=38133.28841146 | Val Loss=1.40943599 | Data=381.31203715 | Physics=1.97928346 | Val RMSE: 2.12510943 | ‚àö(Val Loss) = 1.18719673 | Current Learning Rate: 0.002\n",
      "Epoch 114/1000 | Train Loss=38000.21809896 | Val Loss=1.30303919 | Data=379.98226929 | Physics=2.08138180 | Val RMSE: 2.12304664 | ‚àö(Val Loss) = 1.14150739 | Current Learning Rate: 0.002\n",
      "Epoch 115/1000 | Train Loss=38165.34505208 | Val Loss=1.22711229 | Data=381.63377380 | Physics=1.87961868 | Val RMSE: 2.11544466 | ‚àö(Val Loss) = 1.10775101 | Current Learning Rate: 0.002\n",
      "Epoch 116/1000 | Train Loss=38645.55403646 | Val Loss=1.29777384 | Data=386.43110657 | Physics=2.03000526 | Val RMSE: 2.12365723 | ‚àö(Val Loss) = 1.13919878 | Current Learning Rate: 0.002\n",
      "Epoch 117/1000 | Train Loss=38108.30273438 | Val Loss=2.33204770 | Data=381.06281535 | Physics=2.26846334 | Val RMSE: 2.11947441 | ‚àö(Val Loss) = 1.52710438 | Current Learning Rate: 0.002\n",
      "Epoch 118/1000 | Train Loss=38123.33593750 | Val Loss=1.31110263 | Data=381.21332804 | Physics=2.08474481 | Val RMSE: 2.12295866 | ‚àö(Val Loss) = 1.14503396 | Current Learning Rate: 0.002\n",
      "Epoch 119/1000 | Train Loss=38140.16536458 | Val Loss=1.37392747 | Data=381.38127645 | Physics=2.11665847 | Val RMSE: 2.12447453 | ‚àö(Val Loss) = 1.17214656 | Current Learning Rate: 0.002\n",
      "Epoch 120/1000 | Train Loss=38158.87955729 | Val Loss=1.29254901 | Data=381.56909688 | Physics=1.95357964 | Val RMSE: 2.12401009 | ‚àö(Val Loss) = 1.13690329 | Current Learning Rate: 0.002\n",
      "Epoch 121/1000 | Train Loss=38590.38411458 | Val Loss=1.14760840 | Data=385.87889099 | Physics=1.94196638 | Val RMSE: 2.12290788 | ‚àö(Val Loss) = 1.07126486 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 122/1000 | Train Loss=37970.33658854 | Val Loss=1.21829307 | Data=379.68358866 | Physics=1.97688382 | Val RMSE: 2.12389660 | ‚àö(Val Loss) = 1.10376310 | Current Learning Rate: 0.002\n",
      "Epoch 123/1000 | Train Loss=38557.66861979 | Val Loss=1.28012931 | Data=385.55578613 | Physics=2.19889427 | Val RMSE: 2.12374306 | ‚àö(Val Loss) = 1.13142800 | Current Learning Rate: 0.002\n",
      "Epoch 124/1000 | Train Loss=38152.33398438 | Val Loss=1.27946949 | Data=381.50358582 | Physics=1.85793183 | Val RMSE: 2.12396479 | ‚àö(Val Loss) = 1.13113642 | Current Learning Rate: 0.002\n",
      "Epoch 125/1000 | Train Loss=38691.75976562 | Val Loss=1.15088189 | Data=386.89308167 | Physics=2.35488718 | Val RMSE: 2.12025762 | ‚àö(Val Loss) = 1.07279158 | Current Learning Rate: 0.002\n",
      "Epoch 126/1000 | Train Loss=38115.36653646 | Val Loss=1.21972370 | Data=381.13399251 | Physics=1.91848192 | Val RMSE: 2.12452340 | ‚àö(Val Loss) = 1.10441101 | Current Learning Rate: 0.002\n",
      "Epoch 127/1000 | Train Loss=38084.81054688 | Val Loss=1.17558157 | Data=380.82783508 | Physics=2.02222533 | Val RMSE: 2.12340188 | ‚àö(Val Loss) = 1.08424234 | Current Learning Rate: 0.002\n",
      "Epoch 128/1000 | Train Loss=37928.41080729 | Val Loss=1.27411318 | Data=379.26438395 | Physics=1.88938578 | Val RMSE: 2.12465024 | ‚àö(Val Loss) = 1.12876618 | Current Learning Rate: 0.002\n",
      "Epoch 129/1000 | Train Loss=38120.36979167 | Val Loss=1.31345689 | Data=381.18390910 | Physics=2.03041787 | Val RMSE: 2.11645031 | ‚àö(Val Loss) = 1.14606142 | Current Learning Rate: 0.002\n",
      "Epoch 130/1000 | Train Loss=38442.15690104 | Val Loss=1.32606697 | Data=384.40019735 | Physics=2.01043045 | Val RMSE: 2.12439585 | ‚àö(Val Loss) = 1.15154982 | Current Learning Rate: 0.002\n",
      "Epoch 131/1000 | Train Loss=38115.35481771 | Val Loss=1.25446546 | Data=381.13365173 | Physics=2.05514291 | Val RMSE: 2.12243748 | ‚àö(Val Loss) = 1.12002921 | Current Learning Rate: 0.002\n",
      "Epoch 132/1000 | Train Loss=38079.25520833 | Val Loss=1.45158625 | Data=380.77270508 | Physics=1.97596524 | Val RMSE: 2.11440420 | ‚àö(Val Loss) = 1.20481789 | Current Learning Rate: 0.002\n",
      "Epoch 133/1000 | Train Loss=38619.49348958 | Val Loss=1.23787177 | Data=386.17107137 | Physics=1.96004376 | Val RMSE: 2.12278461 | ‚àö(Val Loss) = 1.11259687 | Current Learning Rate: 0.002\n",
      "Epoch 134/1000 | Train Loss=38073.21940104 | Val Loss=1.29977739 | Data=380.71232605 | Physics=2.05291421 | Val RMSE: 2.12432837 | ‚àö(Val Loss) = 1.14007783 | Current Learning Rate: 0.002\n",
      "Epoch 135/1000 | Train Loss=38097.50781250 | Val Loss=1.24726045 | Data=380.95498657 | Physics=1.95451154 | Val RMSE: 2.12175989 | ‚àö(Val Loss) = 1.11680818 | Current Learning Rate: 0.002\n",
      "Epoch 136/1000 | Train Loss=37798.43164062 | Val Loss=1.40709102 | Data=377.96387736 | Physics=2.27773155 | Val RMSE: 2.12504101 | ‚àö(Val Loss) = 1.18620872 | Current Learning Rate: 0.002\n",
      "Epoch 137/1000 | Train Loss=38201.41536458 | Val Loss=1.51102006 | Data=381.99416606 | Physics=2.08276162 | Val RMSE: 2.11705446 | ‚àö(Val Loss) = 1.22923553 | Current Learning Rate: 0.002\n",
      "Epoch 138/1000 | Train Loss=38075.72395833 | Val Loss=2.03285527 | Data=380.73567708 | Physics=2.01034997 | Val RMSE: 2.11937857 | ‚àö(Val Loss) = 1.42578232 | Current Learning Rate: 0.002\n",
      "Epoch 139/1000 | Train Loss=38992.40429688 | Val Loss=1.66116357 | Data=389.90012105 | Physics=3.25365158 | Val RMSE: 2.11673069 | ‚àö(Val Loss) = 1.28886139 | Current Learning Rate: 0.002\n",
      "Epoch 140/1000 | Train Loss=38195.03776042 | Val Loss=5.76808310 | Data=381.93018595 | Physics=2.13981654 | Val RMSE: 2.11783791 | ‚àö(Val Loss) = 2.40168333 | Current Learning Rate: 0.002\n",
      "Epoch 141/1000 | Train Loss=38686.23958333 | Val Loss=1.48150301 | Data=386.82937113 | Physics=1.91160781 | Val RMSE: 2.12376714 | ‚àö(Val Loss) = 1.21717012 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 142/1000 | Train Loss=38202.73111979 | Val Loss=1.23434389 | Data=382.00741069 | Physics=2.08059558 | Val RMSE: 2.12608814 | ‚àö(Val Loss) = 1.11101031 | Current Learning Rate: 0.002\n",
      "Epoch 143/1000 | Train Loss=38043.51562500 | Val Loss=1.35964727 | Data=380.40908813 | Physics=1.91157783 | Val RMSE: 2.11980462 | ‚àö(Val Loss) = 1.16603911 | Current Learning Rate: 0.002\n",
      "Epoch 144/1000 | Train Loss=38907.28580729 | Val Loss=2.34545732 | Data=389.04349264 | Physics=1.98006252 | Val RMSE: 2.11752319 | ‚àö(Val Loss) = 1.53148854 | Current Learning Rate: 0.002\n",
      "Epoch 145/1000 | Train Loss=38253.39713542 | Val Loss=1.14480937 | Data=382.51412455 | Physics=2.08692921 | Val RMSE: 2.12344456 | ‚àö(Val Loss) = 1.06995761 | Current Learning Rate: 0.002\n",
      "Epoch 146/1000 | Train Loss=38611.88216146 | Val Loss=1.40946317 | Data=386.09597778 | Physics=2.25820287 | Val RMSE: 2.11553955 | ‚àö(Val Loss) = 1.18720818 | Current Learning Rate: 0.002\n",
      "Epoch 147/1000 | Train Loss=38153.21028646 | Val Loss=1.18370092 | Data=381.51230367 | Physics=2.01963293 | Val RMSE: 2.11287618 | ‚àö(Val Loss) = 1.08798015 | Current Learning Rate: 0.002\n",
      "Epoch 148/1000 | Train Loss=39276.25455729 | Val Loss=1.13246596 | Data=392.73619588 | Physics=3.35563569 | Val RMSE: 2.09486771 | ‚àö(Val Loss) = 1.06417382 | Current Learning Rate: 0.002\n",
      "Epoch 149/1000 | Train Loss=38549.52278646 | Val Loss=1.23644578 | Data=385.43843587 | Physics=3.81100974 | Val RMSE: 2.11624217 | ‚àö(Val Loss) = 1.11195588 | Current Learning Rate: 0.002\n",
      "Epoch 150/1000 | Train Loss=38153.43489583 | Val Loss=1.16833282 | Data=381.51401774 | Physics=2.01296312 | Val RMSE: 2.12518644 | ‚àö(Val Loss) = 1.08089447 | Current Learning Rate: 0.002\n",
      "Epoch 151/1000 | Train Loss=38559.68229167 | Val Loss=1.13696682 | Data=385.57253011 | Physics=2.34815502 | Val RMSE: 2.12246704 | ‚àö(Val Loss) = 1.06628644 | Current Learning Rate: 0.002\n",
      "Epoch 152/1000 | Train Loss=38126.12825521 | Val Loss=1.22094274 | Data=381.24159749 | Physics=1.92677395 | Val RMSE: 2.12525797 | ‚àö(Val Loss) = 1.10496283 | Current Learning Rate: 0.002\n",
      "Epoch 153/1000 | Train Loss=38275.11002604 | Val Loss=1.24189520 | Data=382.72945150 | Physics=2.00639288 | Val RMSE: 2.12562752 | ‚àö(Val Loss) = 1.11440349 | Current Learning Rate: 0.002\n",
      "Epoch 154/1000 | Train Loss=38394.03906250 | Val Loss=3.58865738 | Data=383.91888936 | Physics=2.89000091 | Val RMSE: 2.12081671 | ‚àö(Val Loss) = 1.89437521 | Current Learning Rate: 0.002\n",
      "Epoch 155/1000 | Train Loss=37879.84830729 | Val Loss=1.38710797 | Data=378.77870178 | Physics=1.86542654 | Val RMSE: 2.12469149 | ‚àö(Val Loss) = 1.17775548 | Current Learning Rate: 0.002\n",
      "Epoch 156/1000 | Train Loss=38246.78841146 | Val Loss=1.45131826 | Data=382.44805908 | Physics=1.93308632 | Val RMSE: 2.11949587 | ‚àö(Val Loss) = 1.20470667 | Current Learning Rate: 0.002\n",
      "Epoch 157/1000 | Train Loss=38441.50260417 | Val Loss=1.34376347 | Data=384.39120483 | Physics=1.88359036 | Val RMSE: 2.12440848 | ‚àö(Val Loss) = 1.15920818 | Current Learning Rate: 0.002\n",
      "Epoch 158/1000 | Train Loss=38115.52018229 | Val Loss=1.27659106 | Data=381.13536072 | Physics=1.95762186 | Val RMSE: 2.12270164 | ‚àö(Val Loss) = 1.12986326 | Current Learning Rate: 0.002\n",
      "Epoch 159/1000 | Train Loss=38163.81380208 | Val Loss=1.23176897 | Data=381.61836243 | Physics=1.91620659 | Val RMSE: 2.11500216 | ‚àö(Val Loss) = 1.10985088 | Current Learning Rate: 0.002\n",
      "Epoch 160/1000 | Train Loss=38425.38216146 | Val Loss=1.26899207 | Data=384.23143514 | Physics=1.93952670 | Val RMSE: 2.12321329 | ‚àö(Val Loss) = 1.12649548 | Current Learning Rate: 0.002\n",
      "Epoch 161/1000 | Train Loss=38017.83398438 | Val Loss=2.57213974 | Data=380.15840149 | Physics=2.06751715 | Val RMSE: 2.12088060 | ‚àö(Val Loss) = 1.60378921 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 162/1000 | Train Loss=38068.98046875 | Val Loss=1.56071699 | Data=380.66844686 | Physics=2.06671450 | Val RMSE: 2.12819457 | ‚àö(Val Loss) = 1.24928653 | Current Learning Rate: 0.002\n",
      "Epoch 163/1000 | Train Loss=38004.22200521 | Val Loss=1.39799643 | Data=380.02233887 | Physics=2.04395483 | Val RMSE: 2.12449741 | ‚àö(Val Loss) = 1.18236899 | Current Learning Rate: 0.002\n",
      "Epoch 164/1000 | Train Loss=38087.36002604 | Val Loss=1.23850524 | Data=380.85374451 | Physics=2.14260672 | Val RMSE: 2.11724210 | ‚àö(Val Loss) = 1.11288154 | Current Learning Rate: 0.002\n",
      "Epoch 165/1000 | Train Loss=38519.66471354 | Val Loss=1.34338748 | Data=385.17389425 | Physics=1.97146256 | Val RMSE: 2.12424850 | ‚àö(Val Loss) = 1.15904593 | Current Learning Rate: 0.002\n",
      "Epoch 166/1000 | Train Loss=38179.80338542 | Val Loss=1.26970077 | Data=381.77821859 | Physics=1.93400728 | Val RMSE: 2.12238455 | ‚àö(Val Loss) = 1.12680995 | Current Learning Rate: 0.002\n",
      "Epoch 167/1000 | Train Loss=38204.53190104 | Val Loss=1.43860781 | Data=382.02558899 | Physics=1.93069958 | Val RMSE: 2.11409450 | ‚àö(Val Loss) = 1.19941974 | Current Learning Rate: 0.002\n",
      "Epoch 168/1000 | Train Loss=38500.86783854 | Val Loss=1.26260924 | Data=384.98662313 | Physics=2.00942350 | Val RMSE: 2.12310886 | ‚àö(Val Loss) = 1.12365890 | Current Learning Rate: 0.002\n",
      "Epoch 169/1000 | Train Loss=38026.67187500 | Val Loss=1.26341188 | Data=380.24679057 | Physics=2.02382032 | Val RMSE: 2.12070441 | ‚àö(Val Loss) = 1.12401593 | Current Learning Rate: 0.002\n",
      "Epoch 170/1000 | Train Loss=38051.42578125 | Val Loss=1.34259915 | Data=380.49432882 | Physics=2.06876169 | Val RMSE: 2.12391996 | ‚àö(Val Loss) = 1.15870583 | Current Learning Rate: 0.002\n",
      "Epoch 171/1000 | Train Loss=38375.49088542 | Val Loss=1.34173417 | Data=383.73414612 | Physics=2.06217362 | Val RMSE: 2.12425137 | ‚àö(Val Loss) = 1.15833247 | Current Learning Rate: 0.002\n",
      "Epoch 172/1000 | Train Loss=38196.03190104 | Val Loss=1.19093275 | Data=381.94049072 | Physics=2.02182280 | Val RMSE: 2.12355614 | ‚àö(Val Loss) = 1.09129870 | Current Learning Rate: 0.002\n",
      "Epoch 173/1000 | Train Loss=38913.32226562 | Val Loss=1.19116163 | Data=389.10943604 | Physics=2.79138510 | Val RMSE: 2.11118054 | ‚àö(Val Loss) = 1.09140348 | Current Learning Rate: 0.002\n",
      "Epoch 174/1000 | Train Loss=38050.72786458 | Val Loss=1.47489285 | Data=380.48593140 | Physics=2.07970896 | Val RMSE: 2.12317872 | ‚àö(Val Loss) = 1.21445167 | Current Learning Rate: 0.002\n",
      "Epoch 175/1000 | Train Loss=38116.60807292 | Val Loss=1.17574465 | Data=381.14632670 | Physics=1.83554263 | Val RMSE: 2.12203050 | ‚àö(Val Loss) = 1.08431756 | Current Learning Rate: 0.002\n",
      "Epoch 176/1000 | Train Loss=38148.38541667 | Val Loss=2.54797530 | Data=381.46421814 | Physics=1.95079212 | Val RMSE: 2.10342169 | ‚àö(Val Loss) = 1.59623790 | Current Learning Rate: 0.002\n",
      "Epoch 177/1000 | Train Loss=38899.14518229 | Val Loss=2.41168785 | Data=388.96632385 | Physics=3.11998489 | Val RMSE: 2.11066341 | ‚àö(Val Loss) = 1.55296099 | Current Learning Rate: 0.002\n",
      "Epoch 178/1000 | Train Loss=38156.46484375 | Val Loss=1.62842703 | Data=381.54454549 | Physics=2.18534729 | Val RMSE: 2.12184620 | ‚àö(Val Loss) = 1.27609837 | Current Learning Rate: 0.002\n",
      "Epoch 179/1000 | Train Loss=38306.11393229 | Val Loss=1.43477499 | Data=383.03888448 | Physics=1.96864311 | Val RMSE: 2.12543488 | ‚àö(Val Loss) = 1.19782090 | Current Learning Rate: 0.002\n",
      "Epoch 180/1000 | Train Loss=38255.02408854 | Val Loss=1.29318321 | Data=382.53030396 | Physics=1.97223837 | Val RMSE: 2.12254596 | ‚àö(Val Loss) = 1.13718212 | Current Learning Rate: 0.002\n",
      "Epoch 181/1000 | Train Loss=38080.24869792 | Val Loss=1.23542011 | Data=380.78280640 | Physics=1.85280756 | Val RMSE: 2.11561275 | ‚àö(Val Loss) = 1.11149454 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 182/1000 | Train Loss=38598.06184896 | Val Loss=1.28898466 | Data=385.95646159 | Physics=1.98630142 | Val RMSE: 2.12346506 | ‚àö(Val Loss) = 1.13533461 | Current Learning Rate: 0.002\n",
      "Epoch 183/1000 | Train Loss=37997.19010417 | Val Loss=1.32195854 | Data=379.95213826 | Physics=1.92533647 | Val RMSE: 2.12411690 | ‚àö(Val Loss) = 1.14976454 | Current Learning Rate: 0.002\n",
      "Epoch 184/1000 | Train Loss=38518.48111979 | Val Loss=1.27558601 | Data=385.16376241 | Physics=1.97370127 | Val RMSE: 2.12152028 | ‚àö(Val Loss) = 1.12941849 | Current Learning Rate: 0.002\n",
      "Epoch 185/1000 | Train Loss=38149.42447917 | Val Loss=1.28945339 | Data=381.47446187 | Physics=1.91721466 | Val RMSE: 2.12379932 | ‚àö(Val Loss) = 1.13554096 | Current Learning Rate: 0.002\n",
      "Epoch 186/1000 | Train Loss=38588.26171875 | Val Loss=1.24802375 | Data=385.85931905 | Physics=2.01198330 | Val RMSE: 2.12480569 | ‚àö(Val Loss) = 1.11714983 | Current Learning Rate: 0.002\n",
      "Epoch 187/1000 | Train Loss=38079.03580729 | Val Loss=1.19902849 | Data=380.77066549 | Physics=1.89836684 | Val RMSE: 2.12536573 | ‚àö(Val Loss) = 1.09500158 | Current Learning Rate: 0.002\n",
      "Epoch 188/1000 | Train Loss=38372.93750000 | Val Loss=1.13029110 | Data=383.70362854 | Physics=1.98562684 | Val RMSE: 2.12391067 | ‚àö(Val Loss) = 1.06315148 | Current Learning Rate: 0.002\n",
      "Epoch 189/1000 | Train Loss=38119.52864583 | Val Loss=1.17312968 | Data=381.17548116 | Physics=1.91007807 | Val RMSE: 2.12220716 | ‚àö(Val Loss) = 1.08311117 | Current Learning Rate: 0.002\n",
      "Epoch 190/1000 | Train Loss=38126.74088542 | Val Loss=1.42506218 | Data=381.24774679 | Physics=1.92179849 | Val RMSE: 2.11475468 | ‚àö(Val Loss) = 1.19375968 | Current Learning Rate: 0.002\n",
      "Epoch 191/1000 | Train Loss=38542.19075521 | Val Loss=1.23178113 | Data=385.39894613 | Physics=1.99681783 | Val RMSE: 2.12301612 | ‚àö(Val Loss) = 1.10985637 | Current Learning Rate: 0.002\n",
      "Epoch 192/1000 | Train Loss=37952.31315104 | Val Loss=1.30526352 | Data=379.50351461 | Physics=1.91100180 | Val RMSE: 2.12424660 | ‚àö(Val Loss) = 1.14248133 | Current Learning Rate: 0.002\n",
      "Epoch 193/1000 | Train Loss=38265.82096354 | Val Loss=1.34891415 | Data=382.63768514 | Physics=2.00973183 | Val RMSE: 2.12454939 | ‚àö(Val Loss) = 1.16142762 | Current Learning Rate: 0.002\n",
      "Epoch 194/1000 | Train Loss=38101.03710938 | Val Loss=1.26457119 | Data=380.99058533 | Physics=1.97934234 | Val RMSE: 2.12422991 | ‚àö(Val Loss) = 1.12453151 | Current Learning Rate: 0.002\n",
      "Epoch 195/1000 | Train Loss=38575.56054688 | Val Loss=1.29069030 | Data=385.73186747 | Physics=1.99595562 | Val RMSE: 2.11772537 | ‚àö(Val Loss) = 1.13608551 | Current Learning Rate: 0.002\n",
      "Epoch 196/1000 | Train Loss=37832.90559896 | Val Loss=1.32411039 | Data=378.30905660 | Physics=2.08029726 | Val RMSE: 2.12458229 | ‚àö(Val Loss) = 1.15069997 | Current Learning Rate: 0.002\n",
      "Epoch 197/1000 | Train Loss=38037.87304688 | Val Loss=1.31374550 | Data=380.35880534 | Physics=1.88528598 | Val RMSE: 2.12377501 | ‚àö(Val Loss) = 1.14618742 | Current Learning Rate: 0.002\n",
      "Epoch 198/1000 | Train Loss=38221.17513021 | Val Loss=1.64235842 | Data=382.19100952 | Physics=2.06548721 | Val RMSE: 2.11889791 | ‚àö(Val Loss) = 1.28154528 | Current Learning Rate: 0.002\n",
      "Epoch 199/1000 | Train Loss=38835.47526042 | Val Loss=1.67625141 | Data=388.33332825 | Physics=2.20987390 | Val RMSE: 2.12071896 | ‚àö(Val Loss) = 1.29470134 | Current Learning Rate: 0.002\n",
      "Epoch 200/1000 | Train Loss=38164.52083333 | Val Loss=1.32056737 | Data=381.62543233 | Physics=2.03169542 | Val RMSE: 2.12367892 | ‚àö(Val Loss) = 1.14915943 | Current Learning Rate: 0.002\n",
      "Epoch 201/1000 | Train Loss=38849.37565104 | Val Loss=2.31980515 | Data=388.46694946 | Physics=2.92634434 | Val RMSE: 2.10808206 | ‚àö(Val Loss) = 1.52309060 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 202/1000 | Train Loss=38046.02278646 | Val Loss=1.30395949 | Data=380.44004822 | Physics=1.88572590 | Val RMSE: 2.12283993 | ‚àö(Val Loss) = 1.14191043 | Current Learning Rate: 0.002\n",
      "Epoch 203/1000 | Train Loss=38206.75585938 | Val Loss=1.52172911 | Data=382.04647319 | Physics=2.16228228 | Val RMSE: 2.12818336 | ‚àö(Val Loss) = 1.23358381 | Current Learning Rate: 0.002\n",
      "Epoch 204/1000 | Train Loss=38230.66666667 | Val Loss=1.37919140 | Data=382.28688049 | Physics=2.02050283 | Val RMSE: 2.12647510 | ‚àö(Val Loss) = 1.17438984 | Current Learning Rate: 0.002\n",
      "Epoch 205/1000 | Train Loss=38027.25260417 | Val Loss=1.19001615 | Data=380.25264994 | Physics=1.98855252 | Val RMSE: 2.11743832 | ‚àö(Val Loss) = 1.09087861 | Current Learning Rate: 0.002\n",
      "Epoch 206/1000 | Train Loss=38205.51367188 | Val Loss=1.28344357 | Data=382.03445435 | Physics=1.97974637 | Val RMSE: 2.11772513 | ‚àö(Val Loss) = 1.13289165 | Current Learning Rate: 0.002\n",
      "Epoch 207/1000 | Train Loss=37984.65885417 | Val Loss=1.46284580 | Data=379.82619731 | Physics=2.19796779 | Val RMSE: 2.12667346 | ‚àö(Val Loss) = 1.20948160 | Current Learning Rate: 0.002\n",
      "Epoch 208/1000 | Train Loss=38200.92903646 | Val Loss=1.36704516 | Data=381.98949687 | Physics=2.03000746 | Val RMSE: 2.12497091 | ‚àö(Val Loss) = 1.16920710 | Current Learning Rate: 0.002\n",
      "Epoch 209/1000 | Train Loss=38658.96028646 | Val Loss=1.23438609 | Data=386.56476847 | Physics=2.05037972 | Val RMSE: 2.12416530 | ‚àö(Val Loss) = 1.11102927 | Current Learning Rate: 0.002\n",
      "Epoch 210/1000 | Train Loss=38151.04166667 | Val Loss=1.21900380 | Data=381.49070740 | Physics=1.94556600 | Val RMSE: 2.12198162 | ‚àö(Val Loss) = 1.10408509 | Current Learning Rate: 0.002\n",
      "Epoch 211/1000 | Train Loss=38166.71354167 | Val Loss=1.46962667 | Data=381.64737956 | Physics=1.97423302 | Val RMSE: 2.11440229 | ‚àö(Val Loss) = 1.21228158 | Current Learning Rate: 0.002\n",
      "Epoch 212/1000 | Train Loss=38589.88476562 | Val Loss=1.23927104 | Data=385.87560018 | Physics=1.95879077 | Val RMSE: 2.12280369 | ‚àö(Val Loss) = 1.11322546 | Current Learning Rate: 0.002\n",
      "Epoch 213/1000 | Train Loss=37918.87890625 | Val Loss=1.29664612 | Data=379.16898600 | Physics=1.94478608 | Val RMSE: 2.12362051 | ‚àö(Val Loss) = 1.13870370 | Current Learning Rate: 0.002\n",
      "Epoch 214/1000 | Train Loss=38507.61783854 | Val Loss=1.34991312 | Data=385.05554199 | Physics=2.18257496 | Val RMSE: 2.12408113 | ‚àö(Val Loss) = 1.16185760 | Current Learning Rate: 0.002\n",
      "Epoch 215/1000 | Train Loss=38145.56054688 | Val Loss=1.26949167 | Data=381.43582662 | Physics=1.90258299 | Val RMSE: 2.12347245 | ‚àö(Val Loss) = 1.12671721 | Current Learning Rate: 0.002\n",
      "Epoch 216/1000 | Train Loss=38862.43619792 | Val Loss=1.13829231 | Data=388.59925334 | Physics=2.03146097 | Val RMSE: 2.12038779 | ‚àö(Val Loss) = 1.06690788 | Current Learning Rate: 0.002\n",
      "Epoch 217/1000 | Train Loss=38163.35221354 | Val Loss=1.17592192 | Data=381.61381531 | Physics=1.95953446 | Val RMSE: 2.12408352 | ‚àö(Val Loss) = 1.08439934 | Current Learning Rate: 0.002\n",
      "Epoch 218/1000 | Train Loss=38465.45833333 | Val Loss=1.19099689 | Data=384.63151042 | Physics=1.99517368 | Val RMSE: 2.12462735 | ‚àö(Val Loss) = 1.09132802 | Current Learning Rate: 0.002\n",
      "Epoch 219/1000 | Train Loss=38166.12760417 | Val Loss=1.23199534 | Data=381.64153544 | Physics=1.97653207 | Val RMSE: 2.12479115 | ‚àö(Val Loss) = 1.10995281 | Current Learning Rate: 0.002\n",
      "Epoch 220/1000 | Train Loss=38600.27734375 | Val Loss=1.13341939 | Data=385.97859192 | Physics=2.01718247 | Val RMSE: 2.12323427 | ‚àö(Val Loss) = 1.06462169 | Current Learning Rate: 0.002\n",
      "Epoch 221/1000 | Train Loss=38140.96679688 | Val Loss=1.19623077 | Data=381.39004517 | Physics=1.83845564 | Val RMSE: 2.12412882 | ‚àö(Val Loss) = 1.09372330 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 222/1000 | Train Loss=38403.31054688 | Val Loss=1.17989886 | Data=384.00949605 | Physics=1.88367720 | Val RMSE: 2.12237716 | ‚àö(Val Loss) = 1.08623147 | Current Learning Rate: 0.002\n",
      "Epoch 223/1000 | Train Loss=38042.00716146 | Val Loss=1.22080803 | Data=380.40011088 | Physics=2.05198267 | Val RMSE: 2.12263227 | ‚àö(Val Loss) = 1.10490179 | Current Learning Rate: 0.002\n",
      "Epoch 224/1000 | Train Loss=38117.05013021 | Val Loss=1.20681548 | Data=381.15078227 | Physics=1.90553971 | Val RMSE: 2.11573839 | ‚àö(Val Loss) = 1.09855151 | Current Learning Rate: 0.002\n",
      "Epoch 225/1000 | Train Loss=38525.64583333 | Val Loss=1.26002908 | Data=385.23299662 | Physics=1.90839764 | Val RMSE: 2.12336469 | ‚àö(Val Loss) = 1.12251019 | Current Learning Rate: 0.002\n",
      "Epoch 226/1000 | Train Loss=38121.85351562 | Val Loss=1.26434338 | Data=381.19866435 | Physics=1.95445959 | Val RMSE: 2.12000895 | ‚àö(Val Loss) = 1.12443030 | Current Learning Rate: 0.002\n",
      "Epoch 227/1000 | Train Loss=38019.07291667 | Val Loss=1.33303499 | Data=380.17090352 | Physics=1.93975972 | Val RMSE: 2.12403774 | ‚àö(Val Loss) = 1.15457129 | Current Learning Rate: 0.002\n",
      "Epoch 228/1000 | Train Loss=38213.61197917 | Val Loss=1.38832808 | Data=382.11528524 | Physics=2.37176998 | Val RMSE: 2.12473512 | ‚àö(Val Loss) = 1.17827332 | Current Learning Rate: 0.002\n",
      "Epoch 229/1000 | Train Loss=38139.33138021 | Val Loss=1.28721344 | Data=381.37356059 | Physics=1.97821527 | Val RMSE: 2.12390804 | ‚àö(Val Loss) = 1.13455427 | Current Learning Rate: 0.002\n",
      "Epoch 230/1000 | Train Loss=38965.42317708 | Val Loss=1.17085814 | Data=389.62924703 | Physics=2.35474601 | Val RMSE: 2.12092924 | ‚àö(Val Loss) = 1.08206201 | Current Learning Rate: 0.002\n",
      "Epoch 231/1000 | Train Loss=38217.42122396 | Val Loss=1.19130909 | Data=382.15445455 | Physics=2.00419607 | Val RMSE: 2.12526870 | ‚àö(Val Loss) = 1.09147108 | Current Learning Rate: 0.002\n",
      "Epoch 232/1000 | Train Loss=38009.38411458 | Val Loss=1.17022467 | Data=380.07417297 | Physics=1.93906481 | Val RMSE: 2.12308455 | ‚àö(Val Loss) = 1.08176923 | Current Learning Rate: 0.002\n",
      "Epoch 233/1000 | Train Loss=37981.88411458 | Val Loss=1.41412747 | Data=379.79821269 | Physics=1.96971884 | Val RMSE: 2.12767982 | ‚àö(Val Loss) = 1.18917096 | Current Learning Rate: 0.002\n",
      "Epoch 234/1000 | Train Loss=38127.77929688 | Val Loss=2.35672021 | Data=381.25804647 | Physics=1.94771891 | Val RMSE: 2.12321997 | ‚àö(Val Loss) = 1.53516126 | Current Learning Rate: 0.002\n",
      "Epoch 235/1000 | Train Loss=38098.93815104 | Val Loss=1.35288501 | Data=380.96909587 | Physics=2.08640878 | Val RMSE: 2.12180638 | ‚àö(Val Loss) = 1.16313589 | Current Learning Rate: 0.002\n",
      "Epoch 236/1000 | Train Loss=38141.85742188 | Val Loss=1.32323527 | Data=381.39807129 | Physics=2.02559623 | Val RMSE: 2.11894989 | ‚àö(Val Loss) = 1.15031970 | Current Learning Rate: 0.002\n",
      "Epoch 237/1000 | Train Loss=38492.80664062 | Val Loss=1.38939035 | Data=384.90684001 | Physics=2.02373810 | Val RMSE: 2.12443423 | ‚àö(Val Loss) = 1.17872405 | Current Learning Rate: 0.002\n",
      "Epoch 238/1000 | Train Loss=38156.48177083 | Val Loss=1.26250494 | Data=381.54504903 | Physics=1.95169199 | Val RMSE: 2.12333059 | ‚àö(Val Loss) = 1.12361240 | Current Learning Rate: 0.002\n",
      "Epoch 239/1000 | Train Loss=38824.79947917 | Val Loss=1.33634472 | Data=388.22295634 | Physics=2.37889255 | Val RMSE: 2.11423993 | ‚àö(Val Loss) = 1.15600371 | Current Learning Rate: 0.002\n",
      "Epoch 240/1000 | Train Loss=38110.67447917 | Val Loss=1.27802658 | Data=381.08683268 | Physics=1.85603758 | Val RMSE: 2.12183380 | ‚àö(Val Loss) = 1.13049841 | Current Learning Rate: 0.002\n",
      "Epoch 241/1000 | Train Loss=38047.13085938 | Val Loss=1.23375547 | Data=380.45151265 | Physics=1.86281144 | Val RMSE: 2.12406421 | ‚àö(Val Loss) = 1.11074543 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 242/1000 | Train Loss=38219.76822917 | Val Loss=1.23367476 | Data=382.17716980 | Physics=1.99886553 | Val RMSE: 2.11989331 | ‚àö(Val Loss) = 1.11070907 | Current Learning Rate: 0.002\n",
      "Epoch 243/1000 | Train Loss=37647.98046875 | Val Loss=1.75529480 | Data=376.45936076 | Physics=2.10206596 | Val RMSE: 2.12085009 | ‚àö(Val Loss) = 1.32487535 | Current Learning Rate: 0.002\n",
      "Epoch 244/1000 | Train Loss=38231.48502604 | Val Loss=1.56845498 | Data=382.29227702 | Physics=2.16420509 | Val RMSE: 2.12715244 | ‚àö(Val Loss) = 1.25237978 | Current Learning Rate: 0.002\n",
      "Epoch 245/1000 | Train Loss=38157.56380208 | Val Loss=1.35667348 | Data=381.55588786 | Physics=1.96293283 | Val RMSE: 2.12483048 | ‚àö(Val Loss) = 1.16476321 | Current Learning Rate: 0.002\n",
      "Epoch 246/1000 | Train Loss=38945.75976562 | Val Loss=1.12438786 | Data=389.42703756 | Physics=1.95204442 | Val RMSE: 2.12392712 | ‚àö(Val Loss) = 1.06037152 | Current Learning Rate: 0.002\n",
      "Epoch 247/1000 | Train Loss=38173.97460938 | Val Loss=1.16941869 | Data=381.72003682 | Physics=1.91054321 | Val RMSE: 2.12587142 | ‚àö(Val Loss) = 1.08139658 | Current Learning Rate: 0.002\n",
      "Epoch 248/1000 | Train Loss=38244.05403646 | Val Loss=1.42679524 | Data=382.42072550 | Physics=1.96836936 | Val RMSE: 2.11700082 | ‚àö(Val Loss) = 1.19448531 | Current Learning Rate: 0.002\n",
      "Epoch 249/1000 | Train Loss=37971.19010417 | Val Loss=1.19458508 | Data=379.69224040 | Physics=1.97027116 | Val RMSE: 2.11955786 | ‚àö(Val Loss) = 1.09297073 | Current Learning Rate: 0.002\n",
      "Epoch 250/1000 | Train Loss=38165.54036458 | Val Loss=1.25851786 | Data=381.63512166 | Physics=1.93679732 | Val RMSE: 2.12314224 | ‚àö(Val Loss) = 1.12183678 | Current Learning Rate: 0.002\n",
      "Epoch 251/1000 | Train Loss=38384.43294271 | Val Loss=1.27779698 | Data=383.82310994 | Physics=2.02462170 | Val RMSE: 2.12365913 | ‚àö(Val Loss) = 1.13039684 | Current Learning Rate: 0.002\n",
      "Epoch 252/1000 | Train Loss=38160.60872396 | Val Loss=1.21373808 | Data=381.58635457 | Physics=1.90563156 | Val RMSE: 2.12324500 | ‚àö(Val Loss) = 1.10169780 | Current Learning Rate: 0.002\n",
      "Epoch 253/1000 | Train Loss=38908.23437500 | Val Loss=2.13366866 | Data=389.05754089 | Physics=2.45519380 | Val RMSE: 2.10925412 | ‚àö(Val Loss) = 1.46070826 | Current Learning Rate: 0.002\n",
      "Epoch 254/1000 | Train Loss=38293.99088542 | Val Loss=1.19279671 | Data=382.92004903 | Physics=1.98778871 | Val RMSE: 2.12142491 | ‚àö(Val Loss) = 1.09215236 | Current Learning Rate: 0.002\n",
      "Epoch 255/1000 | Train Loss=38621.34700521 | Val Loss=1.30025983 | Data=386.19031270 | Physics=2.07928008 | Val RMSE: 2.12377381 | ‚àö(Val Loss) = 1.14028931 | Current Learning Rate: 0.002\n",
      "Epoch 256/1000 | Train Loss=38141.51888021 | Val Loss=1.27890813 | Data=381.39542135 | Physics=1.96501845 | Val RMSE: 2.12556076 | ‚àö(Val Loss) = 1.13088822 | Current Learning Rate: 0.002\n",
      "Epoch 257/1000 | Train Loss=38252.45898438 | Val Loss=1.14795136 | Data=382.50146484 | Physics=1.84991628 | Val RMSE: 2.11425972 | ‚àö(Val Loss) = 1.07142496 | Current Learning Rate: 0.002\n",
      "Epoch 258/1000 | Train Loss=39102.20507812 | Val Loss=4.60177946 | Data=390.99593608 | Physics=2.85376586 | Val RMSE: 2.10447907 | ‚àö(Val Loss) = 2.14517593 | Current Learning Rate: 0.002\n",
      "Epoch 259/1000 | Train Loss=38295.77473958 | Val Loss=1.23550868 | Data=382.93758138 | Physics=1.96973589 | Val RMSE: 2.12019467 | ‚àö(Val Loss) = 1.11153436 | Current Learning Rate: 0.002\n",
      "Epoch 260/1000 | Train Loss=38066.99414062 | Val Loss=1.26248980 | Data=380.64988708 | Physics=2.13998013 | Val RMSE: 2.12039876 | ‚àö(Val Loss) = 1.12360573 | Current Learning Rate: 0.002\n",
      "Epoch 261/1000 | Train Loss=38021.32942708 | Val Loss=1.37699139 | Data=380.19320170 | Physics=2.04205722 | Val RMSE: 2.12453628 | ‚àö(Val Loss) = 1.17345273 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 262/1000 | Train Loss=38123.29361979 | Val Loss=1.29948175 | Data=381.21251933 | Physics=1.96312742 | Val RMSE: 2.12262464 | ‚àö(Val Loss) = 1.13994813 | Current Learning Rate: 0.002\n",
      "Epoch 263/1000 | Train Loss=38330.20768229 | Val Loss=1.34309101 | Data=383.28087362 | Physics=1.98569386 | Val RMSE: 2.12410092 | ‚àö(Val Loss) = 1.15891802 | Current Learning Rate: 0.002\n",
      "Epoch 264/1000 | Train Loss=38104.81315104 | Val Loss=1.21874988 | Data=381.02833557 | Physics=1.85886371 | Val RMSE: 2.12366319 | ‚àö(Val Loss) = 1.10397005 | Current Learning Rate: 0.002\n",
      "Epoch 265/1000 | Train Loss=38899.11197917 | Val Loss=1.22507370 | Data=388.96633403 | Physics=2.25302296 | Val RMSE: 2.11812758 | ‚àö(Val Loss) = 1.10683048 | Current Learning Rate: 0.002\n",
      "Epoch 266/1000 | Train Loss=38125.96158854 | Val Loss=1.16960537 | Data=381.23992411 | Physics=1.94768016 | Val RMSE: 2.12382722 | ‚àö(Val Loss) = 1.08148301 | Current Learning Rate: 0.002\n",
      "Epoch 267/1000 | Train Loss=38688.61328125 | Val Loss=2.47979021 | Data=386.86391195 | Physics=2.48063122 | Val RMSE: 2.12137771 | ‚àö(Val Loss) = 1.57473493 | Current Learning Rate: 0.002\n",
      "Epoch 268/1000 | Train Loss=38152.68554688 | Val Loss=1.32148814 | Data=381.50698853 | Physics=1.95558300 | Val RMSE: 2.12603521 | ‚àö(Val Loss) = 1.14955997 | Current Learning Rate: 0.002\n",
      "Epoch 269/1000 | Train Loss=37952.74609375 | Val Loss=1.20090747 | Data=379.50750732 | Physics=1.98331913 | Val RMSE: 2.12197137 | ‚àö(Val Loss) = 1.09585929 | Current Learning Rate: 0.002\n",
      "Epoch 270/1000 | Train Loss=38753.52083333 | Val Loss=1.37943029 | Data=387.51064046 | Physics=1.92857276 | Val RMSE: 2.12202287 | ‚àö(Val Loss) = 1.17449152 | Current Learning Rate: 0.002\n",
      "Epoch 271/1000 | Train Loss=38220.97916667 | Val Loss=1.20944345 | Data=382.18996175 | Physics=1.90509008 | Val RMSE: 2.12627196 | ‚àö(Val Loss) = 1.09974694 | Current Learning Rate: 0.002\n",
      "Epoch 272/1000 | Train Loss=38115.11197917 | Val Loss=1.13657331 | Data=381.12676493 | Physics=1.91193715 | Val RMSE: 2.12599540 | ‚àö(Val Loss) = 1.06610191 | Current Learning Rate: 0.002\n",
      "Epoch 273/1000 | Train Loss=37987.32486979 | Val Loss=1.21433520 | Data=379.85359701 | Physics=1.95051073 | Val RMSE: 2.11775398 | ‚àö(Val Loss) = 1.10196877 | Current Learning Rate: 0.002\n",
      "Epoch 274/1000 | Train Loss=39006.62044271 | Val Loss=1.37536716 | Data=390.04282633 | Physics=2.80691035 | Val RMSE: 2.12046123 | ‚àö(Val Loss) = 1.17276049 | Current Learning Rate: 0.002\n",
      "Epoch 275/1000 | Train Loss=38163.98632812 | Val Loss=1.29739547 | Data=381.62012227 | Physics=1.96907470 | Val RMSE: 2.12553167 | ‚àö(Val Loss) = 1.13903272 | Current Learning Rate: 0.002\n",
      "Epoch 276/1000 | Train Loss=38062.26497396 | Val Loss=1.13756001 | Data=380.60241191 | Physics=1.94823854 | Val RMSE: 2.12415028 | ‚àö(Val Loss) = 1.06656456 | Current Learning Rate: 0.002\n",
      "Epoch 277/1000 | Train Loss=37928.33658854 | Val Loss=1.24071836 | Data=379.26161194 | Physics=2.19659827 | Val RMSE: 2.12430477 | ‚àö(Val Loss) = 1.11387539 | Current Learning Rate: 0.002\n",
      "Epoch 278/1000 | Train Loss=38199.79231771 | Val Loss=1.23337436 | Data=381.97773234 | Physics=2.05435550 | Val RMSE: 2.12156653 | ‚àö(Val Loss) = 1.11057389 | Current Learning Rate: 0.002\n",
      "Epoch 279/1000 | Train Loss=38367.40690104 | Val Loss=1.29676116 | Data=383.65363057 | Physics=2.14871218 | Val RMSE: 2.12353349 | ‚àö(Val Loss) = 1.13875425 | Current Learning Rate: 0.002\n",
      "Epoch 280/1000 | Train Loss=38141.19596354 | Val Loss=1.27979505 | Data=381.39217122 | Physics=1.86492433 | Val RMSE: 2.12521625 | ‚àö(Val Loss) = 1.13128030 | Current Learning Rate: 0.002\n",
      "Epoch 281/1000 | Train Loss=38076.23111979 | Val Loss=1.16099954 | Data=380.74183146 | Physics=1.89117919 | Val RMSE: 2.12373614 | ‚àö(Val Loss) = 1.07749689 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 282/1000 | Train Loss=38359.49414062 | Val Loss=1.22650599 | Data=383.57440694 | Physics=2.21851882 | Val RMSE: 2.12398124 | ‚àö(Val Loss) = 1.10747731 | Current Learning Rate: 0.002\n",
      "Epoch 283/1000 | Train Loss=38181.11914062 | Val Loss=1.21225357 | Data=381.79148356 | Physics=1.91846444 | Val RMSE: 2.12523484 | ‚àö(Val Loss) = 1.10102391 | Current Learning Rate: 0.002\n",
      "Epoch 284/1000 | Train Loss=38854.92838542 | Val Loss=1.15602720 | Data=388.52190653 | Physics=2.00947180 | Val RMSE: 2.12341928 | ‚àö(Val Loss) = 1.07518709 | Current Learning Rate: 0.002\n",
      "Epoch 285/1000 | Train Loss=38186.35156250 | Val Loss=1.18463635 | Data=381.84378052 | Physics=1.92855746 | Val RMSE: 2.12564564 | ‚àö(Val Loss) = 1.08841002 | Current Learning Rate: 0.002\n",
      "Epoch 286/1000 | Train Loss=38587.36783854 | Val Loss=1.13596880 | Data=385.84578451 | Physics=1.95134569 | Val RMSE: 2.12358904 | ‚àö(Val Loss) = 1.06581843 | Current Learning Rate: 0.002\n",
      "Epoch 287/1000 | Train Loss=38176.47656250 | Val Loss=1.16936779 | Data=381.74498494 | Physics=1.93862917 | Val RMSE: 2.12470913 | ‚àö(Val Loss) = 1.08137310 | Current Learning Rate: 0.002\n",
      "Epoch 288/1000 | Train Loss=38145.46744792 | Val Loss=1.16208398 | Data=381.43435669 | Physics=1.96171847 | Val RMSE: 2.12058568 | ‚àö(Val Loss) = 1.07799995 | Current Learning Rate: 0.002\n",
      "Epoch 289/1000 | Train Loss=38835.10481771 | Val Loss=1.70236278 | Data=388.32791138 | Physics=2.30990727 | Val RMSE: 2.12043118 | ‚àö(Val Loss) = 1.30474627 | Current Learning Rate: 0.002\n",
      "Epoch 290/1000 | Train Loss=38141.15169271 | Val Loss=1.35266256 | Data=381.39175415 | Physics=1.92611720 | Val RMSE: 2.12588263 | ‚àö(Val Loss) = 1.16304028 | Current Learning Rate: 0.002\n",
      "Epoch 291/1000 | Train Loss=38354.17252604 | Val Loss=1.19524133 | Data=383.51894633 | Physics=1.99587230 | Val RMSE: 2.12460470 | ‚àö(Val Loss) = 1.09327090 | Current Learning Rate: 0.002\n",
      "Epoch 292/1000 | Train Loss=38043.89778646 | Val Loss=1.20567727 | Data=380.41927592 | Physics=2.03483706 | Val RMSE: 2.12288928 | ‚àö(Val Loss) = 1.09803331 | Current Learning Rate: 0.002\n",
      "Epoch 293/1000 | Train Loss=38092.35286458 | Val Loss=1.19598997 | Data=380.90373739 | Physics=2.01969313 | Val RMSE: 2.11640167 | ‚àö(Val Loss) = 1.09361327 | Current Learning Rate: 0.002\n",
      "Epoch 294/1000 | Train Loss=38521.65364583 | Val Loss=1.26662862 | Data=385.19388835 | Physics=1.99514039 | Val RMSE: 2.12361979 | ‚àö(Val Loss) = 1.12544596 | Current Learning Rate: 0.002\n",
      "Epoch 295/1000 | Train Loss=37992.57617188 | Val Loss=1.29679811 | Data=379.90588379 | Physics=2.07050417 | Val RMSE: 2.12179780 | ‚àö(Val Loss) = 1.13877046 | Current Learning Rate: 0.002\n",
      "Epoch 296/1000 | Train Loss=38539.76888021 | Val Loss=1.39636147 | Data=385.37672933 | Physics=2.49102187 | Val RMSE: 2.11902714 | ‚àö(Val Loss) = 1.18167734 | Current Learning Rate: 0.002\n",
      "Epoch 297/1000 | Train Loss=38199.58333333 | Val Loss=1.36229050 | Data=381.97595723 | Physics=2.03347095 | Val RMSE: 2.12373900 | ‚àö(Val Loss) = 1.16717196 | Current Learning Rate: 0.002\n",
      "Epoch 298/1000 | Train Loss=38655.58919271 | Val Loss=1.16260982 | Data=386.53086853 | Physics=2.41624158 | Val RMSE: 2.12339783 | ‚àö(Val Loss) = 1.07824385 | Current Learning Rate: 0.002\n",
      "Epoch 299/1000 | Train Loss=38222.44921875 | Val Loss=1.21785581 | Data=382.20475260 | Physics=1.91602346 | Val RMSE: 2.12503004 | ‚àö(Val Loss) = 1.10356510 | Current Learning Rate: 0.002\n",
      "Epoch 300/1000 | Train Loss=38130.27929688 | Val Loss=1.17779875 | Data=381.28154500 | Physics=1.91710849 | Val RMSE: 2.11685443 | ‚àö(Val Loss) = 1.08526433 | Current Learning Rate: 0.002\n",
      "Epoch 301/1000 | Train Loss=38917.07031250 | Val Loss=2.74738121 | Data=389.14677938 | Physics=2.31002522 | Val RMSE: 2.11497760 | ‚àö(Val Loss) = 1.65752256 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 302/1000 | Train Loss=38125.39518229 | Val Loss=1.27795684 | Data=381.23415629 | Physics=1.95034261 | Val RMSE: 2.12129617 | ‚àö(Val Loss) = 1.13046753 | Current Learning Rate: 0.002\n",
      "Epoch 303/1000 | Train Loss=38734.21028646 | Val Loss=2.04694319 | Data=387.31652323 | Physics=2.45287384 | Val RMSE: 2.10757089 | ‚àö(Val Loss) = 1.43071425 | Current Learning Rate: 0.002\n",
      "Epoch 304/1000 | Train Loss=38152.55859375 | Val Loss=1.22148907 | Data=381.50572205 | Physics=2.06797524 | Val RMSE: 2.11805773 | ‚àö(Val Loss) = 1.10520995 | Current Learning Rate: 0.002\n",
      "Epoch 305/1000 | Train Loss=38406.93684896 | Val Loss=1.35073102 | Data=384.04693095 | Physics=2.14994770 | Val RMSE: 2.12574482 | ‚àö(Val Loss) = 1.16220951 | Current Learning Rate: 0.002\n",
      "Epoch 306/1000 | Train Loss=38143.25065104 | Val Loss=1.18929458 | Data=381.41275024 | Physics=1.83202985 | Val RMSE: 2.12581372 | ‚àö(Val Loss) = 1.09054780 | Current Learning Rate: 0.002\n",
      "Epoch 307/1000 | Train Loss=38230.74023438 | Val Loss=1.39625406 | Data=382.28347778 | Physics=1.91178805 | Val RMSE: 2.11735845 | ‚àö(Val Loss) = 1.18163192 | Current Learning Rate: 0.002\n",
      "Epoch 308/1000 | Train Loss=39064.61393229 | Val Loss=2.35012364 | Data=390.62100220 | Physics=1.99583447 | Val RMSE: 2.11948824 | ‚àö(Val Loss) = 1.53301132 | Current Learning Rate: 0.002\n",
      "Epoch 309/1000 | Train Loss=38203.55924479 | Val Loss=1.27155185 | Data=382.01586914 | Physics=1.94001500 | Val RMSE: 2.12662268 | ‚àö(Val Loss) = 1.12763107 | Current Learning Rate: 0.002\n",
      "Epoch 310/1000 | Train Loss=38238.30403646 | Val Loss=1.40961897 | Data=382.36328634 | Physics=2.01283118 | Val RMSE: 2.11794758 | ‚àö(Val Loss) = 1.18727374 | Current Learning Rate: 0.002\n",
      "Epoch 311/1000 | Train Loss=38013.65169271 | Val Loss=1.34226727 | Data=380.11681112 | Physics=1.93477470 | Val RMSE: 2.11661410 | ‚àö(Val Loss) = 1.15856254 | Current Learning Rate: 0.002\n",
      "Epoch 312/1000 | Train Loss=38961.23242188 | Val Loss=2.00934410 | Data=389.58761088 | Physics=2.61779657 | Val RMSE: 2.11306453 | ‚àö(Val Loss) = 1.41751337 | Current Learning Rate: 0.002\n",
      "Epoch 313/1000 | Train Loss=38017.96744792 | Val Loss=1.71139860 | Data=380.15988668 | Physics=1.89624447 | Val RMSE: 2.11787820 | ‚àö(Val Loss) = 1.30820429 | Current Learning Rate: 0.002\n",
      "Epoch 314/1000 | Train Loss=38974.28906250 | Val Loss=2.29423928 | Data=389.71934509 | Physics=2.39946088 | Val RMSE: 2.11855459 | ‚àö(Val Loss) = 1.51467466 | Current Learning Rate: 0.002\n",
      "Epoch 315/1000 | Train Loss=38168.36067708 | Val Loss=1.30707228 | Data=381.66385905 | Physics=1.91936648 | Val RMSE: 2.12433076 | ‚àö(Val Loss) = 1.14327264 | Current Learning Rate: 0.002\n",
      "Epoch 316/1000 | Train Loss=38566.08984375 | Val Loss=1.83112180 | Data=385.63421122 | Physics=3.08091352 | Val RMSE: 2.11124945 | ‚àö(Val Loss) = 1.35318947 | Current Learning Rate: 0.002\n",
      "Epoch 317/1000 | Train Loss=38126.10546875 | Val Loss=1.43027210 | Data=381.24051412 | Physics=2.15000813 | Val RMSE: 2.12293935 | ‚àö(Val Loss) = 1.19593990 | Current Learning Rate: 0.002\n",
      "Epoch 318/1000 | Train Loss=38136.15755208 | Val Loss=1.19561207 | Data=381.34174601 | Physics=2.00153632 | Val RMSE: 2.11955523 | ‚àö(Val Loss) = 1.09344053 | Current Learning Rate: 0.002\n",
      "Epoch 319/1000 | Train Loss=38142.90234375 | Val Loss=1.26355422 | Data=381.40912374 | Physics=1.96362035 | Val RMSE: 2.12239122 | ‚àö(Val Loss) = 1.12407923 | Current Learning Rate: 0.002\n",
      "Epoch 320/1000 | Train Loss=38254.10286458 | Val Loss=1.86076200 | Data=382.52132670 | Physics=1.87730696 | Val RMSE: 2.10725403 | ‚àö(Val Loss) = 1.36409748 | Current Learning Rate: 0.002\n",
      "Epoch 321/1000 | Train Loss=38790.75911458 | Val Loss=1.24960613 | Data=387.88221741 | Physics=2.36300730 | Val RMSE: 2.12154293 | ‚àö(Val Loss) = 1.11785781 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 322/1000 | Train Loss=38178.51041667 | Val Loss=1.23319125 | Data=381.76527913 | Physics=1.97138009 | Val RMSE: 2.12535405 | ‚àö(Val Loss) = 1.11049139 | Current Learning Rate: 0.002\n",
      "Epoch 323/1000 | Train Loss=38378.74023438 | Val Loss=1.12713861 | Data=383.76507568 | Physics=1.87015946 | Val RMSE: 2.12596273 | ‚àö(Val Loss) = 1.06166780 | Current Learning Rate: 0.002\n",
      "Epoch 324/1000 | Train Loss=38136.77148438 | Val Loss=1.21882069 | Data=381.34801737 | Physics=1.86419422 | Val RMSE: 2.12630010 | ‚àö(Val Loss) = 1.10400212 | Current Learning Rate: 0.002\n",
      "Epoch 325/1000 | Train Loss=38072.28906250 | Val Loss=1.15566301 | Data=380.70270793 | Physics=1.98868890 | Val RMSE: 2.11885476 | ‚àö(Val Loss) = 1.07501769 | Current Learning Rate: 0.002\n",
      "Epoch 326/1000 | Train Loss=38198.22070312 | Val Loss=1.25100088 | Data=381.96140035 | Physics=1.90031191 | Val RMSE: 2.11748362 | ‚àö(Val Loss) = 1.11848152 | Current Learning Rate: 0.002\n",
      "Epoch 327/1000 | Train Loss=38122.07486979 | Val Loss=1.42692995 | Data=381.19956970 | Physics=1.95539787 | Val RMSE: 2.12701249 | ‚àö(Val Loss) = 1.19454169 | Current Learning Rate: 0.002\n",
      "Epoch 328/1000 | Train Loss=38023.08723958 | Val Loss=1.56752813 | Data=380.21106974 | Physics=1.99548297 | Val RMSE: 2.11820054 | ‚àö(Val Loss) = 1.25200963 | Current Learning Rate: 0.002\n",
      "Epoch 329/1000 | Train Loss=38910.57747396 | Val Loss=1.33733344 | Data=389.08158875 | Physics=2.15830455 | Val RMSE: 2.12448120 | ‚àö(Val Loss) = 1.15643132 | Current Learning Rate: 0.002\n",
      "Epoch 330/1000 | Train Loss=38172.53580729 | Val Loss=1.24357378 | Data=381.70549520 | Physics=2.04453214 | Val RMSE: 2.12547255 | ‚àö(Val Loss) = 1.11515641 | Current Learning Rate: 0.002\n",
      "Epoch 331/1000 | Train Loss=38132.27343750 | Val Loss=1.13337433 | Data=381.30118815 | Physics=1.92700196 | Val RMSE: 2.12115359 | ‚àö(Val Loss) = 1.06460059 | Current Learning Rate: 0.002\n",
      "Epoch 332/1000 | Train Loss=38793.15755208 | Val Loss=1.48370159 | Data=387.90487162 | Physics=1.88726765 | Val RMSE: 2.12093997 | ‚àö(Val Loss) = 1.21807289 | Current Learning Rate: 0.002\n",
      "Epoch 333/1000 | Train Loss=38176.08723958 | Val Loss=1.21547043 | Data=381.74110921 | Physics=1.93815132 | Val RMSE: 2.12601042 | ‚àö(Val Loss) = 1.10248375 | Current Learning Rate: 0.002\n",
      "Epoch 334/1000 | Train Loss=38408.68945312 | Val Loss=1.17163074 | Data=384.06314596 | Physics=1.89302222 | Val RMSE: 2.12509155 | ‚àö(Val Loss) = 1.08241892 | Current Learning Rate: 0.002\n",
      "Epoch 335/1000 | Train Loss=38098.87239583 | Val Loss=1.20830965 | Data=380.96856181 | Physics=2.24238196 | Val RMSE: 2.12342429 | ‚àö(Val Loss) = 1.09923136 | Current Learning Rate: 0.002\n",
      "Epoch 336/1000 | Train Loss=38134.82421875 | Val Loss=1.18091810 | Data=381.32846578 | Physics=2.00452406 | Val RMSE: 2.11812830 | ‚àö(Val Loss) = 1.08670056 | Current Learning Rate: 0.002\n",
      "Epoch 337/1000 | Train Loss=38523.92903646 | Val Loss=1.28357208 | Data=385.21672058 | Physics=1.98261909 | Val RMSE: 2.12365508 | ‚àö(Val Loss) = 1.13294840 | Current Learning Rate: 0.002\n",
      "Epoch 338/1000 | Train Loss=38069.68033854 | Val Loss=4.08155251 | Data=380.67620341 | Physics=2.50810535 | Val RMSE: 2.12383556 | ‚àö(Val Loss) = 2.02028537 | Current Learning Rate: 0.002\n",
      "Epoch 339/1000 | Train Loss=38556.70898438 | Val Loss=2.33169293 | Data=385.54691569 | Physics=2.15926448 | Val RMSE: 2.12449265 | ‚àö(Val Loss) = 1.52698815 | Current Learning Rate: 0.002\n",
      "Epoch 340/1000 | Train Loss=38124.31901042 | Val Loss=1.35713065 | Data=381.22204081 | Physics=2.07178990 | Val RMSE: 2.12182522 | ‚àö(Val Loss) = 1.16495955 | Current Learning Rate: 0.002\n",
      "Epoch 341/1000 | Train Loss=38210.90429688 | Val Loss=1.90635872 | Data=382.08927409 | Physics=2.06374931 | Val RMSE: 2.11432648 | ‚àö(Val Loss) = 1.38070953 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9537892   -3.9150712  -19.561138  ]\n",
      " [  0.95378846  -3.9150703  -19.561155  ]\n",
      " [  0.95378786  -3.9150693  -19.56117   ]\n",
      " ...\n",
      " [  0.95334566  -3.914432   -19.5697    ]\n",
      " [  0.9533445   -3.9144306  -19.569714  ]\n",
      " [  0.9533438   -3.9144297  -19.569725  ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95334333  -3.914429   -19.569736  ]\n",
      " [  0.9533429   -3.9144282  -19.569746  ]\n",
      " [  0.95334154  -3.9144266  -19.569763  ]\n",
      " ...\n",
      " [  0.9525149   -3.9132175  -19.585976  ]\n",
      " [  0.9525122   -3.913214   -19.58602   ]\n",
      " [  0.9525105   -3.913211   -19.586058  ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9525082   -3.913208   -19.5861    ]\n",
      " [  0.9525057   -3.9132044  -19.586143  ]\n",
      " [  0.95250404  -3.9132016  -19.586184  ]\n",
      " [  0.9525018   -3.9131985  -19.586226  ]\n",
      " [  0.9524994   -3.913195   -19.586271  ]\n",
      " [  0.95249677  -3.913191   -19.586319  ]\n",
      " [  0.9524948   -3.9131882  -19.58636   ]\n",
      " [  0.9524933   -3.9131858  -19.5864    ]\n",
      " [  0.95249206  -3.913184   -19.586432  ]\n",
      " [  0.95249015  -3.913181   -19.58647   ]\n",
      " [  0.9524878   -3.9131777  -19.586512  ]\n",
      " [  0.9524862   -3.913175   -19.586548  ]\n",
      " [  0.9524841   -3.913172   -19.586588  ]\n",
      " [  0.9524826   -3.9131699  -19.586624  ]\n",
      " [  0.95248044  -3.9131668  -19.586662  ]\n",
      " [  0.9524795   -3.913165   -19.586693  ]\n",
      " [  0.95247716  -3.9131618  -19.586733  ]\n",
      " [  0.95247567  -3.9131594  -19.586767  ]\n",
      " [  0.9524735   -3.9131563  -19.586805  ]\n",
      " [  0.95247203  -3.9131541  -19.58684   ]\n",
      " [  0.95247     -3.913151   -19.586878  ]\n",
      " [  0.9524677   -3.9131477  -19.58692   ]\n",
      " [  0.952466    -3.9131453  -19.586956  ]\n",
      " [  0.9524662   -3.913145   -19.586975  ]\n",
      " [  0.95246273  -3.9131405  -19.587023  ]\n",
      " [  0.95246065  -3.9131372  -19.587063  ]\n",
      " [  0.9524582   -3.9131339  -19.587107  ]\n",
      " [  0.95245653  -3.9131312  -19.587143  ]\n",
      " [  0.9524543   -3.913128   -19.587185  ]\n",
      " [  0.95245236  -3.913125   -19.587225  ]\n",
      " [  0.95245045  -3.9131222  -19.587265  ]\n",
      " [  0.9524481   -3.9131188  -19.587309  ]\n",
      " [  0.95244646  -3.9131162  -19.587347  ]\n",
      " [  0.95244473  -3.9131136  -19.587383  ]\n",
      " [  0.952443    -3.913111   -19.587421  ]\n",
      " [  0.9524408   -3.9131079  -19.587461  ]\n",
      " [  0.95243925  -3.9131055  -19.587498  ]\n",
      " [  0.95243716  -3.9131024  -19.587538  ]\n",
      " [  0.9524348   -3.9130988  -19.58758   ]\n",
      " [  0.9524331   -3.9130964  -19.587618  ]\n",
      " [  0.9524309   -3.913093   -19.58766   ]\n",
      " [  0.9524294   -3.9130907  -19.587694  ]\n",
      " [  0.95242727  -3.9130876  -19.587734  ]\n",
      " [  0.9524258   -3.9130855  -19.58777   ]\n",
      " [  0.95242375  -3.9130824  -19.587809  ]\n",
      " [  0.95242137  -3.913079   -19.587852  ]\n",
      " [  0.95241976  -3.9130764  -19.587889  ]\n",
      " [  0.9524176   -3.9130733  -19.58793   ]\n",
      " [  0.95241517  -3.9130697  -19.587975  ]\n",
      " [  0.95241356  -3.9130673  -19.58801   ]\n",
      " [  0.95241135  -3.913064   -19.588053  ]\n",
      " [  0.95240897  -3.9130607  -19.588099  ]\n",
      " [  0.95240635  -3.9130569  -19.588144  ]\n",
      " [  0.9524068   -3.9130569  -19.588165  ]\n",
      " [  0.95240355  -3.9130526  -19.588213  ]\n",
      " [  0.9524006   -3.9130483  -19.588263  ]\n",
      " [  0.9523982   -3.9130447  -19.588308  ]\n",
      " [  0.9523959   -3.9130414  -19.588354  ]\n",
      " [  0.9523947   -3.9130392  -19.58839   ]\n",
      " [  0.95239234  -3.9130359  -19.588434  ]\n",
      " [  0.9523907   -3.9130332  -19.588472  ]\n",
      " [  0.9523885   -3.9130301  -19.588514  ]\n",
      " [  0.952387    -3.9130278  -19.58855   ]\n",
      " [  0.9523849   -3.9130247  -19.588593  ]\n",
      " [  0.9523825   -3.913021   -19.588636  ]\n",
      " [  0.9523813   -3.9130192  -19.58867   ]\n",
      " [  0.9523789   -3.9130158  -19.588713  ]\n",
      " [  0.9523773   -3.9130132  -19.588749  ]\n",
      " [  0.9523752   -3.9130101  -19.588789  ]\n",
      " [  0.9523728   -3.9130068  -19.588833  ]\n",
      " [  0.9523712   -3.9130042  -19.588871  ]\n",
      " [  0.95236856  -3.9130003  -19.588917  ]\n",
      " [  0.95236814  -3.9129994  -19.588943  ]\n",
      " [  0.9523657   -3.912996   -19.588985  ]\n",
      " [  0.9523632   -3.9129925  -19.58903   ]\n",
      " [  0.9523615   -3.9129899  -19.589067  ]\n",
      " [  0.9523593   -3.9129865  -19.58911   ]\n",
      " [  0.95235777  -3.9129841  -19.589146  ]\n",
      " [  0.9523557   -3.912981   -19.589186  ]\n",
      " [  0.9523533   -3.9129777  -19.58923   ]\n",
      " [  0.9523517   -3.912975   -19.589268  ]\n",
      " [  0.9523495   -3.912972   -19.589308  ]\n",
      " [  0.95234704  -3.9129684  -19.589354  ]\n",
      " [  0.9523454   -3.9129658  -19.589392  ]\n",
      " [  0.95234317  -3.9129627  -19.589434  ]\n",
      " [  0.9523416   -3.91296    -19.589472  ]\n",
      " [  0.95233953  -3.9129572  -19.589512  ]\n",
      " [  0.95233715  -3.9129536  -19.589556  ]\n",
      " [  0.95233554  -3.912951   -19.589594  ]\n",
      " [  0.95233333  -3.912948   -19.589636  ]\n",
      " [  0.95233184  -3.9129455  -19.589672  ]\n",
      " [  0.95232975  -3.9129424  -19.589712  ]\n",
      " [  0.9523274   -3.912939   -19.589756  ]\n",
      " [  0.95232576  -3.9129364  -19.589794  ]\n",
      " [  0.9523241   -3.9129338  -19.58983   ]\n",
      " [  0.9523219   -3.9129307  -19.589872  ]\n",
      " [  0.95231986  -3.9129279  -19.589912  ]\n",
      " [  0.9523176   -3.9129243  -19.589954  ]\n",
      " [  0.952316    -3.912922   -19.589993  ]\n",
      " [  0.9523139   -3.9129188  -19.590033  ]\n",
      " [  0.9523124   -3.9129164  -19.590069  ]\n",
      " [  0.9523103   -3.9129133  -19.590109  ]\n",
      " [  0.95230806  -3.91291    -19.590153  ]\n",
      " [  0.95230645  -3.9129076  -19.590189  ]\n",
      " [  0.9523034   -3.9129033  -19.590239  ]\n",
      " [  0.95230144  -3.9129004  -19.59028   ]\n",
      " [  0.9523      -3.912898   -19.590317  ]\n",
      " [  0.9523002   -3.9128978  -19.590338  ]\n",
      " [  0.9522968   -3.9128933  -19.590385  ]\n",
      " [  0.95229477  -3.9128902  -19.590425  ]\n",
      " [  0.9522924   -3.9128869  -19.59047   ]\n",
      " [  0.9522899   -3.912883   -19.590515  ]\n",
      " [  0.9522881   -3.9128804  -19.590555  ]\n",
      " [  0.9522859   -3.912877   -19.5906    ]\n",
      " [  0.95228434  -3.9128747  -19.590637  ]\n",
      " [  0.95228225  -3.9128716  -19.590677  ]\n",
      " [  0.95227987  -3.912868   -19.590721  ]\n",
      " [  0.9522782   -3.9128654  -19.590761  ]\n",
      " [  0.95227605  -3.9128623  -19.590803  ]\n",
      " [  0.95227367  -3.9128587  -19.590847  ]\n",
      " [  0.952272    -3.9128563  -19.590887  ]\n",
      " [  0.95226985  -3.912853   -19.590929  ]\n",
      " [  0.9522674   -3.9128494  -19.590973  ]\n",
      " [  0.9522658   -3.912847   -19.591013  ]\n",
      " [  0.9522636   -3.9128437  -19.591055  ]\n",
      " [  0.9522612   -3.9128401  -19.5911    ]\n",
      " [  0.95225954  -3.9128377  -19.591139  ]\n",
      " [  0.9522574   -3.9128344  -19.591183  ]\n",
      " [  0.95225495  -3.9128308  -19.591227  ]\n",
      " [  0.9522533   -3.9128282  -19.591267  ]\n",
      " [  0.95225114  -3.912825   -19.59131   ]\n",
      " [  0.9522496   -3.9128227  -19.591347  ]\n",
      " [  0.9522475   -3.9128196  -19.591389  ]\n",
      " [  0.9522452   -3.9128163  -19.591433  ]\n",
      " [  0.95224357  -3.9128137  -19.59147   ]\n",
      " [  0.9522436   -3.9128134  -19.591492  ]\n",
      " [  0.9522411   -3.9128098  -19.59153   ]\n",
      " [  0.9522387   -3.9128063  -19.591572  ]\n",
      " [  0.9522362   -3.9128027  -19.591618  ]\n",
      " [  0.95223445  -3.9128     -19.591656  ]\n",
      " [  0.95223224  -3.9127967  -19.5917    ]\n",
      " [  0.9522298   -3.9127932  -19.591743  ]\n",
      " [  0.9522282   -3.9127905  -19.591784  ]\n",
      " [  0.952226    -3.9127874  -19.591825  ]\n",
      " [  0.9522241   -3.9127846  -19.591866  ]\n",
      " [  0.95222265  -3.9127822  -19.591902  ]\n",
      " [  0.9522202   -3.9127789  -19.591946  ]\n",
      " [  0.9522186   -3.9127762  -19.591984  ]\n",
      " [  0.95221645  -3.9127731  -19.592024  ]\n",
      " [  0.9522141   -3.9127696  -19.592068  ]\n",
      " [  0.9522125   -3.9127672  -19.592106  ]\n",
      " [  0.95221037  -3.912764   -19.592148  ]\n",
      " [  0.952208    -3.9127605  -19.592192  ]\n",
      " [  0.9522059   -3.9127574  -19.592236  ]\n",
      " [  0.952204    -3.9127545  -19.592276  ]\n",
      " [  0.95220166  -3.912751   -19.59232   ]\n",
      " [  0.9522001   -3.9127486  -19.592358  ]\n",
      " [  0.95219797  -3.9127455  -19.5924    ]\n",
      " [  0.95219654  -3.912743   -19.592436  ]\n",
      " [  0.9521954   -3.9127412  -19.592468  ]\n",
      " [  0.95219266  -3.9127374  -19.592512  ]\n",
      " [  0.95219094  -3.9127347  -19.59255   ]\n",
      " [  0.95218873  -3.9127316  -19.592592  ]\n",
      " [  0.9521864   -3.912728   -19.592636  ]\n",
      " [  0.95218474  -3.9127254  -19.592674  ]\n",
      " [  0.9521826   -3.9127223  -19.592716  ]\n",
      " [  0.95218116  -3.91272    -19.592752  ]\n",
      " [  0.95217913  -3.912717   -19.592793  ]\n",
      " [  0.9521768   -3.9127138  -19.592836  ]\n",
      " [  0.95217526  -3.9127111  -19.592873  ]\n",
      " [  0.9521732   -3.912708   -19.592913  ]\n",
      " [  0.95217085  -3.9127047  -19.592957  ]\n",
      " [  0.95216924  -3.9127023  -19.592995  ]\n",
      " [  0.95216715  -3.9126992  -19.593037  ]\n",
      " [  0.95216566  -3.9126968  -19.593073  ]\n",
      " [  0.9521658   -3.9126966  -19.593092  ]\n",
      " [  0.9521634   -3.9126933  -19.59313   ]\n",
      " [  0.952161    -3.91269    -19.59317   ]\n",
      " [  0.9521581   -3.9126856  -19.593218  ]\n",
      " [  0.9521566   -3.9126832  -19.593256  ]\n",
      " [  0.952155    -3.9126809  -19.593292  ]\n",
      " [  0.9521525   -3.9126773  -19.593336  ]\n",
      " [  0.9521508   -3.9126747  -19.593374  ]\n",
      " [  0.9521487   -3.9126713  -19.593416  ]\n",
      " [  0.95214725  -3.9126692  -19.59345   ]\n",
      " [  0.9521452   -3.912666   -19.59349   ]\n",
      " [  0.95214295  -3.9126627  -19.593534  ]\n",
      " [  0.95214134  -3.9126604  -19.59357   ]\n",
      " [  0.9521393   -3.9126573  -19.59361   ]\n",
      " [  0.952137    -3.912654   -19.593655  ]\n",
      " [  0.95213544  -3.9126515  -19.59369   ]\n",
      " [  0.95213336  -3.9126484  -19.593733  ]\n",
      " [  0.9521319   -3.912646   -19.593767  ]\n",
      " [  0.95212996  -3.9126432  -19.593807  ]\n",
      " [  0.9521284   -3.9126408  -19.593842  ]\n",
      " [  0.9521264   -3.9126377  -19.593882  ]\n",
      " [  0.9521241   -3.9126344  -19.593924  ]\n",
      " [  0.95212257  -3.912632   -19.59396   ]\n",
      " [  0.9521206   -3.9126291  -19.593998  ]\n",
      " [  0.95211846  -3.912626   -19.59404   ]\n",
      " [  0.95211697  -3.9126236  -19.594076  ]\n",
      " [  0.95211494  -3.9126205  -19.594116  ]\n",
      " [  0.9521126   -3.9126172  -19.594158  ]\n",
      " [  0.95211065  -3.9126143  -19.5942    ]\n",
      " [  0.9521088   -3.9126115  -19.594238  ]\n",
      " [  0.9521066   -3.9126081  -19.594282  ]\n",
      " [  0.9521051   -3.9126058  -19.594318  ]\n",
      " [  0.952103    -3.9126027  -19.594358  ]\n",
      " [  0.95210075  -3.9125993  -19.594402  ]\n",
      " [  0.9520992   -3.912597   -19.59444   ]\n",
      " [  0.9520971   -3.9125938  -19.59448   ]\n",
      " [  0.9520957   -3.9125915  -19.594517  ]\n",
      " [  0.9520937   -3.9125886  -19.594555  ]\n",
      " [  0.95209277  -3.912587   -19.594585  ]\n",
      " [  0.9520906   -3.912584   -19.594624  ]\n",
      " [  0.95208836  -3.9125807  -19.594664  ]\n",
      " [  0.9520868   -3.912578   -19.5947    ]\n",
      " [  0.9520848   -3.9125752  -19.59474   ]\n",
      " [  0.9520834   -3.912573   -19.594774  ]\n",
      " [  0.95208144  -3.91257    -19.594812  ]\n",
      " [  0.95207924  -3.912567   -19.594852  ]\n",
      " [  0.95207775  -3.9125645  -19.594889  ]\n",
      " [  0.9520758   -3.9125617  -19.594927  ]\n",
      " [  0.9520735   -3.9125583  -19.594969  ]\n",
      " [  0.952072    -3.912556   -19.595005  ]\n",
      " [  0.95207     -3.9125528  -19.595045  ]\n",
      " [  0.95206773  -3.9125495  -19.595087  ]\n",
      " [  0.9520662   -3.912547   -19.595123  ]\n",
      " [  0.95206416  -3.912544   -19.595163  ]\n",
      " [  0.9520619   -3.9125407  -19.595207  ]\n",
      " [  0.95206034  -3.9125383  -19.595243  ]\n",
      " [  0.95205826  -3.9125352  -19.595285  ]\n",
      " [  0.9520569   -3.912533   -19.59532   ]\n",
      " [  0.9520549   -3.9125302  -19.595358  ]\n",
      " [  0.9520527   -3.9125268  -19.5954    ]\n",
      " [  0.9520512   -3.9125245  -19.595436  ]\n",
      " [  0.9520492   -3.9125216  -19.595476  ]\n",
      " [  0.9520478   -3.9125195  -19.59551   ]\n",
      " [  0.95204586  -3.9125166  -19.595549  ]\n",
      " [  0.95204365  -3.9125133  -19.595589  ]\n",
      " [  0.95204437  -3.9125137  -19.595604  ]\n",
      " [  0.9520414   -3.9125097  -19.595646  ]\n",
      " [  0.9520396   -3.912507   -19.595684  ]\n",
      " [  0.95203745  -3.9125037  -19.595722  ]\n",
      " [  0.9520351   -3.9125004  -19.595764  ]\n",
      " [  0.9520336   -3.912498   -19.595802  ]\n",
      " [  0.95203376  -3.9124978  -19.595821  ]\n",
      " [  0.9520305   -3.9124932  -19.595867  ]\n",
      " [  0.9520286   -3.9124904  -19.595905  ]\n",
      " [  0.9520263   -3.912487   -19.595947  ]\n",
      " [  0.9520239   -3.9124835  -19.595991  ]\n",
      " [  0.9520223   -3.912481   -19.596031  ]\n",
      " [  0.9520202   -3.912478   -19.596071  ]\n",
      " [  0.95201796  -3.9124746  -19.596115  ]\n",
      " [  0.9520164   -3.912472   -19.596153  ]\n",
      " [  0.9520143   -3.912469   -19.596195  ]\n",
      " [  0.9520129   -3.9124668  -19.59623   ]\n",
      " [  0.9520109   -3.912464   -19.59627   ]\n",
      " [  0.9520096   -3.9124618  -19.596304  ]\n",
      " [  0.9520077   -3.912459   -19.596342  ]\n",
      " [  0.9520064   -3.9124568  -19.596375  ]\n",
      " [  0.9520045   -3.912454   -19.59641   ]\n",
      " [  0.95200366  -3.9124525  -19.59644   ]\n",
      " [  0.9520016   -3.9124496  -19.596476  ]\n",
      " [  0.95199937  -3.9124463  -19.596516  ]\n",
      " [  0.95199794  -3.9124439  -19.59655   ]\n",
      " [  0.95199597  -3.912441   -19.596586  ]\n",
      " [  0.9519946   -3.9124389  -19.596619  ]\n",
      " [  0.95199275  -3.9124362  -19.596657  ]\n",
      " [  0.9519906   -3.912433   -19.596695  ]\n",
      " [  0.9519892   -3.9124308  -19.59673   ]\n",
      " [  0.9519872   -3.912428   -19.596767  ]\n",
      " [  0.9519859   -3.9124258  -19.596802  ]\n",
      " [  0.95198447  -3.9124236  -19.596834  ]\n",
      " [  0.95198214  -3.9124203  -19.596874  ]\n",
      " [  0.95198065  -3.912418   -19.596909  ]\n",
      " [  0.9519786   -3.9124148  -19.596947  ]\n",
      " [  0.9519773   -3.912413   -19.59698   ]\n",
      " [  0.9519754   -3.91241    -19.597015  ]\n",
      " [  0.9519733   -3.912407   -19.597055  ]\n",
      " [  0.9519719   -3.9124045  -19.59709   ]\n",
      " [  0.9519699   -3.9124017  -19.597128  ]\n",
      " [  0.9519686   -3.9123995  -19.59716   ]] \n",
      "\n",
      "Final Test RMSE:  0.936108390490214\n",
      "Epoch 342/1000 | Train Loss=38535.13802083 | Val Loss=1.27733636 | Data=385.32816569 | Physics=1.94642626 | Val RMSE: 2.12336421 | ‚àö(Val Loss) = 1.13019311 | Current Learning Rate: 0.002\n",
      "Epoch 343/1000 | Train Loss=37969.27148438 | Val Loss=1.24997258 | Data=379.67282104 | Physics=2.01872532 | Val RMSE: 2.12188554 | ‚àö(Val Loss) = 1.11802173 | Current Learning Rate: 0.002\n",
      "Epoch 344/1000 | Train Loss=37993.74088542 | Val Loss=1.35090137 | Data=379.91744995 | Physics=1.93770269 | Val RMSE: 2.12401199 | ‚àö(Val Loss) = 1.16228282 | Current Learning Rate: 0.002\n",
      "Epoch 345/1000 | Train Loss=38162.86848958 | Val Loss=1.56176770 | Data=381.60851542 | Physics=2.07909498 | Val RMSE: 2.11755395 | ‚àö(Val Loss) = 1.24970698 | Current Learning Rate: 0.002\n",
      "Epoch 346/1000 | Train Loss=37914.70377604 | Val Loss=1.59139419 | Data=379.12656657 | Physics=2.07939001 | Val RMSE: 2.12762094 | ‚àö(Val Loss) = 1.26150477 | Current Learning Rate: 0.002\n",
      "Epoch 347/1000 | Train Loss=37995.55598958 | Val Loss=2.95337391 | Data=379.93553162 | Physics=2.09007289 | Val RMSE: 2.12609076 | ‚àö(Val Loss) = 1.71853828 | Current Learning Rate: 0.002\n",
      "Epoch 348/1000 | Train Loss=39248.21614583 | Val Loss=2.09853864 | Data=392.45767212 | Physics=2.60018431 | Val RMSE: 2.12221575 | ‚àö(Val Loss) = 1.44863331 | Current Learning Rate: 0.002\n",
      "Epoch 349/1000 | Train Loss=38204.20117188 | Val Loss=1.28471041 | Data=382.02219645 | Physics=1.92209396 | Val RMSE: 2.12523198 | ‚àö(Val Loss) = 1.13345063 | Current Learning Rate: 0.002\n",
      "Epoch 350/1000 | Train Loss=38649.94335938 | Val Loss=1.11930740 | Data=386.47260030 | Physics=1.88772767 | Val RMSE: 2.12379980 | ‚àö(Val Loss) = 1.05797327 | Current Learning Rate: 0.002\n",
      "Epoch 351/1000 | Train Loss=38192.13932292 | Val Loss=1.13455546 | Data=381.90166728 | Physics=2.00953122 | Val RMSE: 2.12512589 | ‚àö(Val Loss) = 1.06515515 | Current Learning Rate: 0.002\n",
      "Epoch 352/1000 | Train Loss=38085.90169271 | Val Loss=1.13457775 | Data=380.83842977 | Physics=1.92876160 | Val RMSE: 2.12105107 | ‚àö(Val Loss) = 1.06516564 | Current Learning Rate: 0.002\n",
      "Epoch 353/1000 | Train Loss=38835.38411458 | Val Loss=1.40505993 | Data=388.33015951 | Physics=2.10489307 | Val RMSE: 2.12167835 | ‚àö(Val Loss) = 1.18535221 | Current Learning Rate: 0.002\n",
      "Epoch 354/1000 | Train Loss=38199.45507812 | Val Loss=1.20016158 | Data=381.97476705 | Physics=1.95274431 | Val RMSE: 2.12598777 | ‚àö(Val Loss) = 1.09551883 | Current Learning Rate: 0.002\n",
      "Epoch 355/1000 | Train Loss=38735.95507812 | Val Loss=1.14881933 | Data=387.32783508 | Physics=2.03666503 | Val RMSE: 2.12371182 | ‚àö(Val Loss) = 1.07182992 | Current Learning Rate: 0.002\n",
      "Epoch 356/1000 | Train Loss=38183.21940104 | Val Loss=1.14230013 | Data=381.81246440 | Physics=2.04132025 | Val RMSE: 2.12589145 | ‚àö(Val Loss) = 1.06878436 | Current Learning Rate: 0.002\n",
      "Epoch 357/1000 | Train Loss=37873.82421875 | Val Loss=1.20399344 | Data=378.71809387 | Physics=1.86702134 | Val RMSE: 2.11959338 | ‚àö(Val Loss) = 1.09726632 | Current Learning Rate: 0.002\n",
      "Epoch 358/1000 | Train Loss=38723.14648438 | Val Loss=1.38636565 | Data=387.20815023 | Physics=1.94919267 | Val RMSE: 2.12690997 | ‚àö(Val Loss) = 1.17744029 | Current Learning Rate: 0.002\n",
      "Epoch 359/1000 | Train Loss=38224.86197917 | Val Loss=1.22343898 | Data=382.22882080 | Physics=1.98494829 | Val RMSE: 2.12583399 | ‚àö(Val Loss) = 1.10609174 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 360 (Val Loss = 1.11302102)\n",
      "Epoch 360/1000 | Train Loss=38573.81315104 | Val Loss=1.11302102 | Data=385.70904032 | Physics=1.91857292 | Val RMSE: 2.12276745 | ‚àö(Val Loss) = 1.05499816 | Current Learning Rate: 0.002\n",
      "Epoch 361/1000 | Train Loss=38146.02864583 | Val Loss=1.16512346 | Data=381.44025675 | Physics=1.99081071 | Val RMSE: 2.11689377 | ‚àö(Val Loss) = 1.07940888 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9545765   -3.9179218  -19.203447  ]\n",
      " [  0.9545752   -3.9179184  -19.203571  ]\n",
      " [  0.954574    -3.9179153  -19.20369   ]\n",
      " ...\n",
      " [  0.95428777  -3.9169636  -19.244171  ]\n",
      " [  0.95428795  -3.916963   -19.244219  ]\n",
      " [  0.95428777  -3.9169621  -19.244263  ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542875   -3.9169612  -19.244307  ]\n",
      " [  0.95428735  -3.9169605  -19.244349  ]\n",
      " [  0.95428735  -3.9169598  -19.2444    ]\n",
      " ...\n",
      " [  0.95432895  -3.9162562  -19.28887   ]\n",
      " [  0.95432985  -3.9162557  -19.288961  ]\n",
      " [  0.9543302   -3.9162545  -19.289047  ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9543308   -3.9162538  -19.289137  ]\n",
      " [  0.95433146  -3.9162529  -19.28923   ]\n",
      " [  0.9543318   -3.9162517  -19.289316  ]\n",
      " [  0.9543324   -3.9162507  -19.289408  ]\n",
      " [  0.95433307  -3.9162498  -19.289501  ]\n",
      " [  0.95433366  -3.9162488  -19.2896    ]\n",
      " [  0.9543341   -3.9162476  -19.289692  ]\n",
      " [  0.9543345   -3.9162464  -19.289778  ]\n",
      " [  0.954335    -3.9162455  -19.289856  ]\n",
      " [  0.9543357   -3.9162447  -19.289942  ]\n",
      " [  0.9543364   -3.916244   -19.290031  ]\n",
      " [  0.95433676  -3.9162428  -19.290115  ]\n",
      " [  0.9543374   -3.9162421  -19.290203  ]\n",
      " [  0.95433784  -3.916241   -19.290283  ]\n",
      " [  0.9543385   -3.9162402  -19.29037   ]\n",
      " [  0.9543388   -3.916239   -19.290445  ]\n",
      " [  0.95433956  -3.9162385  -19.290531  ]\n",
      " [  0.95434     -3.9162376  -19.29061   ]\n",
      " [  0.95434064  -3.9162369  -19.290693  ]\n",
      " [  0.954341    -3.916236   -19.290771  ]\n",
      " [  0.95434165  -3.9162352  -19.290855  ]\n",
      " [  0.9543423   -3.9162343  -19.290943  ]\n",
      " [  0.95434266  -3.9162333  -19.291023  ]\n",
      " [  0.9543427   -3.9162319  -19.291082  ]\n",
      " [  0.95434403  -3.916232   -19.291174  ]\n",
      " [  0.9543444   -3.9162312  -19.29126   ]\n",
      " [  0.954345    -3.9162302  -19.29135   ]\n",
      " [  0.95434535  -3.916229   -19.291431  ]\n",
      " [  0.954346    -3.9162283  -19.29152   ]\n",
      " [  0.95434654  -3.9162273  -19.291605  ]\n",
      " [  0.9543471   -3.9162264  -19.291689  ]\n",
      " [  0.9543477   -3.9162254  -19.291779  ]\n",
      " [  0.9543482   -3.9162245  -19.291862  ]\n",
      " [  0.95434874  -3.9162235  -19.291945  ]\n",
      " [  0.95434934  -3.9162228  -19.292025  ]\n",
      " [  0.95435     -3.916222   -19.29211   ]\n",
      " [  0.9543505   -3.9162211  -19.29219   ]\n",
      " [  0.9543511   -3.9162204  -19.292274  ]\n",
      " [  0.9543518   -3.9162197  -19.292364  ]\n",
      " [  0.9543522   -3.9162185  -19.292446  ]\n",
      " [  0.95435286  -3.9162178  -19.292532  ]\n",
      " [  0.95435333  -3.9162166  -19.292612  ]\n",
      " [  0.95435405  -3.9162161  -19.292698  ]\n",
      " [  0.95435447  -3.916215   -19.292776  ]\n",
      " [  0.9543552   -3.9162145  -19.29286   ]\n",
      " [  0.95435584  -3.9162138  -19.292948  ]\n",
      " [  0.95435625  -3.9162126  -19.293028  ]\n",
      " [  0.95435697  -3.9162118  -19.293114  ]\n",
      " [  0.9543576   -3.9162111  -19.293203  ]\n",
      " [  0.9543581   -3.91621    -19.293285  ]\n",
      " [  0.95435876  -3.9162092  -19.293371  ]\n",
      " [  0.9543595   -3.9162085  -19.293463  ]\n",
      " [  0.9543602   -3.9162076  -19.293556  ]\n",
      " [  0.95436     -3.916206   -19.293615  ]\n",
      " [  0.9543614   -3.9162061  -19.293709  ]\n",
      " [  0.9543621   -3.9162054  -19.293806  ]\n",
      " [  0.95436263  -3.9162042  -19.293901  ]\n",
      " [  0.9543632   -3.9162033  -19.293993  ]\n",
      " [  0.9543636   -3.9162018  -19.294073  ]\n",
      " [  0.9543645   -3.9162014  -19.294163  ]\n",
      " [  0.954365    -3.9162004  -19.294247  ]\n",
      " [  0.95436573  -3.9161997  -19.294334  ]\n",
      " [  0.9543662   -3.9161987  -19.294415  ]\n",
      " [  0.954367    -3.9161983  -19.2945    ]\n",
      " [  0.9543677   -3.9161975  -19.29459   ]\n",
      " [  0.95436805  -3.9161963  -19.294668  ]\n",
      " [  0.95436895  -3.9161959  -19.294754  ]\n",
      " [  0.9543694   -3.9161947  -19.294836  ]\n",
      " [  0.95437014  -3.9161942  -19.29492   ]\n",
      " [  0.95437086  -3.9161935  -19.29501   ]\n",
      " [  0.95437133  -3.9161923  -19.29509   ]\n",
      " [  0.9543722   -3.9161918  -19.295181  ]\n",
      " [  0.95437235  -3.9161904  -19.29525   ]\n",
      " [  0.95437336  -3.9161901  -19.295336  ]\n",
      " [  0.9543741   -3.9161894  -19.295424  ]\n",
      " [  0.95437455  -3.9161885  -19.295506  ]\n",
      " [  0.95437527  -3.9161878  -19.295591  ]\n",
      " [  0.9543758   -3.9161868  -19.29567   ]\n",
      " [  0.9543765   -3.916186   -19.295753  ]\n",
      " [  0.9543773   -3.9161854  -19.295841  ]\n",
      " [  0.9543778   -3.9161844  -19.295921  ]\n",
      " [  0.9543785   -3.916184   -19.296007  ]\n",
      " [  0.95437926  -3.9161832  -19.296097  ]\n",
      " [  0.95437974  -3.916182   -19.296179  ]\n",
      " [  0.9543805   -3.9161813  -19.296265  ]\n",
      " [  0.95438105  -3.9161804  -19.296345  ]\n",
      " [  0.9543818   -3.91618    -19.296429  ]\n",
      " [  0.9543826   -3.9161792  -19.296516  ]\n",
      " [  0.9543831   -3.9161782  -19.296598  ]\n",
      " [  0.95438385  -3.9161775  -19.296682  ]\n",
      " [  0.9543844   -3.9161766  -19.29676   ]\n",
      " [  0.95438516  -3.916176   -19.296844  ]\n",
      " [  0.9543859   -3.9161754  -19.296932  ]\n",
      " [  0.9543864   -3.9161744  -19.29701   ]\n",
      " [  0.95438707  -3.9161737  -19.29709   ]\n",
      " [  0.95438784  -3.9161732  -19.297174  ]\n",
      " [  0.9543885   -3.9161723  -19.297256  ]\n",
      " [  0.9543892   -3.9161716  -19.297344  ]\n",
      " [  0.95438975  -3.9161706  -19.297422  ]\n",
      " [  0.9543905   -3.9161701  -19.297506  ]\n",
      " [  0.95439106  -3.9161692  -19.297583  ]\n",
      " [  0.95439184  -3.9161687  -19.297665  ]\n",
      " [  0.9543926   -3.916168   -19.29775   ]\n",
      " [  0.9543931   -3.916167   -19.297829  ]\n",
      " [  0.95439416  -3.9161668  -19.297922  ]\n",
      " [  0.95439464  -3.9161656  -19.298008  ]\n",
      " [  0.9543952   -3.9161646  -19.298086  ]\n",
      " [  0.95439535  -3.9161634  -19.298141  ]\n",
      " [  0.95439684  -3.916164   -19.298231  ]\n",
      " [  0.9543974   -3.916163   -19.298313  ]\n",
      " [  0.9543981   -3.9161623  -19.298399  ]\n",
      " [  0.9543988   -3.9161615  -19.298489  ]\n",
      " [  0.95439935  -3.9161606  -19.29857   ]\n",
      " [  0.9544001   -3.9161599  -19.298656  ]\n",
      " [  0.9544007   -3.916159   -19.298737  ]\n",
      " [  0.9544015   -3.9161584  -19.298819  ]\n",
      " [  0.9544023   -3.916158   -19.298906  ]\n",
      " [  0.95440286  -3.916157   -19.298986  ]\n",
      " [  0.9544037   -3.9161563  -19.29907   ]\n",
      " [  0.95440453  -3.9161558  -19.299158  ]\n",
      " [  0.95440507  -3.9161549  -19.299238  ]\n",
      " [  0.9544059   -3.9161541  -19.299324  ]\n",
      " [  0.9544067   -3.9161537  -19.299412  ]\n",
      " [  0.9544073   -3.9161525  -19.299492  ]\n",
      " [  0.9544081   -3.916152   -19.299578  ]\n",
      " [  0.95440894  -3.9161515  -19.299665  ]\n",
      " [  0.9544095   -3.9161503  -19.299747  ]\n",
      " [  0.9544103   -3.9161499  -19.299831  ]\n",
      " [  0.95441115  -3.9161494  -19.299921  ]\n",
      " [  0.95441175  -3.9161484  -19.300001  ]\n",
      " [  0.9544126   -3.9161477  -19.300087  ]\n",
      " [  0.9544132   -3.916147   -19.300165  ]\n",
      " [  0.954414    -3.9161463  -19.300247  ]\n",
      " [  0.95441484  -3.9161458  -19.300333  ]\n",
      " [  0.95441544  -3.9161448  -19.300413  ]\n",
      " [  0.9544156   -3.9161437  -19.300468  ]\n",
      " [  0.9544169   -3.916144   -19.300547  ]\n",
      " [  0.9544177   -3.9161434  -19.30063   ]\n",
      " [  0.9544185   -3.916143   -19.300716  ]\n",
      " [  0.954419    -3.9161417  -19.300797  ]\n",
      " [  0.9544198   -3.9161413  -19.300879  ]\n",
      " [  0.9544206   -3.9161406  -19.300964  ]\n",
      " [  0.95442116  -3.9161398  -19.301044  ]\n",
      " [  0.95442206  -3.9161391  -19.301128  ]\n",
      " [  0.9544228   -3.9161384  -19.301208  ]\n",
      " [  0.95442337  -3.9161377  -19.301283  ]\n",
      " [  0.9544243   -3.9161375  -19.301369  ]\n",
      " [  0.9544249   -3.9161365  -19.301445  ]\n",
      " [  0.95442575  -3.916136   -19.301527  ]\n",
      " [  0.9544266   -3.9161355  -19.30161   ]\n",
      " [  0.9544272   -3.9161346  -19.30169   ]\n",
      " [  0.954428    -3.916134   -19.301771  ]\n",
      " [  0.95442885  -3.9161334  -19.301857  ]\n",
      " [  0.95442957  -3.9161327  -19.301939  ]\n",
      " [  0.9544303   -3.916132   -19.302021  ]\n",
      " [  0.9544312   -3.9161315  -19.302107  ]\n",
      " [  0.9544318   -3.9161305  -19.302185  ]\n",
      " [  0.95443267  -3.91613    -19.302267  ]\n",
      " [  0.95443326  -3.9161294  -19.302341  ]\n",
      " [  0.95443386  -3.9161286  -19.30241   ]\n",
      " [  0.954435    -3.9161286  -19.302494  ]\n",
      " [  0.9544356   -3.9161277  -19.302572  ]\n",
      " [  0.9544364   -3.9161272  -19.302652  ]\n",
      " [  0.95443726  -3.9161267  -19.302736  ]\n",
      " [  0.95443785  -3.9161258  -19.302814  ]\n",
      " [  0.9544387   -3.9161253  -19.302895  ]\n",
      " [  0.95443934  -3.9161246  -19.302969  ]\n",
      " [  0.9544402   -3.916124   -19.303047  ]\n",
      " [  0.9544411   -3.9161236  -19.303131  ]\n",
      " [  0.95444167  -3.916123   -19.303205  ]\n",
      " [  0.9544425   -3.9161224  -19.303286  ]\n",
      " [  0.9544434   -3.916122   -19.303368  ]\n",
      " [  0.954444    -3.916121   -19.303444  ]\n",
      " [  0.9544448   -3.9161205  -19.303524  ]\n",
      " [  0.9544455   -3.9161198  -19.303596  ]\n",
      " [  0.95444566  -3.9161186  -19.303648  ]\n",
      " [  0.954447    -3.916119   -19.303722  ]\n",
      " [  0.9544478   -3.9161189  -19.3038    ]\n",
      " [  0.95444876  -3.9161184  -19.303888  ]\n",
      " [  0.9544492   -3.9161172  -19.303963  ]\n",
      " [  0.95444983  -3.9161165  -19.304035  ]\n",
      " [  0.95445085  -3.9161162  -19.304117  ]\n",
      " [  0.95445144  -3.9161155  -19.304192  ]\n",
      " [  0.95445234  -3.916115   -19.304272  ]\n",
      " [  0.95445293  -3.9161143  -19.304344  ]\n",
      " [  0.9544538   -3.9161139  -19.30442   ]\n",
      " [  0.95445466  -3.9161136  -19.3045    ]\n",
      " [  0.9544553   -3.9161127  -19.304575  ]\n",
      " [  0.95445615  -3.9161122  -19.304651  ]\n",
      " [  0.95445704  -3.916112   -19.304733  ]\n",
      " [  0.95445764  -3.916111   -19.304808  ]\n",
      " [  0.95445853  -3.9161105  -19.304886  ]\n",
      " [  0.95445913  -3.9161098  -19.304956  ]\n",
      " [  0.95446     -3.9161096  -19.305033  ]\n",
      " [  0.9544607   -3.9161088  -19.305103  ]\n",
      " [  0.9544616   -3.9161086  -19.30518   ]\n",
      " [  0.95446247  -3.9161081  -19.305258  ]\n",
      " [  0.954463    -3.9161074  -19.30533   ]\n",
      " [  0.9544639   -3.916107   -19.305405  ]\n",
      " [  0.95446473  -3.9161065  -19.305483  ]\n",
      " [  0.9544654   -3.9161057  -19.305555  ]\n",
      " [  0.9544662   -3.9161053  -19.30563   ]\n",
      " [  0.9544671   -3.916105   -19.30571   ]\n",
      " [  0.9544679   -3.9161043  -19.305788  ]\n",
      " [  0.9544686   -3.9161036  -19.305864  ]\n",
      " [  0.9544695   -3.9161034  -19.305944  ]\n",
      " [  0.95447016  -3.9161026  -19.306017  ]\n",
      " [  0.95447105  -3.9161022  -19.306093  ]\n",
      " [  0.954472    -3.9161017  -19.306173  ]\n",
      " [  0.9544726   -3.916101   -19.306248  ]\n",
      " [  0.9544735   -3.9161005  -19.306324  ]\n",
      " [  0.95447415  -3.9161     -19.306395  ]\n",
      " [  0.95447505  -3.9160995  -19.30647   ]\n",
      " [  0.9544756   -3.9160988  -19.306534  ]\n",
      " [  0.9544766   -3.9160988  -19.306606  ]\n",
      " [  0.9544775   -3.9160984  -19.306684  ]\n",
      " [  0.9544781   -3.9160976  -19.306753  ]\n",
      " [  0.954479    -3.9160972  -19.30683   ]\n",
      " [  0.9544796   -3.9160967  -19.306896  ]\n",
      " [  0.95448047  -3.9160962  -19.306969  ]\n",
      " [  0.95448136  -3.916096   -19.307045  ]\n",
      " [  0.95448196  -3.9160953  -19.307114  ]\n",
      " [  0.95448285  -3.9160948  -19.307188  ]\n",
      " [  0.95448375  -3.9160945  -19.307266  ]\n",
      " [  0.95448434  -3.9160938  -19.307335  ]\n",
      " [  0.95448524  -3.9160933  -19.307411  ]\n",
      " [  0.95448613  -3.916093   -19.30749   ]\n",
      " [  0.9544868   -3.9160924  -19.30756   ]\n",
      " [  0.9544877   -3.916092   -19.307634  ]\n",
      " [  0.95448864  -3.9160917  -19.307714  ]\n",
      " [  0.9544893   -3.916091   -19.307785  ]\n",
      " [  0.9544902   -3.9160905  -19.307861  ]\n",
      " [  0.95449084  -3.9160898  -19.30793   ]\n",
      " [  0.95449173  -3.9160895  -19.308004  ]\n",
      " [  0.9544927   -3.9160893  -19.30808   ]\n",
      " [  0.95449334  -3.9160886  -19.308151  ]\n",
      " [  0.95449424  -3.9160883  -19.308226  ]\n",
      " [  0.9544949   -3.9160876  -19.308292  ]\n",
      " [  0.9544958   -3.9160874  -19.308365  ]\n",
      " [  0.95449674  -3.916087   -19.30844   ]\n",
      " [  0.9544966   -3.9160855  -19.308481  ]\n",
      " [  0.9544982   -3.9160864  -19.30856   ]\n",
      " [  0.9544989   -3.9160857  -19.308628  ]\n",
      " [  0.9544997   -3.9160855  -19.3087    ]\n",
      " [  0.95450056  -3.916085   -19.308777  ]\n",
      " [  0.95450115  -3.9160843  -19.308846  ]\n",
      " [  0.9545014   -3.9160833  -19.308893  ]\n",
      " [  0.95450294  -3.916084   -19.308973  ]\n",
      " [  0.9545036   -3.9160833  -19.309046  ]\n",
      " [  0.9545045   -3.9160829  -19.309122  ]\n",
      " [  0.9545054   -3.9160824  -19.309202  ]\n",
      " [  0.954506    -3.9160817  -19.309273  ]\n",
      " [  0.95450693  -3.9160814  -19.30935   ]\n",
      " [  0.9545079   -3.916081   -19.309427  ]\n",
      " [  0.95450854  -3.9160802  -19.3095    ]\n",
      " [  0.9545095   -3.91608    -19.309576  ]\n",
      " [  0.9545102   -3.9160793  -19.309643  ]\n",
      " [  0.95451117  -3.916079   -19.309717  ]\n",
      " [  0.9545118   -3.9160786  -19.309782  ]\n",
      " [  0.9545128   -3.9160783  -19.309853  ]\n",
      " [  0.95451343  -3.9160779  -19.309917  ]\n",
      " [  0.9545144   -3.9160776  -19.309986  ]\n",
      " [  0.95451486  -3.916077   -19.310043  ]\n",
      " [  0.9545159   -3.916077   -19.310112  ]\n",
      " [  0.9545168   -3.9160767  -19.310184  ]\n",
      " [  0.9545174   -3.916076   -19.31025   ]\n",
      " [  0.9545183   -3.9160757  -19.31032   ]\n",
      " [  0.9545189   -3.9160752  -19.310383  ]\n",
      " [  0.9545198   -3.916075   -19.310452  ]\n",
      " [  0.95452076  -3.9160748  -19.310522  ]\n",
      " [  0.95452136  -3.916074   -19.310589  ]\n",
      " [  0.95452225  -3.9160738  -19.310658  ]\n",
      " [  0.9545229   -3.916073   -19.31072   ]\n",
      " [  0.9545237   -3.9160728  -19.310783  ]\n",
      " [  0.9545247   -3.9160728  -19.310856  ]\n",
      " [  0.95452535  -3.9160721  -19.31092   ]\n",
      " [  0.95452625  -3.916072   -19.31099   ]\n",
      " [  0.9545269   -3.9160714  -19.311052  ]\n",
      " [  0.9545278   -3.9160712  -19.311121  ]\n",
      " [  0.9545287   -3.916071   -19.311192  ]\n",
      " [  0.95452935  -3.9160702  -19.311256  ]\n",
      " [  0.95453024  -3.91607    -19.311327  ]\n",
      " [  0.9545309   -3.9160695  -19.311388  ]] \n",
      "\n",
      "Final Test RMSE:  0.9098828236262003\n",
      "Epoch 362/1000 | Train Loss=38125.68294271 | Val Loss=1.13691735 | Data=381.23469543 | Physics=1.93571920 | Val RMSE: 2.12703848 | ‚àö(Val Loss) = 1.06626332 | Current Learning Rate: 0.002\n",
      "Epoch 363/1000 | Train Loss=38135.48046875 | Val Loss=1.25812197 | Data=381.33413696 | Physics=1.81770418 | Val RMSE: 2.12245631 | ‚àö(Val Loss) = 1.12166035 | Current Learning Rate: 0.002\n",
      "Epoch 364/1000 | Train Loss=38137.71809896 | Val Loss=1.18791687 | Data=381.35754395 | Physics=1.86146585 | Val RMSE: 2.12204289 | ‚àö(Val Loss) = 1.08991599 | Current Learning Rate: 0.002\n",
      "Epoch 365/1000 | Train Loss=38579.32617188 | Val Loss=1.49711812 | Data=385.77334595 | Physics=2.02429218 | Val RMSE: 2.10353780 | ‚àö(Val Loss) = 1.22356772 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 366 (Val Loss = 1.11284888)\n",
      "Epoch 366/1000 | Train Loss=38305.77734375 | Val Loss=1.11284888 | Data=383.03821818 | Physics=1.95938546 | Val RMSE: 2.11998868 | ‚àö(Val Loss) = 1.05491650 | Current Learning Rate: 0.002\n",
      "Epoch 367/1000 | Train Loss=38241.68880208 | Val Loss=5.55060387 | Data=382.39302063 | Physics=2.71589784 | Val RMSE: 2.14726615 | ‚àö(Val Loss) = 2.35597205 | Current Learning Rate: 0.002\n",
      "Epoch 368/1000 | Train Loss=38165.10742188 | Val Loss=11.18708229 | Data=381.60767619 | Physics=4.17501192 | Val RMSE: 2.12464976 | ‚àö(Val Loss) = 3.34470963 | Current Learning Rate: 0.002\n",
      "Epoch 369/1000 | Train Loss=38155.63541667 | Val Loss=1.13947213 | Data=381.53123474 | Physics=2.55425434 | Val RMSE: 2.12732172 | ‚àö(Val Loss) = 1.06746054 | Current Learning Rate: 0.002\n",
      "Epoch 370/1000 | Train Loss=38170.38216146 | Val Loss=1.14219129 | Data=381.68429057 | Physics=1.97638978 | Val RMSE: 2.12516737 | ‚àö(Val Loss) = 1.06873345 | Current Learning Rate: 0.002\n",
      "Epoch 371/1000 | Train Loss=38179.37630208 | Val Loss=1.64913177 | Data=381.77404277 | Physics=2.01528314 | Val RMSE: 2.12817240 | ‚àö(Val Loss) = 1.28418529 | Current Learning Rate: 0.002\n",
      "Epoch 372/1000 | Train Loss=38089.66927083 | Val Loss=1.19482255 | Data=380.87731934 | Physics=1.89609701 | Val RMSE: 2.12830234 | ‚àö(Val Loss) = 1.09307933 | Current Learning Rate: 0.002\n",
      "Epoch 373/1000 | Train Loss=38062.56315104 | Val Loss=1.67699409 | Data=380.60616557 | Physics=1.88618272 | Val RMSE: 2.13560319 | ‚àö(Val Loss) = 1.29498804 | Current Learning Rate: 0.002\n",
      "Epoch 374/1000 | Train Loss=38117.83919271 | Val Loss=1.17883110 | Data=381.15830485 | Physics=2.36238061 | Val RMSE: 2.12781262 | ‚àö(Val Loss) = 1.08573985 | Current Learning Rate: 0.002\n",
      "Epoch 375/1000 | Train Loss=38156.81184896 | Val Loss=2.62569571 | Data=381.54750061 | Physics=2.52957375 | Val RMSE: 2.12457085 | ‚àö(Val Loss) = 1.62039983 | Current Learning Rate: 0.002\n",
      "Epoch 376/1000 | Train Loss=38107.07747396 | Val Loss=2.01432920 | Data=381.05122375 | Physics=1.88068300 | Val RMSE: 2.13231611 | ‚àö(Val Loss) = 1.41927063 | Current Learning Rate: 0.002\n",
      "Epoch 377/1000 | Train Loss=38126.33723958 | Val Loss=1.24716318 | Data=381.24276733 | Physics=1.95306997 | Val RMSE: 2.13037753 | ‚àö(Val Loss) = 1.11676455 | Current Learning Rate: 0.002\n",
      "Epoch 378/1000 | Train Loss=38115.67252604 | Val Loss=1.24879384 | Data=381.13691711 | Physics=1.89506899 | Val RMSE: 2.13673544 | ‚àö(Val Loss) = 1.11749446 | Current Learning Rate: 0.002\n",
      "Epoch 379/1000 | Train Loss=38130.94205729 | Val Loss=1.49190652 | Data=381.28996277 | Physics=1.99087810 | Val RMSE: 2.12689734 | ‚àö(Val Loss) = 1.22143626 | Current Learning Rate: 0.002\n",
      "Epoch 380/1000 | Train Loss=38159.09765625 | Val Loss=3.36784911 | Data=381.57098897 | Physics=2.23708731 | Val RMSE: 2.12802219 | ‚àö(Val Loss) = 1.83517003 | Current Learning Rate: 0.002\n",
      "Epoch 381/1000 | Train Loss=38200.00455729 | Val Loss=2.62548208 | Data=381.98023987 | Physics=2.06430528 | Val RMSE: 2.12692761 | ‚àö(Val Loss) = 1.62033391 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9521028   -3.9376738  -19.235357  ]\n",
      " [  0.95210296  -3.9376736  -19.235357  ]\n",
      " [  0.95210314  -3.9376736  -19.235357  ]\n",
      " ...\n",
      " [  0.9522758   -3.9376621  -19.236149  ]\n",
      " [  0.9522759   -3.9376621  -19.23615   ]\n",
      " [  0.95227623  -3.9376621  -19.236153  ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95227647  -3.9376621  -19.236153  ]\n",
      " [  0.9522766   -3.9376621  -19.236153  ]\n",
      " [  0.952277    -3.9376621  -19.236156  ]\n",
      " ...\n",
      " [  0.9526756   -3.9376144  -19.23804   ]\n",
      " [  0.9526766   -3.937614   -19.238049  ]\n",
      " [  0.9526778   -3.937614   -19.238054  ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95267886  -3.9376137  -19.23806   ]\n",
      " [  0.9526802   -3.9376137  -19.238068  ]\n",
      " [  0.9526813   -3.9376135  -19.238071  ]\n",
      " [  0.9526824   -3.9376132  -19.238077  ]\n",
      " [  0.9526836   -3.937613   -19.238083  ]\n",
      " [  0.95268506  -3.937613   -19.23809   ]\n",
      " [  0.95268625  -3.9376128  -19.238096  ]\n",
      " [  0.95268726  -3.9376125  -19.2381    ]\n",
      " [  0.95268804  -3.9376123  -19.238102  ]\n",
      " [  0.95268893  -3.937612   -19.238108  ]\n",
      " [  0.9526901   -3.9376118  -19.238113  ]\n",
      " [  0.95269114  -3.9376118  -19.238117  ]\n",
      " [  0.95269215  -3.9376116  -19.238123  ]\n",
      " [  0.9526931   -3.9376113  -19.238127  ]\n",
      " [  0.9526941   -3.937611   -19.238132  ]\n",
      " [  0.95269495  -3.937611   -19.238134  ]\n",
      " [  0.9526959   -3.9376109  -19.23814   ]\n",
      " [  0.95269686  -3.9376106  -19.238144  ]\n",
      " [  0.9526979   -3.9376106  -19.23815   ]\n",
      " [  0.9526988   -3.9376104  -19.238153  ]\n",
      " [  0.9526998   -3.9376101  -19.23816   ]\n",
      " [  0.952701    -3.9376101  -19.238165  ]\n",
      " [  0.95270205  -3.93761    -19.23817   ]\n",
      " [  0.9527024   -3.9376097  -19.238169  ]\n",
      " [  0.9527035   -3.9376094  -19.238178  ]\n",
      " [  0.95270485  -3.9376094  -19.238184  ]\n",
      " [  0.9527061   -3.9376092  -19.238192  ]\n",
      " [  0.9527072   -3.9376092  -19.238195  ]\n",
      " [  0.9527083   -3.937609   -19.238201  ]\n",
      " [  0.95270944  -3.9376087  -19.238207  ]\n",
      " [  0.95271057  -3.9376085  -19.238213  ]\n",
      " [  0.95271176  -3.9376082  -19.238218  ]\n",
      " [  0.95271283  -3.9376082  -19.238222  ]\n",
      " [  0.9527138   -3.937608   -19.238228  ]\n",
      " [  0.9527148   -3.9376078  -19.238232  ]\n",
      " [  0.9527159   -3.9376075  -19.238237  ]\n",
      " [  0.9527169   -3.9376073  -19.238241  ]\n",
      " [  0.9527179   -3.937607   -19.238247  ]\n",
      " [  0.95271915  -3.937607   -19.238255  ]\n",
      " [  0.9527202   -3.9376068  -19.238258  ]\n",
      " [  0.95272136  -3.9376066  -19.238264  ]\n",
      " [  0.9527224   -3.9376063  -19.238268  ]\n",
      " [  0.9527234   -3.937606   -19.238274  ]\n",
      " [  0.9527244   -3.937606   -19.238277  ]\n",
      " [  0.9527254   -3.9376059  -19.238283  ]\n",
      " [  0.9527266   -3.9376056  -19.23829   ]\n",
      " [  0.9527277   -3.9376056  -19.238295  ]\n",
      " [  0.95272875  -3.9376054  -19.2383    ]\n",
      " [  0.95273006  -3.9376051  -19.238308  ]\n",
      " [  0.95273113  -3.937605   -19.238312  ]\n",
      " [  0.95273226  -3.9376047  -19.238317  ]\n",
      " [  0.9527335   -3.9376044  -19.238325  ]\n",
      " [  0.9527349   -3.9376044  -19.23833   ]\n",
      " [  0.9527354   -3.9376042  -19.238329  ]\n",
      " [  0.95273644  -3.9376037  -19.238338  ]\n",
      " [  0.9527381   -3.9376037  -19.238346  ]\n",
      " [  0.95273954  -3.9376037  -19.238354  ]\n",
      " [  0.95274085  -3.9376035  -19.238361  ]\n",
      " [  0.9527418   -3.9376032  -19.238363  ]\n",
      " [  0.95274293  -3.9376028  -19.23837   ]\n",
      " [  0.95274407  -3.9376028  -19.238375  ]\n",
      " [  0.95274514  -3.9376025  -19.23838   ]\n",
      " [  0.95274615  -3.9376023  -19.238384  ]\n",
      " [  0.9527472   -3.937602   -19.23839   ]\n",
      " [  0.9527485   -3.9376018  -19.238396  ]\n",
      " [  0.95274943  -3.9376016  -19.2384    ]\n",
      " [  0.9527505   -3.9376013  -19.238405  ]\n",
      " [  0.9527516   -3.9376013  -19.23841   ]\n",
      " [  0.95275265  -3.937601   -19.238417  ]\n",
      " [  0.9527539   -3.9376009  -19.238422  ]\n",
      " [  0.952755    -3.9376006  -19.238426  ]\n",
      " [  0.9527562   -3.9376004  -19.238434  ]\n",
      " [  0.95275706  -3.9376001  -19.238436  ]\n",
      " [  0.95275795  -3.9376     -19.238441  ]\n",
      " [  0.9527593   -3.9376     -19.23845   ]\n",
      " [  0.9527604   -3.9375997  -19.238453  ]\n",
      " [  0.95276153  -3.9375994  -19.238459  ]\n",
      " [  0.9527626   -3.9375992  -19.238464  ]\n",
      " [  0.9527637   -3.937599   -19.23847   ]\n",
      " [  0.9527649   -3.9375987  -19.238476  ]\n",
      " [  0.952766    -3.9375987  -19.23848   ]\n",
      " [  0.95276713  -3.9375982  -19.238487  ]\n",
      " [  0.95276845  -3.9375982  -19.238493  ]\n",
      " [  0.9527696   -3.937598   -19.238499  ]\n",
      " [  0.9527707   -3.9375978  -19.238504  ]\n",
      " [  0.9527718   -3.9375975  -19.238508  ]\n",
      " [  0.95277286  -3.9375973  -19.238514  ]\n",
      " [  0.9527741   -3.937597   -19.238522  ]\n",
      " [  0.95277524  -3.9375968  -19.238525  ]\n",
      " [  0.9527764   -3.9375966  -19.238531  ]\n",
      " [  0.9527774   -3.9375966  -19.238535  ]\n",
      " [  0.95277846  -3.937596   -19.23854   ]\n",
      " [  0.9527797   -3.937596   -19.238548  ]\n",
      " [  0.9527808   -3.9375958  -19.238552  ]\n",
      " [  0.9527818   -3.9375956  -19.238558  ]\n",
      " [  0.95278287  -3.9375954  -19.238564  ]\n",
      " [  0.95278406  -3.9375951  -19.23857   ]\n",
      " [  0.9527853   -3.937595   -19.238575  ]\n",
      " [  0.9527864   -3.9375947  -19.238579  ]\n",
      " [  0.95278746  -3.9375944  -19.238585  ]\n",
      " [  0.95278853  -3.9375944  -19.23859   ]\n",
      " [  0.95278955  -3.937594   -19.238596  ]\n",
      " [  0.9527908   -3.937594   -19.238602  ]\n",
      " [  0.95279187  -3.9375937  -19.238605  ]\n",
      " [  0.95279324  -3.9375935  -19.238615  ]\n",
      " [  0.9527946   -3.9375932  -19.23862   ]\n",
      " [  0.9527956   -3.937593   -19.238625  ]\n",
      " [  0.95279604  -3.9375927  -19.238623  ]\n",
      " [  0.9527971   -3.9375923  -19.238632  ]\n",
      " [  0.95279855  -3.9375925  -19.238638  ]\n",
      " [  0.95279974  -3.9375923  -19.238646  ]\n",
      " [  0.95280117  -3.937592   -19.238653  ]\n",
      " [  0.95280236  -3.9375918  -19.238659  ]\n",
      " [  0.95280355  -3.9375916  -19.238665  ]\n",
      " [  0.9528046   -3.9375913  -19.238668  ]\n",
      " [  0.95280576  -3.937591   -19.238674  ]\n",
      " [  0.95280707  -3.9375908  -19.238682  ]\n",
      " [  0.9528082   -3.9375906  -19.238686  ]\n",
      " [  0.95280933  -3.9375904  -19.238693  ]\n",
      " [  0.95281065  -3.9375901  -19.238699  ]\n",
      " [  0.9528118   -3.93759    -19.238705  ]\n",
      " [  0.95281297  -3.9375896  -19.23871   ]\n",
      " [  0.9528143   -3.9375894  -19.238718  ]\n",
      " [  0.9528154   -3.9375892  -19.238722  ]\n",
      " [  0.9528166   -3.937589   -19.238728  ]\n",
      " [  0.9528179   -3.9375887  -19.238735  ]\n",
      " [  0.95281905  -3.9375885  -19.238739  ]\n",
      " [  0.95282024  -3.9375882  -19.238747  ]\n",
      " [  0.95282155  -3.937588   -19.238752  ]\n",
      " [  0.9528227   -3.9375877  -19.238758  ]\n",
      " [  0.9528239   -3.9375875  -19.238764  ]\n",
      " [  0.95282495  -3.9375873  -19.238768  ]\n",
      " [  0.952826    -3.937587   -19.238773  ]\n",
      " [  0.95282733  -3.9375868  -19.238781  ]\n",
      " [  0.95282847  -3.9375865  -19.238785  ]\n",
      " [  0.95282894  -3.9375863  -19.238783  ]\n",
      " [  0.9528298   -3.9375858  -19.23879   ]\n",
      " [  0.9528311   -3.9375858  -19.238798  ]\n",
      " [  0.95283246  -3.9375856  -19.238806  ]\n",
      " [  0.95283365  -3.9375854  -19.23881   ]\n",
      " [  0.95283484  -3.937585   -19.238817  ]\n",
      " [  0.95283616  -3.9375849  -19.238823  ]\n",
      " [  0.95283735  -3.9375846  -19.238829  ]\n",
      " [  0.95283854  -3.9375844  -19.238834  ]\n",
      " [  0.95283973  -3.9375842  -19.23884   ]\n",
      " [  0.95284075  -3.937584   -19.238844  ]\n",
      " [  0.9528419   -3.9375837  -19.238852  ]\n",
      " [  0.95284307  -3.9375834  -19.238855  ]\n",
      " [  0.9528442   -3.9375832  -19.238863  ]\n",
      " [  0.9528455   -3.937583   -19.238869  ]\n",
      " [  0.95284665  -3.9375827  -19.238874  ]\n",
      " [  0.9528478   -3.9375825  -19.23888   ]\n",
      " [  0.9528491   -3.9375823  -19.238888  ]\n",
      " [  0.9528504   -3.937582   -19.238894  ]\n",
      " [  0.9528516   -3.9375818  -19.2389    ]\n",
      " [  0.95285285  -3.9375815  -19.238905  ]\n",
      " [  0.952854    -3.9375813  -19.23891   ]\n",
      " [  0.9528551   -3.937581   -19.238916  ]\n",
      " [  0.9528562   -3.9375808  -19.23892   ]\n",
      " [  0.952857    -3.9375806  -19.238922  ]\n",
      " [  0.95285815  -3.9375803  -19.238932  ]\n",
      " [  0.9528594   -3.93758    -19.238935  ]\n",
      " [  0.95286053  -3.9375799  -19.238943  ]\n",
      " [  0.9528619   -3.9375796  -19.238949  ]\n",
      " [  0.95286304  -3.9375794  -19.238955  ]\n",
      " [  0.95286417  -3.9375792  -19.23896   ]\n",
      " [  0.95286524  -3.937579   -19.238964  ]\n",
      " [  0.9528663   -3.9375787  -19.23897   ]\n",
      " [  0.9528676   -3.9375784  -19.238977  ]\n",
      " [  0.9528687   -3.9375782  -19.238981  ]\n",
      " [  0.95286983  -3.937578   -19.238987  ]\n",
      " [  0.95287114  -3.9375777  -19.238995  ]\n",
      " [  0.9528723   -3.9375775  -19.238998  ]\n",
      " [  0.9528734   -3.937577   -19.239006  ]\n",
      " [  0.9528745   -3.937577   -19.23901   ]\n",
      " [  0.9528749   -3.9375765  -19.239008  ]\n",
      " [  0.95287573  -3.9375763  -19.239016  ]\n",
      " [  0.95287704  -3.9375763  -19.239021  ]\n",
      " [  0.95287853  -3.937576   -19.23903   ]\n",
      " [  0.9528797   -3.9375758  -19.239035  ]\n",
      " [  0.9528807   -3.9375756  -19.23904   ]\n",
      " [  0.95288193  -3.9375753  -19.239048  ]\n",
      " [  0.9528832   -3.937575   -19.239052  ]\n",
      " [  0.9528843   -3.9375749  -19.23906   ]\n",
      " [  0.9528854   -3.9375746  -19.239063  ]\n",
      " [  0.9528865   -3.9375741  -19.239069  ]\n",
      " [  0.9528878   -3.937574   -19.239077  ]\n",
      " [  0.9528889   -3.937574   -19.23908   ]\n",
      " [  0.95289004  -3.9375734  -19.239086  ]\n",
      " [  0.9528913   -3.9375732  -19.239094  ]\n",
      " [  0.9528924   -3.937573   -19.239098  ]\n",
      " [  0.95289356  -3.9375727  -19.239103  ]\n",
      " [  0.9528946   -3.9375725  -19.239107  ]\n",
      " [  0.9528957   -3.9375722  -19.239115  ]\n",
      " [  0.9528967   -3.937572   -19.239119  ]\n",
      " [  0.95289785  -3.9375718  -19.239124  ]\n",
      " [  0.9528991   -3.9375715  -19.23913   ]\n",
      " [  0.9529002   -3.9375713  -19.239136  ]\n",
      " [  0.95290124  -3.937571   -19.239141  ]\n",
      " [  0.9529025   -3.9375708  -19.239147  ]\n",
      " [  0.95290357  -3.9375706  -19.239151  ]\n",
      " [  0.95290464  -3.9375703  -19.239159  ]\n",
      " [  0.95290595  -3.93757    -19.239164  ]\n",
      " [  0.9529072   -3.9375699  -19.23917   ]\n",
      " [  0.95290834  -3.9375694  -19.239176  ]\n",
      " [  0.9529096   -3.9375691  -19.239183  ]\n",
      " [  0.95291066  -3.937569   -19.239187  ]\n",
      " [  0.9529118   -3.9375687  -19.239193  ]\n",
      " [  0.9529131   -3.9375684  -19.2392    ]\n",
      " [  0.95291424  -3.9375682  -19.239204  ]\n",
      " [  0.9529154   -3.937568   -19.23921   ]\n",
      " [  0.95291644  -3.9375677  -19.239214  ]\n",
      " [  0.95291746  -3.9375672  -19.239222  ]\n",
      " [  0.95291835  -3.937567   -19.239223  ]\n",
      " [  0.9529193   -3.9375668  -19.23923   ]\n",
      " [  0.9529206   -3.9375665  -19.239235  ]\n",
      " [  0.9529217   -3.9375663  -19.23924   ]\n",
      " [  0.95292276  -3.937566   -19.239246  ]\n",
      " [  0.9529238   -3.9375658  -19.23925   ]\n",
      " [  0.95292485  -3.9375656  -19.239256  ]\n",
      " [  0.95292604  -3.9375653  -19.239262  ]\n",
      " [  0.9529271   -3.937565   -19.239267  ]\n",
      " [  0.9529282   -3.9375648  -19.239273  ]\n",
      " [  0.95292944  -3.9375646  -19.239279  ]\n",
      " [  0.95293057  -3.9375644  -19.239285  ]\n",
      " [  0.95293164  -3.9375641  -19.23929   ]\n",
      " [  0.95293295  -3.937564   -19.239296  ]\n",
      " [  0.952934    -3.9375637  -19.239302  ]\n",
      " [  0.95293516  -3.9375632  -19.239307  ]\n",
      " [  0.9529365   -3.937563   -19.239313  ]\n",
      " [  0.9529376   -3.9375627  -19.239319  ]\n",
      " [  0.95293874  -3.9375625  -19.239325  ]\n",
      " [  0.95293975  -3.9375622  -19.239328  ]\n",
      " [  0.9529408   -3.937562   -19.239334  ]\n",
      " [  0.9529421   -3.9375618  -19.239342  ]\n",
      " [  0.95294315  -3.9375615  -19.239346  ]\n",
      " [  0.9529442   -3.937561   -19.239351  ]\n",
      " [  0.95294523  -3.9375608  -19.239355  ]\n",
      " [  0.95294625  -3.9375606  -19.23936   ]\n",
      " [  0.9529475   -3.9375603  -19.239367  ]\n",
      " [  0.95294785  -3.93756    -19.239363  ]\n",
      " [  0.95294875  -3.9375596  -19.239372  ]\n",
      " [  0.95295006  -3.9375596  -19.239378  ]\n",
      " [  0.95295113  -3.9375594  -19.239384  ]\n",
      " [  0.95295244  -3.9375591  -19.239391  ]\n",
      " [  0.9529536   -3.937559   -19.239395  ]\n",
      " [  0.952954    -3.9375584  -19.239393  ]\n",
      " [  0.95295507  -3.9375582  -19.239403  ]\n",
      " [  0.9529565   -3.9375582  -19.239408  ]\n",
      " [  0.9529577   -3.9375577  -19.239416  ]\n",
      " [  0.95295906  -3.9375575  -19.239424  ]\n",
      " [  0.9529602   -3.9375572  -19.239428  ]\n",
      " [  0.9529614   -3.937557   -19.239433  ]\n",
      " [  0.9529627   -3.9375567  -19.23944   ]\n",
      " [  0.9529638   -3.9375563  -19.239445  ]\n",
      " [  0.952965    -3.937556   -19.239452  ]\n",
      " [  0.95296603  -3.9375558  -19.239456  ]\n",
      " [  0.9529671   -3.9375553  -19.239462  ]\n",
      " [  0.95296806  -3.937555   -19.239464  ]\n",
      " [  0.9529691   -3.9375548  -19.23947   ]\n",
      " [  0.95297     -3.9375546  -19.239473  ]\n",
      " [  0.952971    -3.9375544  -19.239477  ]\n",
      " [  0.95297176  -3.9375541  -19.23948   ]\n",
      " [  0.95297265  -3.9375536  -19.239485  ]\n",
      " [  0.95297384  -3.9375536  -19.239492  ]\n",
      " [  0.95297486  -3.9375534  -19.239496  ]\n",
      " [  0.95297587  -3.937553   -19.2395    ]\n",
      " [  0.9529768   -3.9375527  -19.239504  ]\n",
      " [  0.9529778   -3.9375525  -19.23951   ]\n",
      " [  0.9529789   -3.9375522  -19.239515  ]\n",
      " [  0.9529799   -3.937552   -19.23952   ]\n",
      " [  0.95298094  -3.9375515  -19.239525  ]\n",
      " [  0.9529819   -3.9375513  -19.239529  ]\n",
      " [  0.9529827   -3.937551   -19.23953   ]\n",
      " [  0.9529838   -3.9375508  -19.239538  ]\n",
      " [  0.9529848   -3.9375505  -19.239542  ]\n",
      " [  0.9529858   -3.9375503  -19.239548  ]\n",
      " [  0.9529868   -3.93755    -19.23955   ]\n",
      " [  0.95298773  -3.9375496  -19.239555  ]\n",
      " [  0.95298886  -3.9375494  -19.239561  ]\n",
      " [  0.9529899   -3.937549   -19.239565  ]\n",
      " [  0.95299083  -3.9375486  -19.239569  ]\n",
      " [  0.9529918   -3.9375484  -19.239573  ]] \n",
      "\n",
      "Final Test RMSE:  0.8998799473047256\n",
      "Epoch 382/1000 | Train Loss=38109.26367188 | Val Loss=1.54357088 | Data=381.07324727 | Physics=1.90537485 | Val RMSE: 2.13612843 | ‚àö(Val Loss) = 1.24240530 | Current Learning Rate: 0.002\n",
      "Epoch 383/1000 | Train Loss=38118.30078125 | Val Loss=1.30615199 | Data=381.16362000 | Physics=1.93723202 | Val RMSE: 2.12872934 | ‚àö(Val Loss) = 1.14287007 | Current Learning Rate: 0.002\n",
      "Epoch 384/1000 | Train Loss=38111.51106771 | Val Loss=1.28367102 | Data=381.09575907 | Physics=1.91405186 | Val RMSE: 2.13178349 | ‚àö(Val Loss) = 1.13299203 | Current Learning Rate: 0.002\n",
      "Epoch 385/1000 | Train Loss=38163.36197917 | Val Loss=1.27222431 | Data=381.61426799 | Physics=1.95300325 | Val RMSE: 2.12787938 | ‚àö(Val Loss) = 1.12792921 | Current Learning Rate: 0.002\n",
      "Epoch 386/1000 | Train Loss=38060.70963542 | Val Loss=1.34196043 | Data=380.58777364 | Physics=1.86376037 | Val RMSE: 2.12650323 | ‚àö(Val Loss) = 1.15843010 | Current Learning Rate: 0.002\n",
      "Epoch 387/1000 | Train Loss=38051.74739583 | Val Loss=1.33205032 | Data=380.49812317 | Physics=1.87567872 | Val RMSE: 2.12591767 | ‚àö(Val Loss) = 1.15414488 | Current Learning Rate: 0.002\n",
      "Epoch 388/1000 | Train Loss=38119.58919271 | Val Loss=1.14032829 | Data=381.17649841 | Physics=1.94138128 | Val RMSE: 2.12616396 | ‚àö(Val Loss) = 1.06786156 | Current Learning Rate: 0.002\n",
      "Epoch 389/1000 | Train Loss=38131.75781250 | Val Loss=3.68014359 | Data=381.29432170 | Physics=2.03800277 | Val RMSE: 2.12798214 | ‚àö(Val Loss) = 1.91837001 | Current Learning Rate: 0.002\n",
      "Epoch 390/1000 | Train Loss=38081.35026042 | Val Loss=1.50391686 | Data=380.78954569 | Physics=2.89757220 | Val RMSE: 2.13505673 | ‚àö(Val Loss) = 1.22634292 | Current Learning Rate: 0.002\n",
      "Epoch 391/1000 | Train Loss=38123.10351562 | Val Loss=2.88729620 | Data=381.19819641 | Physics=3.08278872 | Val RMSE: 2.12924981 | ‚àö(Val Loss) = 1.69920456 | Current Learning Rate: 0.002\n",
      "Epoch 392/1000 | Train Loss=38094.35481771 | Val Loss=1.15714347 | Data=380.91470846 | Physics=1.88909772 | Val RMSE: 2.12845564 | ‚àö(Val Loss) = 1.07570601 | Current Learning Rate: 0.002\n",
      "Epoch 393/1000 | Train Loss=38159.14583333 | Val Loss=1.68194962 | Data=381.57051086 | Physics=2.03180041 | Val RMSE: 2.12631989 | ‚àö(Val Loss) = 1.29690003 | Current Learning Rate: 0.002\n",
      "Epoch 394/1000 | Train Loss=38152.90429688 | Val Loss=1.46243191 | Data=381.50966899 | Physics=1.91547558 | Val RMSE: 2.13239312 | ‚àö(Val Loss) = 1.20931053 | Current Learning Rate: 0.002\n",
      "Epoch 395/1000 | Train Loss=38113.77734375 | Val Loss=1.93349934 | Data=381.11827087 | Physics=1.96673392 | Val RMSE: 2.12690854 | ‚àö(Val Loss) = 1.39050329 | Current Learning Rate: 0.002\n",
      "Epoch 396/1000 | Train Loss=38109.75976562 | Val Loss=1.26949906 | Data=381.07808940 | Physics=1.93758649 | Val RMSE: 2.12556458 | ‚àö(Val Loss) = 1.12672055 | Current Learning Rate: 0.002\n",
      "Epoch 397/1000 | Train Loss=38070.47265625 | Val Loss=1.32201481 | Data=380.68527730 | Physics=1.87753378 | Val RMSE: 2.12945914 | ‚àö(Val Loss) = 1.14978898 | Current Learning Rate: 0.002\n",
      "Epoch 398/1000 | Train Loss=38108.19531250 | Val Loss=1.15680826 | Data=381.06253560 | Physics=1.97925439 | Val RMSE: 2.13011956 | ‚àö(Val Loss) = 1.07555020 | Current Learning Rate: 0.002\n",
      "Epoch 399/1000 | Train Loss=38121.17252604 | Val Loss=1.48061860 | Data=381.19228109 | Physics=1.96258962 | Val RMSE: 2.13085151 | ‚àö(Val Loss) = 1.21680677 | Current Learning Rate: 0.002\n",
      "Epoch 400/1000 | Train Loss=38121.47395833 | Val Loss=1.27590883 | Data=381.19539897 | Physics=1.91625663 | Val RMSE: 2.12486029 | ‚àö(Val Loss) = 1.12956131 | Current Learning Rate: 0.0002\n",
      "Epoch 401/1000 | Train Loss=38109.51302083 | Val Loss=1.12672293 | Data=381.07572428 | Physics=2.00911945 | Val RMSE: 2.12095547 | ‚àö(Val Loss) = 1.06147206 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.9521028   -3.9376738  -19.235357  ]\n",
      " [  0.95210296  -3.9376736  -19.235357  ]\n",
      " [  0.95210314  -3.9376736  -19.235357  ]\n",
      " ...\n",
      " [  0.9522758   -3.9376621  -19.236149  ]\n",
      " [  0.9522759   -3.9376621  -19.23615   ]\n",
      " [  0.95227623  -3.9376621  -19.236153  ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95227647  -3.9376621  -19.236153  ]\n",
      " [  0.9522766   -3.9376621  -19.236153  ]\n",
      " [  0.952277    -3.9376621  -19.236156  ]\n",
      " ...\n",
      " [  0.9526756   -3.9376144  -19.23804   ]\n",
      " [  0.9526766   -3.937614   -19.238049  ]\n",
      " [  0.9526778   -3.937614   -19.238054  ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.95267886  -3.9376137  -19.23806   ]\n",
      " [  0.9526802   -3.9376137  -19.238068  ]\n",
      " [  0.9526813   -3.9376135  -19.238071  ]\n",
      " [  0.9526824   -3.9376132  -19.238077  ]\n",
      " [  0.9526836   -3.937613   -19.238083  ]\n",
      " [  0.95268506  -3.937613   -19.23809   ]\n",
      " [  0.95268625  -3.9376128  -19.238096  ]\n",
      " [  0.95268726  -3.9376125  -19.2381    ]\n",
      " [  0.95268804  -3.9376123  -19.238102  ]\n",
      " [  0.95268893  -3.937612   -19.238108  ]\n",
      " [  0.9526901   -3.9376118  -19.238113  ]\n",
      " [  0.95269114  -3.9376118  -19.238117  ]\n",
      " [  0.95269215  -3.9376116  -19.238123  ]\n",
      " [  0.9526931   -3.9376113  -19.238127  ]\n",
      " [  0.9526941   -3.937611   -19.238132  ]\n",
      " [  0.95269495  -3.937611   -19.238134  ]\n",
      " [  0.9526959   -3.9376109  -19.23814   ]\n",
      " [  0.95269686  -3.9376106  -19.238144  ]\n",
      " [  0.9526979   -3.9376106  -19.23815   ]\n",
      " [  0.9526988   -3.9376104  -19.238153  ]\n",
      " [  0.9526998   -3.9376101  -19.23816   ]\n",
      " [  0.952701    -3.9376101  -19.238165  ]\n",
      " [  0.95270205  -3.93761    -19.23817   ]\n",
      " [  0.9527024   -3.9376097  -19.238169  ]\n",
      " [  0.9527035   -3.9376094  -19.238178  ]\n",
      " [  0.95270485  -3.9376094  -19.238184  ]\n",
      " [  0.9527061   -3.9376092  -19.238192  ]\n",
      " [  0.9527072   -3.9376092  -19.238195  ]\n",
      " [  0.9527083   -3.937609   -19.238201  ]\n",
      " [  0.95270944  -3.9376087  -19.238207  ]\n",
      " [  0.95271057  -3.9376085  -19.238213  ]\n",
      " [  0.95271176  -3.9376082  -19.238218  ]\n",
      " [  0.95271283  -3.9376082  -19.238222  ]\n",
      " [  0.9527138   -3.937608   -19.238228  ]\n",
      " [  0.9527148   -3.9376078  -19.238232  ]\n",
      " [  0.9527159   -3.9376075  -19.238237  ]\n",
      " [  0.9527169   -3.9376073  -19.238241  ]\n",
      " [  0.9527179   -3.937607   -19.238247  ]\n",
      " [  0.95271915  -3.937607   -19.238255  ]\n",
      " [  0.9527202   -3.9376068  -19.238258  ]\n",
      " [  0.95272136  -3.9376066  -19.238264  ]\n",
      " [  0.9527224   -3.9376063  -19.238268  ]\n",
      " [  0.9527234   -3.937606   -19.238274  ]\n",
      " [  0.9527244   -3.937606   -19.238277  ]\n",
      " [  0.9527254   -3.9376059  -19.238283  ]\n",
      " [  0.9527266   -3.9376056  -19.23829   ]\n",
      " [  0.9527277   -3.9376056  -19.238295  ]\n",
      " [  0.95272875  -3.9376054  -19.2383    ]\n",
      " [  0.95273006  -3.9376051  -19.238308  ]\n",
      " [  0.95273113  -3.937605   -19.238312  ]\n",
      " [  0.95273226  -3.9376047  -19.238317  ]\n",
      " [  0.9527335   -3.9376044  -19.238325  ]\n",
      " [  0.9527349   -3.9376044  -19.23833   ]\n",
      " [  0.9527354   -3.9376042  -19.238329  ]\n",
      " [  0.95273644  -3.9376037  -19.238338  ]\n",
      " [  0.9527381   -3.9376037  -19.238346  ]\n",
      " [  0.95273954  -3.9376037  -19.238354  ]\n",
      " [  0.95274085  -3.9376035  -19.238361  ]\n",
      " [  0.9527418   -3.9376032  -19.238363  ]\n",
      " [  0.95274293  -3.9376028  -19.23837   ]\n",
      " [  0.95274407  -3.9376028  -19.238375  ]\n",
      " [  0.95274514  -3.9376025  -19.23838   ]\n",
      " [  0.95274615  -3.9376023  -19.238384  ]\n",
      " [  0.9527472   -3.937602   -19.23839   ]\n",
      " [  0.9527485   -3.9376018  -19.238396  ]\n",
      " [  0.95274943  -3.9376016  -19.2384    ]\n",
      " [  0.9527505   -3.9376013  -19.238405  ]\n",
      " [  0.9527516   -3.9376013  -19.23841   ]\n",
      " [  0.95275265  -3.937601   -19.238417  ]\n",
      " [  0.9527539   -3.9376009  -19.238422  ]\n",
      " [  0.952755    -3.9376006  -19.238426  ]\n",
      " [  0.9527562   -3.9376004  -19.238434  ]\n",
      " [  0.95275706  -3.9376001  -19.238436  ]\n",
      " [  0.95275795  -3.9376     -19.238441  ]\n",
      " [  0.9527593   -3.9376     -19.23845   ]\n",
      " [  0.9527604   -3.9375997  -19.238453  ]\n",
      " [  0.95276153  -3.9375994  -19.238459  ]\n",
      " [  0.9527626   -3.9375992  -19.238464  ]\n",
      " [  0.9527637   -3.937599   -19.23847   ]\n",
      " [  0.9527649   -3.9375987  -19.238476  ]\n",
      " [  0.952766    -3.9375987  -19.23848   ]\n",
      " [  0.95276713  -3.9375982  -19.238487  ]\n",
      " [  0.95276845  -3.9375982  -19.238493  ]\n",
      " [  0.9527696   -3.937598   -19.238499  ]\n",
      " [  0.9527707   -3.9375978  -19.238504  ]\n",
      " [  0.9527718   -3.9375975  -19.238508  ]\n",
      " [  0.95277286  -3.9375973  -19.238514  ]\n",
      " [  0.9527741   -3.937597   -19.238522  ]\n",
      " [  0.95277524  -3.9375968  -19.238525  ]\n",
      " [  0.9527764   -3.9375966  -19.238531  ]\n",
      " [  0.9527774   -3.9375966  -19.238535  ]\n",
      " [  0.95277846  -3.937596   -19.23854   ]\n",
      " [  0.9527797   -3.937596   -19.238548  ]\n",
      " [  0.9527808   -3.9375958  -19.238552  ]\n",
      " [  0.9527818   -3.9375956  -19.238558  ]\n",
      " [  0.95278287  -3.9375954  -19.238564  ]\n",
      " [  0.95278406  -3.9375951  -19.23857   ]\n",
      " [  0.9527853   -3.937595   -19.238575  ]\n",
      " [  0.9527864   -3.9375947  -19.238579  ]\n",
      " [  0.95278746  -3.9375944  -19.238585  ]\n",
      " [  0.95278853  -3.9375944  -19.23859   ]\n",
      " [  0.95278955  -3.937594   -19.238596  ]\n",
      " [  0.9527908   -3.937594   -19.238602  ]\n",
      " [  0.95279187  -3.9375937  -19.238605  ]\n",
      " [  0.95279324  -3.9375935  -19.238615  ]\n",
      " [  0.9527946   -3.9375932  -19.23862   ]\n",
      " [  0.9527956   -3.937593   -19.238625  ]\n",
      " [  0.95279604  -3.9375927  -19.238623  ]\n",
      " [  0.9527971   -3.9375923  -19.238632  ]\n",
      " [  0.95279855  -3.9375925  -19.238638  ]\n",
      " [  0.95279974  -3.9375923  -19.238646  ]\n",
      " [  0.95280117  -3.937592   -19.238653  ]\n",
      " [  0.95280236  -3.9375918  -19.238659  ]\n",
      " [  0.95280355  -3.9375916  -19.238665  ]\n",
      " [  0.9528046   -3.9375913  -19.238668  ]\n",
      " [  0.95280576  -3.937591   -19.238674  ]\n",
      " [  0.95280707  -3.9375908  -19.238682  ]\n",
      " [  0.9528082   -3.9375906  -19.238686  ]\n",
      " [  0.95280933  -3.9375904  -19.238693  ]\n",
      " [  0.95281065  -3.9375901  -19.238699  ]\n",
      " [  0.9528118   -3.93759    -19.238705  ]\n",
      " [  0.95281297  -3.9375896  -19.23871   ]\n",
      " [  0.9528143   -3.9375894  -19.238718  ]\n",
      " [  0.9528154   -3.9375892  -19.238722  ]\n",
      " [  0.9528166   -3.937589   -19.238728  ]\n",
      " [  0.9528179   -3.9375887  -19.238735  ]\n",
      " [  0.95281905  -3.9375885  -19.238739  ]\n",
      " [  0.95282024  -3.9375882  -19.238747  ]\n",
      " [  0.95282155  -3.937588   -19.238752  ]\n",
      " [  0.9528227   -3.9375877  -19.238758  ]\n",
      " [  0.9528239   -3.9375875  -19.238764  ]\n",
      " [  0.95282495  -3.9375873  -19.238768  ]\n",
      " [  0.952826    -3.937587   -19.238773  ]\n",
      " [  0.95282733  -3.9375868  -19.238781  ]\n",
      " [  0.95282847  -3.9375865  -19.238785  ]\n",
      " [  0.95282894  -3.9375863  -19.238783  ]\n",
      " [  0.9528298   -3.9375858  -19.23879   ]\n",
      " [  0.9528311   -3.9375858  -19.238798  ]\n",
      " [  0.95283246  -3.9375856  -19.238806  ]\n",
      " [  0.95283365  -3.9375854  -19.23881   ]\n",
      " [  0.95283484  -3.937585   -19.238817  ]\n",
      " [  0.95283616  -3.9375849  -19.238823  ]\n",
      " [  0.95283735  -3.9375846  -19.238829  ]\n",
      " [  0.95283854  -3.9375844  -19.238834  ]\n",
      " [  0.95283973  -3.9375842  -19.23884   ]\n",
      " [  0.95284075  -3.937584   -19.238844  ]\n",
      " [  0.9528419   -3.9375837  -19.238852  ]\n",
      " [  0.95284307  -3.9375834  -19.238855  ]\n",
      " [  0.9528442   -3.9375832  -19.238863  ]\n",
      " [  0.9528455   -3.937583   -19.238869  ]\n",
      " [  0.95284665  -3.9375827  -19.238874  ]\n",
      " [  0.9528478   -3.9375825  -19.23888   ]\n",
      " [  0.9528491   -3.9375823  -19.238888  ]\n",
      " [  0.9528504   -3.937582   -19.238894  ]\n",
      " [  0.9528516   -3.9375818  -19.2389    ]\n",
      " [  0.95285285  -3.9375815  -19.238905  ]\n",
      " [  0.952854    -3.9375813  -19.23891   ]\n",
      " [  0.9528551   -3.937581   -19.238916  ]\n",
      " [  0.9528562   -3.9375808  -19.23892   ]\n",
      " [  0.952857    -3.9375806  -19.238922  ]\n",
      " [  0.95285815  -3.9375803  -19.238932  ]\n",
      " [  0.9528594   -3.93758    -19.238935  ]\n",
      " [  0.95286053  -3.9375799  -19.238943  ]\n",
      " [  0.9528619   -3.9375796  -19.238949  ]\n",
      " [  0.95286304  -3.9375794  -19.238955  ]\n",
      " [  0.95286417  -3.9375792  -19.23896   ]\n",
      " [  0.95286524  -3.937579   -19.238964  ]\n",
      " [  0.9528663   -3.9375787  -19.23897   ]\n",
      " [  0.9528676   -3.9375784  -19.238977  ]\n",
      " [  0.9528687   -3.9375782  -19.238981  ]\n",
      " [  0.95286983  -3.937578   -19.238987  ]\n",
      " [  0.95287114  -3.9375777  -19.238995  ]\n",
      " [  0.9528723   -3.9375775  -19.238998  ]\n",
      " [  0.9528734   -3.937577   -19.239006  ]\n",
      " [  0.9528745   -3.937577   -19.23901   ]\n",
      " [  0.9528749   -3.9375765  -19.239008  ]\n",
      " [  0.95287573  -3.9375763  -19.239016  ]\n",
      " [  0.95287704  -3.9375763  -19.239021  ]\n",
      " [  0.95287853  -3.937576   -19.23903   ]\n",
      " [  0.9528797   -3.9375758  -19.239035  ]\n",
      " [  0.9528807   -3.9375756  -19.23904   ]\n",
      " [  0.95288193  -3.9375753  -19.239048  ]\n",
      " [  0.9528832   -3.937575   -19.239052  ]\n",
      " [  0.9528843   -3.9375749  -19.23906   ]\n",
      " [  0.9528854   -3.9375746  -19.239063  ]\n",
      " [  0.9528865   -3.9375741  -19.239069  ]\n",
      " [  0.9528878   -3.937574   -19.239077  ]\n",
      " [  0.9528889   -3.937574   -19.23908   ]\n",
      " [  0.95289004  -3.9375734  -19.239086  ]\n",
      " [  0.9528913   -3.9375732  -19.239094  ]\n",
      " [  0.9528924   -3.937573   -19.239098  ]\n",
      " [  0.95289356  -3.9375727  -19.239103  ]\n",
      " [  0.9528946   -3.9375725  -19.239107  ]\n",
      " [  0.9528957   -3.9375722  -19.239115  ]\n",
      " [  0.9528967   -3.937572   -19.239119  ]\n",
      " [  0.95289785  -3.9375718  -19.239124  ]\n",
      " [  0.9528991   -3.9375715  -19.23913   ]\n",
      " [  0.9529002   -3.9375713  -19.239136  ]\n",
      " [  0.95290124  -3.937571   -19.239141  ]\n",
      " [  0.9529025   -3.9375708  -19.239147  ]\n",
      " [  0.95290357  -3.9375706  -19.239151  ]\n",
      " [  0.95290464  -3.9375703  -19.239159  ]\n",
      " [  0.95290595  -3.93757    -19.239164  ]\n",
      " [  0.9529072   -3.9375699  -19.23917   ]\n",
      " [  0.95290834  -3.9375694  -19.239176  ]\n",
      " [  0.9529096   -3.9375691  -19.239183  ]\n",
      " [  0.95291066  -3.937569   -19.239187  ]\n",
      " [  0.9529118   -3.9375687  -19.239193  ]\n",
      " [  0.9529131   -3.9375684  -19.2392    ]\n",
      " [  0.95291424  -3.9375682  -19.239204  ]\n",
      " [  0.9529154   -3.937568   -19.23921   ]\n",
      " [  0.95291644  -3.9375677  -19.239214  ]\n",
      " [  0.95291746  -3.9375672  -19.239222  ]\n",
      " [  0.95291835  -3.937567   -19.239223  ]\n",
      " [  0.9529193   -3.9375668  -19.23923   ]\n",
      " [  0.9529206   -3.9375665  -19.239235  ]\n",
      " [  0.9529217   -3.9375663  -19.23924   ]\n",
      " [  0.95292276  -3.937566   -19.239246  ]\n",
      " [  0.9529238   -3.9375658  -19.23925   ]\n",
      " [  0.95292485  -3.9375656  -19.239256  ]\n",
      " [  0.95292604  -3.9375653  -19.239262  ]\n",
      " [  0.9529271   -3.937565   -19.239267  ]\n",
      " [  0.9529282   -3.9375648  -19.239273  ]\n",
      " [  0.95292944  -3.9375646  -19.239279  ]\n",
      " [  0.95293057  -3.9375644  -19.239285  ]\n",
      " [  0.95293164  -3.9375641  -19.23929   ]\n",
      " [  0.95293295  -3.937564   -19.239296  ]\n",
      " [  0.952934    -3.9375637  -19.239302  ]\n",
      " [  0.95293516  -3.9375632  -19.239307  ]\n",
      " [  0.9529365   -3.937563   -19.239313  ]\n",
      " [  0.9529376   -3.9375627  -19.239319  ]\n",
      " [  0.95293874  -3.9375625  -19.239325  ]\n",
      " [  0.95293975  -3.9375622  -19.239328  ]\n",
      " [  0.9529408   -3.937562   -19.239334  ]\n",
      " [  0.9529421   -3.9375618  -19.239342  ]\n",
      " [  0.95294315  -3.9375615  -19.239346  ]\n",
      " [  0.9529442   -3.937561   -19.239351  ]\n",
      " [  0.95294523  -3.9375608  -19.239355  ]\n",
      " [  0.95294625  -3.9375606  -19.23936   ]\n",
      " [  0.9529475   -3.9375603  -19.239367  ]\n",
      " [  0.95294785  -3.93756    -19.239363  ]\n",
      " [  0.95294875  -3.9375596  -19.239372  ]\n",
      " [  0.95295006  -3.9375596  -19.239378  ]\n",
      " [  0.95295113  -3.9375594  -19.239384  ]\n",
      " [  0.95295244  -3.9375591  -19.239391  ]\n",
      " [  0.9529536   -3.937559   -19.239395  ]\n",
      " [  0.952954    -3.9375584  -19.239393  ]\n",
      " [  0.95295507  -3.9375582  -19.239403  ]\n",
      " [  0.9529565   -3.9375582  -19.239408  ]\n",
      " [  0.9529577   -3.9375577  -19.239416  ]\n",
      " [  0.95295906  -3.9375575  -19.239424  ]\n",
      " [  0.9529602   -3.9375572  -19.239428  ]\n",
      " [  0.9529614   -3.937557   -19.239433  ]\n",
      " [  0.9529627   -3.9375567  -19.23944   ]\n",
      " [  0.9529638   -3.9375563  -19.239445  ]\n",
      " [  0.952965    -3.937556   -19.239452  ]\n",
      " [  0.95296603  -3.9375558  -19.239456  ]\n",
      " [  0.9529671   -3.9375553  -19.239462  ]\n",
      " [  0.95296806  -3.937555   -19.239464  ]\n",
      " [  0.9529691   -3.9375548  -19.23947   ]\n",
      " [  0.95297     -3.9375546  -19.239473  ]\n",
      " [  0.952971    -3.9375544  -19.239477  ]\n",
      " [  0.95297176  -3.9375541  -19.23948   ]\n",
      " [  0.95297265  -3.9375536  -19.239485  ]\n",
      " [  0.95297384  -3.9375536  -19.239492  ]\n",
      " [  0.95297486  -3.9375534  -19.239496  ]\n",
      " [  0.95297587  -3.937553   -19.2395    ]\n",
      " [  0.9529768   -3.9375527  -19.239504  ]\n",
      " [  0.9529778   -3.9375525  -19.23951   ]\n",
      " [  0.9529789   -3.9375522  -19.239515  ]\n",
      " [  0.9529799   -3.937552   -19.23952   ]\n",
      " [  0.95298094  -3.9375515  -19.239525  ]\n",
      " [  0.9529819   -3.9375513  -19.239529  ]\n",
      " [  0.9529827   -3.937551   -19.23953   ]\n",
      " [  0.9529838   -3.9375508  -19.239538  ]\n",
      " [  0.9529848   -3.9375505  -19.239542  ]\n",
      " [  0.9529858   -3.9375503  -19.239548  ]\n",
      " [  0.9529868   -3.93755    -19.23955   ]\n",
      " [  0.95298773  -3.9375496  -19.239555  ]\n",
      " [  0.95298886  -3.9375494  -19.239561  ]\n",
      " [  0.9529899   -3.937549   -19.239565  ]\n",
      " [  0.95299083  -3.9375486  -19.239569  ]\n",
      " [  0.9529918   -3.9375484  -19.239573  ]] \n",
      "\n",
      "Final Test RMSE:  0.8998799473047256\n",
      "Epoch 402/1000 | Train Loss=967.21145630 | Val Loss=1.28138745 | Data=9.65274302 | Physics=1.87905802 | Val RMSE: 2.12143970 | ‚àö(Val Loss) = 1.13198388 | Current Learning Rate: 0.0002\n",
      "‚úÖ Saved best model at epoch 403 (Val Loss = 1.11219752)\n",
      "Epoch 403/1000 | Train Loss=991.05116781 | Val Loss=1.11219752 | Data=9.89090792 | Physics=1.94822141 | Val RMSE: 2.12076402 | ‚àö(Val Loss) = 1.05460775 | Current Learning Rate: 0.0002\n",
      "Epoch 404/1000 | Train Loss=981.94620768 | Val Loss=1.33225012 | Data=9.79987510 | Physics=1.94219615 | Val RMSE: 2.12120843 | ‚àö(Val Loss) = 1.15423143 | Current Learning Rate: 0.0002\n",
      "Epoch 405/1000 | Train Loss=965.21397909 | Val Loss=1.13395011 | Data=9.63277022 | Physics=1.93180088 | Val RMSE: 2.12101054 | ‚àö(Val Loss) = 1.06487095 | Current Learning Rate: 0.0002\n",
      "Epoch 406/1000 | Train Loss=964.39596558 | Val Loss=1.12220967 | Data=9.62454875 | Physics=1.90070969 | Val RMSE: 2.12091446 | ‚àö(Val Loss) = 1.05934393 | Current Learning Rate: 0.0002\n",
      "Epoch 407/1000 | Train Loss=964.76109823 | Val Loss=1.11810541 | Data=9.62821531 | Physics=1.98975357 | Val RMSE: 2.12085319 | ‚àö(Val Loss) = 1.05740499 | Current Learning Rate: 0.0002\n",
      "Epoch 408/1000 | Train Loss=963.83914185 | Val Loss=1.12076604 | Data=9.61898692 | Physics=1.91461603 | Val RMSE: 2.12086177 | ‚àö(Val Loss) = 1.05866241 | Current Learning Rate: 0.0002\n",
      "Epoch 409/1000 | Train Loss=964.29652913 | Val Loss=1.12593389 | Data=9.62355121 | Physics=1.97667484 | Val RMSE: 2.12088609 | ‚àö(Val Loss) = 1.06110036 | Current Learning Rate: 0.0002\n",
      "Epoch 410/1000 | Train Loss=977.70091756 | Val Loss=1.36706984 | Data=9.75753864 | Physics=2.01967384 | Val RMSE: 2.12084746 | ‚àö(Val Loss) = 1.16921759 | Current Learning Rate: 0.0002\n",
      "Epoch 411/1000 | Train Loss=966.11052450 | Val Loss=1.18485343 | Data=9.64168167 | Physics=1.93189995 | Val RMSE: 2.12081146 | ‚àö(Val Loss) = 1.08850968 | Current Learning Rate: 0.0002\n",
      "Epoch 412/1000 | Train Loss=974.56867472 | Val Loss=1.48933780 | Data=9.72615560 | Physics=2.02882189 | Val RMSE: 2.12081742 | ‚àö(Val Loss) = 1.22038424 | Current Learning Rate: 0.0002\n",
      "Epoch 413/1000 | Train Loss=963.52286784 | Val Loss=1.12509179 | Data=9.61584441 | Physics=1.94632210 | Val RMSE: 2.12080932 | ‚àö(Val Loss) = 1.06070340 | Current Learning Rate: 0.0002\n",
      "Epoch 414/1000 | Train Loss=972.51602173 | Val Loss=1.36437833 | Data=9.70567910 | Physics=1.93654310 | Val RMSE: 2.12081957 | ‚àö(Val Loss) = 1.16806602 | Current Learning Rate: 0.0002\n",
      "Epoch 415/1000 | Train Loss=970.24744670 | Val Loss=1.20684123 | Data=9.68305588 | Physics=1.97964896 | Val RMSE: 2.12081122 | ‚àö(Val Loss) = 1.09856331 | Current Learning Rate: 0.0002\n",
      "Epoch 416/1000 | Train Loss=970.18425496 | Val Loss=1.26839852 | Data=9.68237321 | Physics=1.90223358 | Val RMSE: 2.12079072 | ‚àö(Val Loss) = 1.12623203 | Current Learning Rate: 0.0002\n",
      "Epoch 417/1000 | Train Loss=973.90237427 | Val Loss=1.37771046 | Data=9.71957413 | Physics=1.91482172 | Val RMSE: 2.12079811 | ‚àö(Val Loss) = 1.17375910 | Current Learning Rate: 0.0002\n",
      "Epoch 418/1000 | Train Loss=963.80575562 | Val Loss=1.19866192 | Data=9.61866856 | Physics=1.87486509 | Val RMSE: 2.12079072 | ‚àö(Val Loss) = 1.09483421 | Current Learning Rate: 0.0002\n",
      "Epoch 419/1000 | Train Loss=968.31416829 | Val Loss=1.40535378 | Data=9.66370821 | Physics=1.91155109 | Val RMSE: 2.12079549 | ‚àö(Val Loss) = 1.18547618 | Current Learning Rate: 0.0002\n",
      "Epoch 420/1000 | Train Loss=963.10518392 | Val Loss=1.17822969 | Data=9.61165110 | Physics=1.96985311 | Val RMSE: 2.12080288 | ‚àö(Val Loss) = 1.08546293 | Current Learning Rate: 0.0002\n",
      "Epoch 421/1000 | Train Loss=971.65677897 | Val Loss=1.33510590 | Data=9.69709810 | Physics=2.01785629 | Val RMSE: 2.12079620 | ‚àö(Val Loss) = 1.15546787 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 422/1000 | Train Loss=963.49222819 | Val Loss=1.18724263 | Data=9.61554050 | Physics=1.83510432 | Val RMSE: 2.12079477 | ‚àö(Val Loss) = 1.08960664 | Current Learning Rate: 0.0002\n",
      "Epoch 423/1000 | Train Loss=968.80686442 | Val Loss=1.36339688 | Data=9.66861884 | Physics=1.95042507 | Val RMSE: 2.12080002 | ‚àö(Val Loss) = 1.16764581 | Current Learning Rate: 0.0002\n",
      "Epoch 424/1000 | Train Loss=972.94658407 | Val Loss=1.38978755 | Data=9.70990276 | Physics=2.03977171 | Val RMSE: 2.12080073 | ‚àö(Val Loss) = 1.17889249 | Current Learning Rate: 0.0002\n",
      "Epoch 425/1000 | Train Loss=964.09446208 | Val Loss=1.20378327 | Data=9.62154420 | Physics=1.91321291 | Val RMSE: 2.12079620 | ‚àö(Val Loss) = 1.09717059 | Current Learning Rate: 0.0002\n",
      "Epoch 426/1000 | Train Loss=967.62940470 | Val Loss=1.35851634 | Data=9.65688165 | Physics=1.95358046 | Val RMSE: 2.12079287 | ‚àö(Val Loss) = 1.16555405 | Current Learning Rate: 0.0002\n",
      "Epoch 427/1000 | Train Loss=970.88562012 | Val Loss=1.37208939 | Data=9.68939575 | Physics=1.98561781 | Val RMSE: 2.12078643 | ‚àö(Val Loss) = 1.17136216 | Current Learning Rate: 0.0002\n",
      "Epoch 428/1000 | Train Loss=964.42819214 | Val Loss=1.19888449 | Data=9.62490336 | Physics=1.87722599 | Val RMSE: 2.12078738 | ‚àö(Val Loss) = 1.09493589 | Current Learning Rate: 0.0002\n",
      "Epoch 429/1000 | Train Loss=967.25640869 | Val Loss=1.34124863 | Data=9.65313117 | Physics=1.95648662 | Val RMSE: 2.12079668 | ‚àö(Val Loss) = 1.15812290 | Current Learning Rate: 0.0002\n",
      "Epoch 430/1000 | Train Loss=969.93030802 | Val Loss=1.35384774 | Data=9.67984883 | Physics=1.93267323 | Val RMSE: 2.12079525 | ‚àö(Val Loss) = 1.16354966 | Current Learning Rate: 0.0002\n",
      "Epoch 431/1000 | Train Loss=964.36276245 | Val Loss=1.19353163 | Data=9.62419430 | Physics=1.98929621 | Val RMSE: 2.12079382 | ‚àö(Val Loss) = 1.09248877 | Current Learning Rate: 0.0002\n",
      "Epoch 432/1000 | Train Loss=968.21547445 | Val Loss=1.32416201 | Data=9.66272513 | Physics=1.90706194 | Val RMSE: 2.12079287 | ‚àö(Val Loss) = 1.15072238 | Current Learning Rate: 0.0002\n",
      "Epoch 433/1000 | Train Loss=969.00084432 | Val Loss=1.33728528 | Data=9.67051681 | Physics=2.03385326 | Val RMSE: 2.12080503 | ‚àö(Val Loss) = 1.15641057 | Current Learning Rate: 0.0002\n",
      "Epoch 434/1000 | Train Loss=964.74790446 | Val Loss=1.18833089 | Data=9.62806066 | Physics=1.95833102 | Val RMSE: 2.12081552 | ‚àö(Val Loss) = 1.09010589 | Current Learning Rate: 0.0002\n",
      "Epoch 435/1000 | Train Loss=966.83393351 | Val Loss=1.30953455 | Data=9.64890242 | Physics=1.99251486 | Val RMSE: 2.12081385 | ‚àö(Val Loss) = 1.14434898 | Current Learning Rate: 0.0002\n",
      "Epoch 436/1000 | Train Loss=968.79224650 | Val Loss=1.32195902 | Data=9.66846482 | Physics=2.02945447 | Val RMSE: 2.12080193 | ‚àö(Val Loss) = 1.14976478 | Current Learning Rate: 0.0002\n",
      "Epoch 437/1000 | Train Loss=968.26854451 | Val Loss=1.39580810 | Data=9.66320451 | Physics=2.03489287 | Val RMSE: 2.12079477 | ‚àö(Val Loss) = 1.18144321 | Current Learning Rate: 0.0002\n",
      "Epoch 438/1000 | Train Loss=969.56604004 | Val Loss=1.36126697 | Data=9.67620579 | Physics=1.93345505 | Val RMSE: 2.12079310 | ‚àö(Val Loss) = 1.16673350 | Current Learning Rate: 0.0002\n",
      "Epoch 439/1000 | Train Loss=964.20584106 | Val Loss=1.19630742 | Data=9.62264538 | Physics=1.88442425 | Val RMSE: 2.12080812 | ‚àö(Val Loss) = 1.09375834 | Current Learning Rate: 0.0002\n",
      "Epoch 440/1000 | Train Loss=967.07567342 | Val Loss=1.30326700 | Data=9.65135368 | Physics=1.87304871 | Val RMSE: 2.12080789 | ‚àö(Val Loss) = 1.14160717 | Current Learning Rate: 0.0002\n",
      "Epoch 441/1000 | Train Loss=966.45944214 | Val Loss=1.30929470 | Data=9.64514891 | Physics=1.90981765 | Val RMSE: 2.12080193 | ‚àö(Val Loss) = 1.14424419 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 442/1000 | Train Loss=967.84397380 | Val Loss=1.37364376 | Data=9.65899706 | Physics=1.98609426 | Val RMSE: 2.12079334 | ‚àö(Val Loss) = 1.17202544 | Current Learning Rate: 0.0002\n",
      "Epoch 443/1000 | Train Loss=966.94436646 | Val Loss=1.34060419 | Data=9.64999723 | Physics=1.95909405 | Val RMSE: 2.12078643 | ‚àö(Val Loss) = 1.15784466 | Current Learning Rate: 0.0002\n",
      "Epoch 444/1000 | Train Loss=965.78430176 | Val Loss=1.27185678 | Data=9.63839102 | Physics=1.93638617 | Val RMSE: 2.12089372 | ‚àö(Val Loss) = 1.12776625 | Current Learning Rate: 0.0002\n",
      "Epoch 445/1000 | Train Loss=965.82720947 | Val Loss=1.30232763 | Data=9.63884719 | Physics=1.94788518 | Val RMSE: 2.12085605 | ‚àö(Val Loss) = 1.14119565 | Current Learning Rate: 0.0002\n",
      "Epoch 446/1000 | Train Loss=964.85782878 | Val Loss=1.23141980 | Data=9.62916915 | Physics=1.90897452 | Val RMSE: 2.12081480 | ‚àö(Val Loss) = 1.10969353 | Current Learning Rate: 0.0002\n",
      "Epoch 447/1000 | Train Loss=966.45926921 | Val Loss=1.31594706 | Data=9.64514748 | Physics=1.99390396 | Val RMSE: 2.12080002 | ‚àö(Val Loss) = 1.14714730 | Current Learning Rate: 0.0002\n",
      "Epoch 448/1000 | Train Loss=966.76195272 | Val Loss=1.30590653 | Data=9.64820830 | Physics=1.91580959 | Val RMSE: 2.12079573 | ‚àö(Val Loss) = 1.14276266 | Current Learning Rate: 0.0002\n",
      "Epoch 449/1000 | Train Loss=967.64229329 | Val Loss=1.35747778 | Data=9.65696367 | Physics=1.98309230 | Val RMSE: 2.12078738 | ‚àö(Val Loss) = 1.16510844 | Current Learning Rate: 0.0002\n",
      "Epoch 450/1000 | Train Loss=967.05661011 | Val Loss=1.32382452 | Data=9.65112813 | Physics=1.94869491 | Val RMSE: 2.12078524 | ‚àö(Val Loss) = 1.15057576 | Current Learning Rate: 0.0002\n",
      "Epoch 451/1000 | Train Loss=966.99736532 | Val Loss=1.36523700 | Data=9.65054941 | Physics=1.88119691 | Val RMSE: 2.12079024 | ‚àö(Val Loss) = 1.16843355 | Current Learning Rate: 0.0002\n",
      "Epoch 452/1000 | Train Loss=965.68547567 | Val Loss=1.32582581 | Data=9.63744386 | Physics=1.93049177 | Val RMSE: 2.12079358 | ‚àö(Val Loss) = 1.15144515 | Current Learning Rate: 0.0002\n",
      "Epoch 453/1000 | Train Loss=967.18688965 | Val Loss=1.36006415 | Data=9.65243769 | Physics=1.95688932 | Val RMSE: 2.12082362 | ‚àö(Val Loss) = 1.16621792 | Current Learning Rate: 0.0002\n",
      "Epoch 454/1000 | Train Loss=967.11613973 | Val Loss=1.31838334 | Data=9.65169621 | Physics=2.00279779 | Val RMSE: 2.12082624 | ‚àö(Val Loss) = 1.14820874 | Current Learning Rate: 0.0002\n",
      "Epoch 455/1000 | Train Loss=967.30665080 | Val Loss=1.35340476 | Data=9.65358273 | Physics=2.05827394 | Val RMSE: 2.12080169 | ‚àö(Val Loss) = 1.16335928 | Current Learning Rate: 0.0002\n",
      "Epoch 456/1000 | Train Loss=967.02177938 | Val Loss=1.31378257 | Data=9.65083011 | Physics=1.90632862 | Val RMSE: 2.12078619 | ‚àö(Val Loss) = 1.14620352 | Current Learning Rate: 0.0002\n",
      "Epoch 457/1000 | Train Loss=966.58809408 | Val Loss=1.34846830 | Data=9.64645831 | Physics=1.97578512 | Val RMSE: 2.12078023 | ‚àö(Val Loss) = 1.16123569 | Current Learning Rate: 0.0002\n",
      "Epoch 458/1000 | Train Loss=967.26880900 | Val Loss=1.31024384 | Data=9.65327374 | Physics=1.96741896 | Val RMSE: 2.12077713 | ‚àö(Val Loss) = 1.14465880 | Current Learning Rate: 0.0002\n",
      "Epoch 459/1000 | Train Loss=967.32420858 | Val Loss=1.34455967 | Data=9.65384722 | Physics=1.87442118 | Val RMSE: 2.12078357 | ‚àö(Val Loss) = 1.15955150 | Current Learning Rate: 0.0002\n",
      "Epoch 460/1000 | Train Loss=965.92324829 | Val Loss=1.30707264 | Data=9.63980071 | Physics=1.92641351 | Val RMSE: 2.12078619 | ‚àö(Val Loss) = 1.14327276 | Current Learning Rate: 0.0002\n",
      "Epoch 461/1000 | Train Loss=966.43167114 | Val Loss=1.33886337 | Data=9.64489826 | Physics=1.87008226 | Val RMSE: 2.12078071 | ‚àö(Val Loss) = 1.15709269 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 462/1000 | Train Loss=966.28792318 | Val Loss=1.30215323 | Data=9.64348714 | Physics=1.88377932 | Val RMSE: 2.12078071 | ‚àö(Val Loss) = 1.14111924 | Current Learning Rate: 0.0002\n",
      "Epoch 463/1000 | Train Loss=966.56090291 | Val Loss=1.33516121 | Data=9.64618826 | Physics=1.95794839 | Val RMSE: 2.12078619 | ‚àö(Val Loss) = 1.15549171 | Current Learning Rate: 0.0002\n",
      "Epoch 464/1000 | Train Loss=965.79063924 | Val Loss=1.29962993 | Data=9.63851388 | Physics=1.90416517 | Val RMSE: 2.12079191 | ‚àö(Val Loss) = 1.14001310 | Current Learning Rate: 0.0002\n",
      "Epoch 465/1000 | Train Loss=965.97035726 | Val Loss=1.33093834 | Data=9.64028279 | Physics=1.94843933 | Val RMSE: 2.12079215 | ‚àö(Val Loss) = 1.15366304 | Current Learning Rate: 0.0002\n",
      "Epoch 466/1000 | Train Loss=965.03715007 | Val Loss=1.29544055 | Data=9.63098176 | Physics=1.89880922 | Val RMSE: 2.12079287 | ‚àö(Val Loss) = 1.13817418 | Current Learning Rate: 0.0002\n",
      "Epoch 467/1000 | Train Loss=966.32218424 | Val Loss=1.32614732 | Data=9.64382013 | Physics=1.86430278 | Val RMSE: 2.12079811 | ‚àö(Val Loss) = 1.15158474 | Current Learning Rate: 0.0002\n",
      "Epoch 468/1000 | Train Loss=966.24743652 | Val Loss=1.29164386 | Data=9.64306355 | Physics=2.01737743 | Val RMSE: 2.12079716 | ‚àö(Val Loss) = 1.13650513 | Current Learning Rate: 0.0002\n",
      "Epoch 469/1000 | Train Loss=965.12773641 | Val Loss=1.32186675 | Data=9.63192113 | Physics=1.79401529 | Val RMSE: 2.12078834 | ‚àö(Val Loss) = 1.14972460 | Current Learning Rate: 0.0002\n",
      "Epoch 470/1000 | Train Loss=966.83529663 | Val Loss=1.28812444 | Data=9.64888827 | Physics=2.10075562 | Val RMSE: 2.12078285 | ‚àö(Val Loss) = 1.13495564 | Current Learning Rate: 0.0002\n",
      "Epoch 471/1000 | Train Loss=966.79573568 | Val Loss=1.31857467 | Data=9.64852015 | Physics=1.97765433 | Val RMSE: 2.12078357 | ‚àö(Val Loss) = 1.14829206 | Current Learning Rate: 0.0002\n",
      "Epoch 472/1000 | Train Loss=964.53064982 | Val Loss=1.28550005 | Data=9.62591505 | Physics=1.93566256 | Val RMSE: 2.12078309 | ‚àö(Val Loss) = 1.13379896 | Current Learning Rate: 0.0002\n",
      "Epoch 473/1000 | Train Loss=966.43116252 | Val Loss=1.31491590 | Data=9.64487155 | Physics=1.94111583 | Val RMSE: 2.12078404 | ‚àö(Val Loss) = 1.14669788 | Current Learning Rate: 0.0002\n",
      "Epoch 474/1000 | Train Loss=967.34002686 | Val Loss=1.28257191 | Data=9.65391588 | Physics=2.13929405 | Val RMSE: 2.12078309 | ‚àö(Val Loss) = 1.13250697 | Current Learning Rate: 0.0002\n",
      "Epoch 475/1000 | Train Loss=966.54798381 | Val Loss=1.31167281 | Data=9.64605093 | Physics=2.01236401 | Val RMSE: 2.12078500 | ‚àö(Val Loss) = 1.14528286 | Current Learning Rate: 0.0002\n",
      "Epoch 476/1000 | Train Loss=964.98299154 | Val Loss=1.28005517 | Data=9.63040543 | Physics=1.94588577 | Val RMSE: 2.12078428 | ‚àö(Val Loss) = 1.13139522 | Current Learning Rate: 0.0002\n",
      "Epoch 477/1000 | Train Loss=968.51430257 | Val Loss=1.30816197 | Data=9.66566181 | Physics=2.04237145 | Val RMSE: 2.12077928 | ‚àö(Val Loss) = 1.14374912 | Current Learning Rate: 0.0002\n",
      "Epoch 478/1000 | Train Loss=965.30320231 | Val Loss=1.27714050 | Data=9.63362312 | Physics=1.94937977 | Val RMSE: 2.12078071 | ‚àö(Val Loss) = 1.13010645 | Current Learning Rate: 0.0002\n",
      "Epoch 479/1000 | Train Loss=966.57537842 | Val Loss=1.30535424 | Data=9.64632432 | Physics=1.93909529 | Val RMSE: 2.12078261 | ‚àö(Val Loss) = 1.14252102 | Current Learning Rate: 0.0002\n",
      "Epoch 480/1000 | Train Loss=964.92113241 | Val Loss=1.27488291 | Data=9.62983433 | Physics=1.92895885 | Val RMSE: 2.12078524 | ‚àö(Val Loss) = 1.12910712 | Current Learning Rate: 0.0002\n",
      "Epoch 481/1000 | Train Loss=965.56528727 | Val Loss=1.30181181 | Data=9.63626083 | Physics=1.91467931 | Val RMSE: 2.12077904 | ‚àö(Val Loss) = 1.14096963 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 482/1000 | Train Loss=966.20243327 | Val Loss=1.27126682 | Data=9.64259736 | Physics=1.97078475 | Val RMSE: 2.12077045 | ‚àö(Val Loss) = 1.12750471 | Current Learning Rate: 0.0002\n",
      "Epoch 483/1000 | Train Loss=965.81318156 | Val Loss=1.29868472 | Data=9.63869190 | Physics=1.99121077 | Val RMSE: 2.12077188 | ‚àö(Val Loss) = 1.13959849 | Current Learning Rate: 0.0002\n",
      "Epoch 484/1000 | Train Loss=966.00750732 | Val Loss=1.26924610 | Data=9.64068524 | Physics=1.91791143 | Val RMSE: 2.12077546 | ‚àö(Val Loss) = 1.12660825 | Current Learning Rate: 0.0002\n",
      "Epoch 485/1000 | Train Loss=966.19957479 | Val Loss=1.29656076 | Data=9.64260530 | Physics=1.87821391 | Val RMSE: 2.12077785 | ‚àö(Val Loss) = 1.13866627 | Current Learning Rate: 0.0002\n",
      "Epoch 486/1000 | Train Loss=963.87886556 | Val Loss=1.26732266 | Data=9.61939828 | Physics=1.91273216 | Val RMSE: 2.12077737 | ‚àö(Val Loss) = 1.12575424 | Current Learning Rate: 0.0002\n",
      "Epoch 487/1000 | Train Loss=966.28824870 | Val Loss=1.29414082 | Data=9.64346043 | Physics=1.94634453 | Val RMSE: 2.12077999 | ‚àö(Val Loss) = 1.13760304 | Current Learning Rate: 0.0002\n",
      "Epoch 488/1000 | Train Loss=964.45977783 | Val Loss=1.26587915 | Data=9.62516276 | Physics=1.94298958 | Val RMSE: 2.12078381 | ‚àö(Val Loss) = 1.12511289 | Current Learning Rate: 0.0002\n",
      "Epoch 489/1000 | Train Loss=966.51833089 | Val Loss=1.29167187 | Data=9.64575656 | Physics=1.90780837 | Val RMSE: 2.12078023 | ‚àö(Val Loss) = 1.13651741 | Current Learning Rate: 0.0002\n",
      "Epoch 490/1000 | Train Loss=965.51620483 | Val Loss=1.26388717 | Data=9.63572613 | Physics=2.01342294 | Val RMSE: 2.12078309 | ‚àö(Val Loss) = 1.12422740 | Current Learning Rate: 0.0002\n",
      "Epoch 491/1000 | Train Loss=966.01189168 | Val Loss=1.28975594 | Data=9.64067841 | Physics=2.03137279 | Val RMSE: 2.12078261 | ‚àö(Val Loss) = 1.13567424 | Current Learning Rate: 0.0002\n",
      "Epoch 492/1000 | Train Loss=963.75094604 | Val Loss=1.26172733 | Data=9.61813958 | Physics=1.86190679 | Val RMSE: 2.12078309 | ‚àö(Val Loss) = 1.12326634 | Current Learning Rate: 0.0002\n",
      "Epoch 493/1000 | Train Loss=965.68708293 | Val Loss=1.28713346 | Data=9.63745642 | Physics=1.89590432 | Val RMSE: 2.12078047 | ‚àö(Val Loss) = 1.13451910 | Current Learning Rate: 0.0002\n",
      "Epoch 494/1000 | Train Loss=963.59861247 | Val Loss=1.26036572 | Data=9.61660401 | Physics=1.86535246 | Val RMSE: 2.12078643 | ‚àö(Val Loss) = 1.12266016 | Current Learning Rate: 0.0002\n",
      "Epoch 495/1000 | Train Loss=965.60352580 | Val Loss=1.28547513 | Data=9.63662068 | Physics=1.97828093 | Val RMSE: 2.12078547 | ‚àö(Val Loss) = 1.13378799 | Current Learning Rate: 0.0002\n",
      "Epoch 496/1000 | Train Loss=963.98296102 | Val Loss=1.25833559 | Data=9.62045829 | Physics=1.84057424 | Val RMSE: 2.12078595 | ‚àö(Val Loss) = 1.12175560 | Current Learning Rate: 0.0002\n",
      "Epoch 497/1000 | Train Loss=965.86980184 | Val Loss=1.28273821 | Data=9.63924074 | Physics=1.97353134 | Val RMSE: 2.12077975 | ‚àö(Val Loss) = 1.13258028 | Current Learning Rate: 0.0002\n",
      "Epoch 498/1000 | Train Loss=963.87004598 | Val Loss=1.25603771 | Data=9.61931705 | Physics=1.90924080 | Val RMSE: 2.12078023 | ‚àö(Val Loss) = 1.12073088 | Current Learning Rate: 0.0002\n",
      "Epoch 499/1000 | Train Loss=966.59899902 | Val Loss=1.28068292 | Data=9.64657497 | Physics=1.95763016 | Val RMSE: 2.12077451 | ‚àö(Val Loss) = 1.13167262 | Current Learning Rate: 0.0002\n",
      "Epoch 500/1000 | Train Loss=964.81388346 | Val Loss=1.25452363 | Data=9.62875175 | Physics=1.95295369 | Val RMSE: 2.12077069 | ‚àö(Val Loss) = 1.12005520 | Current Learning Rate: 0.0002\n",
      "Epoch 501/1000 | Train Loss=964.59402466 | Val Loss=1.27908373 | Data=9.62655020 | Physics=1.88608389 | Val RMSE: 2.12077141 | ‚àö(Val Loss) = 1.13096583 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 502/1000 | Train Loss=964.92867025 | Val Loss=1.25351024 | Data=9.62987836 | Physics=1.98199172 | Val RMSE: 2.12077665 | ‚àö(Val Loss) = 1.11960268 | Current Learning Rate: 0.0002\n",
      "Epoch 503/1000 | Train Loss=965.96995036 | Val Loss=1.27785254 | Data=9.64028184 | Physics=1.94265199 | Val RMSE: 2.12077785 | ‚àö(Val Loss) = 1.13042140 | Current Learning Rate: 0.0002\n",
      "Epoch 504/1000 | Train Loss=964.63460286 | Val Loss=1.25267148 | Data=9.62691911 | Physics=1.91839517 | Val RMSE: 2.12078524 | ‚àö(Val Loss) = 1.11922812 | Current Learning Rate: 0.0002\n",
      "Epoch 505/1000 | Train Loss=964.20413208 | Val Loss=1.27691805 | Data=9.62265730 | Physics=1.87478862 | Val RMSE: 2.12079048 | ‚àö(Val Loss) = 1.13000798 | Current Learning Rate: 0.0002\n",
      "Epoch 506/1000 | Train Loss=963.55849202 | Val Loss=1.25102317 | Data=9.61619282 | Physics=1.93338841 | Val RMSE: 2.12078762 | ‚àö(Val Loss) = 1.11849153 | Current Learning Rate: 0.0002\n",
      "Epoch 507/1000 | Train Loss=965.26715088 | Val Loss=1.27466309 | Data=9.63326708 | Physics=1.94750255 | Val RMSE: 2.12078333 | ‚àö(Val Loss) = 1.12900972 | Current Learning Rate: 0.0002\n",
      "Epoch 508/1000 | Train Loss=964.34835815 | Val Loss=1.24938214 | Data=9.62404585 | Physics=2.03976280 | Val RMSE: 2.12078404 | ‚àö(Val Loss) = 1.11775768 | Current Learning Rate: 0.0002\n",
      "Epoch 509/1000 | Train Loss=966.16830444 | Val Loss=1.27247691 | Data=9.64226755 | Physics=1.96347962 | Val RMSE: 2.12077975 | ‚àö(Val Loss) = 1.12804115 | Current Learning Rate: 0.0002\n",
      "Epoch 510/1000 | Train Loss=964.07421875 | Val Loss=1.24743223 | Data=9.62136793 | Physics=1.87401965 | Val RMSE: 2.12077880 | ‚àö(Val Loss) = 1.11688507 | Current Learning Rate: 0.0002\n",
      "Epoch 511/1000 | Train Loss=965.35042318 | Val Loss=1.27087367 | Data=9.63406134 | Physics=1.98696129 | Val RMSE: 2.12077928 | ‚àö(Val Loss) = 1.12733030 | Current Learning Rate: 0.0002\n",
      "Epoch 512/1000 | Train Loss=964.21301270 | Val Loss=1.24741447 | Data=9.62270069 | Physics=1.94635757 | Val RMSE: 2.12078691 | ‚àö(Val Loss) = 1.11687708 | Current Learning Rate: 0.0002\n",
      "Epoch 513/1000 | Train Loss=965.60628255 | Val Loss=1.27019727 | Data=9.63663658 | Physics=1.95015647 | Val RMSE: 2.12078381 | ‚àö(Val Loss) = 1.12703025 | Current Learning Rate: 0.0002\n",
      "Epoch 514/1000 | Train Loss=964.46861776 | Val Loss=1.24637294 | Data=9.62529357 | Physics=1.95198972 | Val RMSE: 2.12078524 | ‚àö(Val Loss) = 1.11641073 | Current Learning Rate: 0.0002\n",
      "Epoch 515/1000 | Train Loss=965.35009766 | Val Loss=1.26966393 | Data=9.63409662 | Physics=1.94050396 | Val RMSE: 2.12078595 | ‚àö(Val Loss) = 1.12679362 | Current Learning Rate: 0.0002\n",
      "Epoch 516/1000 | Train Loss=965.13171387 | Val Loss=1.24627101 | Data=9.63191334 | Physics=1.99175224 | Val RMSE: 2.12077165 | ‚àö(Val Loss) = 1.11636508 | Current Learning Rate: 0.0002\n",
      "Epoch 517/1000 | Train Loss=966.95448812 | Val Loss=1.27049387 | Data=9.65007432 | Physics=2.04060410 | Val RMSE: 2.12077045 | ‚àö(Val Loss) = 1.12716186 | Current Learning Rate: 0.0002\n",
      "Epoch 518/1000 | Train Loss=963.73131307 | Val Loss=1.24568057 | Data=9.61792469 | Physics=1.96534714 | Val RMSE: 2.12077832 | ‚àö(Val Loss) = 1.11610067 | Current Learning Rate: 0.0002\n",
      "Epoch 519/1000 | Train Loss=966.17621867 | Val Loss=1.26747429 | Data=9.64232254 | Physics=1.97615627 | Val RMSE: 2.12077904 | ‚àö(Val Loss) = 1.12582159 | Current Learning Rate: 0.0002\n",
      "Epoch 520/1000 | Train Loss=963.83747355 | Val Loss=1.24329257 | Data=9.61895514 | Physics=1.96174498 | Val RMSE: 2.12078714 | ‚àö(Val Loss) = 1.11503029 | Current Learning Rate: 0.0002\n",
      "Epoch 521/1000 | Train Loss=965.79675293 | Val Loss=1.26508904 | Data=9.63854249 | Physics=1.96579610 | Val RMSE: 2.12078261 | ‚àö(Val Loss) = 1.12476182 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 522/1000 | Train Loss=963.46011353 | Val Loss=1.24176025 | Data=9.61520545 | Physics=1.90522993 | Val RMSE: 2.12078714 | ‚àö(Val Loss) = 1.11434293 | Current Learning Rate: 0.0002\n",
      "Epoch 523/1000 | Train Loss=964.59292603 | Val Loss=1.26352239 | Data=9.62653971 | Physics=1.90011963 | Val RMSE: 2.12078524 | ‚àö(Val Loss) = 1.12406516 | Current Learning Rate: 0.0002\n",
      "Epoch 524/1000 | Train Loss=964.61069743 | Val Loss=1.24047661 | Data=9.62667100 | Physics=1.98911773 | Val RMSE: 2.12078476 | ‚àö(Val Loss) = 1.11376691 | Current Learning Rate: 0.0002\n",
      "Epoch 525/1000 | Train Loss=964.93597412 | Val Loss=1.26228738 | Data=9.62994258 | Physics=1.93515247 | Val RMSE: 2.12078476 | ‚àö(Val Loss) = 1.12351561 | Current Learning Rate: 0.0002\n",
      "Epoch 526/1000 | Train Loss=963.44877116 | Val Loss=1.24050391 | Data=9.61509116 | Physics=1.93666669 | Val RMSE: 2.12079644 | ‚àö(Val Loss) = 1.11377907 | Current Learning Rate: 0.0002\n",
      "Epoch 527/1000 | Train Loss=964.73952230 | Val Loss=1.26253903 | Data=9.62798643 | Physics=1.93101035 | Val RMSE: 2.12079453 | ‚àö(Val Loss) = 1.12362766 | Current Learning Rate: 0.0002\n",
      "Epoch 528/1000 | Train Loss=963.47549438 | Val Loss=1.23856556 | Data=9.61533276 | Physics=1.99393869 | Val RMSE: 2.12078714 | ‚àö(Val Loss) = 1.11290860 | Current Learning Rate: 0.0002\n",
      "Epoch 529/1000 | Train Loss=965.25247192 | Val Loss=1.26062655 | Data=9.63313516 | Physics=1.93710373 | Val RMSE: 2.12078857 | ‚àö(Val Loss) = 1.12277627 | Current Learning Rate: 0.0002\n",
      "Epoch 530/1000 | Train Loss=964.33614095 | Val Loss=1.25885201 | Data=9.62399515 | Physics=1.87912446 | Val RMSE: 2.12075639 | ‚àö(Val Loss) = 1.12198579 | Current Learning Rate: 0.0002\n",
      "Epoch 531/1000 | Train Loss=965.56382243 | Val Loss=1.28858054 | Data=9.63620965 | Physics=1.93107423 | Val RMSE: 2.12074661 | ‚àö(Val Loss) = 1.13515663 | Current Learning Rate: 0.0002\n",
      "Epoch 532/1000 | Train Loss=963.96875000 | Val Loss=1.25245845 | Data=9.62025881 | Physics=2.02483605 | Val RMSE: 2.12077022 | ‚àö(Val Loss) = 1.11913288 | Current Learning Rate: 0.0002\n",
      "Epoch 533/1000 | Train Loss=965.28474935 | Val Loss=1.26673520 | Data=9.63345146 | Physics=1.88221243 | Val RMSE: 2.12077069 | ‚àö(Val Loss) = 1.12549329 | Current Learning Rate: 0.0002\n",
      "Epoch 534/1000 | Train Loss=964.65553792 | Val Loss=1.24045777 | Data=9.62715435 | Physics=1.98716388 | Val RMSE: 2.12077975 | ‚àö(Val Loss) = 1.11375844 | Current Learning Rate: 0.0002\n",
      "Epoch 535/1000 | Train Loss=964.76281738 | Val Loss=1.25964165 | Data=9.62823264 | Physics=1.93677891 | Val RMSE: 2.12077880 | ‚àö(Val Loss) = 1.12233758 | Current Learning Rate: 0.0002\n",
      "Epoch 536/1000 | Train Loss=964.59546916 | Val Loss=1.23803318 | Data=9.62654718 | Physics=1.95932577 | Val RMSE: 2.12078691 | ‚àö(Val Loss) = 1.11266935 | Current Learning Rate: 0.0002\n",
      "Epoch 537/1000 | Train Loss=965.04908244 | Val Loss=1.25899875 | Data=9.63108333 | Physics=1.93050012 | Val RMSE: 2.12078834 | ‚àö(Val Loss) = 1.12205112 | Current Learning Rate: 0.0002\n",
      "Epoch 538/1000 | Train Loss=963.54250081 | Val Loss=1.23740673 | Data=9.61605326 | Physics=1.89771191 | Val RMSE: 2.12079835 | ‚àö(Val Loss) = 1.11238790 | Current Learning Rate: 0.0002\n",
      "Epoch 539/1000 | Train Loss=965.75080363 | Val Loss=1.25780904 | Data=9.63806597 | Physics=1.99636309 | Val RMSE: 2.12079453 | ‚àö(Val Loss) = 1.12152088 | Current Learning Rate: 0.0002\n",
      "Epoch 540/1000 | Train Loss=964.63954671 | Val Loss=1.23718286 | Data=9.62698619 | Physics=1.99276822 | Val RMSE: 2.12078619 | ‚àö(Val Loss) = 1.11228716 | Current Learning Rate: 0.0002\n",
      "Epoch 541/1000 | Train Loss=966.49890137 | Val Loss=1.26004410 | Data=9.64553897 | Physics=1.97298452 | Val RMSE: 2.12078118 | ‚àö(Val Loss) = 1.12251687 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 542/1000 | Train Loss=963.55213420 | Val Loss=1.23948812 | Data=9.61613989 | Physics=1.88707984 | Val RMSE: 2.12078714 | ‚àö(Val Loss) = 1.11332297 | Current Learning Rate: 0.0002\n",
      "Epoch 543/1000 | Train Loss=966.03667196 | Val Loss=1.26011288 | Data=9.64093256 | Physics=1.93440474 | Val RMSE: 2.12078238 | ‚àö(Val Loss) = 1.12254751 | Current Learning Rate: 0.0002\n",
      "Epoch 544/1000 | Train Loss=964.76261393 | Val Loss=1.23800290 | Data=9.62823598 | Physics=1.89192758 | Val RMSE: 2.12078714 | ‚àö(Val Loss) = 1.11265576 | Current Learning Rate: 0.0002\n",
      "Epoch 545/1000 | Train Loss=966.11303711 | Val Loss=1.25783873 | Data=9.64168040 | Physics=2.00862460 | Val RMSE: 2.12078643 | ‚àö(Val Loss) = 1.12153411 | Current Learning Rate: 0.0002\n",
      "Epoch 546/1000 | Train Loss=963.42657471 | Val Loss=1.23523080 | Data=9.61488326 | Physics=1.90644781 | Val RMSE: 2.12079406 | ‚àö(Val Loss) = 1.11140943 | Current Learning Rate: 0.0002\n",
      "Epoch 547/1000 | Train Loss=965.25422160 | Val Loss=1.25546789 | Data=9.63313182 | Physics=1.94606915 | Val RMSE: 2.12079549 | ‚àö(Val Loss) = 1.12047660 | Current Learning Rate: 0.0002\n",
      "Epoch 548/1000 | Train Loss=964.38716634 | Val Loss=1.23724806 | Data=9.62447516 | Physics=1.89337962 | Val RMSE: 2.12081790 | ‚àö(Val Loss) = 1.11231649 | Current Learning Rate: 0.0002\n",
      "Epoch 549/1000 | Train Loss=966.08731079 | Val Loss=1.25856197 | Data=9.64142958 | Physics=1.96304479 | Val RMSE: 2.12082458 | ‚àö(Val Loss) = 1.12185645 | Current Learning Rate: 0.0002\n",
      "Epoch 550/1000 | Train Loss=964.55151367 | Val Loss=1.23271537 | Data=9.62607908 | Physics=1.98401459 | Val RMSE: 2.12080932 | ‚àö(Val Loss) = 1.11027718 | Current Learning Rate: 0.0002\n",
      "Epoch 551/1000 | Train Loss=966.00087484 | Val Loss=1.25168824 | Data=9.64060052 | Physics=1.90987772 | Val RMSE: 2.12080050 | ‚àö(Val Loss) = 1.11878872 | Current Learning Rate: 0.0002\n",
      "Epoch 552/1000 | Train Loss=964.07533773 | Val Loss=1.23194790 | Data=9.62133519 | Physics=1.91923869 | Val RMSE: 2.12080765 | ‚àö(Val Loss) = 1.10993147 | Current Learning Rate: 0.0002\n",
      "Epoch 553/1000 | Train Loss=965.44215902 | Val Loss=1.25093508 | Data=9.63499896 | Physics=1.95983215 | Val RMSE: 2.12079835 | ‚àö(Val Loss) = 1.11845207 | Current Learning Rate: 0.0002\n",
      "Epoch 554/1000 | Train Loss=963.45711263 | Val Loss=1.23437607 | Data=9.61520211 | Physics=1.88447920 | Val RMSE: 2.12080741 | ‚àö(Val Loss) = 1.11102474 | Current Learning Rate: 0.0002\n",
      "Epoch 555/1000 | Train Loss=964.74589030 | Val Loss=1.25426984 | Data=9.62809340 | Physics=1.87186379 | Val RMSE: 2.12079358 | ‚àö(Val Loss) = 1.11994195 | Current Learning Rate: 0.0002\n",
      "Epoch 556/1000 | Train Loss=963.53685506 | Val Loss=1.23166609 | Data=9.61595074 | Physics=1.90254335 | Val RMSE: 2.12080836 | ‚àö(Val Loss) = 1.10980451 | Current Learning Rate: 0.0002\n",
      "Epoch 557/1000 | Train Loss=965.05006917 | Val Loss=1.32438934 | Data=9.63111369 | Physics=1.88924383 | Val RMSE: 2.12078667 | ‚àö(Val Loss) = 1.15082121 | Current Learning Rate: 0.0002\n",
      "Epoch 558/1000 | Train Loss=967.81591797 | Val Loss=1.37969291 | Data=9.65875721 | Physics=1.90612276 | Val RMSE: 2.12076616 | ‚àö(Val Loss) = 1.17460334 | Current Learning Rate: 0.0002\n",
      "Epoch 559/1000 | Train Loss=964.98999023 | Val Loss=1.32502139 | Data=9.63049205 | Physics=1.92448291 | Val RMSE: 2.12078118 | ‚àö(Val Loss) = 1.15109575 | Current Learning Rate: 0.0002\n",
      "Epoch 560/1000 | Train Loss=963.18001302 | Val Loss=1.26361227 | Data=9.61244901 | Physics=1.84706691 | Val RMSE: 2.12079024 | ‚àö(Val Loss) = 1.12410510 | Current Learning Rate: 0.0002\n",
      "Epoch 561/1000 | Train Loss=964.82907104 | Val Loss=1.26769757 | Data=9.62890689 | Physics=1.89618204 | Val RMSE: 2.12078667 | ‚àö(Val Loss) = 1.12592077 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 562/1000 | Train Loss=964.69235229 | Val Loss=1.23669529 | Data=9.62751802 | Physics=1.95326801 | Val RMSE: 2.12080002 | ‚àö(Val Loss) = 1.11206806 | Current Learning Rate: 0.0002\n",
      "Epoch 563/1000 | Train Loss=965.76520793 | Val Loss=1.25243652 | Data=9.63821395 | Physics=1.95991890 | Val RMSE: 2.12080145 | ‚àö(Val Loss) = 1.11912310 | Current Learning Rate: 0.0002\n",
      "Epoch 564/1000 | Train Loss=964.87031047 | Val Loss=1.23213780 | Data=9.62927866 | Physics=2.00523780 | Val RMSE: 2.12081265 | ‚àö(Val Loss) = 1.11001706 | Current Learning Rate: 0.0002\n",
      "Epoch 565/1000 | Train Loss=965.51139323 | Val Loss=1.25027382 | Data=9.63572423 | Physics=1.87891095 | Val RMSE: 2.12080860 | ‚àö(Val Loss) = 1.11815643 | Current Learning Rate: 0.0002\n",
      "Epoch 566/1000 | Train Loss=966.44249471 | Val Loss=1.30371141 | Data=9.64497884 | Physics=2.03706853 | Val RMSE: 2.12082744 | ‚àö(Val Loss) = 1.14180183 | Current Learning Rate: 0.0002\n",
      "Epoch 567/1000 | Train Loss=964.50868734 | Val Loss=1.30309212 | Data=9.62565358 | Physics=1.96053123 | Val RMSE: 2.12083507 | ‚àö(Val Loss) = 1.14153063 | Current Learning Rate: 0.0002\n",
      "Epoch 568/1000 | Train Loss=965.70288086 | Val Loss=1.34231639 | Data=9.63763380 | Physics=1.85959048 | Val RMSE: 2.12082434 | ‚àö(Val Loss) = 1.15858376 | Current Learning Rate: 0.0002\n",
      "Epoch 569/1000 | Train Loss=965.12546794 | Val Loss=1.26069307 | Data=9.63181416 | Physics=1.99985123 | Val RMSE: 2.12079954 | ‚àö(Val Loss) = 1.12280583 | Current Learning Rate: 0.0002\n",
      "Epoch 570/1000 | Train Loss=963.98119100 | Val Loss=1.26488209 | Data=9.62040949 | Physics=1.91334802 | Val RMSE: 2.12079191 | ‚àö(Val Loss) = 1.12466979 | Current Learning Rate: 0.0002\n",
      "Epoch 571/1000 | Train Loss=964.10725911 | Val Loss=1.23720479 | Data=9.62165499 | Physics=1.95252809 | Val RMSE: 2.12080073 | ‚àö(Val Loss) = 1.11229706 | Current Learning Rate: 0.0002\n",
      "Epoch 572/1000 | Train Loss=966.19625854 | Val Loss=1.26821244 | Data=9.64252774 | Physics=1.98434325 | Val RMSE: 2.12078285 | ‚àö(Val Loss) = 1.12614942 | Current Learning Rate: 0.0002\n",
      "Epoch 573/1000 | Train Loss=965.60661825 | Val Loss=1.25976229 | Data=9.63662020 | Physics=1.98863194 | Val RMSE: 2.12076855 | ‚àö(Val Loss) = 1.12239134 | Current Learning Rate: 0.0002\n",
      "Epoch 574/1000 | Train Loss=963.36111450 | Val Loss=1.26433432 | Data=9.61426767 | Physics=1.84692437 | Val RMSE: 2.12076712 | ‚àö(Val Loss) = 1.12442625 | Current Learning Rate: 0.0002\n",
      "Epoch 575/1000 | Train Loss=963.67553711 | Val Loss=1.23422992 | Data=9.61735725 | Physics=2.01689262 | Val RMSE: 2.12075806 | ‚àö(Val Loss) = 1.11095905 | Current Learning Rate: 0.0002\n",
      "Epoch 576/1000 | Train Loss=965.07636515 | Val Loss=1.24857521 | Data=9.63136117 | Physics=1.86323314 | Val RMSE: 2.12076020 | ‚àö(Val Loss) = 1.11739659 | Current Learning Rate: 0.0002\n",
      "Epoch 577/1000 | Train Loss=966.21484375 | Val Loss=1.23173726 | Data=9.64269257 | Physics=2.02070713 | Val RMSE: 2.12079883 | ‚àö(Val Loss) = 1.10983658 | Current Learning Rate: 0.0002\n",
      "Epoch 578/1000 | Train Loss=964.94433594 | Val Loss=1.25200844 | Data=9.63003461 | Physics=1.94613742 | Val RMSE: 2.12081099 | ‚àö(Val Loss) = 1.11893177 | Current Learning Rate: 0.0002\n",
      "Epoch 579/1000 | Train Loss=966.62837728 | Val Loss=1.35635698 | Data=9.64683088 | Physics=2.03743181 | Val RMSE: 2.12083077 | ‚àö(Val Loss) = 1.16462743 | Current Learning Rate: 0.0002\n",
      "Epoch 580/1000 | Train Loss=964.92325846 | Val Loss=1.27508950 | Data=9.62980636 | Physics=1.99295978 | Val RMSE: 2.12084031 | ‚àö(Val Loss) = 1.12919855 | Current Learning Rate: 0.0002\n",
      "Epoch 581/1000 | Train Loss=964.34060669 | Val Loss=1.26946509 | Data=9.62401390 | Physics=1.85580041 | Val RMSE: 2.12080741 | ‚àö(Val Loss) = 1.12670541 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 582/1000 | Train Loss=963.96383667 | Val Loss=1.23900390 | Data=9.62026946 | Physics=1.88482430 | Val RMSE: 2.12080693 | ‚àö(Val Loss) = 1.11310554 | Current Learning Rate: 0.0002\n",
      "Epoch 583/1000 | Train Loss=965.44203695 | Val Loss=1.25355351 | Data=9.63503663 | Physics=1.88912713 | Val RMSE: 2.12080526 | ‚àö(Val Loss) = 1.11962199 | Current Learning Rate: 0.0002\n",
      "Epoch 584/1000 | Train Loss=965.43680827 | Val Loss=1.35474980 | Data=9.63498576 | Physics=1.93164365 | Val RMSE: 2.12077427 | ‚àö(Val Loss) = 1.16393721 | Current Learning Rate: 0.0002\n",
      "Epoch 585/1000 | Train Loss=964.54919434 | Val Loss=1.27294636 | Data=9.62608957 | Physics=1.97161360 | Val RMSE: 2.12076759 | ‚àö(Val Loss) = 1.12824929 | Current Learning Rate: 0.0002\n",
      "Epoch 586/1000 | Train Loss=964.88485718 | Val Loss=1.26879609 | Data=9.62944412 | Physics=1.88414043 | Val RMSE: 2.12077427 | ‚àö(Val Loss) = 1.12640846 | Current Learning Rate: 0.0002\n",
      "Epoch 587/1000 | Train Loss=966.41367594 | Val Loss=1.36371040 | Data=9.64472946 | Physics=1.97575199 | Val RMSE: 2.12076640 | ‚àö(Val Loss) = 1.16778016 | Current Learning Rate: 0.0002\n",
      "Epoch 588/1000 | Train Loss=965.26781209 | Val Loss=1.27668929 | Data=9.63322624 | Physics=2.10922654 | Val RMSE: 2.12077379 | ‚àö(Val Loss) = 1.12990677 | Current Learning Rate: 0.0002\n",
      "Epoch 589/1000 | Train Loss=965.36376953 | Val Loss=1.27041602 | Data=9.63423109 | Physics=1.97251558 | Val RMSE: 2.12077427 | ‚àö(Val Loss) = 1.12712729 | Current Learning Rate: 0.0002\n",
      "Epoch 590/1000 | Train Loss=964.59527588 | Val Loss=1.36796796 | Data=9.62659836 | Physics=1.83422716 | Val RMSE: 2.12077904 | ‚àö(Val Loss) = 1.16960168 | Current Learning Rate: 0.0002\n",
      "Epoch 591/1000 | Train Loss=963.91402181 | Val Loss=1.27950656 | Data=9.61976067 | Physics=1.95380459 | Val RMSE: 2.12078619 | ‚àö(Val Loss) = 1.13115275 | Current Learning Rate: 0.0002\n",
      "Epoch 592/1000 | Train Loss=965.41001383 | Val Loss=1.27070940 | Data=9.63466930 | Physics=1.92451396 | Val RMSE: 2.12077785 | ‚àö(Val Loss) = 1.12725747 | Current Learning Rate: 0.0002\n",
      "Epoch 593/1000 | Train Loss=965.05664062 | Val Loss=1.28094172 | Data=9.63111178 | Physics=1.99872449 | Val RMSE: 2.12098670 | ‚àö(Val Loss) = 1.13178694 | Current Learning Rate: 0.0002\n",
      "Epoch 594/1000 | Train Loss=966.38711548 | Val Loss=1.30557239 | Data=9.64439090 | Physics=2.02007326 | Val RMSE: 2.12103248 | ‚àö(Val Loss) = 1.14261651 | Current Learning Rate: 0.0002\n",
      "Epoch 595/1000 | Train Loss=964.20775350 | Val Loss=1.24931836 | Data=9.62263584 | Physics=1.97358032 | Val RMSE: 2.12091160 | ‚àö(Val Loss) = 1.11772907 | Current Learning Rate: 0.0002\n",
      "Epoch 596/1000 | Train Loss=966.07905070 | Val Loss=1.25445175 | Data=9.64135265 | Physics=2.00504775 | Val RMSE: 2.12085533 | ‚àö(Val Loss) = 1.12002313 | Current Learning Rate: 0.0002\n",
      "Epoch 597/1000 | Train Loss=963.23705037 | Val Loss=1.23589981 | Data=9.61299515 | Physics=1.89711076 | Val RMSE: 2.12086439 | ‚àö(Val Loss) = 1.11171031 | Current Learning Rate: 0.0002\n",
      "Epoch 598/1000 | Train Loss=964.12781779 | Val Loss=1.25190473 | Data=9.62190406 | Physics=1.89309842 | Val RMSE: 2.12084341 | ‚àö(Val Loss) = 1.11888552 | Current Learning Rate: 0.0002\n",
      "Epoch 599/1000 | Train Loss=964.96650187 | Val Loss=1.27973247 | Data=9.63018640 | Physics=1.99280347 | Val RMSE: 2.12089396 | ‚àö(Val Loss) = 1.13125265 | Current Learning Rate: 0.0002\n",
      "Epoch 600/1000 | Train Loss=966.18543498 | Val Loss=1.31232846 | Data=9.64238930 | Physics=2.03266825 | Val RMSE: 2.12090039 | ‚àö(Val Loss) = 1.14556909 | Current Learning Rate: 0.0002\n",
      "Epoch 601/1000 | Train Loss=963.72612508 | Val Loss=1.25200868 | Data=9.61783791 | Physics=1.93662259 | Val RMSE: 2.12084055 | ‚àö(Val Loss) = 1.11893189 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 602/1000 | Train Loss=4392.87377930 | Val Loss=1.25511670 | Data=43.90931384 | Physics=1.93834508 | Val RMSE: 2.12083197 | ‚àö(Val Loss) = 1.12031996 | Current Learning Rate: 0.0002\n",
      "Epoch 603/1000 | Train Loss=4389.91064453 | Val Loss=1.31675541 | Data=43.87968191 | Physics=1.96187003 | Val RMSE: 2.12137222 | ‚àö(Val Loss) = 1.14749968 | Current Learning Rate: 0.0002\n",
      "Epoch 604/1000 | Train Loss=4394.88159180 | Val Loss=1.25647140 | Data=43.92937787 | Physics=1.99740645 | Val RMSE: 2.12152362 | ‚àö(Val Loss) = 1.12092435 | Current Learning Rate: 0.0002\n",
      "Epoch 605/1000 | Train Loss=4390.59383138 | Val Loss=1.27288997 | Data=43.88648351 | Physics=1.94795130 | Val RMSE: 2.12136126 | ‚àö(Val Loss) = 1.12822425 | Current Learning Rate: 0.0002\n",
      "Epoch 606/1000 | Train Loss=4389.50561523 | Val Loss=1.23666024 | Data=43.87567520 | Physics=1.89217350 | Val RMSE: 2.12143826 | ‚àö(Val Loss) = 1.11205232 | Current Learning Rate: 0.0002\n",
      "Epoch 607/1000 | Train Loss=4391.02360026 | Val Loss=1.24840057 | Data=43.89082654 | Physics=1.93878520 | Val RMSE: 2.12133837 | ‚àö(Val Loss) = 1.11731851 | Current Learning Rate: 0.0002\n",
      "Epoch 608/1000 | Train Loss=4393.99544271 | Val Loss=1.34678054 | Data=43.92053668 | Physics=1.98931928 | Val RMSE: 2.12109351 | ‚àö(Val Loss) = 1.16050875 | Current Learning Rate: 0.0002\n",
      "Epoch 609/1000 | Train Loss=4393.57820638 | Val Loss=1.26790452 | Data=43.91638819 | Physics=1.92979938 | Val RMSE: 2.12103987 | ‚àö(Val Loss) = 1.12601268 | Current Learning Rate: 0.0002\n",
      "Epoch 610/1000 | Train Loss=4388.67301432 | Val Loss=1.26381254 | Data=43.86733500 | Physics=1.86013039 | Val RMSE: 2.12104249 | ‚àö(Val Loss) = 1.12419415 | Current Learning Rate: 0.0002\n",
      "Epoch 611/1000 | Train Loss=4395.81005859 | Val Loss=1.35316980 | Data=43.93866984 | Physics=1.97580867 | Val RMSE: 2.12093282 | ‚àö(Val Loss) = 1.16325831 | Current Learning Rate: 0.0002\n",
      "Epoch 612/1000 | Train Loss=4393.68554688 | Val Loss=1.26857567 | Data=43.91743724 | Physics=1.97954880 | Val RMSE: 2.12087321 | ‚àö(Val Loss) = 1.12631059 | Current Learning Rate: 0.0002\n",
      "Epoch 613/1000 | Train Loss=4388.09912109 | Val Loss=1.26379085 | Data=43.86160978 | Physics=1.86346252 | Val RMSE: 2.12093759 | ‚àö(Val Loss) = 1.12418449 | Current Learning Rate: 0.0002\n",
      "Epoch 614/1000 | Train Loss=4393.02880859 | Val Loss=1.33343494 | Data=43.91084290 | Physics=1.99064184 | Val RMSE: 2.12138844 | ‚àö(Val Loss) = 1.15474451 | Current Learning Rate: 0.0002\n",
      "Epoch 615/1000 | Train Loss=4395.70597331 | Val Loss=1.24888134 | Data=43.93763606 | Physics=1.98869498 | Val RMSE: 2.12156248 | ‚àö(Val Loss) = 1.11753356 | Current Learning Rate: 0.0002\n",
      "Epoch 616/1000 | Train Loss=4394.15437826 | Val Loss=1.25201821 | Data=43.92212105 | Physics=1.97244596 | Val RMSE: 2.12134242 | ‚àö(Val Loss) = 1.11893618 | Current Learning Rate: 0.0002\n",
      "Epoch 617/1000 | Train Loss=4386.34781901 | Val Loss=1.26609886 | Data=43.84406789 | Physics=1.91365595 | Val RMSE: 2.12166333 | ‚àö(Val Loss) = 1.12521052 | Current Learning Rate: 0.0002\n",
      "Epoch 618/1000 | Train Loss=4392.96728516 | Val Loss=1.29354858 | Data=43.91025861 | Physics=1.93826123 | Val RMSE: 2.12166739 | ‚àö(Val Loss) = 1.13734281 | Current Learning Rate: 0.0002\n",
      "Epoch 619/1000 | Train Loss=4389.45532227 | Val Loss=1.24378812 | Data=43.87517929 | Physics=1.89721899 | Val RMSE: 2.12146449 | ‚àö(Val Loss) = 1.11525249 | Current Learning Rate: 0.0002\n",
      "Epoch 620/1000 | Train Loss=4395.59635417 | Val Loss=1.25187492 | Data=43.93654378 | Physics=1.99982594 | Val RMSE: 2.12123489 | ‚àö(Val Loss) = 1.11887217 | Current Learning Rate: 0.0002\n",
      "Epoch 621/1000 | Train Loss=4391.86507161 | Val Loss=1.35064435 | Data=43.89922396 | Physics=1.92513328 | Val RMSE: 2.12105250 | ‚àö(Val Loss) = 1.16217220 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 622/1000 | Train Loss=4394.16430664 | Val Loss=1.26943076 | Data=43.92222150 | Physics=1.99380433 | Val RMSE: 2.12113237 | ‚àö(Val Loss) = 1.12669015 | Current Learning Rate: 0.0002\n",
      "Epoch 623/1000 | Train Loss=4388.05802409 | Val Loss=1.26300883 | Data=43.86117236 | Physics=1.92467494 | Val RMSE: 2.12105250 | ‚àö(Val Loss) = 1.12383664 | Current Learning Rate: 0.0002\n",
      "Epoch 624/1000 | Train Loss=4388.15104167 | Val Loss=1.23367190 | Data=43.86213621 | Physics=1.91087069 | Val RMSE: 2.12107015 | ‚àö(Val Loss) = 1.11070788 | Current Learning Rate: 0.0002\n",
      "Epoch 625/1000 | Train Loss=4395.82413737 | Val Loss=1.24617684 | Data=43.93878873 | Physics=2.01427571 | Val RMSE: 2.12108135 | ‚àö(Val Loss) = 1.11632288 | Current Learning Rate: 0.0002\n",
      "Epoch 626/1000 | Train Loss=4395.94913737 | Val Loss=1.33620548 | Data=43.94004059 | Physics=2.02531880 | Val RMSE: 2.12128258 | ‚àö(Val Loss) = 1.15594351 | Current Learning Rate: 0.0002\n",
      "Epoch 627/1000 | Train Loss=4389.49983724 | Val Loss=1.25882459 | Data=43.87563260 | Physics=1.85314704 | Val RMSE: 2.12131047 | ‚àö(Val Loss) = 1.12197351 | Current Learning Rate: 0.0002\n",
      "Epoch 628/1000 | Train Loss=4387.87296549 | Val Loss=1.25646651 | Data=43.85935148 | Physics=1.89145731 | Val RMSE: 2.12117887 | ‚àö(Val Loss) = 1.12092221 | Current Learning Rate: 0.0002\n",
      "Epoch 629/1000 | Train Loss=4392.80330404 | Val Loss=1.34718442 | Data=43.90860875 | Physics=1.95798975 | Val RMSE: 2.12108707 | ‚àö(Val Loss) = 1.16068280 | Current Learning Rate: 0.0002\n",
      "Epoch 630/1000 | Train Loss=4398.51261393 | Val Loss=1.26642811 | Data=43.96569888 | Physics=1.99023970 | Val RMSE: 2.12103844 | ‚àö(Val Loss) = 1.12535691 | Current Learning Rate: 0.0002\n",
      "Epoch 631/1000 | Train Loss=4385.10839844 | Val Loss=1.26101005 | Data=43.83172925 | Physics=1.83261970 | Val RMSE: 2.12104273 | ‚àö(Val Loss) = 1.12294710 | Current Learning Rate: 0.0002\n",
      "Epoch 632/1000 | Train Loss=4391.39925130 | Val Loss=1.26342440 | Data=43.89458974 | Physics=1.92919194 | Val RMSE: 2.12143373 | ‚àö(Val Loss) = 1.12402153 | Current Learning Rate: 0.0002\n",
      "Epoch 633/1000 | Train Loss=4391.71093750 | Val Loss=1.28593493 | Data=43.89764977 | Physics=2.01680885 | Val RMSE: 2.12156844 | ‚àö(Val Loss) = 1.13399076 | Current Learning Rate: 0.0002\n",
      "Epoch 634/1000 | Train Loss=4389.03108724 | Val Loss=1.24442327 | Data=43.87090747 | Physics=1.94185507 | Val RMSE: 2.12153792 | ‚àö(Val Loss) = 1.11553717 | Current Learning Rate: 0.0002\n",
      "Epoch 635/1000 | Train Loss=4388.84781901 | Val Loss=1.25254512 | Data=43.86909612 | Physics=1.90872965 | Val RMSE: 2.12142110 | ‚àö(Val Loss) = 1.11917162 | Current Learning Rate: 0.0002\n",
      "Epoch 636/1000 | Train Loss=4388.62768555 | Val Loss=1.30112731 | Data=43.86684926 | Physics=1.92321583 | Val RMSE: 2.12167573 | ‚àö(Val Loss) = 1.14066970 | Current Learning Rate: 0.0002\n",
      "Epoch 637/1000 | Train Loss=4397.79964193 | Val Loss=1.33954906 | Data=43.95853424 | Physics=2.02771572 | Val RMSE: 2.12170863 | ‚àö(Val Loss) = 1.15738893 | Current Learning Rate: 0.0002\n",
      "Epoch 638/1000 | Train Loss=4393.54793294 | Val Loss=1.26210082 | Data=43.91606394 | Physics=1.95043694 | Val RMSE: 2.12140107 | ‚àö(Val Loss) = 1.12343264 | Current Learning Rate: 0.0002\n",
      "Epoch 639/1000 | Train Loss=4392.41918945 | Val Loss=1.25796020 | Data=43.90482839 | Physics=1.91139622 | Val RMSE: 2.12119508 | ‚àö(Val Loss) = 1.12158823 | Current Learning Rate: 0.0002\n",
      "Epoch 640/1000 | Train Loss=4394.63989258 | Val Loss=1.27954245 | Data=43.92698987 | Physics=1.94458830 | Val RMSE: 2.12150645 | ‚àö(Val Loss) = 1.13116860 | Current Learning Rate: 0.0002\n",
      "Epoch 641/1000 | Train Loss=4384.80411784 | Val Loss=1.30736756 | Data=43.82865461 | Physics=1.84071269 | Val RMSE: 2.12162566 | ‚àö(Val Loss) = 1.14340174 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 642/1000 | Train Loss=4395.51749674 | Val Loss=1.24834108 | Data=43.93574969 | Physics=2.00648026 | Val RMSE: 2.12138295 | ‚àö(Val Loss) = 1.11729181 | Current Learning Rate: 0.0002\n",
      "Epoch 643/1000 | Train Loss=4394.07047526 | Val Loss=1.25031412 | Data=43.92130725 | Physics=1.90448238 | Val RMSE: 2.12125278 | ‚àö(Val Loss) = 1.11817443 | Current Learning Rate: 0.0002\n",
      "Epoch 644/1000 | Train Loss=4390.66389974 | Val Loss=1.23715627 | Data=43.88724836 | Physics=1.94607908 | Val RMSE: 2.12149954 | ‚àö(Val Loss) = 1.11227524 | Current Learning Rate: 0.0002\n",
      "Epoch 645/1000 | Train Loss=4397.39420573 | Val Loss=1.25379241 | Data=43.95452118 | Physics=1.97852587 | Val RMSE: 2.12146592 | ‚àö(Val Loss) = 1.11972868 | Current Learning Rate: 0.0002\n",
      "Epoch 646/1000 | Train Loss=4396.38256836 | Val Loss=1.34338534 | Data=43.94443130 | Physics=1.96956469 | Val RMSE: 2.12137294 | ‚àö(Val Loss) = 1.15904498 | Current Learning Rate: 0.0002\n",
      "Epoch 647/1000 | Train Loss=4394.03019206 | Val Loss=1.26543379 | Data=43.92091433 | Physics=1.92623831 | Val RMSE: 2.12124205 | ‚àö(Val Loss) = 1.12491500 | Current Learning Rate: 0.0002\n",
      "Epoch 648/1000 | Train Loss=4395.85563151 | Val Loss=1.26068389 | Data=43.93914795 | Physics=1.91170204 | Val RMSE: 2.12119174 | ‚àö(Val Loss) = 1.12280178 | Current Learning Rate: 0.0002\n",
      "Epoch 649/1000 | Train Loss=4393.70418294 | Val Loss=1.34153199 | Data=43.91761271 | Physics=1.99633050 | Val RMSE: 2.12146497 | ‚àö(Val Loss) = 1.15824521 | Current Learning Rate: 0.0002\n",
      "Epoch 650/1000 | Train Loss=4394.16967773 | Val Loss=1.26057506 | Data=43.92231814 | Physics=1.93416457 | Val RMSE: 2.12141776 | ‚àö(Val Loss) = 1.12275338 | Current Learning Rate: 0.0002\n",
      "Epoch 651/1000 | Train Loss=4394.72371419 | Val Loss=1.25831497 | Data=43.92778460 | Physics=1.99293582 | Val RMSE: 2.12117195 | ‚àö(Val Loss) = 1.12174642 | Current Learning Rate: 0.0002\n",
      "Epoch 652/1000 | Train Loss=4393.01416016 | Val Loss=1.34778881 | Data=43.91072655 | Physics=1.99561483 | Val RMSE: 2.12118435 | ‚àö(Val Loss) = 1.16094303 | Current Learning Rate: 0.0002\n",
      "Epoch 653/1000 | Train Loss=4388.22428385 | Val Loss=1.26689827 | Data=43.86286926 | Physics=1.89455920 | Val RMSE: 2.12112784 | ‚àö(Val Loss) = 1.12556577 | Current Learning Rate: 0.0002\n",
      "Epoch 654/1000 | Train Loss=4390.06323242 | Val Loss=1.26071894 | Data=43.88124720 | Physics=1.86575845 | Val RMSE: 2.12107563 | ‚àö(Val Loss) = 1.12281740 | Current Learning Rate: 0.0002\n",
      "Epoch 655/1000 | Train Loss=4395.98323568 | Val Loss=1.27103400 | Data=43.94037946 | Physics=2.03264920 | Val RMSE: 2.12158775 | ‚àö(Val Loss) = 1.12740147 | Current Learning Rate: 0.0002\n",
      "Epoch 656/1000 | Train Loss=4390.03279622 | Val Loss=1.29335523 | Data=43.88091977 | Physics=1.96330142 | Val RMSE: 2.12171435 | ‚àö(Val Loss) = 1.13725781 | Current Learning Rate: 0.0002\n",
      "Epoch 657/1000 | Train Loss=4389.14550781 | Val Loss=1.24237311 | Data=43.87204742 | Physics=1.96570664 | Val RMSE: 2.12148809 | ‚àö(Val Loss) = 1.11461794 | Current Learning Rate: 0.0002\n",
      "Epoch 658/1000 | Train Loss=4386.80045573 | Val Loss=1.24643636 | Data=43.84860102 | Physics=1.87224434 | Val RMSE: 2.12135506 | ‚àö(Val Loss) = 1.11643910 | Current Learning Rate: 0.0002\n",
      "Epoch 659/1000 | Train Loss=4391.39485677 | Val Loss=1.33721447 | Data=43.89452680 | Physics=1.98707870 | Val RMSE: 2.12124848 | ‚àö(Val Loss) = 1.15637994 | Current Learning Rate: 0.0002\n",
      "Epoch 660/1000 | Train Loss=4385.65551758 | Val Loss=1.26249409 | Data=43.83714867 | Physics=1.95942784 | Val RMSE: 2.12118554 | ‚àö(Val Loss) = 1.12360764 | Current Learning Rate: 0.0002\n",
      "Epoch 661/1000 | Train Loss=4391.86702474 | Val Loss=1.25756681 | Data=43.89927991 | Physics=1.90948749 | Val RMSE: 2.12112164 | ‚àö(Val Loss) = 1.12141287 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 662/1000 | Train Loss=4389.89908854 | Val Loss=1.34544694 | Data=43.87957382 | Physics=1.95757415 | Val RMSE: 2.12105203 | ‚àö(Val Loss) = 1.15993404 | Current Learning Rate: 0.0002\n",
      "Epoch 663/1000 | Train Loss=4389.91691081 | Val Loss=1.26467359 | Data=43.87975311 | Physics=1.97889134 | Val RMSE: 2.12101603 | ‚àö(Val Loss) = 1.12457705 | Current Learning Rate: 0.0002\n",
      "Epoch 664/1000 | Train Loss=4392.13598633 | Val Loss=1.25825441 | Data=43.90193621 | Physics=1.94476342 | Val RMSE: 2.12102890 | ‚àö(Val Loss) = 1.12171936 | Current Learning Rate: 0.0002\n",
      "Epoch 665/1000 | Train Loss=4389.76888021 | Val Loss=1.25220549 | Data=43.87832006 | Physics=1.91268890 | Val RMSE: 2.12158775 | ‚àö(Val Loss) = 1.11901987 | Current Learning Rate: 0.0002\n",
      "Epoch 666/1000 | Train Loss=4393.19197591 | Val Loss=1.27083361 | Data=43.91249720 | Physics=1.97952780 | Val RMSE: 2.12161875 | ‚àö(Val Loss) = 1.12731254 | Current Learning Rate: 0.0002\n",
      "Epoch 667/1000 | Train Loss=4395.95694987 | Val Loss=1.23980331 | Data=43.94015312 | Physics=2.00297980 | Val RMSE: 2.12145138 | ‚àö(Val Loss) = 1.11346459 | Current Learning Rate: 0.0002\n",
      "Epoch 668/1000 | Train Loss=4386.37158203 | Val Loss=1.25034440 | Data=43.84430250 | Physics=1.93288467 | Val RMSE: 2.12142396 | ‚àö(Val Loss) = 1.11818802 | Current Learning Rate: 0.0002\n",
      "Epoch 669/1000 | Train Loss=4390.78133138 | Val Loss=1.33450139 | Data=43.88839086 | Physics=1.92201880 | Val RMSE: 2.12146282 | ‚àö(Val Loss) = 1.15520620 | Current Learning Rate: 0.0002\n",
      "Epoch 670/1000 | Train Loss=4390.02303060 | Val Loss=1.25809598 | Data=43.88085810 | Physics=1.91147882 | Val RMSE: 2.12138319 | ‚àö(Val Loss) = 1.12164879 | Current Learning Rate: 0.0002\n",
      "Epoch 671/1000 | Train Loss=4395.10611979 | Val Loss=1.25463068 | Data=43.93163935 | Physics=1.94829403 | Val RMSE: 2.12119126 | ‚àö(Val Loss) = 1.12010300 | Current Learning Rate: 0.0002\n",
      "Epoch 672/1000 | Train Loss=4383.78165690 | Val Loss=1.33983696 | Data=43.81846746 | Physics=1.82150012 | Val RMSE: 2.12117195 | ‚àö(Val Loss) = 1.15751326 | Current Learning Rate: 0.0002\n",
      "Epoch 673/1000 | Train Loss=4386.21004232 | Val Loss=1.26098239 | Data=43.84273847 | Physics=1.84043154 | Val RMSE: 2.12109208 | ‚àö(Val Loss) = 1.12293470 | Current Learning Rate: 0.0002\n",
      "Epoch 674/1000 | Train Loss=4392.91438802 | Val Loss=1.25624716 | Data=43.90970548 | Physics=1.97463197 | Val RMSE: 2.12104559 | ‚àö(Val Loss) = 1.12082434 | Current Learning Rate: 0.0002\n",
      "Epoch 675/1000 | Train Loss=4395.29728190 | Val Loss=1.23635471 | Data=43.93355624 | Physics=1.94597111 | Val RMSE: 2.12132907 | ‚àö(Val Loss) = 1.11191487 | Current Learning Rate: 0.0002\n",
      "Epoch 676/1000 | Train Loss=4389.67708333 | Val Loss=1.25044894 | Data=43.87736829 | Physics=1.92457517 | Val RMSE: 2.12141252 | ‚àö(Val Loss) = 1.11823475 | Current Learning Rate: 0.0002\n",
      "Epoch 677/1000 | Train Loss=4387.33292643 | Val Loss=1.26526880 | Data=43.85392443 | Physics=1.94716982 | Val RMSE: 2.12183976 | ‚àö(Val Loss) = 1.12484169 | Current Learning Rate: 0.0002\n",
      "Epoch 678/1000 | Train Loss=4384.41625977 | Val Loss=1.29146969 | Data=43.82477315 | Physics=1.90205089 | Val RMSE: 2.12193084 | ‚àö(Val Loss) = 1.13642848 | Current Learning Rate: 0.0002\n",
      "Epoch 679/1000 | Train Loss=4385.87548828 | Val Loss=1.35034060 | Data=43.83940379 | Physics=1.86934925 | Val RMSE: 2.12161231 | ‚àö(Val Loss) = 1.16204154 | Current Learning Rate: 0.0002\n",
      "Epoch 680/1000 | Train Loss=4394.45963542 | Val Loss=1.26112092 | Data=43.92517598 | Physics=2.01079017 | Val RMSE: 2.12132978 | ‚àö(Val Loss) = 1.12299645 | Current Learning Rate: 0.0002\n",
      "Epoch 681/1000 | Train Loss=4388.33056641 | Val Loss=1.25677967 | Data=43.86394564 | Physics=1.85447907 | Val RMSE: 2.12111282 | ‚àö(Val Loss) = 1.12106180 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 682/1000 | Train Loss=4395.89778646 | Val Loss=1.34534502 | Data=43.93957265 | Physics=1.93843789 | Val RMSE: 2.12098074 | ‚àö(Val Loss) = 1.15989006 | Current Learning Rate: 0.0002\n",
      "Epoch 683/1000 | Train Loss=4394.63924154 | Val Loss=1.26537478 | Data=43.92698034 | Physics=1.93306954 | Val RMSE: 2.12093973 | ‚àö(Val Loss) = 1.12488878 | Current Learning Rate: 0.0002\n",
      "Epoch 684/1000 | Train Loss=4391.59155273 | Val Loss=1.25840056 | Data=43.89651044 | Physics=1.92087847 | Val RMSE: 2.12103653 | ‚àö(Val Loss) = 1.12178457 | Current Learning Rate: 0.0002\n",
      "Epoch 685/1000 | Train Loss=4394.01505534 | Val Loss=1.34251070 | Data=43.92074839 | Physics=1.95262831 | Val RMSE: 2.12098694 | ‚àö(Val Loss) = 1.15866768 | Current Learning Rate: 0.0002\n",
      "Epoch 686/1000 | Train Loss=4388.80541992 | Val Loss=1.26306343 | Data=43.86868159 | Physics=1.86553561 | Val RMSE: 2.12095118 | ‚àö(Val Loss) = 1.12386096 | Current Learning Rate: 0.0002\n",
      "Epoch 687/1000 | Train Loss=4389.12841797 | Val Loss=1.25754142 | Data=43.87191200 | Physics=1.86048454 | Val RMSE: 2.12101531 | ‚àö(Val Loss) = 1.12140155 | Current Learning Rate: 0.0002\n",
      "Epoch 688/1000 | Train Loss=4398.01733398 | Val Loss=1.32602310 | Data=43.96069590 | Physics=2.03406166 | Val RMSE: 2.12141824 | ‚àö(Val Loss) = 1.15153074 | Current Learning Rate: 0.0002\n",
      "Epoch 689/1000 | Train Loss=4392.67773438 | Val Loss=1.24657083 | Data=43.90734800 | Physics=1.97539697 | Val RMSE: 2.12168884 | ‚àö(Val Loss) = 1.11649930 | Current Learning Rate: 0.0002\n",
      "Epoch 690/1000 | Train Loss=4391.61027018 | Val Loss=1.24716759 | Data=43.89670436 | Physics=1.93872881 | Val RMSE: 2.12153053 | ‚àö(Val Loss) = 1.11676657 | Current Learning Rate: 0.0002\n",
      "Epoch 691/1000 | Train Loss=4400.32348633 | Val Loss=1.33556461 | Data=43.98378054 | Physics=2.04594567 | Val RMSE: 2.12143946 | ‚àö(Val Loss) = 1.15566635 | Current Learning Rate: 0.0002\n",
      "Epoch 692/1000 | Train Loss=4396.27783203 | Val Loss=1.25982559 | Data=43.94332123 | Physics=2.08363763 | Val RMSE: 2.12135577 | ‚àö(Val Loss) = 1.12241948 | Current Learning Rate: 0.0002\n",
      "Epoch 693/1000 | Train Loss=4386.94010417 | Val Loss=1.25451803 | Data=43.85001119 | Physics=1.90543326 | Val RMSE: 2.12123275 | ‚àö(Val Loss) = 1.12005270 | Current Learning Rate: 0.0002\n",
      "Epoch 694/1000 | Train Loss=4394.05810547 | Val Loss=1.33877146 | Data=43.92110825 | Physics=2.02092619 | Val RMSE: 2.12118649 | ‚àö(Val Loss) = 1.15705287 | Current Learning Rate: 0.0002\n",
      "Epoch 695/1000 | Train Loss=4392.52604167 | Val Loss=1.26092672 | Data=43.90582720 | Physics=1.99837586 | Val RMSE: 2.12118864 | ‚àö(Val Loss) = 1.12290990 | Current Learning Rate: 0.0002\n",
      "Epoch 696/1000 | Train Loss=4385.72338867 | Val Loss=1.25585866 | Data=43.83783277 | Physics=1.91850916 | Val RMSE: 2.12112498 | ‚àö(Val Loss) = 1.12065101 | Current Learning Rate: 0.0002\n",
      "Epoch 697/1000 | Train Loss=4392.13517253 | Val Loss=1.27723086 | Data=43.90191650 | Physics=1.94060433 | Val RMSE: 2.12154698 | ‚àö(Val Loss) = 1.13014638 | Current Learning Rate: 0.0002\n",
      "Epoch 698/1000 | Train Loss=4392.26717122 | Val Loss=1.30267084 | Data=43.90321477 | Physics=2.01591641 | Val RMSE: 2.12182164 | ‚àö(Val Loss) = 1.14134610 | Current Learning Rate: 0.0002\n",
      "Epoch 699/1000 | Train Loss=4391.55590820 | Val Loss=1.24389458 | Data=43.89612834 | Physics=1.99100245 | Val RMSE: 2.12159753 | ‚àö(Val Loss) = 1.11530018 | Current Learning Rate: 0.0002\n",
      "Epoch 700/1000 | Train Loss=4389.50496419 | Val Loss=1.24580240 | Data=43.87567012 | Physics=1.83691235 | Val RMSE: 2.12143636 | ‚àö(Val Loss) = 1.11615515 | Current Learning Rate: 0.0002\n",
      "Epoch 701/1000 | Train Loss=4388.45987956 | Val Loss=1.32396114 | Data=43.86519686 | Physics=1.93240162 | Val RMSE: 2.12170553 | ‚àö(Val Loss) = 1.15063512 | Current Learning Rate: 0.0002\n",
      "‚úÖ Learning Rate updated to 0.001\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 702/1000 | Train Loss=4388.65681966 | Val Loss=1.11735594 | Data=43.86713219 | Physics=1.88769823 | Val RMSE: 2.12562418 | ‚àö(Val Loss) = 1.05705059 | Current Learning Rate: 0.001\n",
      "Epoch 703/1000 | Train Loss=4401.31575521 | Val Loss=1.32755411 | Data=43.99365616 | Physics=1.89362931 | Val RMSE: 2.12469625 | ‚àö(Val Loss) = 1.15219533 | Current Learning Rate: 0.001\n",
      "Epoch 704/1000 | Train Loss=4402.17317708 | Val Loss=1.20825851 | Data=44.00219981 | Physics=1.99511296 | Val RMSE: 2.12362528 | ‚àö(Val Loss) = 1.09920812 | Current Learning Rate: 0.001\n",
      "Epoch 705/1000 | Train Loss=4398.46663411 | Val Loss=1.16169369 | Data=43.96521505 | Physics=1.90829641 | Val RMSE: 2.12270260 | ‚àö(Val Loss) = 1.07781899 | Current Learning Rate: 0.001\n",
      "Epoch 706/1000 | Train Loss=4392.30598958 | Val Loss=1.14415956 | Data=43.90363185 | Physics=1.93366059 | Val RMSE: 2.12239766 | ‚àö(Val Loss) = 1.06965399 | Current Learning Rate: 0.001\n",
      "Epoch 707/1000 | Train Loss=4388.86010742 | Val Loss=1.13556433 | Data=43.86921755 | Physics=1.84025956 | Val RMSE: 2.12215734 | ‚àö(Val Loss) = 1.06562865 | Current Learning Rate: 0.001\n",
      "Epoch 708/1000 | Train Loss=4396.17447917 | Val Loss=1.13242733 | Data=43.94227028 | Physics=2.02148610 | Val RMSE: 2.12215018 | ‚àö(Val Loss) = 1.06415570 | Current Learning Rate: 0.001\n",
      "Epoch 709/1000 | Train Loss=4395.82462565 | Val Loss=1.12955809 | Data=43.93883451 | Physics=1.96068586 | Val RMSE: 2.12226987 | ‚àö(Val Loss) = 1.06280673 | Current Learning Rate: 0.001\n",
      "Epoch 710/1000 | Train Loss=4394.32763672 | Val Loss=1.12859523 | Data=43.92386119 | Physics=1.92560953 | Val RMSE: 2.12199593 | ‚àö(Val Loss) = 1.06235361 | Current Learning Rate: 0.001\n",
      "Epoch 711/1000 | Train Loss=4398.15861003 | Val Loss=1.12826216 | Data=43.96212133 | Physics=1.93560722 | Val RMSE: 2.12196159 | ‚àö(Val Loss) = 1.06219685 | Current Learning Rate: 0.001\n",
      "Epoch 712/1000 | Train Loss=4390.36083984 | Val Loss=1.12895608 | Data=43.88416227 | Physics=1.90078700 | Val RMSE: 2.12243295 | ‚àö(Val Loss) = 1.06252348 | Current Learning Rate: 0.001\n",
      "Epoch 713/1000 | Train Loss=4395.96736654 | Val Loss=1.12815213 | Data=43.94022814 | Physics=1.89973450 | Val RMSE: 2.12213492 | ‚àö(Val Loss) = 1.06214511 | Current Learning Rate: 0.001\n",
      "Epoch 714/1000 | Train Loss=4392.78849284 | Val Loss=1.13094008 | Data=43.90847842 | Physics=1.89204835 | Val RMSE: 2.12220979 | ‚àö(Val Loss) = 1.06345665 | Current Learning Rate: 0.001\n",
      "Epoch 715/1000 | Train Loss=4398.34147135 | Val Loss=1.12812150 | Data=43.96396510 | Physics=2.01033598 | Val RMSE: 2.12223530 | ‚àö(Val Loss) = 1.06213069 | Current Learning Rate: 0.001\n",
      "Epoch 716/1000 | Train Loss=4395.38370768 | Val Loss=1.12755418 | Data=43.93437322 | Physics=2.04909777 | Val RMSE: 2.12237787 | ‚àö(Val Loss) = 1.06186354 | Current Learning Rate: 0.001\n",
      "Epoch 717/1000 | Train Loss=4389.39184570 | Val Loss=1.12721920 | Data=43.87448756 | Physics=1.88023727 | Val RMSE: 2.12223721 | ‚àö(Val Loss) = 1.06170583 | Current Learning Rate: 0.001\n",
      "Epoch 718/1000 | Train Loss=4391.16398112 | Val Loss=1.12877727 | Data=43.89219030 | Physics=1.93179130 | Val RMSE: 2.12208819 | ‚àö(Val Loss) = 1.06243932 | Current Learning Rate: 0.001\n",
      "Epoch 719/1000 | Train Loss=4391.23974609 | Val Loss=1.12967658 | Data=43.89292971 | Physics=1.89897117 | Val RMSE: 2.12179804 | ‚àö(Val Loss) = 1.06286240 | Current Learning Rate: 0.001\n",
      "Epoch 720/1000 | Train Loss=4394.96704102 | Val Loss=1.13442111 | Data=43.93026924 | Physics=1.84334405 | Val RMSE: 2.12227750 | ‚àö(Val Loss) = 1.06509209 | Current Learning Rate: 0.001\n",
      "Epoch 721/1000 | Train Loss=4391.60245768 | Val Loss=1.12887025 | Data=43.89662425 | Physics=1.85170044 | Val RMSE: 2.12205815 | ‚àö(Val Loss) = 1.06248307 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 722/1000 | Train Loss=4394.93269857 | Val Loss=1.12939227 | Data=43.92984962 | Physics=1.96174615 | Val RMSE: 2.12213278 | ‚àö(Val Loss) = 1.06272864 | Current Learning Rate: 0.001\n",
      "Epoch 723/1000 | Train Loss=4393.91414388 | Val Loss=1.12829328 | Data=43.91969872 | Physics=1.88819307 | Val RMSE: 2.12251735 | ‚àö(Val Loss) = 1.06221151 | Current Learning Rate: 0.001\n",
      "Epoch 724/1000 | Train Loss=4387.05460612 | Val Loss=1.12904072 | Data=43.85109584 | Physics=1.88019107 | Val RMSE: 2.12277937 | ‚àö(Val Loss) = 1.06256330 | Current Learning Rate: 0.001\n",
      "Epoch 725/1000 | Train Loss=4388.86360677 | Val Loss=1.12681890 | Data=43.86919212 | Physics=1.92098928 | Val RMSE: 2.12251401 | ‚àö(Val Loss) = 1.06151724 | Current Learning Rate: 0.001\n",
      "Epoch 726/1000 | Train Loss=4394.28426107 | Val Loss=1.12824941 | Data=43.92338181 | Physics=1.94199894 | Val RMSE: 2.12243581 | ‚àö(Val Loss) = 1.06219089 | Current Learning Rate: 0.001\n",
      "Epoch 727/1000 | Train Loss=4395.82153320 | Val Loss=1.12616992 | Data=43.93875949 | Physics=1.94767498 | Val RMSE: 2.12241197 | ‚àö(Val Loss) = 1.06121159 | Current Learning Rate: 0.001\n",
      "Epoch 728/1000 | Train Loss=4395.54931641 | Val Loss=1.12869084 | Data=43.93603770 | Physics=1.93836745 | Val RMSE: 2.12228823 | ‚àö(Val Loss) = 1.06239867 | Current Learning Rate: 0.001\n",
      "Epoch 729/1000 | Train Loss=4391.99763997 | Val Loss=1.12839842 | Data=43.90056801 | Physics=1.87238548 | Val RMSE: 2.12249470 | ‚àö(Val Loss) = 1.06226099 | Current Learning Rate: 0.001\n",
      "Epoch 730/1000 | Train Loss=4391.19767253 | Val Loss=1.12653232 | Data=43.89253743 | Physics=1.90796895 | Val RMSE: 2.12222290 | ‚àö(Val Loss) = 1.06138229 | Current Learning Rate: 0.001\n",
      "Epoch 731/1000 | Train Loss=4395.18066406 | Val Loss=1.12841940 | Data=43.93235397 | Physics=1.93312662 | Val RMSE: 2.12226701 | ‚àö(Val Loss) = 1.06227088 | Current Learning Rate: 0.001\n",
      "Epoch 732/1000 | Train Loss=4395.57185872 | Val Loss=1.12857664 | Data=43.93624496 | Physics=1.98169232 | Val RMSE: 2.12255526 | ‚àö(Val Loss) = 1.06234491 | Current Learning Rate: 0.001\n",
      "Epoch 733/1000 | Train Loss=4386.67871094 | Val Loss=1.12793899 | Data=43.84735044 | Physics=1.88108892 | Val RMSE: 2.12280631 | ‚àö(Val Loss) = 1.06204474 | Current Learning Rate: 0.001\n",
      "Epoch 734/1000 | Train Loss=4394.24723307 | Val Loss=1.12795043 | Data=43.92299080 | Physics=2.05312083 | Val RMSE: 2.12250447 | ‚àö(Val Loss) = 1.06205010 | Current Learning Rate: 0.001\n",
      "Epoch 735/1000 | Train Loss=4389.98356120 | Val Loss=1.12815058 | Data=43.88037046 | Physics=1.95031737 | Val RMSE: 2.12215781 | ‚àö(Val Loss) = 1.06214428 | Current Learning Rate: 0.001\n",
      "Epoch 736/1000 | Train Loss=4393.57226562 | Val Loss=1.13179016 | Data=43.91631635 | Physics=1.87330579 | Val RMSE: 2.12181830 | ‚àö(Val Loss) = 1.06385624 | Current Learning Rate: 0.001\n",
      "Epoch 737/1000 | Train Loss=4392.16088867 | Val Loss=1.12877905 | Data=43.90216192 | Physics=1.93061568 | Val RMSE: 2.12187409 | ‚àö(Val Loss) = 1.06244016 | Current Learning Rate: 0.001\n",
      "Epoch 738/1000 | Train Loss=4390.16845703 | Val Loss=1.12966394 | Data=43.88224157 | Physics=1.92315183 | Val RMSE: 2.12217951 | ‚àö(Val Loss) = 1.06285655 | Current Learning Rate: 0.001\n",
      "Epoch 739/1000 | Train Loss=4395.73331706 | Val Loss=1.12855589 | Data=43.93785985 | Physics=1.95280567 | Val RMSE: 2.12213612 | ‚àö(Val Loss) = 1.06233513 | Current Learning Rate: 0.001\n",
      "Epoch 740/1000 | Train Loss=4396.54622396 | Val Loss=1.13070977 | Data=43.94605192 | Physics=1.89406365 | Val RMSE: 2.12216258 | ‚àö(Val Loss) = 1.06334841 | Current Learning Rate: 0.001\n",
      "Epoch 741/1000 | Train Loss=4395.02416992 | Val Loss=1.12967372 | Data=43.93082364 | Physics=1.90762375 | Val RMSE: 2.12241745 | ‚àö(Val Loss) = 1.06286108 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 742/1000 | Train Loss=4396.92895508 | Val Loss=1.12803268 | Data=43.94988759 | Physics=1.95446166 | Val RMSE: 2.12232041 | ‚àö(Val Loss) = 1.06208885 | Current Learning Rate: 0.001\n",
      "Epoch 743/1000 | Train Loss=4390.83186849 | Val Loss=1.12796593 | Data=43.88887914 | Physics=1.92216292 | Val RMSE: 2.12221265 | ‚àö(Val Loss) = 1.06205738 | Current Learning Rate: 0.001\n",
      "Epoch 744/1000 | Train Loss=4391.93994141 | Val Loss=1.12823153 | Data=43.89998372 | Physics=1.86495024 | Val RMSE: 2.12158060 | ‚àö(Val Loss) = 1.06218243 | Current Learning Rate: 0.001\n",
      "Epoch 745/1000 | Train Loss=4395.29996745 | Val Loss=1.13280046 | Data=43.93358294 | Physics=1.93305386 | Val RMSE: 2.12212586 | ‚àö(Val Loss) = 1.06433094 | Current Learning Rate: 0.001\n",
      "Epoch 746/1000 | Train Loss=4395.93684896 | Val Loss=1.12941313 | Data=43.93990835 | Physics=1.97477863 | Val RMSE: 2.12198782 | ‚àö(Val Loss) = 1.06273854 | Current Learning Rate: 0.001\n",
      "Epoch 747/1000 | Train Loss=4390.72119141 | Val Loss=1.13078451 | Data=43.88777033 | Physics=1.93819888 | Val RMSE: 2.12231851 | ‚àö(Val Loss) = 1.06338358 | Current Learning Rate: 0.001\n",
      "Epoch 748/1000 | Train Loss=4393.13444010 | Val Loss=1.12855911 | Data=43.91190211 | Physics=1.97009125 | Val RMSE: 2.12220693 | ‚àö(Val Loss) = 1.06233668 | Current Learning Rate: 0.001\n",
      "Epoch 749/1000 | Train Loss=4400.19197591 | Val Loss=1.12984967 | Data=43.98243777 | Physics=2.07086033 | Val RMSE: 2.12219834 | ‚àö(Val Loss) = 1.06294382 | Current Learning Rate: 0.001\n",
      "Epoch 750/1000 | Train Loss=4396.36897786 | Val Loss=1.12873828 | Data=43.94424184 | Physics=1.91785555 | Val RMSE: 2.12267590 | ‚àö(Val Loss) = 1.06242096 | Current Learning Rate: 0.001\n",
      "Epoch 751/1000 | Train Loss=4397.27604167 | Val Loss=1.12887537 | Data=43.95330429 | Physics=1.95119560 | Val RMSE: 2.12264705 | ‚àö(Val Loss) = 1.06248546 | Current Learning Rate: 0.001\n",
      "Epoch 752/1000 | Train Loss=4397.69165039 | Val Loss=1.12931359 | Data=43.95742798 | Physics=1.99000386 | Val RMSE: 2.12265205 | ‚àö(Val Loss) = 1.06269169 | Current Learning Rate: 0.001\n",
      "Epoch 753/1000 | Train Loss=4394.56429036 | Val Loss=1.12997580 | Data=43.92621867 | Physics=1.88866447 | Val RMSE: 2.12293887 | ‚àö(Val Loss) = 1.06300318 | Current Learning Rate: 0.001\n",
      "Epoch 754/1000 | Train Loss=4394.44596354 | Val Loss=1.12818420 | Data=43.92503421 | Physics=1.87406599 | Val RMSE: 2.12267137 | ‚àö(Val Loss) = 1.06216013 | Current Learning Rate: 0.001\n",
      "Epoch 755/1000 | Train Loss=4397.74934896 | Val Loss=1.12915373 | Data=43.95805486 | Physics=2.02766613 | Val RMSE: 2.12246156 | ‚àö(Val Loss) = 1.06261647 | Current Learning Rate: 0.001\n",
      "Epoch 756/1000 | Train Loss=4391.05354818 | Val Loss=1.12825024 | Data=43.89108531 | Physics=1.93483127 | Val RMSE: 2.12239480 | ‚àö(Val Loss) = 1.06219125 | Current Learning Rate: 0.001\n",
      "Epoch 757/1000 | Train Loss=4396.20524089 | Val Loss=1.12939727 | Data=43.94261614 | Physics=1.93715413 | Val RMSE: 2.12235856 | ‚àö(Val Loss) = 1.06273103 | Current Learning Rate: 0.001\n",
      "Epoch 758/1000 | Train Loss=4390.62141927 | Val Loss=1.12845540 | Data=43.88675690 | Physics=1.95024954 | Val RMSE: 2.12228894 | ‚àö(Val Loss) = 1.06228781 | Current Learning Rate: 0.001\n",
      "Epoch 759/1000 | Train Loss=4396.57552083 | Val Loss=1.12907100 | Data=43.94628843 | Physics=2.03388391 | Val RMSE: 2.12243414 | ‚àö(Val Loss) = 1.06257749 | Current Learning Rate: 0.001\n",
      "Epoch 760/1000 | Train Loss=4389.37402344 | Val Loss=1.13082886 | Data=43.87431272 | Physics=1.87678540 | Val RMSE: 2.12202835 | ‚àö(Val Loss) = 1.06340432 | Current Learning Rate: 0.001\n",
      "Epoch 761/1000 | Train Loss=4395.52441406 | Val Loss=1.13454878 | Data=43.93576686 | Physics=2.01819444 | Val RMSE: 2.12196231 | ‚àö(Val Loss) = 1.06515205 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 762/1000 | Train Loss=4397.93929036 | Val Loss=1.12984324 | Data=43.95991516 | Physics=2.01043036 | Val RMSE: 2.12221670 | ‚àö(Val Loss) = 1.06294084 | Current Learning Rate: 0.001\n",
      "Epoch 763/1000 | Train Loss=4403.01725260 | Val Loss=1.13056600 | Data=44.01071739 | Physics=2.04407448 | Val RMSE: 2.12247968 | ‚àö(Val Loss) = 1.06328082 | Current Learning Rate: 0.001\n",
      "Epoch 764/1000 | Train Loss=4391.43611654 | Val Loss=1.13008046 | Data=43.89493052 | Physics=1.89782824 | Val RMSE: 2.12262630 | ‚àö(Val Loss) = 1.06305242 | Current Learning Rate: 0.001\n",
      "Epoch 765/1000 | Train Loss=4393.87508138 | Val Loss=1.12830091 | Data=43.91930326 | Physics=1.96691906 | Val RMSE: 2.12257600 | ‚àö(Val Loss) = 1.06221509 | Current Learning Rate: 0.001\n",
      "Epoch 766/1000 | Train Loss=4391.80485026 | Val Loss=1.12806809 | Data=43.89862569 | Physics=1.91564398 | Val RMSE: 2.12255216 | ‚àö(Val Loss) = 1.06210554 | Current Learning Rate: 0.001\n",
      "Epoch 767/1000 | Train Loss=4396.81787109 | Val Loss=1.12917352 | Data=43.94874382 | Physics=1.98150023 | Val RMSE: 2.12230539 | ‚àö(Val Loss) = 1.06262577 | Current Learning Rate: 0.001\n",
      "Epoch 768/1000 | Train Loss=4400.49015299 | Val Loss=1.12873971 | Data=43.98542976 | Physics=2.03063776 | Val RMSE: 2.12245178 | ‚àö(Val Loss) = 1.06242168 | Current Learning Rate: 0.001\n",
      "Epoch 769/1000 | Train Loss=4393.81323242 | Val Loss=1.12821364 | Data=43.91872025 | Physics=1.86227523 | Val RMSE: 2.12244821 | ‚àö(Val Loss) = 1.06217396 | Current Learning Rate: 0.001\n",
      "Epoch 770/1000 | Train Loss=4387.08740234 | Val Loss=1.13009655 | Data=43.85147413 | Physics=1.80725140 | Val RMSE: 2.12240624 | ‚àö(Val Loss) = 1.06306005 | Current Learning Rate: 0.001\n",
      "Epoch 771/1000 | Train Loss=4394.36059570 | Val Loss=1.12887871 | Data=43.92419052 | Physics=1.92476155 | Val RMSE: 2.12212062 | ‚àö(Val Loss) = 1.06248701 | Current Learning Rate: 0.001\n",
      "Epoch 772/1000 | Train Loss=4394.61035156 | Val Loss=1.12925267 | Data=43.92669996 | Physics=1.89427804 | Val RMSE: 2.12211490 | ‚àö(Val Loss) = 1.06266296 | Current Learning Rate: 0.001\n",
      "Epoch 773/1000 | Train Loss=4391.71541341 | Val Loss=1.12920332 | Data=43.89774450 | Physics=1.92459247 | Val RMSE: 2.12220955 | ‚àö(Val Loss) = 1.06263983 | Current Learning Rate: 0.001\n",
      "Epoch 774/1000 | Train Loss=4397.70906576 | Val Loss=1.12867022 | Data=43.95765877 | Physics=1.98409579 | Val RMSE: 2.12203264 | ‚àö(Val Loss) = 1.06238890 | Current Learning Rate: 0.001\n",
      "Epoch 775/1000 | Train Loss=4392.80053711 | Val Loss=1.13083160 | Data=43.90855662 | Physics=1.93385676 | Val RMSE: 2.12157917 | ‚àö(Val Loss) = 1.06340563 | Current Learning Rate: 0.001\n",
      "Epoch 776/1000 | Train Loss=4395.66601562 | Val Loss=1.13311911 | Data=43.93720245 | Physics=1.98843342 | Val RMSE: 2.12205505 | ‚àö(Val Loss) = 1.06448066 | Current Learning Rate: 0.001\n",
      "Epoch 777/1000 | Train Loss=4392.10774740 | Val Loss=1.13111126 | Data=43.90164185 | Physics=1.93475558 | Val RMSE: 2.12238240 | ‚àö(Val Loss) = 1.06353712 | Current Learning Rate: 0.001\n",
      "Epoch 778/1000 | Train Loss=4386.95776367 | Val Loss=1.13348913 | Data=43.85021973 | Physics=1.77944420 | Val RMSE: 2.12261462 | ‚àö(Val Loss) = 1.06465447 | Current Learning Rate: 0.001\n",
      "Epoch 779/1000 | Train Loss=4388.47159831 | Val Loss=1.13117313 | Data=43.86530368 | Physics=1.88188345 | Val RMSE: 2.12231803 | ‚àö(Val Loss) = 1.06356621 | Current Learning Rate: 0.001\n",
      "Epoch 780/1000 | Train Loss=4391.33878581 | Val Loss=1.13077843 | Data=43.89397748 | Physics=1.89749104 | Val RMSE: 2.12196469 | ‚àö(Val Loss) = 1.06338072 | Current Learning Rate: 0.001\n",
      "Epoch 781/1000 | Train Loss=4389.91585286 | Val Loss=1.13059092 | Data=43.87973531 | Physics=1.85006314 | Val RMSE: 2.12196875 | ‚àö(Val Loss) = 1.06329250 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 782/1000 | Train Loss=4395.63191732 | Val Loss=1.13132215 | Data=43.93689982 | Physics=1.92177468 | Val RMSE: 2.12219763 | ‚àö(Val Loss) = 1.06363630 | Current Learning Rate: 0.001\n",
      "Epoch 783/1000 | Train Loss=4395.23803711 | Val Loss=1.13107634 | Data=43.93297005 | Physics=1.90255499 | Val RMSE: 2.12257266 | ‚àö(Val Loss) = 1.06352067 | Current Learning Rate: 0.001\n",
      "Epoch 784/1000 | Train Loss=4392.61263021 | Val Loss=1.13008976 | Data=43.90669696 | Physics=1.88491043 | Val RMSE: 2.12252283 | ‚àö(Val Loss) = 1.06305683 | Current Learning Rate: 0.001\n",
      "Epoch 785/1000 | Train Loss=4400.31396484 | Val Loss=1.13102162 | Data=43.98366229 | Physics=2.02046332 | Val RMSE: 2.12250257 | ‚àö(Val Loss) = 1.06349504 | Current Learning Rate: 0.001\n",
      "Epoch 786/1000 | Train Loss=4393.99365234 | Val Loss=1.13087332 | Data=43.92051824 | Physics=1.88296531 | Val RMSE: 2.12276983 | ‚àö(Val Loss) = 1.06342530 | Current Learning Rate: 0.001\n",
      "Epoch 787/1000 | Train Loss=4394.92521159 | Val Loss=1.13204896 | Data=43.92985598 | Physics=1.85541089 | Val RMSE: 2.12280869 | ‚àö(Val Loss) = 1.06397784 | Current Learning Rate: 0.001\n",
      "Epoch 788/1000 | Train Loss=4393.68933105 | Val Loss=1.13019288 | Data=43.91745313 | Physics=2.01304040 | Val RMSE: 2.12261009 | ‚àö(Val Loss) = 1.06310534 | Current Learning Rate: 0.001\n",
      "Epoch 789/1000 | Train Loss=4392.22656250 | Val Loss=1.12974727 | Data=43.90285238 | Physics=1.89084265 | Val RMSE: 2.12263799 | ‚àö(Val Loss) = 1.06289566 | Current Learning Rate: 0.001\n",
      "Epoch 790/1000 | Train Loss=4395.27547201 | Val Loss=1.13063633 | Data=43.93333371 | Physics=1.98795563 | Val RMSE: 2.12252569 | ‚àö(Val Loss) = 1.06331384 | Current Learning Rate: 0.001\n",
      "Epoch 791/1000 | Train Loss=4394.48144531 | Val Loss=1.12837648 | Data=43.92538134 | Physics=1.95776853 | Val RMSE: 2.12233877 | ‚àö(Val Loss) = 1.06225061 | Current Learning Rate: 0.001\n",
      "Epoch 792/1000 | Train Loss=4393.46907552 | Val Loss=1.13042223 | Data=43.91524506 | Physics=1.95345185 | Val RMSE: 2.12210512 | ‚àö(Val Loss) = 1.06321311 | Current Learning Rate: 0.001\n",
      "Epoch 793/1000 | Train Loss=4391.23201497 | Val Loss=1.13109481 | Data=43.89294116 | Physics=1.87936322 | Val RMSE: 2.12222266 | ‚àö(Val Loss) = 1.06352937 | Current Learning Rate: 0.001\n",
      "Epoch 794/1000 | Train Loss=4389.92122396 | Val Loss=1.13041449 | Data=43.87979253 | Physics=1.87792388 | Val RMSE: 2.12208200 | ‚àö(Val Loss) = 1.06320953 | Current Learning Rate: 0.001\n",
      "Epoch 795/1000 | Train Loss=4395.71638997 | Val Loss=1.13110685 | Data=43.93771172 | Physics=1.93716136 | Val RMSE: 2.12224603 | ‚àö(Val Loss) = 1.06353509 | Current Learning Rate: 0.001\n",
      "Epoch 796/1000 | Train Loss=4393.63818359 | Val Loss=1.13065004 | Data=43.91696612 | Physics=1.93176381 | Val RMSE: 2.12239861 | ‚àö(Val Loss) = 1.06332028 | Current Learning Rate: 0.001\n",
      "Epoch 797/1000 | Train Loss=4388.74047852 | Val Loss=1.13044941 | Data=43.86802038 | Physics=1.85793219 | Val RMSE: 2.12229419 | ‚àö(Val Loss) = 1.06322598 | Current Learning Rate: 0.001\n",
      "Epoch 798/1000 | Train Loss=4392.56868490 | Val Loss=1.12996018 | Data=43.90625191 | Physics=1.95535894 | Val RMSE: 2.12227917 | ‚àö(Val Loss) = 1.06299579 | Current Learning Rate: 0.001\n",
      "Epoch 799/1000 | Train Loss=4398.62280273 | Val Loss=1.12959826 | Data=43.96677844 | Physics=1.94430276 | Val RMSE: 2.12212515 | ‚àö(Val Loss) = 1.06282556 | Current Learning Rate: 0.001\n",
      "Epoch 800/1000 | Train Loss=4398.41031901 | Val Loss=1.13164270 | Data=43.96463267 | Physics=1.93610118 | Val RMSE: 2.12250710 | ‚àö(Val Loss) = 1.06378698 | Current Learning Rate: 0.0001\n",
      "Epoch 801/1000 | Train Loss=4405.25008138 | Val Loss=1.15496349 | Data=44.03302320 | Physics=1.96447257 | Val RMSE: 2.12094164 | ‚àö(Val Loss) = 1.07469225 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 802/1000 | Train Loss=4390.84318034 | Val Loss=1.28591919 | Data=43.88898277 | Physics=2.05625483 | Val RMSE: 2.12128711 | ‚àö(Val Loss) = 1.13398373 | Current Learning Rate: 0.0001\n",
      "Epoch 803/1000 | Train Loss=4396.10587565 | Val Loss=1.24258065 | Data=43.94163195 | Physics=1.97640898 | Val RMSE: 2.12150311 | ‚àö(Val Loss) = 1.11471105 | Current Learning Rate: 0.0001\n",
      "Epoch 804/1000 | Train Loss=4394.11588542 | Val Loss=1.26614058 | Data=43.92174085 | Physics=1.96810605 | Val RMSE: 2.12143016 | ‚àö(Val Loss) = 1.12522912 | Current Learning Rate: 0.0001\n",
      "Epoch 805/1000 | Train Loss=4388.26944987 | Val Loss=1.29791141 | Data=43.86329969 | Physics=1.93150560 | Val RMSE: 2.12120843 | ‚àö(Val Loss) = 1.13925910 | Current Learning Rate: 0.0001\n",
      "Epoch 806/1000 | Train Loss=4389.56567383 | Val Loss=1.25595176 | Data=43.87622770 | Physics=2.01261632 | Val RMSE: 2.12117147 | ‚àö(Val Loss) = 1.12069249 | Current Learning Rate: 0.0001\n",
      "Epoch 807/1000 | Train Loss=4395.83959961 | Val Loss=1.23655891 | Data=43.93896739 | Physics=1.96340178 | Val RMSE: 2.12117100 | ‚àö(Val Loss) = 1.11200666 | Current Learning Rate: 0.0001\n",
      "Epoch 808/1000 | Train Loss=4386.73201497 | Val Loss=1.23028314 | Data=43.84790357 | Physics=1.91262931 | Val RMSE: 2.12138319 | ‚àö(Val Loss) = 1.10918128 | Current Learning Rate: 0.0001\n",
      "Epoch 809/1000 | Train Loss=4388.47900391 | Val Loss=1.26186657 | Data=43.86539078 | Physics=1.95578564 | Val RMSE: 2.12148356 | ‚àö(Val Loss) = 1.12332833 | Current Learning Rate: 0.0001\n",
      "Epoch 810/1000 | Train Loss=4387.13484701 | Val Loss=1.29478061 | Data=43.85194397 | Physics=1.94993553 | Val RMSE: 2.12126684 | ‚àö(Val Loss) = 1.13788426 | Current Learning Rate: 0.0001\n",
      "Epoch 811/1000 | Train Loss=4392.63793945 | Val Loss=1.25456238 | Data=43.90696780 | Physics=1.94397555 | Val RMSE: 2.12115073 | ‚àö(Val Loss) = 1.12007248 | Current Learning Rate: 0.0001\n",
      "Epoch 812/1000 | Train Loss=4392.05297852 | Val Loss=1.27111006 | Data=43.90111860 | Physics=1.91811519 | Val RMSE: 2.12111402 | ‚àö(Val Loss) = 1.12743521 | Current Learning Rate: 0.0001\n",
      "Epoch 813/1000 | Train Loss=4392.91731771 | Val Loss=1.29776073 | Data=43.90977669 | Physics=1.95816004 | Val RMSE: 2.12110758 | ‚àö(Val Loss) = 1.13919306 | Current Learning Rate: 0.0001\n",
      "Epoch 814/1000 | Train Loss=4391.05908203 | Val Loss=1.25552189 | Data=43.89117495 | Physics=1.92731299 | Val RMSE: 2.12112093 | ‚àö(Val Loss) = 1.12050068 | Current Learning Rate: 0.0001\n",
      "Epoch 815/1000 | Train Loss=4388.23404948 | Val Loss=1.23652482 | Data=43.86291631 | Physics=2.00445664 | Val RMSE: 2.12110949 | ‚àö(Val Loss) = 1.11199141 | Current Learning Rate: 0.0001\n",
      "Epoch 816/1000 | Train Loss=4386.87231445 | Val Loss=1.26388228 | Data=43.84931628 | Physics=1.92958046 | Val RMSE: 2.12112045 | ‚àö(Val Loss) = 1.12422514 | Current Learning Rate: 0.0001\n",
      "Epoch 817/1000 | Train Loss=4385.60498047 | Val Loss=1.29638684 | Data=43.83668454 | Physics=1.84683590 | Val RMSE: 2.12121463 | ‚àö(Val Loss) = 1.13858986 | Current Learning Rate: 0.0001\n",
      "Epoch 818/1000 | Train Loss=4394.36604818 | Val Loss=1.25505304 | Data=43.92426300 | Physics=1.96003452 | Val RMSE: 2.12113166 | ‚àö(Val Loss) = 1.12029147 | Current Learning Rate: 0.0001\n",
      "Epoch 819/1000 | Train Loss=4393.17993164 | Val Loss=1.23649430 | Data=43.91238403 | Physics=1.98845584 | Val RMSE: 2.12114382 | ‚àö(Val Loss) = 1.11197770 | Current Learning Rate: 0.0001\n",
      "Epoch 820/1000 | Train Loss=4381.94539388 | Val Loss=1.24643672 | Data=43.80007744 | Physics=1.88274635 | Val RMSE: 2.12152219 | ‚àö(Val Loss) = 1.11643934 | Current Learning Rate: 0.0001\n",
      "Epoch 821/1000 | Train Loss=4393.00268555 | Val Loss=1.26019061 | Data=43.91060448 | Physics=2.01285759 | Val RMSE: 2.12174320 | ‚àö(Val Loss) = 1.12258208 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 822/1000 | Train Loss=4399.01106771 | Val Loss=1.23912013 | Data=43.97066561 | Physics=2.07883669 | Val RMSE: 2.12142229 | ‚àö(Val Loss) = 1.11315775 | Current Learning Rate: 0.0001\n",
      "Epoch 823/1000 | Train Loss=4388.62329102 | Val Loss=1.25235653 | Data=43.86684036 | Physics=1.93286229 | Val RMSE: 2.12160039 | ‚àö(Val Loss) = 1.11908734 | Current Learning Rate: 0.0001\n",
      "Epoch 824/1000 | Train Loss=4393.23502604 | Val Loss=1.26937091 | Data=43.91291300 | Physics=1.99155068 | Val RMSE: 2.12172627 | ‚àö(Val Loss) = 1.12666357 | Current Learning Rate: 0.0001\n",
      "Epoch 825/1000 | Train Loss=4392.80143229 | Val Loss=1.24142468 | Data=43.90858523 | Physics=1.98588401 | Val RMSE: 2.12142324 | ‚àö(Val Loss) = 1.11419237 | Current Learning Rate: 0.0001\n",
      "Epoch 826/1000 | Train Loss=4389.67643229 | Val Loss=1.26469934 | Data=43.87735367 | Physics=1.97004676 | Val RMSE: 2.12123775 | ‚àö(Val Loss) = 1.12458849 | Current Learning Rate: 0.0001\n",
      "Epoch 827/1000 | Train Loss=4382.59106445 | Val Loss=1.29764390 | Data=43.80655352 | Physics=1.82495407 | Val RMSE: 2.12101650 | ‚àö(Val Loss) = 1.13914168 | Current Learning Rate: 0.0001\n",
      "Epoch 828/1000 | Train Loss=4386.01424154 | Val Loss=1.25573480 | Data=43.84075546 | Physics=1.89176871 | Val RMSE: 2.12100720 | ‚àö(Val Loss) = 1.12059569 | Current Learning Rate: 0.0001\n",
      "Epoch 829/1000 | Train Loss=4392.98413086 | Val Loss=1.23628712 | Data=43.91044680 | Physics=1.95565244 | Val RMSE: 2.12108946 | ‚àö(Val Loss) = 1.11188447 | Current Learning Rate: 0.0001\n",
      "Epoch 830/1000 | Train Loss=4388.09293620 | Val Loss=1.22878587 | Data=43.86153475 | Physics=1.94477694 | Val RMSE: 2.12117195 | ‚àö(Val Loss) = 1.10850620 | Current Learning Rate: 0.0001\n",
      "Epoch 831/1000 | Train Loss=4390.23201497 | Val Loss=1.25960755 | Data=43.88291041 | Physics=1.92467188 | Val RMSE: 2.12126756 | ‚àö(Val Loss) = 1.12232244 | Current Learning Rate: 0.0001\n",
      "Epoch 832/1000 | Train Loss=4395.51652018 | Val Loss=1.29269981 | Data=43.93576558 | Physics=1.93007960 | Val RMSE: 2.12135148 | ‚àö(Val Loss) = 1.13696957 | Current Learning Rate: 0.0001\n",
      "Epoch 833/1000 | Train Loss=4392.57788086 | Val Loss=1.25276411 | Data=43.90637143 | Physics=1.92769155 | Val RMSE: 2.12123990 | ‚àö(Val Loss) = 1.11926949 | Current Learning Rate: 0.0001\n",
      "Epoch 834/1000 | Train Loss=4393.71956380 | Val Loss=1.26982188 | Data=43.91777420 | Physics=1.97898091 | Val RMSE: 2.12112498 | ‚àö(Val Loss) = 1.12686372 | Current Learning Rate: 0.0001\n",
      "Epoch 835/1000 | Train Loss=4396.74471029 | Val Loss=1.27203012 | Data=43.94801394 | Physics=1.99621008 | Val RMSE: 2.12141418 | ‚àö(Val Loss) = 1.12784314 | Current Learning Rate: 0.0001\n",
      "Epoch 836/1000 | Train Loss=4387.24877930 | Val Loss=1.25286019 | Data=43.85309919 | Physics=1.84658748 | Val RMSE: 2.12178874 | ‚àö(Val Loss) = 1.11931241 | Current Learning Rate: 0.0001\n",
      "Epoch 837/1000 | Train Loss=4389.51562500 | Val Loss=1.27268505 | Data=43.87578519 | Physics=1.86985705 | Val RMSE: 2.12189579 | ‚àö(Val Loss) = 1.12813342 | Current Learning Rate: 0.0001\n",
      "Epoch 838/1000 | Train Loss=4394.09611003 | Val Loss=1.24402201 | Data=43.92155520 | Physics=1.98285845 | Val RMSE: 2.12145686 | ‚àö(Val Loss) = 1.11535740 | Current Learning Rate: 0.0001\n",
      "Epoch 839/1000 | Train Loss=4385.63476562 | Val Loss=1.26480901 | Data=43.83695793 | Physics=1.90178976 | Val RMSE: 2.12123990 | ‚àö(Val Loss) = 1.12463725 | Current Learning Rate: 0.0001\n",
      "Epoch 840/1000 | Train Loss=4386.83723958 | Val Loss=1.29519629 | Data=43.84900729 | Physics=1.92274468 | Val RMSE: 2.12115932 | ‚àö(Val Loss) = 1.13806689 | Current Learning Rate: 0.0001\n",
      "Epoch 841/1000 | Train Loss=4395.94661458 | Val Loss=1.25519741 | Data=43.94002088 | Physics=1.99127786 | Val RMSE: 2.12106371 | ‚àö(Val Loss) = 1.12035596 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 842/1000 | Train Loss=4388.90820312 | Val Loss=1.23669755 | Data=43.86967087 | Physics=1.95471605 | Val RMSE: 2.12111115 | ‚àö(Val Loss) = 1.11206901 | Current Learning Rate: 0.0001\n",
      "Epoch 843/1000 | Train Loss=4389.53320312 | Val Loss=1.26303315 | Data=43.87593206 | Physics=1.96044121 | Val RMSE: 2.12111807 | ‚àö(Val Loss) = 1.12384748 | Current Learning Rate: 0.0001\n",
      "Epoch 844/1000 | Train Loss=4390.52473958 | Val Loss=1.29612899 | Data=43.88585854 | Physics=1.89741712 | Val RMSE: 2.12114406 | ‚àö(Val Loss) = 1.13847661 | Current Learning Rate: 0.0001\n",
      "Epoch 845/1000 | Train Loss=4389.03312174 | Val Loss=1.25570440 | Data=43.87094180 | Physics=1.88921531 | Val RMSE: 2.12114859 | ‚àö(Val Loss) = 1.12058222 | Current Learning Rate: 0.0001\n",
      "Epoch 846/1000 | Train Loss=4393.36612956 | Val Loss=1.27018356 | Data=43.91425133 | Physics=1.96230472 | Val RMSE: 2.12108064 | ‚àö(Val Loss) = 1.12702417 | Current Learning Rate: 0.0001\n",
      "Epoch 847/1000 | Train Loss=4394.13126628 | Val Loss=1.29538620 | Data=43.92189852 | Physics=1.93569714 | Val RMSE: 2.12098980 | ‚àö(Val Loss) = 1.13815033 | Current Learning Rate: 0.0001\n",
      "Epoch 848/1000 | Train Loss=4387.14404297 | Val Loss=1.25523555 | Data=43.85206286 | Physics=1.88015421 | Val RMSE: 2.12099791 | ‚àö(Val Loss) = 1.12037301 | Current Learning Rate: 0.0001\n",
      "Epoch 849/1000 | Train Loss=4390.07438151 | Val Loss=1.23578811 | Data=43.88133558 | Physics=1.92918254 | Val RMSE: 2.12099242 | ‚àö(Val Loss) = 1.11166012 | Current Learning Rate: 0.0001\n",
      "Epoch 850/1000 | Train Loss=4393.78784180 | Val Loss=1.26226866 | Data=43.91848310 | Physics=1.93811483 | Val RMSE: 2.12094855 | ‚àö(Val Loss) = 1.12350726 | Current Learning Rate: 0.0001\n",
      "Epoch 851/1000 | Train Loss=4388.67521159 | Val Loss=1.29696453 | Data=43.86738014 | Physics=1.90704528 | Val RMSE: 2.12086320 | ‚àö(Val Loss) = 1.13884354 | Current Learning Rate: 0.0001\n",
      "Epoch 852/1000 | Train Loss=4388.56453451 | Val Loss=1.25602555 | Data=43.86626371 | Physics=1.90430575 | Val RMSE: 2.12088513 | ‚àö(Val Loss) = 1.12072551 | Current Learning Rate: 0.0001\n",
      "Epoch 853/1000 | Train Loss=4387.45572917 | Val Loss=1.23666966 | Data=43.85519346 | Physics=1.87565843 | Val RMSE: 2.12105346 | ‚àö(Val Loss) = 1.11205649 | Current Learning Rate: 0.0001\n",
      "Epoch 854/1000 | Train Loss=4387.12939453 | Val Loss=1.26223826 | Data=43.85191790 | Physics=1.83584285 | Val RMSE: 2.12121344 | ‚àö(Val Loss) = 1.12349379 | Current Learning Rate: 0.0001\n",
      "Epoch 855/1000 | Train Loss=4393.41341146 | Val Loss=1.29519343 | Data=43.91469892 | Physics=2.00952950 | Val RMSE: 2.12122655 | ‚àö(Val Loss) = 1.13806570 | Current Learning Rate: 0.0001\n",
      "Epoch 856/1000 | Train Loss=4391.34122721 | Val Loss=1.25463855 | Data=43.89399719 | Physics=1.95097955 | Val RMSE: 2.12109852 | ‚àö(Val Loss) = 1.12010646 | Current Learning Rate: 0.0001\n",
      "Epoch 857/1000 | Train Loss=4392.38069661 | Val Loss=1.23639524 | Data=43.90440877 | Physics=1.91109296 | Val RMSE: 2.12115312 | ‚àö(Val Loss) = 1.11193311 | Current Learning Rate: 0.0001\n",
      "Epoch 858/1000 | Train Loss=4391.58048503 | Val Loss=1.22986889 | Data=43.89641889 | Physics=1.88676374 | Val RMSE: 2.12133360 | ‚àö(Val Loss) = 1.10899448 | Current Learning Rate: 0.0001\n",
      "Epoch 859/1000 | Train Loss=4388.73494466 | Val Loss=1.26118934 | Data=43.86795489 | Physics=1.97594479 | Val RMSE: 2.12132335 | ‚àö(Val Loss) = 1.12302685 | Current Learning Rate: 0.0001\n",
      "Epoch 860/1000 | Train Loss=4389.91243490 | Val Loss=1.29419816 | Data=43.87970352 | Physics=1.99898957 | Val RMSE: 2.12109399 | ‚àö(Val Loss) = 1.13762832 | Current Learning Rate: 0.0001\n",
      "Epoch 861/1000 | Train Loss=4388.28629557 | Val Loss=1.25497580 | Data=43.86344592 | Physics=1.95170260 | Val RMSE: 2.12105203 | ‚àö(Val Loss) = 1.12025702 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 862/1000 | Train Loss=4391.11263021 | Val Loss=1.27239323 | Data=43.89169947 | Physics=1.96883900 | Val RMSE: 2.12106872 | ‚àö(Val Loss) = 1.12800407 | Current Learning Rate: 0.0001\n",
      "Epoch 863/1000 | Train Loss=4390.54964193 | Val Loss=1.25742567 | Data=43.88609314 | Physics=1.90552039 | Val RMSE: 2.12136149 | ‚àö(Val Loss) = 1.12134993 | Current Learning Rate: 0.0001\n",
      "Epoch 864/1000 | Train Loss=4392.65372721 | Val Loss=1.24440086 | Data=43.90711276 | Physics=1.96099157 | Val RMSE: 2.12155771 | ‚àö(Val Loss) = 1.11552715 | Current Learning Rate: 0.0001\n",
      "Epoch 865/1000 | Train Loss=4389.46264648 | Val Loss=1.28319561 | Data=43.87522316 | Physics=1.93850518 | Val RMSE: 2.12146568 | ‚àö(Val Loss) = 1.13278222 | Current Learning Rate: 0.0001\n",
      "Epoch 866/1000 | Train Loss=4390.70768229 | Val Loss=1.24943626 | Data=43.88764826 | Physics=1.97857235 | Val RMSE: 2.12125134 | ‚àö(Val Loss) = 1.11778188 | Current Learning Rate: 0.0001\n",
      "Epoch 867/1000 | Train Loss=4385.00301107 | Val Loss=1.25944948 | Data=43.83064651 | Physics=1.89643209 | Val RMSE: 2.12154913 | ‚àö(Val Loss) = 1.12225199 | Current Learning Rate: 0.0001\n",
      "Epoch 868/1000 | Train Loss=4388.59602865 | Val Loss=1.27725041 | Data=43.86655617 | Physics=1.99790870 | Val RMSE: 2.12174726 | ‚àö(Val Loss) = 1.13015509 | Current Learning Rate: 0.0001\n",
      "Epoch 869/1000 | Train Loss=4389.74096680 | Val Loss=1.24711025 | Data=43.87800217 | Physics=1.95631048 | Val RMSE: 2.12139559 | ‚àö(Val Loss) = 1.11674094 | Current Learning Rate: 0.0001\n",
      "Epoch 870/1000 | Train Loss=4385.37044271 | Val Loss=1.26790416 | Data=43.83435504 | Physics=1.85573088 | Val RMSE: 2.12119484 | ‚àö(Val Loss) = 1.12601244 | Current Learning Rate: 0.0001\n",
      "Epoch 871/1000 | Train Loss=4387.79581706 | Val Loss=1.29770327 | Data=43.85861460 | Physics=1.86946570 | Val RMSE: 2.12095737 | ‚àö(Val Loss) = 1.13916779 | Current Learning Rate: 0.0001\n",
      "Epoch 872/1000 | Train Loss=4393.78531901 | Val Loss=1.25677586 | Data=43.91844050 | Physics=1.95481549 | Val RMSE: 2.12097526 | ‚àö(Val Loss) = 1.12106013 | Current Learning Rate: 0.0001\n",
      "Epoch 873/1000 | Train Loss=4392.40283203 | Val Loss=1.23764479 | Data=43.90462748 | Physics=1.97058102 | Val RMSE: 2.12112164 | ‚àö(Val Loss) = 1.11249483 | Current Learning Rate: 0.0001\n",
      "Epoch 874/1000 | Train Loss=4396.61604818 | Val Loss=1.23024070 | Data=43.94672648 | Physics=1.99567845 | Val RMSE: 2.12129712 | ‚àö(Val Loss) = 1.10916221 | Current Learning Rate: 0.0001\n",
      "Epoch 875/1000 | Train Loss=4389.05493164 | Val Loss=1.26085401 | Data=43.87114143 | Physics=1.91857271 | Val RMSE: 2.12131691 | ‚àö(Val Loss) = 1.12287760 | Current Learning Rate: 0.0001\n",
      "Epoch 876/1000 | Train Loss=4390.47005208 | Val Loss=1.29263151 | Data=43.88529015 | Physics=1.97815345 | Val RMSE: 2.12119842 | ‚àö(Val Loss) = 1.13693953 | Current Learning Rate: 0.0001\n",
      "Epoch 877/1000 | Train Loss=4393.98185221 | Val Loss=1.25402260 | Data=43.92039045 | Physics=1.96208058 | Val RMSE: 2.12120318 | ‚àö(Val Loss) = 1.11983156 | Current Learning Rate: 0.0001\n",
      "Epoch 878/1000 | Train Loss=4389.47664388 | Val Loss=1.23579407 | Data=43.87534587 | Physics=1.95457687 | Val RMSE: 2.12121606 | ‚àö(Val Loss) = 1.11166275 | Current Learning Rate: 0.0001\n",
      "Epoch 879/1000 | Train Loss=4393.19620768 | Val Loss=1.26154780 | Data=43.91253408 | Physics=1.97421686 | Val RMSE: 2.12136507 | ‚àö(Val Loss) = 1.12318647 | Current Learning Rate: 0.0001\n",
      "Epoch 880/1000 | Train Loss=4390.71809896 | Val Loss=1.29301453 | Data=43.88775253 | Physics=1.98984161 | Val RMSE: 2.12140584 | ‚àö(Val Loss) = 1.13710797 | Current Learning Rate: 0.0001\n",
      "Epoch 881/1000 | Train Loss=4392.29410807 | Val Loss=1.25382304 | Data=43.90356318 | Physics=1.89344627 | Val RMSE: 2.12123609 | ‚àö(Val Loss) = 1.11974239 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 882/1000 | Train Loss=4393.35701497 | Val Loss=1.26978266 | Data=43.91415723 | Physics=1.97270187 | Val RMSE: 2.12112451 | ‚àö(Val Loss) = 1.12684631 | Current Learning Rate: 0.0001\n",
      "Epoch 883/1000 | Train Loss=4391.60139974 | Val Loss=1.28302062 | Data=43.89660390 | Physics=1.95895212 | Val RMSE: 2.12133670 | ‚àö(Val Loss) = 1.13270497 | Current Learning Rate: 0.0001\n",
      "Epoch 884/1000 | Train Loss=4387.99308268 | Val Loss=1.24093783 | Data=43.86050034 | Physics=1.97533725 | Val RMSE: 2.12146664 | ‚àö(Val Loss) = 1.11397386 | Current Learning Rate: 0.0001\n",
      "Epoch 885/1000 | Train Loss=4386.09204102 | Val Loss=1.23191357 | Data=43.84153748 | Physics=1.87698903 | Val RMSE: 2.12151408 | ‚àö(Val Loss) = 1.10991597 | Current Learning Rate: 0.0001\n",
      "Epoch 886/1000 | Train Loss=4389.82397461 | Val Loss=1.26583517 | Data=43.87884776 | Physics=1.96167063 | Val RMSE: 2.12153101 | ‚àö(Val Loss) = 1.12509346 | Current Learning Rate: 0.0001\n",
      "Epoch 887/1000 | Train Loss=4397.59871419 | Val Loss=1.29647756 | Data=43.95654996 | Physics=2.03708118 | Val RMSE: 2.12113261 | ‚àö(Val Loss) = 1.13862967 | Current Learning Rate: 0.0001\n",
      "Epoch 888/1000 | Train Loss=4396.09586589 | Val Loss=1.25600934 | Data=43.94153913 | Physics=1.94923265 | Val RMSE: 2.12104893 | ‚àö(Val Loss) = 1.12071824 | Current Learning Rate: 0.0001\n",
      "Epoch 889/1000 | Train Loss=4386.03108724 | Val Loss=1.23747218 | Data=43.84089661 | Physics=1.93814186 | Val RMSE: 2.12111831 | ‚àö(Val Loss) = 1.11241722 | Current Learning Rate: 0.0001\n",
      "Epoch 890/1000 | Train Loss=4393.27514648 | Val Loss=1.23205543 | Data=43.91332245 | Physics=1.99274480 | Val RMSE: 2.12129903 | ‚àö(Val Loss) = 1.10997987 | Current Learning Rate: 0.0001\n",
      "Epoch 891/1000 | Train Loss=4386.89501953 | Val Loss=1.26545799 | Data=43.84960492 | Physics=1.86573439 | Val RMSE: 2.12138939 | ‚àö(Val Loss) = 1.12492573 | Current Learning Rate: 0.0001\n",
      "Epoch 892/1000 | Train Loss=4386.64314779 | Val Loss=1.29776454 | Data=43.84702110 | Physics=1.96640999 | Val RMSE: 2.12111282 | ‚àö(Val Loss) = 1.13919473 | Current Learning Rate: 0.0001\n",
      "Epoch 893/1000 | Train Loss=4386.70068359 | Val Loss=1.25626636 | Data=43.84762891 | Physics=1.85478176 | Val RMSE: 2.12105846 | ‚àö(Val Loss) = 1.12083292 | Current Learning Rate: 0.0001\n",
      "Epoch 894/1000 | Train Loss=4391.59065755 | Val Loss=1.27148664 | Data=43.89651807 | Physics=1.97056326 | Val RMSE: 2.12100196 | ‚àö(Val Loss) = 1.12760222 | Current Learning Rate: 0.0001\n",
      "Epoch 895/1000 | Train Loss=4392.55444336 | Val Loss=1.29723966 | Data=43.90613111 | Physics=1.95315636 | Val RMSE: 2.12087107 | ‚àö(Val Loss) = 1.13896430 | Current Learning Rate: 0.0001\n",
      "Epoch 896/1000 | Train Loss=4385.81103516 | Val Loss=1.25692761 | Data=43.83870061 | Physics=1.95376571 | Val RMSE: 2.12093091 | ‚àö(Val Loss) = 1.12112784 | Current Learning Rate: 0.0001\n",
      "Epoch 897/1000 | Train Loss=4383.34863281 | Val Loss=1.23797846 | Data=43.81412633 | Physics=1.80671550 | Val RMSE: 2.12093425 | ‚àö(Val Loss) = 1.11264479 | Current Learning Rate: 0.0001\n",
      "Epoch 898/1000 | Train Loss=4389.14461263 | Val Loss=1.24720871 | Data=43.87205760 | Physics=1.86380761 | Val RMSE: 2.12140441 | ‚àö(Val Loss) = 1.11678505 | Current Learning Rate: 0.0001\n",
      "Epoch 899/1000 | Train Loss=4389.24291992 | Val Loss=1.26069069 | Data=43.87302589 | Physics=1.92847288 | Val RMSE: 2.12179875 | ‚àö(Val Loss) = 1.12280488 | Current Learning Rate: 0.0001\n",
      "Epoch 900/1000 | Train Loss=4384.72029622 | Val Loss=1.23905683 | Data=43.82784398 | Physics=1.81061770 | Val RMSE: 2.12150836 | ‚àö(Val Loss) = 1.11312926 | Current Learning Rate: 0.0001\n",
      "Epoch 901/1000 | Train Loss=4390.76114909 | Val Loss=1.24352396 | Data=43.88817978 | Physics=1.97758243 | Val RMSE: 2.12172365 | ‚àö(Val Loss) = 1.11513400 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 902/1000 | Train Loss=4393.38647461 | Val Loss=1.25188494 | Data=43.91446368 | Physics=1.98662502 | Val RMSE: 2.12190366 | ‚àö(Val Loss) = 1.11887670 | Current Learning Rate: 0.0001\n",
      "Epoch 903/1000 | Train Loss=4395.40966797 | Val Loss=1.26980555 | Data=43.93468412 | Physics=1.94756743 | Val RMSE: 2.12142611 | ‚àö(Val Loss) = 1.12685645 | Current Learning Rate: 0.0001\n",
      "Epoch 904/1000 | Train Loss=4394.00911458 | Val Loss=1.29908240 | Data=43.92066193 | Physics=1.98150281 | Val RMSE: 2.12114668 | ‚àö(Val Loss) = 1.13977301 | Current Learning Rate: 0.0001\n",
      "Epoch 905/1000 | Train Loss=4389.42708333 | Val Loss=1.25717616 | Data=43.87489573 | Physics=1.91061311 | Val RMSE: 2.12111330 | ‚àö(Val Loss) = 1.12123871 | Current Learning Rate: 0.0001\n",
      "Epoch 906/1000 | Train Loss=4392.13273112 | Val Loss=1.23792911 | Data=43.90191460 | Physics=1.92724257 | Val RMSE: 2.12099695 | ‚àö(Val Loss) = 1.11262262 | Current Learning Rate: 0.0001\n",
      "Epoch 907/1000 | Train Loss=4393.98518880 | Val Loss=1.26601732 | Data=43.92043495 | Physics=1.97037396 | Val RMSE: 2.12087536 | ‚àö(Val Loss) = 1.12517440 | Current Learning Rate: 0.0001\n",
      "Epoch 908/1000 | Train Loss=4388.71826172 | Val Loss=1.30008423 | Data=43.86776924 | Physics=1.91239808 | Val RMSE: 2.12086821 | ‚àö(Val Loss) = 1.14021242 | Current Learning Rate: 0.0001\n",
      "Epoch 909/1000 | Train Loss=4389.50870768 | Val Loss=1.25797558 | Data=43.87568728 | Physics=1.93165995 | Val RMSE: 2.12097096 | ‚àö(Val Loss) = 1.12159514 | Current Learning Rate: 0.0001\n",
      "Epoch 910/1000 | Train Loss=4392.90348307 | Val Loss=1.23854160 | Data=43.90963236 | Physics=1.94364698 | Val RMSE: 2.12103724 | ‚àö(Val Loss) = 1.11289787 | Current Learning Rate: 0.0001\n",
      "Epoch 911/1000 | Train Loss=4391.09847005 | Val Loss=1.26522815 | Data=43.89161174 | Physics=1.84130046 | Val RMSE: 2.12112737 | ‚àö(Val Loss) = 1.12482357 | Current Learning Rate: 0.0001\n",
      "Epoch 912/1000 | Train Loss=4386.74291992 | Val Loss=1.29735506 | Data=43.84802437 | Physics=1.92895852 | Val RMSE: 2.12116718 | ‚àö(Val Loss) = 1.13901496 | Current Learning Rate: 0.0001\n",
      "Epoch 913/1000 | Train Loss=4390.67765299 | Val Loss=1.25613809 | Data=43.88737615 | Physics=1.90378849 | Val RMSE: 2.12109351 | ‚àö(Val Loss) = 1.12077570 | Current Learning Rate: 0.0001\n",
      "Epoch 914/1000 | Train Loss=4394.06591797 | Val Loss=1.23783541 | Data=43.92124112 | Physics=1.97034619 | Val RMSE: 2.12114263 | ‚àö(Val Loss) = 1.11258054 | Current Learning Rate: 0.0001\n",
      "Epoch 915/1000 | Train Loss=4392.99267578 | Val Loss=1.23296916 | Data=43.91051356 | Physics=1.94747889 | Val RMSE: 2.12136984 | ‚àö(Val Loss) = 1.11039150 | Current Learning Rate: 0.0001\n",
      "Epoch 916/1000 | Train Loss=4392.07722982 | Val Loss=1.26533341 | Data=43.90137800 | Physics=1.94136824 | Val RMSE: 2.12151599 | ‚àö(Val Loss) = 1.12487042 | Current Learning Rate: 0.0001\n",
      "Epoch 917/1000 | Train Loss=4387.91414388 | Val Loss=1.29572308 | Data=43.85975838 | Physics=1.86700959 | Val RMSE: 2.12137222 | ‚àö(Val Loss) = 1.13829827 | Current Learning Rate: 0.0001\n",
      "Epoch 918/1000 | Train Loss=4391.12150065 | Val Loss=1.25718594 | Data=43.89179039 | Physics=1.98471218 | Val RMSE: 2.12112260 | ‚àö(Val Loss) = 1.12124300 | Current Learning Rate: 0.0001\n",
      "Epoch 919/1000 | Train Loss=4390.45377604 | Val Loss=1.24043918 | Data=43.88515218 | Physics=1.89343874 | Val RMSE: 2.12093830 | ‚àö(Val Loss) = 1.11375010 | Current Learning Rate: 0.0001\n",
      "Epoch 920/1000 | Train Loss=4390.23632812 | Val Loss=1.25152647 | Data=43.88294411 | Physics=2.00636444 | Val RMSE: 2.12141991 | ‚àö(Val Loss) = 1.11871648 | Current Learning Rate: 0.0001\n",
      "Epoch 921/1000 | Train Loss=4389.92220052 | Val Loss=1.26419699 | Data=43.87984594 | Physics=1.93823923 | Val RMSE: 2.12175131 | ‚àö(Val Loss) = 1.12436509 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 922/1000 | Train Loss=4390.61857096 | Val Loss=1.24066877 | Data=43.88678869 | Physics=1.95255439 | Val RMSE: 2.12132573 | ‚àö(Val Loss) = 1.11385310 | Current Learning Rate: 0.0001\n",
      "Epoch 923/1000 | Train Loss=4392.17805990 | Val Loss=1.24460745 | Data=43.90237363 | Physics=1.91069107 | Val RMSE: 2.12129426 | ‚àö(Val Loss) = 1.11561978 | Current Learning Rate: 0.0001\n",
      "Epoch 924/1000 | Train Loss=4382.58658854 | Val Loss=1.25523186 | Data=43.80647341 | Physics=1.89823060 | Val RMSE: 2.12145710 | ‚àö(Val Loss) = 1.12037134 | Current Learning Rate: 0.0001\n",
      "Epoch 925/1000 | Train Loss=4389.74194336 | Val Loss=1.23639190 | Data=43.87797737 | Physics=2.01611611 | Val RMSE: 2.12125182 | ‚àö(Val Loss) = 1.11193156 | Current Learning Rate: 0.0001\n",
      "Epoch 926/1000 | Train Loss=4392.21842448 | Val Loss=1.26421368 | Data=43.90278244 | Physics=1.94436202 | Val RMSE: 2.12121296 | ‚àö(Val Loss) = 1.12437260 | Current Learning Rate: 0.0001\n",
      "Epoch 927/1000 | Train Loss=4388.14331055 | Val Loss=1.29818416 | Data=43.86206182 | Physics=1.84691997 | Val RMSE: 2.12117553 | ‚àö(Val Loss) = 1.13937879 | Current Learning Rate: 0.0001\n",
      "Epoch 928/1000 | Train Loss=4393.34676107 | Val Loss=1.25678146 | Data=43.91404851 | Physics=2.01570957 | Val RMSE: 2.12117505 | ‚àö(Val Loss) = 1.12106264 | Current Learning Rate: 0.0001\n",
      "Epoch 929/1000 | Train Loss=4392.70353190 | Val Loss=1.27174759 | Data=43.90760803 | Physics=1.96680746 | Val RMSE: 2.12115240 | ‚àö(Val Loss) = 1.12771785 | Current Learning Rate: 0.0001\n",
      "Epoch 930/1000 | Train Loss=4386.24658203 | Val Loss=1.29767942 | Data=43.84308243 | Physics=1.84671883 | Val RMSE: 2.12126207 | ‚àö(Val Loss) = 1.13915730 | Current Learning Rate: 0.0001\n",
      "Epoch 931/1000 | Train Loss=4388.62109375 | Val Loss=1.25687683 | Data=43.86681811 | Physics=1.90066288 | Val RMSE: 2.12124515 | ‚àö(Val Loss) = 1.12110519 | Current Learning Rate: 0.0001\n",
      "Epoch 932/1000 | Train Loss=4390.98681641 | Val Loss=1.23817253 | Data=43.89044762 | Physics=1.97320122 | Val RMSE: 2.12119818 | ‚àö(Val Loss) = 1.11273205 | Current Learning Rate: 0.0001\n",
      "Epoch 933/1000 | Train Loss=4386.64998372 | Val Loss=1.26446068 | Data=43.84710248 | Physics=1.90516267 | Val RMSE: 2.12117743 | ‚àö(Val Loss) = 1.12448239 | Current Learning Rate: 0.0001\n",
      "Epoch 934/1000 | Train Loss=4391.83308919 | Val Loss=1.29777634 | Data=43.89891942 | Physics=1.94650654 | Val RMSE: 2.12113118 | ‚àö(Val Loss) = 1.13919985 | Current Learning Rate: 0.0001\n",
      "Epoch 935/1000 | Train Loss=4390.10856120 | Val Loss=1.25731540 | Data=43.88169988 | Physics=1.91536679 | Val RMSE: 2.12110066 | ‚àö(Val Loss) = 1.12130082 | Current Learning Rate: 0.0001\n",
      "Epoch 936/1000 | Train Loss=4389.65185547 | Val Loss=1.23862922 | Data=43.87706629 | Physics=2.03822436 | Val RMSE: 2.12111568 | ‚àö(Val Loss) = 1.11293721 | Current Learning Rate: 0.0001\n",
      "Epoch 937/1000 | Train Loss=4389.80639648 | Val Loss=1.24527550 | Data=43.87864812 | Physics=1.94975943 | Val RMSE: 2.12152600 | ‚àö(Val Loss) = 1.11591911 | Current Learning Rate: 0.0001\n",
      "Epoch 938/1000 | Train Loss=4384.64957682 | Val Loss=1.25501704 | Data=43.82711347 | Physics=1.84092667 | Val RMSE: 2.12185788 | ‚àö(Val Loss) = 1.12027538 | Current Learning Rate: 0.0001\n",
      "Epoch 939/1000 | Train Loss=4389.61279297 | Val Loss=1.23607111 | Data=43.87672361 | Physics=1.96676379 | Val RMSE: 2.12146211 | ‚àö(Val Loss) = 1.11178732 | Current Learning Rate: 0.0001\n",
      "Epoch 940/1000 | Train Loss=4393.50195312 | Val Loss=1.26333058 | Data=43.91560427 | Physics=1.94163823 | Val RMSE: 2.12110233 | ‚àö(Val Loss) = 1.12397981 | Current Learning Rate: 0.0001\n",
      "Epoch 941/1000 | Train Loss=4388.35953776 | Val Loss=1.29897177 | Data=43.86420759 | Physics=1.91926516 | Val RMSE: 2.12097526 | ‚àö(Val Loss) = 1.13972437 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 942/1000 | Train Loss=4392.96736654 | Val Loss=1.25810337 | Data=43.91025798 | Physics=1.93781937 | Val RMSE: 2.12098503 | ‚àö(Val Loss) = 1.12165213 | Current Learning Rate: 0.0001\n",
      "Epoch 943/1000 | Train Loss=4392.87011719 | Val Loss=1.23935735 | Data=43.90928205 | Physics=1.98141971 | Val RMSE: 2.12108755 | ‚àö(Val Loss) = 1.11326432 | Current Learning Rate: 0.0001\n",
      "Epoch 944/1000 | Train Loss=4393.01920573 | Val Loss=1.23127103 | Data=43.91078568 | Physics=1.98140090 | Val RMSE: 2.12121439 | ‚àö(Val Loss) = 1.10962653 | Current Learning Rate: 0.0001\n",
      "Epoch 945/1000 | Train Loss=4389.23120117 | Val Loss=1.23183048 | Data=43.87290319 | Physics=1.95465148 | Val RMSE: 2.12155437 | ‚àö(Val Loss) = 1.10987854 | Current Learning Rate: 0.0001\n",
      "Epoch 946/1000 | Train Loss=4394.08634440 | Val Loss=1.24815524 | Data=43.92141215 | Physics=2.04620568 | Val RMSE: 2.12209797 | ‚àö(Val Loss) = 1.11720872 | Current Learning Rate: 0.0001\n",
      "Epoch 947/1000 | Train Loss=4392.38753255 | Val Loss=1.25682962 | Data=43.90446218 | Physics=1.96610620 | Val RMSE: 2.12206292 | ‚àö(Val Loss) = 1.12108409 | Current Learning Rate: 0.0001\n",
      "Epoch 948/1000 | Train Loss=4390.30737305 | Val Loss=1.27393568 | Data=43.88367780 | Physics=1.95069504 | Val RMSE: 2.12141442 | ‚àö(Val Loss) = 1.12868762 | Current Learning Rate: 0.0001\n",
      "Epoch 949/1000 | Train Loss=4396.82226562 | Val Loss=1.29960740 | Data=43.94878006 | Physics=2.03384143 | Val RMSE: 2.12113142 | ‚àö(Val Loss) = 1.14000320 | Current Learning Rate: 0.0001\n",
      "Epoch 950/1000 | Train Loss=4392.36140951 | Val Loss=1.25778854 | Data=43.90420914 | Physics=1.92535637 | Val RMSE: 2.12116361 | ‚àö(Val Loss) = 1.12151170 | Current Learning Rate: 0.0001\n",
      "Epoch 951/1000 | Train Loss=4389.09016927 | Val Loss=1.24054801 | Data=43.87148539 | Physics=1.94431731 | Val RMSE: 2.12123370 | ‚àö(Val Loss) = 1.11379886 | Current Learning Rate: 0.0001\n",
      "Epoch 952/1000 | Train Loss=4389.45515951 | Val Loss=1.26110744 | Data=43.87511953 | Physics=1.97716433 | Val RMSE: 2.12135005 | ‚àö(Val Loss) = 1.12299037 | Current Learning Rate: 0.0001\n",
      "Epoch 953/1000 | Train Loss=4389.88541667 | Val Loss=1.28384137 | Data=43.87942886 | Physics=1.96750628 | Val RMSE: 2.12128282 | ‚àö(Val Loss) = 1.13306725 | Current Learning Rate: 0.0001\n",
      "Epoch 954/1000 | Train Loss=4394.61791992 | Val Loss=1.24981868 | Data=43.92675082 | Physics=2.01945129 | Val RMSE: 2.12106800 | ‚àö(Val Loss) = 1.11795294 | Current Learning Rate: 0.0001\n",
      "Epoch 955/1000 | Train Loss=4387.23876953 | Val Loss=1.23646772 | Data=43.85296885 | Physics=1.95401240 | Val RMSE: 2.12104630 | ‚àö(Val Loss) = 1.11196566 | Current Learning Rate: 0.0001\n",
      "Epoch 956/1000 | Train Loss=4390.81363932 | Val Loss=1.23228741 | Data=43.88874181 | Physics=1.95919245 | Val RMSE: 2.12131119 | ‚àö(Val Loss) = 1.11008441 | Current Learning Rate: 0.0001\n",
      "Epoch 957/1000 | Train Loss=4384.03458659 | Val Loss=1.26276064 | Data=43.82095400 | Physics=1.92929090 | Val RMSE: 2.12132549 | ‚àö(Val Loss) = 1.12372625 | Current Learning Rate: 0.0001\n",
      "Epoch 958/1000 | Train Loss=4393.47420247 | Val Loss=1.29607093 | Data=43.91532135 | Physics=2.03370474 | Val RMSE: 2.12113500 | ‚àö(Val Loss) = 1.13845110 | Current Learning Rate: 0.0001\n",
      "Epoch 959/1000 | Train Loss=4394.70467122 | Val Loss=1.25710237 | Data=43.92760022 | Physics=2.02295041 | Val RMSE: 2.12112689 | ‚àö(Val Loss) = 1.12120581 | Current Learning Rate: 0.0001\n",
      "Epoch 960/1000 | Train Loss=4387.83162435 | Val Loss=1.23815382 | Data=43.85893885 | Physics=1.84941421 | Val RMSE: 2.12116480 | ‚àö(Val Loss) = 1.11272359 | Current Learning Rate: 0.0001\n",
      "Epoch 961/1000 | Train Loss=4391.04874674 | Val Loss=1.24589193 | Data=43.89108594 | Physics=1.92187440 | Val RMSE: 2.12160945 | ‚àö(Val Loss) = 1.11619532 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 962/1000 | Train Loss=4389.88110352 | Val Loss=1.25851762 | Data=43.87941806 | Physics=1.91440454 | Val RMSE: 2.12194180 | ‚àö(Val Loss) = 1.12183666 | Current Learning Rate: 0.0001\n",
      "Epoch 963/1000 | Train Loss=4389.99519857 | Val Loss=1.26907742 | Data=43.88055484 | Physics=1.92124501 | Val RMSE: 2.12139440 | ‚àö(Val Loss) = 1.12653339 | Current Learning Rate: 0.0001\n",
      "Epoch 964/1000 | Train Loss=4384.56266276 | Val Loss=1.29049277 | Data=43.82623418 | Physics=1.88338039 | Val RMSE: 2.12098622 | ‚àö(Val Loss) = 1.13599861 | Current Learning Rate: 0.0001\n",
      "Epoch 965/1000 | Train Loss=4385.39860026 | Val Loss=1.25486815 | Data=43.83457565 | Physics=1.95671706 | Val RMSE: 2.12101364 | ‚àö(Val Loss) = 1.12020898 | Current Learning Rate: 0.0001\n",
      "Epoch 966/1000 | Train Loss=4390.73697917 | Val Loss=1.23815155 | Data=43.88798459 | Physics=1.91323527 | Val RMSE: 2.12109017 | ‚àö(Val Loss) = 1.11272264 | Current Learning Rate: 0.0001\n",
      "Epoch 967/1000 | Train Loss=4387.98844401 | Val Loss=1.26543963 | Data=43.86052577 | Physics=1.85989840 | Val RMSE: 2.12107921 | ‚àö(Val Loss) = 1.12491763 | Current Learning Rate: 0.0001\n",
      "Epoch 968/1000 | Train Loss=4391.15877279 | Val Loss=1.29353571 | Data=43.89216805 | Physics=2.00296155 | Val RMSE: 2.12124157 | ‚àö(Val Loss) = 1.13733709 | Current Learning Rate: 0.0001\n",
      "Epoch 969/1000 | Train Loss=4385.42293294 | Val Loss=1.25231624 | Data=43.83483378 | Physics=1.88865833 | Val RMSE: 2.12134933 | ‚àö(Val Loss) = 1.11906934 | Current Learning Rate: 0.0001\n",
      "Epoch 970/1000 | Train Loss=4390.32657878 | Val Loss=1.23674953 | Data=43.88387871 | Physics=1.88726220 | Val RMSE: 2.12136173 | ‚àö(Val Loss) = 1.11209238 | Current Learning Rate: 0.0001\n",
      "Epoch 971/1000 | Train Loss=4389.14526367 | Val Loss=1.24832428 | Data=43.87202136 | Physics=1.95611085 | Val RMSE: 2.12173486 | ‚àö(Val Loss) = 1.11728430 | Current Learning Rate: 0.0001\n",
      "Epoch 972/1000 | Train Loss=4384.06689453 | Val Loss=1.26114643 | Data=43.82128779 | Physics=1.86844429 | Val RMSE: 2.12184167 | ‚àö(Val Loss) = 1.12300777 | Current Learning Rate: 0.0001\n",
      "Epoch 973/1000 | Train Loss=4385.60555013 | Val Loss=1.24115825 | Data=43.83666611 | Physics=1.93350881 | Val RMSE: 2.12142754 | ‚àö(Val Loss) = 1.11407280 | Current Learning Rate: 0.0001\n",
      "Epoch 974/1000 | Train Loss=4390.88614909 | Val Loss=1.23248839 | Data=43.88947868 | Physics=1.90660261 | Val RMSE: 2.12107110 | ‚àö(Val Loss) = 1.11017489 | Current Learning Rate: 0.0001\n",
      "Epoch 975/1000 | Train Loss=4385.71809896 | Val Loss=1.26556802 | Data=43.83780034 | Physics=1.89696885 | Val RMSE: 2.12082815 | ‚àö(Val Loss) = 1.12497473 | Current Learning Rate: 0.0001\n",
      "Epoch 976/1000 | Train Loss=4394.36189779 | Val Loss=1.30131662 | Data=43.92416382 | Physics=2.05592379 | Val RMSE: 2.12084460 | ‚àö(Val Loss) = 1.14075267 | Current Learning Rate: 0.0001\n",
      "Epoch 977/1000 | Train Loss=4388.10709635 | Val Loss=1.25926757 | Data=43.86170069 | Physics=1.90783259 | Val RMSE: 2.12093282 | ‚àö(Val Loss) = 1.12217093 | Current Learning Rate: 0.0001\n",
      "Epoch 978/1000 | Train Loss=4390.55297852 | Val Loss=1.23947227 | Data=43.88611857 | Physics=1.97724024 | Val RMSE: 2.12098408 | ‚àö(Val Loss) = 1.11331594 | Current Learning Rate: 0.0001\n",
      "Epoch 979/1000 | Train Loss=4392.31933594 | Val Loss=1.23038280 | Data=43.90379333 | Physics=1.94661538 | Val RMSE: 2.12112808 | ‚àö(Val Loss) = 1.10922623 | Current Learning Rate: 0.0001\n",
      "Epoch 980/1000 | Train Loss=4390.90576172 | Val Loss=1.25866997 | Data=43.88963381 | Physics=1.97186104 | Val RMSE: 2.12134075 | ‚àö(Val Loss) = 1.12190461 | Current Learning Rate: 0.0001\n",
      "Epoch 981/1000 | Train Loss=4398.49877930 | Val Loss=1.29200113 | Data=43.96556918 | Physics=1.96907582 | Val RMSE: 2.12145567 | ‚àö(Val Loss) = 1.13666224 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        [  0.9646,  -4.0112, -18.3159],\n",
      "        ...,\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630]]) \n",
      " Prediction :  [[  0.95371073  -3.9243014  -19.244375  ]\n",
      " [  0.9537109   -3.9243014  -19.244375  ]\n",
      " [  0.9537111   -3.9243014  -19.244375  ]\n",
      " ...\n",
      " [  0.95386803  -3.9243946  -19.245125  ]\n",
      " [  0.9538681   -3.9243946  -19.245129  ]\n",
      " [  0.95386845  -3.924395   -19.245129  ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        [  0.9418,  -4.4592, -22.7630],\n",
      "        ...,\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9538686   -3.924395   -19.24513   ]\n",
      " [  0.9538688   -3.924395   -19.24513   ]\n",
      " [  0.95386916  -3.9243953  -19.245134  ]\n",
      " ...\n",
      " [  0.9542296   -3.9246006  -19.24692   ]\n",
      " [  0.95423055  -3.924601   -19.246927  ]\n",
      " [  0.95423156  -3.9246016  -19.246931  ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191],\n",
      "        [  0.9476,  -4.0221, -18.8191]]) \n",
      " Prediction :  [[  0.9542326   -3.9246023  -19.246937  ]\n",
      " [  0.9542337   -3.9246027  -19.246944  ]\n",
      " [  0.9542347   -3.9246035  -19.246948  ]\n",
      " [  0.95423573  -3.924604   -19.246954  ]\n",
      " [  0.95423687  -3.9246047  -19.24696   ]\n",
      " [  0.9542381   -3.9246054  -19.246967  ]\n",
      " [  0.95423925  -3.9246058  -19.246973  ]\n",
      " [  0.9542401   -3.9246063  -19.246975  ]\n",
      " [  0.9542408   -3.9246066  -19.246979  ]\n",
      " [  0.95424163  -3.924607   -19.246983  ]\n",
      " [  0.9542427   -3.9246078  -19.246988  ]\n",
      " [  0.9542436   -3.9246082  -19.246992  ]\n",
      " [  0.95424455  -3.9246087  -19.246998  ]\n",
      " [  0.9542454   -3.9246092  -19.247002  ]\n",
      " [  0.9542463   -3.9246097  -19.247007  ]\n",
      " [  0.95424706  -3.92461    -19.24701   ]\n",
      " [  0.9542479   -3.9246104  -19.247015  ]\n",
      " [  0.9542488   -3.9246109  -19.247017  ]\n",
      " [  0.9542497   -3.9246113  -19.247023  ]\n",
      " [  0.9542506   -3.9246118  -19.247026  ]\n",
      " [  0.9542514   -3.9246123  -19.247032  ]\n",
      " [  0.9542525   -3.924613   -19.247038  ]\n",
      " [  0.95425344  -3.9246135  -19.247042  ]\n",
      " [  0.9542538   -3.9246135  -19.24704   ]\n",
      " [  0.9542547   -3.9246142  -19.24705   ]\n",
      " [  0.954256    -3.924615   -19.247055  ]\n",
      " [  0.9542571   -3.9246156  -19.247063  ]\n",
      " [  0.9542581   -3.924616   -19.247066  ]\n",
      " [  0.95425904  -3.9246166  -19.247072  ]\n",
      " [  0.9542601   -3.9246173  -19.247076  ]\n",
      " [  0.95426106  -3.9246178  -19.247082  ]\n",
      " [  0.95426214  -3.9246182  -19.247087  ]\n",
      " [  0.95426315  -3.924619   -19.247091  ]\n",
      " [  0.95426404  -3.9246192  -19.247097  ]\n",
      " [  0.9542649   -3.9246197  -19.2471    ]\n",
      " [  0.9542659   -3.9246204  -19.247107  ]\n",
      " [  0.9542668   -3.9246209  -19.24711   ]\n",
      " [  0.95426774  -3.9246213  -19.247116  ]\n",
      " [  0.9542688   -3.9246218  -19.247122  ]\n",
      " [  0.9542698   -3.9246225  -19.247126  ]\n",
      " [  0.9542708   -3.924623   -19.247131  ]\n",
      " [  0.95427173  -3.9246235  -19.247135  ]\n",
      " [  0.9542726   -3.924624   -19.24714   ]\n",
      " [  0.9542735   -3.9246244  -19.247145  ]\n",
      " [  0.9542745   -3.924625   -19.247149  ]\n",
      " [  0.95427555  -3.9246256  -19.247156  ]\n",
      " [  0.9542765   -3.924626   -19.24716   ]\n",
      " [  0.95427746  -3.9246266  -19.247166  ]\n",
      " [  0.9542786   -3.9246273  -19.247171  ]\n",
      " [  0.9542796   -3.9246278  -19.247175  ]\n",
      " [  0.9542806   -3.9246283  -19.24718   ]\n",
      " [  0.95428175  -3.924629   -19.247189  ]\n",
      " [  0.954283    -3.9246297  -19.247194  ]\n",
      " [  0.95428354  -3.9246297  -19.247192  ]\n",
      " [  0.95428437  -3.9246302  -19.247202  ]\n",
      " [  0.95428586  -3.9246314  -19.24721   ]\n",
      " [  0.9542872   -3.924632   -19.247217  ]\n",
      " [  0.9542883   -3.9246325  -19.247223  ]\n",
      " [  0.9542892   -3.924633   -19.247225  ]\n",
      " [  0.9542902   -3.9246335  -19.24723   ]\n",
      " [  0.9542912   -3.924634   -19.247234  ]\n",
      " [  0.95429224  -3.9246347  -19.24724   ]\n",
      " [  0.95429313  -3.9246352  -19.247244  ]\n",
      " [  0.9542941   -3.9246356  -19.24725   ]\n",
      " [  0.9542952   -3.9246361  -19.247255  ]\n",
      " [  0.9542961   -3.9246366  -19.24726   ]\n",
      " [  0.954297    -3.924637   -19.247265  ]\n",
      " [  0.954298    -3.9246378  -19.247269  ]\n",
      " [  0.954299    -3.9246383  -19.247274  ]\n",
      " [  0.95430005  -3.9246387  -19.24728   ]\n",
      " [  0.95430106  -3.9246395  -19.247286  ]\n",
      " [  0.9543022   -3.92464    -19.247292  ]\n",
      " [  0.9543029   -3.9246404  -19.247292  ]\n",
      " [  0.95430374  -3.9246407  -19.2473    ]\n",
      " [  0.95430493  -3.9246414  -19.247305  ]\n",
      " [  0.95430595  -3.924642   -19.24731   ]\n",
      " [  0.95430696  -3.9246426  -19.247316  ]\n",
      " [  0.9543079   -3.924643   -19.24732   ]\n",
      " [  0.95430887  -3.9246435  -19.247326  ]\n",
      " [  0.95431     -3.9246442  -19.247332  ]\n",
      " [  0.95431095  -3.9246447  -19.247335  ]\n",
      " [  0.95431197  -3.9246452  -19.247341  ]\n",
      " [  0.95431316  -3.924646   -19.247349  ]\n",
      " [  0.9543142   -3.9246464  -19.247353  ]\n",
      " [  0.9543152   -3.9246469  -19.247358  ]\n",
      " [  0.95431614  -3.9246476  -19.247362  ]\n",
      " [  0.9543171   -3.924648   -19.247368  ]\n",
      " [  0.9543183   -3.9246485  -19.247374  ]\n",
      " [  0.95431924  -3.9246492  -19.247377  ]\n",
      " [  0.95432025  -3.9246497  -19.247383  ]\n",
      " [  0.9543212   -3.9246502  -19.247387  ]\n",
      " [  0.95432216  -3.9246507  -19.247393  ]\n",
      " [  0.9543233   -3.9246514  -19.2474    ]\n",
      " [  0.95432425  -3.9246519  -19.247404  ]\n",
      " [  0.95432514  -3.9246523  -19.247408  ]\n",
      " [  0.95432615  -3.9246528  -19.247414  ]\n",
      " [  0.9543272   -3.9246533  -19.24742   ]\n",
      " [  0.9543283   -3.924654   -19.247425  ]\n",
      " [  0.9543293   -3.9246545  -19.247429  ]\n",
      " [  0.95433027  -3.924655   -19.247435  ]\n",
      " [  0.9543312   -3.9246554  -19.247438  ]\n",
      " [  0.9543322   -3.924656   -19.247444  ]\n",
      " [  0.95433325  -3.9246566  -19.24745   ]\n",
      " [  0.9543342   -3.924657   -19.247454  ]\n",
      " [  0.95433545  -3.9246578  -19.247463  ]\n",
      " [  0.95433664  -3.9246585  -19.247469  ]\n",
      " [  0.9543376   -3.924659   -19.247473  ]\n",
      " [  0.95433795  -3.924659   -19.24747   ]\n",
      " [  0.9543389   -3.9246595  -19.24748   ]\n",
      " [  0.9543402   -3.9246604  -19.247484  ]\n",
      " [  0.9543413   -3.924661   -19.247492  ]\n",
      " [  0.95434254  -3.9246616  -19.2475    ]\n",
      " [  0.9543436   -3.924662   -19.247503  ]\n",
      " [  0.9543447   -3.9246628  -19.247509  ]\n",
      " [  0.9543457   -3.9246633  -19.247513  ]\n",
      " [  0.95434666  -3.9246638  -19.247519  ]\n",
      " [  0.95434785  -3.9246645  -19.247526  ]\n",
      " [  0.95434886  -3.924665   -19.24753   ]\n",
      " [  0.9543499   -3.9246655  -19.247536  ]\n",
      " [  0.95435107  -3.9246662  -19.247543  ]\n",
      " [  0.9543521   -3.9246666  -19.247547  ]\n",
      " [  0.95435315  -3.9246671  -19.247553  ]\n",
      " [  0.9543543   -3.9246678  -19.247559  ]\n",
      " [  0.95435536  -3.9246683  -19.247564  ]\n",
      " [  0.9543564   -3.924669   -19.24757   ]\n",
      " [  0.95435756  -3.9246697  -19.247576  ]\n",
      " [  0.95435864  -3.9246702  -19.24758   ]\n",
      " [  0.95435965  -3.9246707  -19.247587  ]\n",
      " [  0.95436084  -3.9246714  -19.247593  ]\n",
      " [  0.9543619   -3.924672   -19.247597  ]\n",
      " [  0.9543629   -3.9246724  -19.247602  ]\n",
      " [  0.95436394  -3.9246728  -19.247606  ]\n",
      " [  0.9543649   -3.9246733  -19.247612  ]\n",
      " [  0.954366    -3.924674   -19.24762   ]\n",
      " [  0.95436704  -3.9246745  -19.247623  ]\n",
      " [  0.9543675   -3.9246747  -19.247622  ]\n",
      " [  0.95436823  -3.924675   -19.24763   ]\n",
      " [  0.9543694   -3.924676   -19.247635  ]\n",
      " [  0.9543707   -3.9246767  -19.247643  ]\n",
      " [  0.95437175  -3.9246771  -19.247646  ]\n",
      " [  0.95437276  -3.9246776  -19.247654  ]\n",
      " [  0.95437396  -3.9246783  -19.24766   ]\n",
      " [  0.954375    -3.9246788  -19.247665  ]\n",
      " [  0.95437604  -3.9246793  -19.247671  ]\n",
      " [  0.9543772   -3.92468    -19.247677  ]\n",
      " [  0.95437807  -3.9246805  -19.247679  ]\n",
      " [  0.9543791   -3.924681   -19.247686  ]\n",
      " [  0.95438015  -3.9246814  -19.24769   ]\n",
      " [  0.95438117  -3.9246821  -19.247696  ]\n",
      " [  0.9543823   -3.9246826  -19.247704  ]\n",
      " [  0.9543834   -3.9246833  -19.247707  ]\n",
      " [  0.9543844   -3.9246838  -19.247713  ]\n",
      " [  0.9543856   -3.9246845  -19.24772   ]\n",
      " [  0.9543867   -3.924685   -19.247726  ]\n",
      " [  0.9543878   -3.9246855  -19.24773   ]\n",
      " [  0.9543889   -3.9246862  -19.247738  ]\n",
      " [  0.9543899   -3.9246867  -19.247742  ]\n",
      " [  0.95439094  -3.9246871  -19.247747  ]\n",
      " [  0.9543919   -3.9246876  -19.247751  ]\n",
      " [  0.9543926   -3.924688   -19.247753  ]\n",
      " [  0.9543937   -3.9246886  -19.24776   ]\n",
      " [  0.9543948   -3.9246893  -19.247766  ]\n",
      " [  0.95439583  -3.9246898  -19.247772  ]\n",
      " [  0.954397    -3.9246905  -19.247778  ]\n",
      " [  0.95439804  -3.924691   -19.247784  ]\n",
      " [  0.95439905  -3.9246914  -19.24779   ]\n",
      " [  0.95440006  -3.924692   -19.247793  ]\n",
      " [  0.954401    -3.9246924  -19.247799  ]\n",
      " [  0.95440215  -3.924693   -19.247805  ]\n",
      " [  0.95440316  -3.9246936  -19.247808  ]\n",
      " [  0.9544041   -3.924694   -19.247814  ]\n",
      " [  0.9544053   -3.9246948  -19.247822  ]\n",
      " [  0.9544063   -3.9246953  -19.247826  ]\n",
      " [  0.95440733  -3.9246957  -19.247831  ]\n",
      " [  0.9544083   -3.9246962  -19.247835  ]\n",
      " [  0.9544087   -3.9246964  -19.247833  ]\n",
      " [  0.9544094   -3.9246967  -19.24784   ]\n",
      " [  0.9544106   -3.9246974  -19.247847  ]\n",
      " [  0.9544119   -3.9246984  -19.247856  ]\n",
      " [  0.954413    -3.9246988  -19.24786   ]\n",
      " [  0.9544139   -3.924699   -19.247864  ]\n",
      " [  0.95441496  -3.9246998  -19.247871  ]\n",
      " [  0.9544161   -3.9247005  -19.247875  ]\n",
      " [  0.9544171   -3.924701   -19.24788   ]\n",
      " [  0.9544181   -3.9247015  -19.247887  ]\n",
      " [  0.9544191   -3.924702   -19.24789   ]\n",
      " [  0.9544202   -3.9247026  -19.247898  ]\n",
      " [  0.9544212   -3.9247031  -19.247902  ]\n",
      " [  0.9544222   -3.9247036  -19.247908  ]\n",
      " [  0.95442337  -3.9247043  -19.247915  ]\n",
      " [  0.9544244   -3.9247048  -19.24792   ]\n",
      " [  0.9544254   -3.9247053  -19.247925  ]\n",
      " [  0.95442635  -3.9247057  -19.247929  ]\n",
      " [  0.95442724  -3.9247062  -19.247934  ]\n",
      " [  0.9544282   -3.9247067  -19.247938  ]\n",
      " [  0.9544292   -3.9247072  -19.247944  ]\n",
      " [  0.95443034  -3.924708   -19.24795   ]\n",
      " [  0.9544313   -3.9247084  -19.247953  ]\n",
      " [  0.95443225  -3.9247088  -19.24796   ]\n",
      " [  0.9544334   -3.9247093  -19.247965  ]\n",
      " [  0.95443434  -3.9247098  -19.247969  ]\n",
      " [  0.9544353   -3.9247103  -19.247974  ]\n",
      " [  0.9544364   -3.924711   -19.247982  ]\n",
      " [  0.95443755  -3.9247117  -19.247988  ]\n",
      " [  0.95443857  -3.9247122  -19.247992  ]\n",
      " [  0.9544397   -3.9247127  -19.248     ]\n",
      " [  0.9544407   -3.9247131  -19.248003  ]\n",
      " [  0.95444167  -3.9247136  -19.248009  ]\n",
      " [  0.95444286  -3.9247143  -19.248014  ]\n",
      " [  0.9544439   -3.9247148  -19.248018  ]\n",
      " [  0.9544449   -3.9247153  -19.248026  ]\n",
      " [  0.95444584  -3.9247158  -19.248028  ]\n",
      " [  0.95444673  -3.9247162  -19.248034  ]\n",
      " [  0.9544475   -3.9247167  -19.248035  ]\n",
      " [  0.9544484   -3.9247172  -19.248041  ]\n",
      " [  0.95444953  -3.9247177  -19.248049  ]\n",
      " [  0.9544505   -3.9247181  -19.248053  ]\n",
      " [  0.9544515   -3.9247189  -19.248058  ]\n",
      " [  0.9544524   -3.924719   -19.248062  ]\n",
      " [  0.9544533   -3.9247196  -19.248068  ]\n",
      " [  0.9544544   -3.9247203  -19.248074  ]\n",
      " [  0.9544554   -3.9247208  -19.248077  ]\n",
      " [  0.9544563   -3.9247212  -19.248083  ]\n",
      " [  0.9544574   -3.924722   -19.248089  ]\n",
      " [  0.9544584   -3.9247224  -19.248093  ]\n",
      " [  0.95445937  -3.924723   -19.248098  ]\n",
      " [  0.95446056  -3.9247234  -19.248106  ]\n",
      " [  0.9544615   -3.9247239  -19.24811   ]\n",
      " [  0.9544625   -3.9247243  -19.248116  ]\n",
      " [  0.9544637   -3.924725   -19.248121  ]\n",
      " [  0.9544647   -3.9247255  -19.248127  ]\n",
      " [  0.9544657   -3.924726   -19.248133  ]\n",
      " [  0.95446664  -3.9247265  -19.248135  ]\n",
      " [  0.9544676   -3.924727   -19.24814   ]\n",
      " [  0.95446867  -3.9247277  -19.248146  ]\n",
      " [  0.9544696   -3.9247282  -19.24815   ]\n",
      " [  0.9544706   -3.9247286  -19.248156  ]\n",
      " [  0.9544715   -3.924729   -19.24816   ]\n",
      " [  0.9544724   -3.9247296  -19.248165  ]\n",
      " [  0.9544735   -3.92473    -19.24817   ]\n",
      " [  0.9544739   -3.92473    -19.248167  ]\n",
      " [  0.9544746   -3.9247305  -19.248177  ]\n",
      " [  0.95447576  -3.9247313  -19.24818   ]\n",
      " [  0.9544768   -3.9247317  -19.248186  ]\n",
      " [  0.9544779   -3.9247324  -19.248194  ]\n",
      " [  0.9544789   -3.924733   -19.248198  ]\n",
      " [  0.95447934  -3.924733   -19.248196  ]\n",
      " [  0.9544803   -3.9247334  -19.248205  ]\n",
      " [  0.95448154  -3.924734   -19.24821   ]\n",
      " [  0.9544826   -3.9247348  -19.248217  ]\n",
      " [  0.9544838   -3.9247353  -19.248224  ]\n",
      " [  0.9544848   -3.9247358  -19.248228  ]\n",
      " [  0.95448583  -3.9247365  -19.248234  ]\n",
      " [  0.954487    -3.924737   -19.248241  ]\n",
      " [  0.95448804  -3.9247375  -19.248245  ]\n",
      " [  0.95448905  -3.924738   -19.248251  ]\n",
      " [  0.95449     -3.9247384  -19.248253  ]\n",
      " [  0.95449096  -3.924739   -19.248259  ]\n",
      " [  0.9544918   -3.9247394  -19.248262  ]\n",
      " [  0.9544927   -3.9247398  -19.248266  ]\n",
      " [  0.9544935   -3.92474    -19.24827   ]\n",
      " [  0.95449436  -3.9247406  -19.248274  ]\n",
      " [  0.9544951   -3.9247408  -19.248276  ]\n",
      " [  0.95449585  -3.9247413  -19.248281  ]\n",
      " [  0.9544969   -3.924742   -19.248287  ]\n",
      " [  0.9544978   -3.9247422  -19.248291  ]\n",
      " [  0.9544987   -3.9247427  -19.248295  ]\n",
      " [  0.95449954  -3.9247432  -19.248299  ]\n",
      " [  0.9545004   -3.9247437  -19.248302  ]\n",
      " [  0.9545014   -3.9247441  -19.248308  ]\n",
      " [  0.9545023   -3.9247446  -19.248312  ]\n",
      " [  0.9545032   -3.924745   -19.248318  ]\n",
      " [  0.954504    -3.9247453  -19.24832   ]\n",
      " [  0.9545048   -3.9247458  -19.248323  ]\n",
      " [  0.95450574  -3.9247463  -19.24833   ]\n",
      " [  0.95450664  -3.9247468  -19.248333  ]\n",
      " [  0.9545075   -3.9247472  -19.248337  ]\n",
      " [  0.95450836  -3.9247475  -19.24834   ]\n",
      " [  0.9545092   -3.924748   -19.248344  ]\n",
      " [  0.9545102   -3.9247484  -19.24835   ]\n",
      " [  0.9545111   -3.924749   -19.248352  ]\n",
      " [  0.95451194  -3.9247494  -19.248358  ]\n",
      " [  0.9545128   -3.9247496  -19.24836   ]] \n",
      "\n",
      "Final Test RMSE:  0.9029234250386556\n",
      "Epoch 982/1000 | Train Loss=4389.89428711 | Val Loss=1.25549781 | Data=43.87954203 | Physics=1.94983785 | Val RMSE: 2.12129903 | ‚àö(Val Loss) = 1.12048995 | Current Learning Rate: 0.0001\n",
      "Epoch 983/1000 | Train Loss=4386.04109701 | Val Loss=1.23749495 | Data=43.84103330 | Physics=1.89365224 | Val RMSE: 2.12120819 | ‚àö(Val Loss) = 1.11242747 | Current Learning Rate: 0.0001\n",
      "Epoch 984/1000 | Train Loss=4392.76725260 | Val Loss=1.26570654 | Data=43.90831184 | Physics=1.88032471 | Val RMSE: 2.12116861 | ‚àö(Val Loss) = 1.12503624 | Current Learning Rate: 0.0001\n",
      "Epoch 985/1000 | Train Loss=4388.45157878 | Val Loss=1.29986036 | Data=43.86510404 | Physics=1.93952339 | Val RMSE: 2.12096834 | ‚àö(Val Loss) = 1.14011419 | Current Learning Rate: 0.0001\n",
      "Epoch 986/1000 | Train Loss=4389.62613932 | Val Loss=1.25891423 | Data=43.87685076 | Physics=1.99439858 | Val RMSE: 2.12097883 | ‚àö(Val Loss) = 1.12201345 | Current Learning Rate: 0.0001\n",
      "Epoch 987/1000 | Train Loss=4392.36018880 | Val Loss=1.24016607 | Data=43.90424093 | Physics=1.85948680 | Val RMSE: 2.12097597 | ‚àö(Val Loss) = 1.11362743 | Current Learning Rate: 0.0001\n",
      "Epoch 988/1000 | Train Loss=4390.56022135 | Val Loss=1.23335004 | Data=43.88622983 | Physics=1.85114204 | Val RMSE: 2.12115407 | ‚àö(Val Loss) = 1.11056292 | Current Learning Rate: 0.0001\n",
      "Epoch 989/1000 | Train Loss=4387.37459310 | Val Loss=1.26269174 | Data=43.85432816 | Physics=1.99670223 | Val RMSE: 2.12129211 | ‚àö(Val Loss) = 1.12369561 | Current Learning Rate: 0.0001\n",
      "Epoch 990/1000 | Train Loss=4393.96850586 | Val Loss=1.29560518 | Data=43.92029699 | Physics=1.92206370 | Val RMSE: 2.12104297 | ‚àö(Val Loss) = 1.13824654 | Current Learning Rate: 0.0001\n",
      "Epoch 991/1000 | Train Loss=4389.85213216 | Val Loss=1.25733447 | Data=43.87908936 | Physics=2.02124378 | Val RMSE: 2.12106037 | ‚àö(Val Loss) = 1.12130928 | Current Learning Rate: 0.0001\n",
      "Epoch 992/1000 | Train Loss=4391.22664388 | Val Loss=1.23776031 | Data=43.89287758 | Physics=1.90945812 | Val RMSE: 2.12111163 | ‚àö(Val Loss) = 1.11254680 | Current Learning Rate: 0.0001\n",
      "Epoch 993/1000 | Train Loss=4386.44889323 | Val Loss=1.25640118 | Data=43.84506543 | Physics=1.92887520 | Val RMSE: 2.12146926 | ‚àö(Val Loss) = 1.12089300 | Current Learning Rate: 0.0001\n",
      "Epoch 994/1000 | Train Loss=4385.10001628 | Val Loss=1.28113914 | Data=43.83162689 | Physics=1.89988128 | Val RMSE: 2.12182450 | ‚àö(Val Loss) = 1.13187420 | Current Learning Rate: 0.0001\n",
      "Epoch 995/1000 | Train Loss=4389.27001953 | Val Loss=1.25032496 | Data=43.87330373 | Physics=1.96692161 | Val RMSE: 2.12135816 | ‚àö(Val Loss) = 1.11817932 | Current Learning Rate: 0.0001\n",
      "Epoch 996/1000 | Train Loss=4385.43766276 | Val Loss=1.27360213 | Data=43.83499972 | Physics=1.90182514 | Val RMSE: 2.12095952 | ‚àö(Val Loss) = 1.12853980 | Current Learning Rate: 0.0001\n",
      "Epoch 997/1000 | Train Loss=4389.40600586 | Val Loss=1.30572522 | Data=43.87467957 | Physics=1.88957745 | Val RMSE: 2.12085629 | ‚àö(Val Loss) = 1.14268339 | Current Learning Rate: 0.0001\n",
      "Epoch 998/1000 | Train Loss=4393.30875651 | Val Loss=1.26156592 | Data=43.91368675 | Physics=1.95268964 | Val RMSE: 2.12094092 | ‚àö(Val Loss) = 1.12319458 | Current Learning Rate: 0.0001\n",
      "Epoch 999/1000 | Train Loss=4394.51546224 | Val Loss=1.24128294 | Data=43.92571831 | Physics=1.99284181 | Val RMSE: 2.12108731 | ‚àö(Val Loss) = 1.11412883 | Current Learning Rate: 0.0001\n",
      "‚úÖ Saved last model at epoch 1000 \n",
      "Epoch 1000/1000 | Train Loss=4387.01725260 | Val Loss=1.23220217 | Data=43.85077095 | Physics=1.91353624 | Val RMSE: 2.12133455 | ‚àö(Val Loss) = 1.11004603 | Current Learning Rate: 0.0001\n",
      "‚úÖ Metrics saved successfully!\n",
      "Plot losses after training 3:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE :  2.708770275115967\n",
      "Cutoff SoH :  0.7\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n",
      " X_['train'] shape : torch.Size([46523, 100, 1]) , y_['train'] shape : torch.Size([46523, 3]) Ôºåy_2['train'] shape: torch.Size([46523, 1])\n",
      "load : \n",
      "['train']loader lengths :  15\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n",
      " X_['val'] shape : torch.Size([8451, 100, 1]) , y_['val'] shape : torch.Size([8451, 3]) Ôºåy_2['val'] shape: torch.Size([8451, 1])\n",
      "load : \n",
      "['val']loader lengths :  3\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n",
      " X_['test'] shape : torch.Size([7071, 100, 1]) , y_['test'] shape : torch.Size([7071, 3]) Ôºåy_2['test'] shape: torch.Size([7071, 1])\n",
      "load : \n",
      "['test']loader lengths :  3\n",
      "## üß† Model\n",
      "Last model window :  last_model_window_100_model_pinn_data_mid.pth\n",
      "üöÄ Initializing model output to: k=0.9732674956321716, a=-3.752986431121826, b=-13.829436302185059\n",
      "‚úÖ Model Output Parameters Initialized!\n",
      "##\n",
      "        ### üìà Gompertz Function (Physics Law)\n",
      "        \n",
      "        * `x`: Time (or cycle number)\n",
      "        \n",
      "        * `k`: Max value (e.g., max capacity)\n",
      "        \n",
      "        * `a`, `b`: Shape parameters\n",
      "## üß† Loss Functions\n",
      "\n",
      "## ‚öôÔ∏è 1. Data-Informed Loss Function\n",
      "        a data loss (what the LSTM learns from data)\n",
      "        \n",
      "        * Mean Squared Error for Training\n",
      "        * RMSE for autoregressive approximation of compound error\n",
      "        \n",
      "        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n",
      "        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n",
      "        \n",
      "        * `alpha`: controls how strongly physics is enforced.\n",
      "## üõ†Ô∏è Parameter Strategy\n",
      "## üîÅ Training Loop\n",
      "‚úÖ Saved best model at epoch 1 (Val Loss = 1.50337601)\n",
      "Epoch 1/1000 | Train Loss=13346.90195312 | Val Loss=1.50337601 | Data=133.44672852 | Physics=2.16402529 | Val RMSE: 1.22154307 | ‚àö(Val Loss) = 1.22612238 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9686268   -3.744859   -13.824992  ]\n",
      " [  0.96862674  -3.744859   -13.824992  ]\n",
      " [  0.9686266   -3.744859   -13.82499   ]\n",
      " ...\n",
      " [  0.9684413   -3.7447348  -13.824185  ]\n",
      " [  0.9684411   -3.7447348  -13.824183  ]\n",
      " [  0.96844083  -3.7447345  -13.8241825 ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9684406   -3.7447343  -13.824181  ]\n",
      " [  0.96844023  -3.744734   -13.82418   ]\n",
      " [  0.96844     -3.7447338  -13.82418   ]\n",
      " ...\n",
      " [  0.9685086   -3.7447796  -13.824478  ]\n",
      " [  0.9685085   -3.7447798  -13.824477  ]\n",
      " [  0.9685082   -3.7447793  -13.824477  ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.96850836  -3.7447796  -13.824476  ]\n",
      " [  0.9685081   -3.7447793  -13.824476  ]\n",
      " [  0.96850795  -3.7447793  -13.824474  ]\n",
      " ...\n",
      " [  0.9682743   -3.7446234  -13.823456  ]\n",
      " [  0.9682738   -3.744623   -13.823453  ]\n",
      " [  0.9682733   -3.7446222  -13.823453  ]] \n",
      "\n",
      "Final Test RMSE:  0.45891404151916504\n",
      "‚úÖ Saved best model at epoch 2 (Val Loss = 1.50268122)\n",
      "Epoch 2/1000 | Train Loss=13343.80227865 | Val Loss=1.50268122 | Data=133.41597951 | Physics=2.19973815 | Val RMSE: 1.22183299 | ‚àö(Val Loss) = 1.22583902 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 3 (Val Loss = 1.48155940)\n",
      "Epoch 3/1000 | Train Loss=13323.27669271 | Val Loss=1.48155940 | Data=133.21073659 | Physics=2.20530365 | Val RMSE: 1.22075129 | ‚àö(Val Loss) = 1.21719325 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 4 (Val Loss = 1.47436714)\n",
      "Epoch 4/1000 | Train Loss=13378.83014323 | Val Loss=1.47436714 | Data=133.76632487 | Physics=2.06016220 | Val RMSE: 1.21708250 | ‚àö(Val Loss) = 1.21423519 | Current Learning Rate: 0.002\n",
      "Epoch 5/1000 | Train Loss=13364.15175781 | Val Loss=1.47961299 | Data=133.61945801 | Physics=2.19744164 | Val RMSE: 1.21942663 | ‚àö(Val Loss) = 1.21639347 | Current Learning Rate: 0.002\n",
      "Epoch 6/1000 | Train Loss=13343.17552083 | Val Loss=1.51857364 | Data=133.40968068 | Physics=2.28029416 | Val RMSE: 1.22071481 | ‚àö(Val Loss) = 1.23230422 | Current Learning Rate: 0.002\n",
      "Epoch 7/1000 | Train Loss=13339.80260417 | Val Loss=1.48229420 | Data=133.37598419 | Physics=2.23407230 | Val RMSE: 1.22289491 | ‚àö(Val Loss) = 1.21749508 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 8 (Val Loss = 1.47327864)\n",
      "Epoch 8/1000 | Train Loss=13354.37858073 | Val Loss=1.47327864 | Data=133.52176208 | Physics=2.18230783 | Val RMSE: 1.22185278 | ‚àö(Val Loss) = 1.21378696 | Current Learning Rate: 0.002\n",
      "Epoch 9/1000 | Train Loss=13374.00514323 | Val Loss=1.49021204 | Data=133.71800588 | Physics=2.22900930 | Val RMSE: 1.22270656 | ‚àö(Val Loss) = 1.22074246 | Current Learning Rate: 0.002\n",
      "Epoch 10/1000 | Train Loss=13350.57753906 | Val Loss=1.48337694 | Data=133.48373566 | Physics=2.25542371 | Val RMSE: 1.21966243 | ‚àö(Val Loss) = 1.21793962 | Current Learning Rate: 0.002\n",
      "Epoch 11/1000 | Train Loss=13328.96106771 | Val Loss=1.48504770 | Data=133.26758525 | Physics=2.17790342 | Val RMSE: 1.21985459 | ‚àö(Val Loss) = 1.21862531 | Current Learning Rate: 0.002\n",
      "Epoch 12/1000 | Train Loss=13358.19042969 | Val Loss=1.48369642 | Data=133.55986938 | Physics=2.25395440 | Val RMSE: 1.21873248 | ‚àö(Val Loss) = 1.21807075 | Current Learning Rate: 0.002\n",
      "Epoch 13/1000 | Train Loss=13341.26946615 | Val Loss=1.52395105 | Data=133.39061279 | Physics=2.27572870 | Val RMSE: 1.22024810 | ‚àö(Val Loss) = 1.23448408 | Current Learning Rate: 0.002\n",
      "Epoch 14/1000 | Train Loss=13345.28932292 | Val Loss=1.47677239 | Data=133.43087463 | Physics=2.15866395 | Val RMSE: 1.22076130 | ‚àö(Val Loss) = 1.21522522 | Current Learning Rate: 0.002\n",
      "Epoch 15/1000 | Train Loss=13370.78841146 | Val Loss=1.47932394 | Data=133.68589274 | Physics=2.06035232 | Val RMSE: 1.22082615 | ‚àö(Val Loss) = 1.21627462 | Current Learning Rate: 0.002\n",
      "Epoch 16/1000 | Train Loss=13341.35462240 | Val Loss=1.48753456 | Data=133.39149933 | Physics=2.25445065 | Val RMSE: 1.22141814 | ‚àö(Val Loss) = 1.21964526 | Current Learning Rate: 0.002\n",
      "Epoch 17/1000 | Train Loss=13317.44980469 | Val Loss=1.49304644 | Data=133.15248769 | Physics=2.14288938 | Val RMSE: 1.22040653 | ‚àö(Val Loss) = 1.22190273 | Current Learning Rate: 0.002\n",
      "Epoch 18/1000 | Train Loss=13346.21894531 | Val Loss=1.47331536 | Data=133.44013570 | Physics=2.25506531 | Val RMSE: 1.22134101 | ‚àö(Val Loss) = 1.21380198 | Current Learning Rate: 0.002\n",
      "Epoch 19/1000 | Train Loss=13337.30455729 | Val Loss=1.48837900 | Data=133.35097961 | Physics=2.27273657 | Val RMSE: 1.22150123 | ‚àö(Val Loss) = 1.21999145 | Current Learning Rate: 0.002\n",
      "Epoch 20/1000 | Train Loss=13354.79335938 | Val Loss=1.48958357 | Data=133.52587636 | Physics=2.23823507 | Val RMSE: 1.21951795 | ‚àö(Val Loss) = 1.22048497 | Current Learning Rate: 0.002\n",
      "Epoch 21/1000 | Train Loss=13352.54876302 | Val Loss=1.48239052 | Data=133.50344950 | Physics=2.20729765 | Val RMSE: 1.21925592 | ‚àö(Val Loss) = 1.21753466 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9721354   -3.749971   -13.824271  ]\n",
      " [  0.97213495  -3.7499704  -13.824269  ]\n",
      " [  0.9721342   -3.7499697  -13.824267  ]\n",
      " ...\n",
      " [  0.97152567  -3.7493556  -13.822574  ]\n",
      " [  0.9715246   -3.7493546  -13.82257   ]\n",
      " [  0.9715238   -3.749354   -13.822568  ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9715226   -3.7493527  -13.822564  ]\n",
      " [  0.97152174  -3.7493517  -13.822562  ]\n",
      " [  0.9715212   -3.749351   -13.822561  ]\n",
      " ...\n",
      " [  0.97174734  -3.7495794  -13.823191  ]\n",
      " [  0.9717464   -3.7495785  -13.823188  ]\n",
      " [  0.97174686  -3.749579   -13.823191  ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.97174627  -3.7495782  -13.823188  ]\n",
      " [  0.9717456   -3.7495778  -13.823186  ]\n",
      " [  0.9717444   -3.7495766  -13.823182  ]\n",
      " ...\n",
      " [  0.97097224  -3.7487962  -13.821031  ]\n",
      " [  0.97097045  -3.7487946  -13.821026  ]\n",
      " [  0.97097033  -3.7487943  -13.821027  ]] \n",
      "\n",
      "Final Test RMSE:  0.4584023058414459\n",
      "Epoch 22/1000 | Train Loss=13336.84355469 | Val Loss=1.49889505 | Data=133.34640503 | Physics=2.19824880 | Val RMSE: 1.22191429 | ‚àö(Val Loss) = 1.22429371 | Current Learning Rate: 0.002\n",
      "Epoch 23/1000 | Train Loss=13333.37968750 | Val Loss=1.48312378 | Data=133.31175944 | Physics=2.24699589 | Val RMSE: 1.22305381 | ‚àö(Val Loss) = 1.21783566 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 24 (Val Loss = 1.46966322)\n",
      "Epoch 24/1000 | Train Loss=13345.50149740 | Val Loss=1.46966322 | Data=133.43297577 | Physics=2.19071060 | Val RMSE: 1.22064948 | ‚àö(Val Loss) = 1.21229672 | Current Learning Rate: 0.002\n",
      "Epoch 25/1000 | Train Loss=13348.62695312 | Val Loss=1.47579821 | Data=133.46425374 | Physics=2.17580225 | Val RMSE: 1.21840715 | ‚àö(Val Loss) = 1.21482432 | Current Learning Rate: 0.002\n",
      "Epoch 26/1000 | Train Loss=13366.19856771 | Val Loss=1.47094436 | Data=133.63995412 | Physics=2.21953893 | Val RMSE: 1.21906793 | ‚àö(Val Loss) = 1.21282494 | Current Learning Rate: 0.002\n",
      "Epoch 27/1000 | Train Loss=13315.65429688 | Val Loss=1.56637116 | Data=133.13448842 | Physics=2.29945980 | Val RMSE: 1.22548234 | ‚àö(Val Loss) = 1.25154757 | Current Learning Rate: 0.002\n",
      "Epoch 28/1000 | Train Loss=13352.68509115 | Val Loss=1.47416119 | Data=133.50481771 | Physics=2.21170818 | Val RMSE: 1.22403693 | ‚àö(Val Loss) = 1.21415043 | Current Learning Rate: 0.002\n",
      "Epoch 29/1000 | Train Loss=13348.13483073 | Val Loss=1.47338581 | Data=133.45932109 | Physics=2.21761540 | Val RMSE: 1.21861398 | ‚àö(Val Loss) = 1.21383107 | Current Learning Rate: 0.002\n",
      "Epoch 30/1000 | Train Loss=13354.63151042 | Val Loss=1.50468508 | Data=133.52426961 | Physics=2.20709827 | Val RMSE: 1.21584415 | ‚àö(Val Loss) = 1.22665608 | Current Learning Rate: 0.002\n",
      "Epoch 31/1000 | Train Loss=13332.96009115 | Val Loss=1.51537796 | Data=133.30754598 | Physics=2.21288603 | Val RMSE: 1.21845484 | ‚àö(Val Loss) = 1.23100686 | Current Learning Rate: 0.002\n",
      "Epoch 32/1000 | Train Loss=13332.68750000 | Val Loss=1.47662624 | Data=133.30477956 | Physics=2.13977164 | Val RMSE: 1.21833169 | ‚àö(Val Loss) = 1.21516514 | Current Learning Rate: 0.002\n",
      "Epoch 33/1000 | Train Loss=13361.81595052 | Val Loss=1.47864199 | Data=133.59610952 | Physics=2.26024491 | Val RMSE: 1.22388220 | ‚àö(Val Loss) = 1.21599424 | Current Learning Rate: 0.002\n",
      "Epoch 34/1000 | Train Loss=13344.17154948 | Val Loss=1.48455850 | Data=133.41971944 | Physics=2.10140697 | Val RMSE: 1.21949244 | ‚àö(Val Loss) = 1.21842456 | Current Learning Rate: 0.002\n",
      "Epoch 35/1000 | Train Loss=13360.15787760 | Val Loss=1.47502100 | Data=133.57954508 | Physics=2.25331032 | Val RMSE: 1.22112644 | ‚àö(Val Loss) = 1.21450448 | Current Learning Rate: 0.002\n",
      "Epoch 36/1000 | Train Loss=13371.63593750 | Val Loss=1.47343099 | Data=133.69431051 | Physics=2.26768269 | Val RMSE: 1.21988010 | ‚àö(Val Loss) = 1.21384966 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 37 (Val Loss = 1.46667371)\n",
      "Epoch 37/1000 | Train Loss=13388.32578125 | Val Loss=1.46667371 | Data=133.86122945 | Physics=2.20564167 | Val RMSE: 1.21677303 | ‚àö(Val Loss) = 1.21106303 | Current Learning Rate: 0.002\n",
      "Epoch 38/1000 | Train Loss=13368.51217448 | Val Loss=1.48559352 | Data=133.66308085 | Physics=2.19747846 | Val RMSE: 1.22061634 | ‚àö(Val Loss) = 1.21884930 | Current Learning Rate: 0.002\n",
      "Epoch 39/1000 | Train Loss=13366.25651042 | Val Loss=1.49480049 | Data=133.64051463 | Physics=2.22082171 | Val RMSE: 1.22100079 | ‚àö(Val Loss) = 1.22262037 | Current Learning Rate: 0.002\n",
      "Epoch 40/1000 | Train Loss=13342.66328125 | Val Loss=1.46788927 | Data=133.40456543 | Physics=2.21777930 | Val RMSE: 1.22005844 | ‚àö(Val Loss) = 1.21156478 | Current Learning Rate: 0.002\n",
      "Epoch 41/1000 | Train Loss=13325.97141927 | Val Loss=1.46866010 | Data=133.23756714 | Physics=2.26471616 | Val RMSE: 1.22151363 | ‚àö(Val Loss) = 1.21188283 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9750662   -3.753617   -13.852743  ]\n",
      " [  0.97506404  -3.753615   -13.852736  ]\n",
      " [  0.975061    -3.7536118  -13.852727  ]\n",
      " ...\n",
      " [  0.9723202   -3.750856   -13.844993  ]\n",
      " [  0.97231543  -3.7508512  -13.844978  ]\n",
      " [  0.97231203  -3.7508476  -13.84497   ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9723068   -3.7508423  -13.844954  ]\n",
      " [  0.9723028   -3.7508385  -13.844943  ]\n",
      " [  0.9723      -3.7508357  -13.844935  ]\n",
      " ...\n",
      " [  0.9733223   -3.751864   -13.84782   ]\n",
      " [  0.97331816  -3.7518597  -13.847808  ]\n",
      " [  0.97331923  -3.7518609  -13.847814  ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9733163   -3.7518578  -13.847803  ]\n",
      " [  0.9733138   -3.7518554  -13.847796  ]\n",
      " [  0.9733093   -3.7518508  -13.847782  ]\n",
      " ...\n",
      " [  0.9698001   -3.7483194  -13.837883  ]\n",
      " [  0.96979195  -3.7483113  -13.83786   ]\n",
      " [  0.96979016  -3.7483094  -13.837858  ]] \n",
      "\n",
      "Final Test RMSE:  0.4540870189666748\n",
      "Epoch 42/1000 | Train Loss=13334.84166667 | Val Loss=1.46725168 | Data=133.32634989 | Physics=2.22305547 | Val RMSE: 1.21920490 | ‚àö(Val Loss) = 1.21130168 | Current Learning Rate: 0.002\n",
      "Epoch 43/1000 | Train Loss=13322.81542969 | Val Loss=1.46703003 | Data=133.20607147 | Physics=2.22538087 | Val RMSE: 1.22127426 | ‚àö(Val Loss) = 1.21121013 | Current Learning Rate: 0.002\n",
      "Epoch 44/1000 | Train Loss=13345.31210937 | Val Loss=1.48127818 | Data=133.43106232 | Physics=2.17842727 | Val RMSE: 1.22134674 | ‚àö(Val Loss) = 1.21707773 | Current Learning Rate: 0.002\n",
      "Epoch 45/1000 | Train Loss=13356.36432292 | Val Loss=1.46818807 | Data=133.54161631 | Physics=2.15314258 | Val RMSE: 1.21756721 | ‚àö(Val Loss) = 1.21168804 | Current Learning Rate: 0.002\n",
      "Epoch 46/1000 | Train Loss=13311.75143229 | Val Loss=1.54876824 | Data=133.09547831 | Physics=2.18687213 | Val RMSE: 1.22438502 | ‚àö(Val Loss) = 1.24449515 | Current Learning Rate: 0.002\n",
      "Epoch 47/1000 | Train Loss=13365.36959635 | Val Loss=1.51278631 | Data=133.63162994 | Physics=2.20723194 | Val RMSE: 1.22430778 | ‚àö(Val Loss) = 1.22995377 | Current Learning Rate: 0.002\n",
      "Epoch 48/1000 | Train Loss=13349.03359375 | Val Loss=1.63313289 | Data=133.46827443 | Physics=2.13907662 | Val RMSE: 1.22294712 | ‚àö(Val Loss) = 1.27794087 | Current Learning Rate: 0.002\n",
      "Epoch 49/1000 | Train Loss=13336.12571615 | Val Loss=1.47361447 | Data=133.33918355 | Physics=2.26708293 | Val RMSE: 1.22214329 | ‚àö(Val Loss) = 1.21392524 | Current Learning Rate: 0.002\n",
      "Epoch 50/1000 | Train Loss=13361.09843750 | Val Loss=1.49185733 | Data=133.58894399 | Physics=2.22145033 | Val RMSE: 1.22167945 | ‚àö(Val Loss) = 1.22141612 | Current Learning Rate: 0.002\n",
      "Epoch 51/1000 | Train Loss=13341.07324219 | Val Loss=1.47247144 | Data=133.38865865 | Physics=2.22082294 | Val RMSE: 1.22272038 | ‚àö(Val Loss) = 1.21345437 | Current Learning Rate: 0.002\n",
      "Epoch 52/1000 | Train Loss=13360.75774740 | Val Loss=1.54147128 | Data=133.58556264 | Physics=2.08927534 | Val RMSE: 1.22135019 | ‚àö(Val Loss) = 1.24155998 | Current Learning Rate: 0.002\n",
      "Epoch 53/1000 | Train Loss=13363.37467448 | Val Loss=1.49156769 | Data=133.61172282 | Physics=2.23133437 | Val RMSE: 1.22273254 | ‚àö(Val Loss) = 1.22129750 | Current Learning Rate: 0.002\n",
      "Epoch 54/1000 | Train Loss=13359.55579427 | Val Loss=1.49249359 | Data=133.57354940 | Physics=2.17851917 | Val RMSE: 1.22205245 | ‚àö(Val Loss) = 1.22167659 | Current Learning Rate: 0.002\n",
      "Epoch 55/1000 | Train Loss=13396.10364583 | Val Loss=1.47787748 | Data=133.93896383 | Physics=2.23926355 | Val RMSE: 1.22363758 | ‚àö(Val Loss) = 1.21567988 | Current Learning Rate: 0.002\n",
      "Epoch 56/1000 | Train Loss=13376.42649740 | Val Loss=1.48939236 | Data=133.74222972 | Physics=2.22011930 | Val RMSE: 1.22206831 | ‚àö(Val Loss) = 1.22040665 | Current Learning Rate: 0.002\n",
      "Epoch 57/1000 | Train Loss=13382.51445313 | Val Loss=1.47380193 | Data=133.80308736 | Physics=2.21044289 | Val RMSE: 1.22001696 | ‚àö(Val Loss) = 1.21400249 | Current Learning Rate: 0.002\n",
      "Epoch 58/1000 | Train Loss=13395.39863281 | Val Loss=1.47695645 | Data=133.93191223 | Physics=2.26551577 | Val RMSE: 1.22398543 | ‚àö(Val Loss) = 1.21530104 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 59 (Val Loss = 1.45708048)\n",
      "Epoch 59/1000 | Train Loss=13375.79563802 | Val Loss=1.45708048 | Data=133.73588257 | Physics=2.24420334 | Val RMSE: 1.22044802 | ‚àö(Val Loss) = 1.20709586 | Current Learning Rate: 0.002\n",
      "Epoch 60/1000 | Train Loss=13355.50716146 | Val Loss=1.45762297 | Data=133.53295898 | Physics=2.23040079 | Val RMSE: 1.21992970 | ‚àö(Val Loss) = 1.20732057 | Current Learning Rate: 0.002\n",
      "Epoch 61/1000 | Train Loss=13371.33105469 | Val Loss=1.51664432 | Data=133.69123230 | Physics=2.16584806 | Val RMSE: 1.22148943 | ‚àö(Val Loss) = 1.23152113 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.97436965  -3.763478   -13.832946  ]\n",
      " [  0.97436804  -3.7634728  -13.832942  ]\n",
      " [  0.97436565  -3.763465   -13.832937  ]\n",
      " ...\n",
      " [  0.972456    -3.7573364  -13.828877  ]\n",
      " [  0.9724524   -3.7573245  -13.82887   ]\n",
      " [  0.9724502   -3.7573178  -13.828865  ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.97244626  -3.7573044  -13.828856  ]\n",
      " [  0.97244364  -3.7572966  -13.828851  ]\n",
      " [  0.972442    -3.7572913  -13.828848  ]\n",
      " ...\n",
      " [  0.97315377  -3.7595768  -13.83036   ]\n",
      " [  0.9731506   -3.7595658  -13.830354  ]\n",
      " [  0.97315234  -3.7595732  -13.8303585 ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9731499   -3.759564   -13.830352  ]\n",
      " [  0.97314805  -3.7595582  -13.830348  ]\n",
      " [  0.9731444   -3.7595458  -13.83034   ]\n",
      " ...\n",
      " [  0.9707034   -3.7517054  -13.825155  ]\n",
      " [  0.9706976   -3.7516863  -13.825142  ]\n",
      " [  0.97069764  -3.7516885  -13.825144  ]] \n",
      "\n",
      "Final Test RMSE:  0.45710548758506775\n",
      "Epoch 62/1000 | Train Loss=13383.73268229 | Val Loss=1.46077724 | Data=133.81521098 | Physics=2.20486112 | Val RMSE: 1.22082901 | ‚àö(Val Loss) = 1.20862615 | Current Learning Rate: 0.002\n",
      "Epoch 63/1000 | Train Loss=13359.40397135 | Val Loss=1.46841307 | Data=133.57190145 | Physics=2.25249459 | Val RMSE: 1.21924949 | ‚àö(Val Loss) = 1.21178102 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 64 (Val Loss = 1.45459541)\n",
      "Epoch 64/1000 | Train Loss=13385.16569010 | Val Loss=1.45459541 | Data=133.82956543 | Physics=2.21384030 | Val RMSE: 1.21959305 | ‚àö(Val Loss) = 1.20606613 | Current Learning Rate: 0.002\n",
      "Epoch 65/1000 | Train Loss=13361.02389323 | Val Loss=1.47214337 | Data=133.58818054 | Physics=2.14776148 | Val RMSE: 1.21943593 | ‚àö(Val Loss) = 1.21331918 | Current Learning Rate: 0.002\n",
      "Epoch 66/1000 | Train Loss=13364.77552083 | Val Loss=1.45703892 | Data=133.62559865 | Physics=2.21736033 | Val RMSE: 1.21826291 | ‚àö(Val Loss) = 1.20707870 | Current Learning Rate: 0.002\n",
      "Epoch 67/1000 | Train Loss=13419.71809896 | Val Loss=1.51963174 | Data=134.17512207 | Physics=2.17258139 | Val RMSE: 1.22022343 | ‚àö(Val Loss) = 1.23273349 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 68 (Val Loss = 1.44725386)\n",
      "Epoch 68/1000 | Train Loss=13336.25533854 | Val Loss=1.44725386 | Data=133.34026642 | Physics=2.28096596 | Val RMSE: 1.22249746 | ‚àö(Val Loss) = 1.20301867 | Current Learning Rate: 0.002\n",
      "Epoch 69/1000 | Train Loss=13374.99140625 | Val Loss=1.46174125 | Data=133.72727407 | Physics=2.20411279 | Val RMSE: 1.22123313 | ‚àö(Val Loss) = 1.20902491 | Current Learning Rate: 0.002\n",
      "Epoch 70/1000 | Train Loss=13370.68470052 | Val Loss=1.53087429 | Data=133.68452759 | Physics=2.20292175 | Val RMSE: 1.22062874 | ‚àö(Val Loss) = 1.23728502 | Current Learning Rate: 0.002\n",
      "Epoch 71/1000 | Train Loss=13387.80755208 | Val Loss=1.49415267 | Data=133.85583700 | Physics=2.14474122 | Val RMSE: 1.21847868 | ‚àö(Val Loss) = 1.22235537 | Current Learning Rate: 0.002\n",
      "Epoch 72/1000 | Train Loss=13370.97044271 | Val Loss=1.46917065 | Data=133.68755900 | Physics=2.21459555 | Val RMSE: 1.22200513 | ‚àö(Val Loss) = 1.21209347 | Current Learning Rate: 0.002\n",
      "Epoch 73/1000 | Train Loss=13363.86367188 | Val Loss=1.47229171 | Data=133.61633148 | Physics=2.32474289 | Val RMSE: 1.21981847 | ‚àö(Val Loss) = 1.21338034 | Current Learning Rate: 0.002\n",
      "Epoch 74/1000 | Train Loss=13390.11178385 | Val Loss=1.55662402 | Data=133.87878927 | Physics=2.12304185 | Val RMSE: 1.22251523 | ‚àö(Val Loss) = 1.24764740 | Current Learning Rate: 0.002\n",
      "Epoch 75/1000 | Train Loss=13400.35468750 | Val Loss=1.53345525 | Data=133.98129425 | Physics=2.16191916 | Val RMSE: 1.22283697 | ‚àö(Val Loss) = 1.23832762 | Current Learning Rate: 0.002\n",
      "Epoch 76/1000 | Train Loss=13381.80553385 | Val Loss=1.52281940 | Data=133.79579315 | Physics=2.15314211 | Val RMSE: 1.22479081 | ‚àö(Val Loss) = 1.23402572 | Current Learning Rate: 0.002\n",
      "Epoch 77/1000 | Train Loss=13383.77864583 | Val Loss=1.46752373 | Data=133.81553752 | Physics=2.21609020 | Val RMSE: 1.22031522 | ‚àö(Val Loss) = 1.21141398 | Current Learning Rate: 0.002\n",
      "Epoch 78/1000 | Train Loss=13363.65826823 | Val Loss=1.53793502 | Data=133.61441243 | Physics=2.17062213 | Val RMSE: 1.22162616 | ‚àö(Val Loss) = 1.24013507 | Current Learning Rate: 0.002\n",
      "Epoch 79/1000 | Train Loss=13385.28782552 | Val Loss=1.52304832 | Data=133.83047129 | Physics=2.24207594 | Val RMSE: 1.22207177 | ‚àö(Val Loss) = 1.23411846 | Current Learning Rate: 0.002\n",
      "Epoch 80/1000 | Train Loss=13363.98248698 | Val Loss=1.69151052 | Data=133.61714376 | Physics=2.36184101 | Val RMSE: 1.22074652 | ‚àö(Val Loss) = 1.30058086 | Current Learning Rate: 0.002\n",
      "Epoch 81/1000 | Train Loss=13373.39238281 | Val Loss=1.55264386 | Data=133.71166026 | Physics=2.25979619 | Val RMSE: 1.22073686 | ‚àö(Val Loss) = 1.24605131 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 82/1000 | Train Loss=13373.81790365 | Val Loss=1.49868850 | Data=133.71593018 | Physics=2.23685676 | Val RMSE: 1.22096825 | ‚àö(Val Loss) = 1.22420931 | Current Learning Rate: 0.002\n",
      "Epoch 83/1000 | Train Loss=13347.01171875 | Val Loss=1.50843481 | Data=133.44780172 | Physics=2.30390229 | Val RMSE: 1.22198033 | ‚àö(Val Loss) = 1.22818351 | Current Learning Rate: 0.002\n",
      "Epoch 84/1000 | Train Loss=13400.05364583 | Val Loss=1.51566267 | Data=133.97835541 | Physics=2.24748785 | Val RMSE: 1.21946621 | ‚àö(Val Loss) = 1.23112249 | Current Learning Rate: 0.002\n",
      "Epoch 85/1000 | Train Loss=13382.64615885 | Val Loss=1.54798003 | Data=133.80422719 | Physics=2.19000660 | Val RMSE: 1.21977031 | ‚àö(Val Loss) = 1.24417841 | Current Learning Rate: 0.002\n",
      "Epoch 86/1000 | Train Loss=13402.56347656 | Val Loss=1.48424558 | Data=134.00348816 | Physics=2.16539584 | Val RMSE: 1.21948063 | ‚àö(Val Loss) = 1.21829617 | Current Learning Rate: 0.002\n",
      "Epoch 87/1000 | Train Loss=13366.12122396 | Val Loss=1.57060130 | Data=133.63894043 | Physics=2.21293780 | Val RMSE: 1.22051036 | ‚àö(Val Loss) = 1.25323629 | Current Learning Rate: 0.002\n",
      "Epoch 88/1000 | Train Loss=13363.92792969 | Val Loss=1.55649241 | Data=133.61710103 | Physics=2.18351224 | Val RMSE: 1.22249794 | ‚àö(Val Loss) = 1.24759471 | Current Learning Rate: 0.002\n",
      "Epoch 89/1000 | Train Loss=13363.60618490 | Val Loss=1.54808025 | Data=133.61377309 | Physics=2.20992103 | Val RMSE: 1.22107697 | ‚àö(Val Loss) = 1.24421871 | Current Learning Rate: 0.002\n",
      "Epoch 90/1000 | Train Loss=13358.47395833 | Val Loss=1.52200894 | Data=133.56223755 | Physics=2.17081932 | Val RMSE: 1.22170246 | ‚àö(Val Loss) = 1.23369730 | Current Learning Rate: 0.002\n",
      "Epoch 91/1000 | Train Loss=13373.95234375 | Val Loss=1.57791913 | Data=133.71716258 | Physics=2.25180856 | Val RMSE: 1.22035766 | ‚àö(Val Loss) = 1.25615251 | Current Learning Rate: 0.002\n",
      "Epoch 92/1000 | Train Loss=13392.50989583 | Val Loss=1.49670021 | Data=133.90256958 | Physics=2.21006209 | Val RMSE: 1.21905100 | ‚àö(Val Loss) = 1.22339702 | Current Learning Rate: 0.002\n",
      "Epoch 93/1000 | Train Loss=13341.53281250 | Val Loss=1.52995257 | Data=133.39311981 | Physics=2.22236792 | Val RMSE: 1.22237098 | ‚àö(Val Loss) = 1.23691249 | Current Learning Rate: 0.002\n",
      "Epoch 94/1000 | Train Loss=13375.02500000 | Val Loss=1.54779855 | Data=133.72800598 | Physics=2.26977773 | Val RMSE: 1.22134566 | ‚àö(Val Loss) = 1.24410546 | Current Learning Rate: 0.002\n",
      "Epoch 95/1000 | Train Loss=13356.34244792 | Val Loss=1.57543910 | Data=133.54119263 | Physics=2.13576195 | Val RMSE: 1.22100878 | ‚àö(Val Loss) = 1.25516498 | Current Learning Rate: 0.002\n",
      "Epoch 96/1000 | Train Loss=13387.62773438 | Val Loss=1.57233556 | Data=133.85400289 | Physics=2.16085978 | Val RMSE: 1.21856761 | ‚àö(Val Loss) = 1.25392807 | Current Learning Rate: 0.002\n",
      "Epoch 97/1000 | Train Loss=13359.16848958 | Val Loss=1.52630508 | Data=133.56948649 | Physics=2.21807526 | Val RMSE: 1.22149849 | ‚àö(Val Loss) = 1.23543715 | Current Learning Rate: 0.002\n",
      "Epoch 98/1000 | Train Loss=13371.90644531 | Val Loss=1.63235907 | Data=133.69657186 | Physics=2.30218535 | Val RMSE: 1.22074080 | ‚àö(Val Loss) = 1.27763808 | Current Learning Rate: 0.002\n",
      "Epoch 99/1000 | Train Loss=13339.05891927 | Val Loss=1.52201641 | Data=133.36835429 | Physics=2.22162450 | Val RMSE: 1.22223043 | ‚àö(Val Loss) = 1.23370028 | Current Learning Rate: 0.002\n",
      "Epoch 100/1000 | Train Loss=13367.49954427 | Val Loss=1.52581402 | Data=133.65261943 | Physics=2.27889133 | Val RMSE: 1.22366953 | ‚àö(Val Loss) = 1.23523843 | Current Learning Rate: 0.002\n",
      "Epoch 101/1000 | Train Loss=13388.75338542 | Val Loss=1.56007063 | Data=133.86531525 | Physics=2.24896994 | Val RMSE: 1.22051191 | ‚àö(Val Loss) = 1.24902785 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 102/1000 | Train Loss=13363.97688802 | Val Loss=1.50253308 | Data=133.61752218 | Physics=2.20711581 | Val RMSE: 1.22247982 | ‚àö(Val Loss) = 1.22577858 | Current Learning Rate: 0.002\n",
      "Epoch 103/1000 | Train Loss=13377.59264323 | Val Loss=1.54185092 | Data=133.75355123 | Physics=2.16056971 | Val RMSE: 1.21950340 | ‚àö(Val Loss) = 1.24171293 | Current Learning Rate: 0.002\n",
      "Epoch 104/1000 | Train Loss=13392.72376302 | Val Loss=1.52158026 | Data=133.90496318 | Physics=2.38146634 | Val RMSE: 1.22137833 | ‚àö(Val Loss) = 1.23352349 | Current Learning Rate: 0.002\n",
      "Epoch 105/1000 | Train Loss=13360.54687500 | Val Loss=1.47483913 | Data=133.58324839 | Physics=2.21094434 | Val RMSE: 1.21711695 | ‚àö(Val Loss) = 1.21442950 | Current Learning Rate: 0.002\n",
      "Epoch 106/1000 | Train Loss=13366.73300781 | Val Loss=1.55993386 | Data=133.64502055 | Physics=2.12839906 | Val RMSE: 1.21976447 | ‚àö(Val Loss) = 1.24897313 | Current Learning Rate: 0.002\n",
      "Epoch 107/1000 | Train Loss=13343.05423177 | Val Loss=1.51475926 | Data=133.40836995 | Physics=2.18416981 | Val RMSE: 1.22133815 | ‚àö(Val Loss) = 1.23075557 | Current Learning Rate: 0.002\n",
      "Epoch 108/1000 | Train Loss=13395.15345052 | Val Loss=1.50361808 | Data=133.92935181 | Physics=2.11793939 | Val RMSE: 1.22078788 | ‚àö(Val Loss) = 1.22622108 | Current Learning Rate: 0.002\n",
      "Epoch 109/1000 | Train Loss=13390.67936198 | Val Loss=1.55074795 | Data=133.88456726 | Physics=2.23240201 | Val RMSE: 1.21992278 | ‚àö(Val Loss) = 1.24529028 | Current Learning Rate: 0.002\n",
      "Epoch 110/1000 | Train Loss=13376.55039062 | Val Loss=1.49976591 | Data=133.74333598 | Physics=2.20792607 | Val RMSE: 1.22228706 | ‚àö(Val Loss) = 1.22464931 | Current Learning Rate: 0.002\n",
      "Epoch 111/1000 | Train Loss=13374.10852865 | Val Loss=1.50315674 | Data=133.71883036 | Physics=2.25484705 | Val RMSE: 1.22154284 | ‚àö(Val Loss) = 1.22603297 | Current Learning Rate: 0.002\n",
      "Epoch 112/1000 | Train Loss=13374.56875000 | Val Loss=1.47399720 | Data=133.72350820 | Physics=2.18766803 | Val RMSE: 1.22142482 | ‚àö(Val Loss) = 1.21408284 | Current Learning Rate: 0.002\n",
      "Epoch 113/1000 | Train Loss=13378.51210938 | Val Loss=1.54825139 | Data=133.76284892 | Physics=2.29482958 | Val RMSE: 1.22339070 | ‚àö(Val Loss) = 1.24428749 | Current Learning Rate: 0.002\n",
      "Epoch 114/1000 | Train Loss=13371.91015625 | Val Loss=1.49529942 | Data=133.69685262 | Physics=2.21022290 | Val RMSE: 1.22223854 | ‚àö(Val Loss) = 1.22282434 | Current Learning Rate: 0.002\n",
      "Epoch 115/1000 | Train Loss=13375.85117187 | Val Loss=1.48954952 | Data=133.73589020 | Physics=2.25010594 | Val RMSE: 1.22105205 | ‚àö(Val Loss) = 1.22047102 | Current Learning Rate: 0.002\n",
      "Epoch 116/1000 | Train Loss=13399.71263021 | Val Loss=1.51356057 | Data=133.97492676 | Physics=2.28361634 | Val RMSE: 1.21915102 | ‚àö(Val Loss) = 1.23026848 | Current Learning Rate: 0.002\n",
      "Epoch 117/1000 | Train Loss=13376.10403646 | Val Loss=1.52891739 | Data=133.73880310 | Physics=2.19789345 | Val RMSE: 1.22192538 | ‚àö(Val Loss) = 1.23649406 | Current Learning Rate: 0.002\n",
      "Epoch 118/1000 | Train Loss=13401.17968750 | Val Loss=1.63493800 | Data=133.98945363 | Physics=2.27108753 | Val RMSE: 1.21969306 | ‚àö(Val Loss) = 1.27864695 | Current Learning Rate: 0.002\n",
      "Epoch 119/1000 | Train Loss=13380.81438802 | Val Loss=1.46890783 | Data=133.78593241 | Physics=2.23578518 | Val RMSE: 1.22193027 | ‚àö(Val Loss) = 1.21198511 | Current Learning Rate: 0.002\n",
      "Epoch 120/1000 | Train Loss=13358.58899740 | Val Loss=1.59820199 | Data=133.56351267 | Physics=2.17188440 | Val RMSE: 1.22445476 | ‚àö(Val Loss) = 1.26420009 | Current Learning Rate: 0.002\n",
      "Epoch 121/1000 | Train Loss=13375.05501302 | Val Loss=1.47492695 | Data=133.72843831 | Physics=2.12826110 | Val RMSE: 1.22056246 | ‚àö(Val Loss) = 1.21446574 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 122/1000 | Train Loss=13389.79570313 | Val Loss=1.55698689 | Data=133.87562764 | Physics=2.21183044 | Val RMSE: 1.21699202 | ‚àö(Val Loss) = 1.24779284 | Current Learning Rate: 0.002\n",
      "Epoch 123/1000 | Train Loss=13408.12174479 | Val Loss=1.47517403 | Data=134.05901286 | Physics=2.20339345 | Val RMSE: 1.22180402 | ‚àö(Val Loss) = 1.21456742 | Current Learning Rate: 0.002\n",
      "Epoch 124/1000 | Train Loss=13352.36471354 | Val Loss=1.45742532 | Data=133.50144501 | Physics=2.18976616 | Val RMSE: 1.22185779 | ‚àö(Val Loss) = 1.20723867 | Current Learning Rate: 0.002\n",
      "Epoch 125/1000 | Train Loss=13392.49251302 | Val Loss=1.59830419 | Data=133.90245107 | Physics=2.26790388 | Val RMSE: 1.21957576 | ‚àö(Val Loss) = 1.26424050 | Current Learning Rate: 0.002\n",
      "Epoch 126/1000 | Train Loss=13382.21529948 | Val Loss=1.55212955 | Data=133.79989726 | Physics=2.19051943 | Val RMSE: 1.22072506 | ‚àö(Val Loss) = 1.24584484 | Current Learning Rate: 0.002\n",
      "Epoch 127/1000 | Train Loss=13399.41842448 | Val Loss=1.53814308 | Data=133.97200521 | Physics=2.19383595 | Val RMSE: 1.22035480 | ‚àö(Val Loss) = 1.24021900 | Current Learning Rate: 0.002\n",
      "Epoch 128/1000 | Train Loss=13380.79016927 | Val Loss=1.49996289 | Data=133.78572896 | Physics=2.21697606 | Val RMSE: 1.22173107 | ‚àö(Val Loss) = 1.22472978 | Current Learning Rate: 0.002\n",
      "Epoch 129/1000 | Train Loss=13371.54361979 | Val Loss=1.58531415 | Data=133.69297180 | Physics=2.24229058 | Val RMSE: 1.21952486 | ‚àö(Val Loss) = 1.25909257 | Current Learning Rate: 0.002\n",
      "Epoch 130/1000 | Train Loss=13366.82747396 | Val Loss=1.46542152 | Data=133.64606934 | Physics=2.21399030 | Val RMSE: 1.22059071 | ‚àö(Val Loss) = 1.21054602 | Current Learning Rate: 0.002\n",
      "Epoch 131/1000 | Train Loss=13373.45065104 | Val Loss=1.51579944 | Data=133.71224264 | Physics=2.20488831 | Val RMSE: 1.21929836 | ‚àö(Val Loss) = 1.23117805 | Current Learning Rate: 0.002\n",
      "Epoch 132/1000 | Train Loss=13383.10188802 | Val Loss=1.57660206 | Data=133.80879008 | Physics=2.17796157 | Val RMSE: 1.22215545 | ‚àö(Val Loss) = 1.25562823 | Current Learning Rate: 0.002\n",
      "Epoch 133/1000 | Train Loss=13386.42994792 | Val Loss=1.50008353 | Data=133.84205831 | Physics=2.23194521 | Val RMSE: 1.22162604 | ‚àö(Val Loss) = 1.22477901 | Current Learning Rate: 0.002\n",
      "Epoch 134/1000 | Train Loss=13355.53815104 | Val Loss=1.57808328 | Data=133.53304138 | Physics=2.26900034 | Val RMSE: 1.21923375 | ‚àö(Val Loss) = 1.25621784 | Current Learning Rate: 0.002\n",
      "Epoch 135/1000 | Train Loss=13367.56223958 | Val Loss=1.51382248 | Data=133.65345662 | Physics=2.20427498 | Val RMSE: 1.21919727 | ‚àö(Val Loss) = 1.23037493 | Current Learning Rate: 0.002\n",
      "Epoch 136/1000 | Train Loss=13376.62233073 | Val Loss=1.51885204 | Data=133.74391632 | Physics=2.16347606 | Val RMSE: 1.21819317 | ‚àö(Val Loss) = 1.23241711 | Current Learning Rate: 0.002\n",
      "Epoch 137/1000 | Train Loss=13394.93307292 | Val Loss=1.49931029 | Data=133.92703298 | Physics=2.20532565 | Val RMSE: 1.22207892 | ‚àö(Val Loss) = 1.22446322 | Current Learning Rate: 0.002\n",
      "Epoch 138/1000 | Train Loss=13369.22421875 | Val Loss=1.56899532 | Data=133.66989237 | Physics=2.16780808 | Val RMSE: 1.21890855 | ‚àö(Val Loss) = 1.25259542 | Current Learning Rate: 0.002\n",
      "Epoch 139/1000 | Train Loss=13386.86119792 | Val Loss=1.52580841 | Data=133.84637705 | Physics=2.20971422 | Val RMSE: 1.22182024 | ‚àö(Val Loss) = 1.23523617 | Current Learning Rate: 0.002\n",
      "Epoch 140/1000 | Train Loss=13374.08782552 | Val Loss=1.56258539 | Data=133.71868388 | Physics=2.26556756 | Val RMSE: 1.22041655 | ‚àö(Val Loss) = 1.25003409 | Current Learning Rate: 0.002\n",
      "Epoch 141/1000 | Train Loss=13384.08352865 | Val Loss=1.52154632 | Data=133.81860860 | Physics=2.20918975 | Val RMSE: 1.21927810 | ‚àö(Val Loss) = 1.23350978 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 142/1000 | Train Loss=13395.87011719 | Val Loss=1.57876452 | Data=133.93644562 | Physics=2.17697683 | Val RMSE: 1.22178459 | ‚àö(Val Loss) = 1.25648904 | Current Learning Rate: 0.002\n",
      "Epoch 143/1000 | Train Loss=13377.21699219 | Val Loss=1.64174648 | Data=133.74958598 | Physics=2.41443535 | Val RMSE: 1.21890092 | ‚àö(Val Loss) = 1.28130662 | Current Learning Rate: 0.002\n",
      "Epoch 144/1000 | Train Loss=13375.41764323 | Val Loss=1.57240339 | Data=133.73192952 | Physics=2.19738085 | Val RMSE: 1.22196627 | ‚àö(Val Loss) = 1.25395513 | Current Learning Rate: 0.002\n",
      "Epoch 145/1000 | Train Loss=13361.91894531 | Val Loss=1.55451743 | Data=133.59698486 | Physics=2.20867888 | Val RMSE: 1.22046566 | ‚àö(Val Loss) = 1.24680293 | Current Learning Rate: 0.002\n",
      "Epoch 146/1000 | Train Loss=13389.90123698 | Val Loss=1.53396396 | Data=133.87678121 | Physics=2.21353512 | Val RMSE: 1.22049212 | ‚àö(Val Loss) = 1.23853302 | Current Learning Rate: 0.002\n",
      "Epoch 147/1000 | Train Loss=13333.17753906 | Val Loss=1.46524485 | Data=133.30962931 | Physics=2.18006409 | Val RMSE: 1.21871710 | ‚àö(Val Loss) = 1.21047306 | Current Learning Rate: 0.002\n",
      "Epoch 148/1000 | Train Loss=13403.93307292 | Val Loss=1.60826862 | Data=134.01681366 | Physics=2.22266421 | Val RMSE: 1.21958125 | ‚àö(Val Loss) = 1.26817536 | Current Learning Rate: 0.002\n",
      "Epoch 149/1000 | Train Loss=13355.66380208 | Val Loss=1.54532468 | Data=133.53435211 | Physics=2.24365495 | Val RMSE: 1.22092700 | ‚àö(Val Loss) = 1.24311090 | Current Learning Rate: 0.002\n",
      "Epoch 150/1000 | Train Loss=13363.80996094 | Val Loss=1.58740668 | Data=133.61591288 | Physics=2.15402162 | Val RMSE: 1.22061515 | ‚àö(Val Loss) = 1.25992322 | Current Learning Rate: 0.002\n",
      "Epoch 151/1000 | Train Loss=13409.89993490 | Val Loss=1.52566493 | Data=134.07684530 | Physics=2.22018077 | Val RMSE: 1.22193754 | ‚àö(Val Loss) = 1.23517811 | Current Learning Rate: 0.002\n",
      "Epoch 152/1000 | Train Loss=13372.15123698 | Val Loss=1.56751319 | Data=133.69924266 | Physics=2.18684150 | Val RMSE: 1.21925306 | ‚àö(Val Loss) = 1.25200367 | Current Learning Rate: 0.002\n",
      "Epoch 153/1000 | Train Loss=13392.37402344 | Val Loss=1.52073594 | Data=133.90138194 | Physics=2.34190852 | Val RMSE: 1.21791697 | ‚àö(Val Loss) = 1.23318124 | Current Learning Rate: 0.002\n",
      "Epoch 154/1000 | Train Loss=13382.03118490 | Val Loss=1.53756368 | Data=133.79801941 | Physics=2.20551659 | Val RMSE: 1.22344255 | ‚àö(Val Loss) = 1.23998535 | Current Learning Rate: 0.002\n",
      "Epoch 155/1000 | Train Loss=13361.03509115 | Val Loss=1.54096166 | Data=133.58801422 | Physics=2.23078046 | Val RMSE: 1.22093344 | ‚àö(Val Loss) = 1.24135470 | Current Learning Rate: 0.002\n",
      "Epoch 156/1000 | Train Loss=13386.13007812 | Val Loss=1.47847450 | Data=133.83893840 | Physics=2.12216260 | Val RMSE: 1.21957755 | ‚àö(Val Loss) = 1.21592534 | Current Learning Rate: 0.002\n",
      "Epoch 157/1000 | Train Loss=13372.16484375 | Val Loss=1.50366815 | Data=133.69916331 | Physics=2.22465513 | Val RMSE: 1.22180498 | ‚àö(Val Loss) = 1.22624147 | Current Learning Rate: 0.002\n",
      "Epoch 158/1000 | Train Loss=13411.73964844 | Val Loss=1.53392605 | Data=134.09519145 | Physics=2.19983467 | Val RMSE: 1.21898675 | ‚àö(Val Loss) = 1.23851764 | Current Learning Rate: 0.002\n",
      "Epoch 159/1000 | Train Loss=13378.03470052 | Val Loss=1.53153765 | Data=133.75812683 | Physics=2.23399402 | Val RMSE: 1.22126842 | ‚àö(Val Loss) = 1.23755312 | Current Learning Rate: 0.002\n",
      "Epoch 160/1000 | Train Loss=13382.99492188 | Val Loss=1.58062911 | Data=133.80772959 | Physics=2.36633847 | Val RMSE: 1.22300506 | ‚àö(Val Loss) = 1.25723076 | Current Learning Rate: 0.002\n",
      "Epoch 161/1000 | Train Loss=13356.35442708 | Val Loss=1.49270296 | Data=133.54144796 | Physics=2.17033514 | Val RMSE: 1.22268677 | ‚àö(Val Loss) = 1.22176218 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 162/1000 | Train Loss=13391.90781250 | Val Loss=1.51854336 | Data=133.89693247 | Physics=2.28896173 | Val RMSE: 1.22041368 | ‚àö(Val Loss) = 1.23229194 | Current Learning Rate: 0.002\n",
      "Epoch 163/1000 | Train Loss=13383.04134115 | Val Loss=1.52638646 | Data=133.80813395 | Physics=2.20816097 | Val RMSE: 1.22054410 | ‚àö(Val Loss) = 1.23547018 | Current Learning Rate: 0.002\n",
      "Epoch 164/1000 | Train Loss=13335.49342448 | Val Loss=1.53989176 | Data=133.33268738 | Physics=2.22209260 | Val RMSE: 1.22392964 | ‚àö(Val Loss) = 1.24092376 | Current Learning Rate: 0.002\n",
      "Epoch 165/1000 | Train Loss=13369.38763021 | Val Loss=1.54736531 | Data=133.67166392 | Physics=2.23367349 | Val RMSE: 1.21940982 | ‚àö(Val Loss) = 1.24393141 | Current Learning Rate: 0.002\n",
      "Epoch 166/1000 | Train Loss=13390.48665365 | Val Loss=1.49345076 | Data=133.88258057 | Physics=2.23815633 | Val RMSE: 1.21816051 | ‚àö(Val Loss) = 1.22206819 | Current Learning Rate: 0.002\n",
      "Epoch 167/1000 | Train Loss=13366.34121094 | Val Loss=1.54603612 | Data=133.64084269 | Physics=2.26316356 | Val RMSE: 1.22165000 | ‚àö(Val Loss) = 1.24339700 | Current Learning Rate: 0.002\n",
      "Epoch 168/1000 | Train Loss=13379.96308594 | Val Loss=1.47534772 | Data=133.77748973 | Physics=2.25084916 | Val RMSE: 1.22098768 | ‚àö(Val Loss) = 1.21463895 | Current Learning Rate: 0.002\n",
      "Epoch 169/1000 | Train Loss=13377.80514323 | Val Loss=1.50267287 | Data=133.75573730 | Physics=2.23713585 | Val RMSE: 1.22128224 | ‚àö(Val Loss) = 1.22583556 | Current Learning Rate: 0.002\n",
      "Epoch 170/1000 | Train Loss=13386.46686198 | Val Loss=1.54621430 | Data=133.84240621 | Physics=2.23633376 | Val RMSE: 1.22002339 | ‚àö(Val Loss) = 1.24346864 | Current Learning Rate: 0.002\n",
      "Epoch 171/1000 | Train Loss=13354.57050781 | Val Loss=1.49725672 | Data=133.52348836 | Physics=2.22774101 | Val RMSE: 1.22051942 | ‚àö(Val Loss) = 1.22362447 | Current Learning Rate: 0.002\n",
      "Epoch 172/1000 | Train Loss=13358.60234375 | Val Loss=1.57694880 | Data=133.56372172 | Physics=2.21765010 | Val RMSE: 1.21940804 | ‚àö(Val Loss) = 1.25576615 | Current Learning Rate: 0.002\n",
      "Epoch 173/1000 | Train Loss=13392.10429688 | Val Loss=1.50619026 | Data=133.89879812 | Physics=2.23080103 | Val RMSE: 1.22101259 | ‚àö(Val Loss) = 1.22726941 | Current Learning Rate: 0.002\n",
      "Epoch 174/1000 | Train Loss=13387.63138021 | Val Loss=1.54411622 | Data=133.85414683 | Physics=2.21880149 | Val RMSE: 1.22252727 | ‚àö(Val Loss) = 1.24262476 | Current Learning Rate: 0.002\n",
      "Epoch 175/1000 | Train Loss=13398.38710938 | Val Loss=1.59442472 | Data=133.96159770 | Physics=2.22769704 | Val RMSE: 1.22041678 | ‚àö(Val Loss) = 1.26270533 | Current Learning Rate: 0.002\n",
      "Epoch 176/1000 | Train Loss=13370.85930990 | Val Loss=1.50471977 | Data=133.68642680 | Physics=2.22089888 | Val RMSE: 1.22552633 | ‚àö(Val Loss) = 1.22667015 | Current Learning Rate: 0.002\n",
      "Epoch 177/1000 | Train Loss=13409.02037760 | Val Loss=1.62110484 | Data=134.06778920 | Physics=2.17837074 | Val RMSE: 1.21928048 | ‚àö(Val Loss) = 1.27322614 | Current Learning Rate: 0.002\n",
      "Epoch 178/1000 | Train Loss=13381.64244792 | Val Loss=1.50845261 | Data=133.79419098 | Physics=2.18860975 | Val RMSE: 1.22188795 | ‚àö(Val Loss) = 1.22819078 | Current Learning Rate: 0.002\n",
      "Epoch 179/1000 | Train Loss=13343.79993490 | Val Loss=1.50741780 | Data=133.41590322 | Physics=2.09619489 | Val RMSE: 1.21982837 | ‚àö(Val Loss) = 1.22776949 | Current Learning Rate: 0.002\n",
      "Epoch 180/1000 | Train Loss=13354.87356771 | Val Loss=1.50845619 | Data=133.52654368 | Physics=2.24050492 | Val RMSE: 1.21995866 | ‚àö(Val Loss) = 1.22819221 | Current Learning Rate: 0.002\n",
      "Epoch 181/1000 | Train Loss=13380.35065104 | Val Loss=1.55068878 | Data=133.78126831 | Physics=2.22421437 | Val RMSE: 1.21913278 | ‚àö(Val Loss) = 1.24526656 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 182/1000 | Train Loss=13356.02356771 | Val Loss=1.50506151 | Data=133.53803558 | Physics=2.27457701 | Val RMSE: 1.22268271 | ‚àö(Val Loss) = 1.22680950 | Current Learning Rate: 0.002\n",
      "Epoch 183/1000 | Train Loss=13351.23274740 | Val Loss=1.51444662 | Data=133.49008484 | Physics=2.20737614 | Val RMSE: 1.22176301 | ‚àö(Val Loss) = 1.23062849 | Current Learning Rate: 0.002\n",
      "Epoch 184/1000 | Train Loss=13329.85572917 | Val Loss=1.51626647 | Data=133.27629242 | Physics=2.26170100 | Val RMSE: 1.21999872 | ‚àö(Val Loss) = 1.23136771 | Current Learning Rate: 0.002\n",
      "Epoch 185/1000 | Train Loss=13360.50260417 | Val Loss=1.48065643 | Data=133.58287913 | Physics=2.13960708 | Val RMSE: 1.22073364 | ‚àö(Val Loss) = 1.21682227 | Current Learning Rate: 0.002\n",
      "Epoch 186/1000 | Train Loss=13387.49856771 | Val Loss=1.50196083 | Data=133.85253448 | Physics=2.41310733 | Val RMSE: 1.21666741 | ‚àö(Val Loss) = 1.22554517 | Current Learning Rate: 0.002\n",
      "Epoch 187/1000 | Train Loss=13360.73079427 | Val Loss=1.52680234 | Data=133.58482819 | Physics=2.22042932 | Val RMSE: 1.22157478 | ‚àö(Val Loss) = 1.23563838 | Current Learning Rate: 0.002\n",
      "Epoch 188/1000 | Train Loss=13380.09055990 | Val Loss=1.56412689 | Data=133.77836812 | Physics=2.31990555 | Val RMSE: 1.22090733 | ‚àö(Val Loss) = 1.25065053 | Current Learning Rate: 0.002\n",
      "Epoch 189/1000 | Train Loss=13360.66959635 | Val Loss=1.52998126 | Data=133.58413493 | Physics=2.29430566 | Val RMSE: 1.21876287 | ‚àö(Val Loss) = 1.23692405 | Current Learning Rate: 0.002\n",
      "Epoch 190/1000 | Train Loss=13392.71256510 | Val Loss=1.50321901 | Data=133.90495555 | Physics=2.19064222 | Val RMSE: 1.22236681 | ‚àö(Val Loss) = 1.22605836 | Current Learning Rate: 0.002\n",
      "Epoch 191/1000 | Train Loss=13377.50227865 | Val Loss=1.47923855 | Data=133.75282440 | Physics=2.15405036 | Val RMSE: 1.22235584 | ‚àö(Val Loss) = 1.21623945 | Current Learning Rate: 0.002\n",
      "Epoch 192/1000 | Train Loss=13352.46204427 | Val Loss=1.66515422 | Data=133.50222829 | Physics=2.18076942 | Val RMSE: 1.22037935 | ‚àö(Val Loss) = 1.29040849 | Current Learning Rate: 0.002\n",
      "Epoch 193/1000 | Train Loss=13385.23229167 | Val Loss=1.58959051 | Data=133.83013509 | Physics=2.15496745 | Val RMSE: 1.22315395 | ‚àö(Val Loss) = 1.26078963 | Current Learning Rate: 0.002\n",
      "Epoch 194/1000 | Train Loss=13378.14095052 | Val Loss=1.50310083 | Data=133.75927022 | Physics=2.21234866 | Val RMSE: 1.22147882 | ‚àö(Val Loss) = 1.22601008 | Current Learning Rate: 0.002\n",
      "Epoch 195/1000 | Train Loss=13367.94108073 | Val Loss=1.55267755 | Data=133.65721029 | Physics=2.16762599 | Val RMSE: 1.22046781 | ‚àö(Val Loss) = 1.24606478 | Current Learning Rate: 0.002\n",
      "Epoch 196/1000 | Train Loss=13400.27460937 | Val Loss=1.48201319 | Data=133.98049164 | Physics=2.21266716 | Val RMSE: 1.22173011 | ‚àö(Val Loss) = 1.21737969 | Current Learning Rate: 0.002\n",
      "Epoch 197/1000 | Train Loss=13395.18261719 | Val Loss=1.56594777 | Data=133.92923279 | Physics=2.28440724 | Val RMSE: 1.21992445 | ‚àö(Val Loss) = 1.25137830 | Current Learning Rate: 0.002\n",
      "Epoch 198/1000 | Train Loss=13359.16217448 | Val Loss=1.55409928 | Data=133.56936646 | Physics=2.22389809 | Val RMSE: 1.22195220 | ‚àö(Val Loss) = 1.24663520 | Current Learning Rate: 0.002\n",
      "Epoch 199/1000 | Train Loss=13363.31119792 | Val Loss=1.54490101 | Data=133.61083425 | Physics=2.25370389 | Val RMSE: 1.22047448 | ‚àö(Val Loss) = 1.24294043 | Current Learning Rate: 0.002\n",
      "Epoch 200/1000 | Train Loss=13406.40794271 | Val Loss=1.46950551 | Data=134.04197947 | Physics=2.17382054 | Val RMSE: 1.22155821 | ‚àö(Val Loss) = 1.21223164 | Current Learning Rate: 0.002\n",
      "Epoch 201/1000 | Train Loss=13370.38828125 | Val Loss=1.54127212 | Data=133.68162028 | Physics=2.21921206 | Val RMSE: 1.21945107 | ‚àö(Val Loss) = 1.24147987 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 202/1000 | Train Loss=13392.21035156 | Val Loss=1.54067357 | Data=133.89987081 | Physics=2.18747056 | Val RMSE: 1.22069740 | ‚àö(Val Loss) = 1.24123871 | Current Learning Rate: 0.002\n",
      "Epoch 203/1000 | Train Loss=13386.18248698 | Val Loss=1.46196822 | Data=133.83955231 | Physics=2.24719165 | Val RMSE: 1.22204816 | ‚àö(Val Loss) = 1.20911872 | Current Learning Rate: 0.002\n",
      "Epoch 204/1000 | Train Loss=13327.74947917 | Val Loss=1.63153938 | Data=133.25510966 | Physics=2.17501846 | Val RMSE: 1.22105598 | ‚àö(Val Loss) = 1.27731729 | Current Learning Rate: 0.002\n",
      "Epoch 205/1000 | Train Loss=13366.78606771 | Val Loss=1.51410294 | Data=133.64571228 | Physics=2.13165740 | Val RMSE: 1.22015643 | ‚àö(Val Loss) = 1.23048890 | Current Learning Rate: 0.002\n",
      "Epoch 206/1000 | Train Loss=13371.49511719 | Val Loss=1.54136757 | Data=133.69275208 | Physics=2.20914054 | Val RMSE: 1.22134054 | ‚àö(Val Loss) = 1.24151826 | Current Learning Rate: 0.002\n",
      "Epoch 207/1000 | Train Loss=13406.21464844 | Val Loss=1.51946437 | Data=134.03990326 | Physics=2.25488931 | Val RMSE: 1.21971345 | ‚àö(Val Loss) = 1.23266554 | Current Learning Rate: 0.002\n",
      "Epoch 208/1000 | Train Loss=13385.53932292 | Val Loss=1.61267316 | Data=133.83308004 | Physics=2.24873271 | Val RMSE: 1.21936965 | ‚àö(Val Loss) = 1.26991069 | Current Learning Rate: 0.002\n",
      "Epoch 209/1000 | Train Loss=13346.82343750 | Val Loss=1.47145776 | Data=133.44611511 | Physics=2.17259741 | Val RMSE: 1.21907890 | ‚àö(Val Loss) = 1.21303654 | Current Learning Rate: 0.002\n",
      "Epoch 210/1000 | Train Loss=13354.31673177 | Val Loss=1.54929221 | Data=133.52087402 | Physics=2.17672466 | Val RMSE: 1.21905494 | ‚àö(Val Loss) = 1.24470568 | Current Learning Rate: 0.002\n",
      "Epoch 211/1000 | Train Loss=13358.93554688 | Val Loss=1.56156099 | Data=133.56704407 | Physics=2.11501658 | Val RMSE: 1.22159696 | ‚àö(Val Loss) = 1.24962437 | Current Learning Rate: 0.002\n",
      "Epoch 212/1000 | Train Loss=13395.11731771 | Val Loss=1.51843266 | Data=133.92891642 | Physics=2.22832255 | Val RMSE: 1.21755886 | ‚àö(Val Loss) = 1.23224699 | Current Learning Rate: 0.002\n",
      "Epoch 213/1000 | Train Loss=13397.14316406 | Val Loss=1.50095205 | Data=133.94931335 | Physics=2.16216525 | Val RMSE: 1.22023284 | ‚àö(Val Loss) = 1.22513342 | Current Learning Rate: 0.002\n",
      "Epoch 214/1000 | Train Loss=13345.47858073 | Val Loss=1.57115026 | Data=133.43253479 | Physics=2.24092142 | Val RMSE: 1.22257710 | ‚àö(Val Loss) = 1.25345540 | Current Learning Rate: 0.002\n",
      "Epoch 215/1000 | Train Loss=13392.04550781 | Val Loss=1.50125480 | Data=133.89819743 | Physics=2.19254217 | Val RMSE: 1.22067595 | ‚àö(Val Loss) = 1.22525704 | Current Learning Rate: 0.002\n",
      "Epoch 216/1000 | Train Loss=13371.74355469 | Val Loss=1.47916603 | Data=133.69521332 | Physics=2.23645423 | Val RMSE: 1.22008252 | ‚àö(Val Loss) = 1.21620965 | Current Learning Rate: 0.002\n",
      "Epoch 217/1000 | Train Loss=13387.85423177 | Val Loss=1.55269893 | Data=133.85615387 | Physics=2.23664850 | Val RMSE: 1.21947038 | ‚àö(Val Loss) = 1.24607337 | Current Learning Rate: 0.002\n",
      "Epoch 218/1000 | Train Loss=13383.91907552 | Val Loss=1.49248485 | Data=133.81695353 | Physics=2.18323256 | Val RMSE: 1.21787095 | ‚àö(Val Loss) = 1.22167301 | Current Learning Rate: 0.002\n",
      "Epoch 219/1000 | Train Loss=13380.75813802 | Val Loss=1.55108424 | Data=133.78521423 | Physics=2.27673349 | Val RMSE: 1.21855330 | ‚àö(Val Loss) = 1.24542534 | Current Learning Rate: 0.002\n",
      "Epoch 220/1000 | Train Loss=13405.36738281 | Val Loss=1.59979578 | Data=134.03135173 | Physics=2.22176300 | Val RMSE: 1.22205496 | ‚àö(Val Loss) = 1.26483035 | Current Learning Rate: 0.002\n",
      "Epoch 221/1000 | Train Loss=13347.11614583 | Val Loss=1.51425640 | Data=133.44885254 | Physics=2.24903905 | Val RMSE: 1.22123551 | ‚àö(Val Loss) = 1.23055124 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 222/1000 | Train Loss=13363.48880208 | Val Loss=1.60539842 | Data=133.61254171 | Physics=2.18378452 | Val RMSE: 1.22011256 | ‚àö(Val Loss) = 1.26704323 | Current Learning Rate: 0.002\n",
      "Epoch 223/1000 | Train Loss=13342.11855469 | Val Loss=1.52885469 | Data=133.39898326 | Physics=2.32858597 | Val RMSE: 1.22019291 | ‚àö(Val Loss) = 1.23646867 | Current Learning Rate: 0.002\n",
      "Epoch 224/1000 | Train Loss=13392.23177083 | Val Loss=1.49366387 | Data=133.90011953 | Physics=2.22106410 | Val RMSE: 1.21588218 | ‚àö(Val Loss) = 1.22215545 | Current Learning Rate: 0.002\n",
      "Epoch 225/1000 | Train Loss=13373.14609375 | Val Loss=1.49713202 | Data=133.70922190 | Physics=2.28653091 | Val RMSE: 1.22203708 | ‚àö(Val Loss) = 1.22357345 | Current Learning Rate: 0.002\n",
      "Epoch 226/1000 | Train Loss=13354.89049479 | Val Loss=1.58417678 | Data=133.52645976 | Physics=2.30721288 | Val RMSE: 1.21826959 | ‚àö(Val Loss) = 1.25864089 | Current Learning Rate: 0.002\n",
      "Epoch 227/1000 | Train Loss=13357.16816406 | Val Loss=1.52264241 | Data=133.54953461 | Physics=2.17837387 | Val RMSE: 1.22161531 | ‚àö(Val Loss) = 1.23395395 | Current Learning Rate: 0.002\n",
      "Epoch 228/1000 | Train Loss=13395.47981771 | Val Loss=1.53631616 | Data=133.93259176 | Physics=2.17166168 | Val RMSE: 1.22132397 | ‚àö(Val Loss) = 1.23948216 | Current Learning Rate: 0.002\n",
      "Epoch 229/1000 | Train Loss=13378.85748698 | Val Loss=1.53243331 | Data=133.76635997 | Physics=2.26599354 | Val RMSE: 1.22172284 | ‚àö(Val Loss) = 1.23791492 | Current Learning Rate: 0.002\n",
      "Epoch 230/1000 | Train Loss=13376.37506510 | Val Loss=1.48602319 | Data=133.74152171 | Physics=2.28781923 | Val RMSE: 1.21709538 | ‚àö(Val Loss) = 1.21902549 | Current Learning Rate: 0.002\n",
      "Epoch 231/1000 | Train Loss=13375.91835938 | Val Loss=1.54042099 | Data=133.73670197 | Physics=2.39592922 | Val RMSE: 1.21546400 | ‚àö(Val Loss) = 1.24113703 | Current Learning Rate: 0.002\n",
      "Epoch 232/1000 | Train Loss=13353.66946615 | Val Loss=1.52567224 | Data=133.51443380 | Physics=2.16836047 | Val RMSE: 1.22141778 | ‚àö(Val Loss) = 1.23518109 | Current Learning Rate: 0.002\n",
      "Epoch 233/1000 | Train Loss=13401.41451823 | Val Loss=1.52567915 | Data=133.99195353 | Physics=2.25431843 | Val RMSE: 1.22161305 | ‚àö(Val Loss) = 1.23518384 | Current Learning Rate: 0.002\n",
      "Epoch 234/1000 | Train Loss=13398.52955729 | Val Loss=1.51225754 | Data=133.96300201 | Physics=2.23651437 | Val RMSE: 1.21920085 | ‚àö(Val Loss) = 1.22973883 | Current Learning Rate: 0.002\n",
      "Epoch 235/1000 | Train Loss=13387.79654948 | Val Loss=1.48754613 | Data=133.85584615 | Physics=2.16749999 | Val RMSE: 1.22090423 | ‚àö(Val Loss) = 1.21965003 | Current Learning Rate: 0.002\n",
      "Epoch 236/1000 | Train Loss=13360.84667969 | Val Loss=1.51202464 | Data=133.58623606 | Physics=2.20703139 | Val RMSE: 1.22139108 | ‚àö(Val Loss) = 1.22964406 | Current Learning Rate: 0.002\n",
      "Epoch 237/1000 | Train Loss=13331.67851563 | Val Loss=1.46404556 | Data=133.29461314 | Physics=2.14391919 | Val RMSE: 1.21926379 | ‚àö(Val Loss) = 1.20997751 | Current Learning Rate: 0.002\n",
      "Epoch 238/1000 | Train Loss=13400.63756510 | Val Loss=1.58866290 | Data=133.98396810 | Physics=2.29379532 | Val RMSE: 1.21813345 | ‚àö(Val Loss) = 1.26042175 | Current Learning Rate: 0.002\n",
      "Epoch 239/1000 | Train Loss=13390.13964844 | Val Loss=1.53435310 | Data=133.87916667 | Physics=2.27764239 | Val RMSE: 1.22014022 | ‚àö(Val Loss) = 1.23869014 | Current Learning Rate: 0.002\n",
      "Epoch 240/1000 | Train Loss=13371.34707031 | Val Loss=1.48971089 | Data=133.69127706 | Physics=2.25015306 | Val RMSE: 1.21934533 | ‚àö(Val Loss) = 1.22053719 | Current Learning Rate: 0.002\n",
      "Epoch 241/1000 | Train Loss=13391.03795573 | Val Loss=1.50692515 | Data=133.88823751 | Physics=2.26645970 | Val RMSE: 1.22181213 | ‚àö(Val Loss) = 1.22756875 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 242/1000 | Train Loss=13353.87102865 | Val Loss=1.46793433 | Data=133.51650543 | Physics=2.24571267 | Val RMSE: 1.21922541 | ‚àö(Val Loss) = 1.21158338 | Current Learning Rate: 0.002\n",
      "Epoch 243/1000 | Train Loss=13373.56048177 | Val Loss=1.58958824 | Data=133.71324259 | Physics=2.19371036 | Val RMSE: 1.21983564 | ‚àö(Val Loss) = 1.26078880 | Current Learning Rate: 0.002\n",
      "Epoch 244/1000 | Train Loss=13387.48111979 | Val Loss=1.48057512 | Data=133.85257314 | Physics=2.28033823 | Val RMSE: 1.22122335 | ‚àö(Val Loss) = 1.21678889 | Current Learning Rate: 0.002\n",
      "Epoch 245/1000 | Train Loss=13410.61595052 | Val Loss=1.56971172 | Data=134.08377533 | Physics=2.22590824 | Val RMSE: 1.21854210 | ‚àö(Val Loss) = 1.25288141 | Current Learning Rate: 0.002\n",
      "Epoch 246/1000 | Train Loss=13348.98261719 | Val Loss=1.48750492 | Data=133.46759694 | Physics=2.20854026 | Val RMSE: 1.22154474 | ‚àö(Val Loss) = 1.21963310 | Current Learning Rate: 0.002\n",
      "Epoch 247/1000 | Train Loss=13380.55247396 | Val Loss=1.54356102 | Data=133.78313751 | Physics=2.27306795 | Val RMSE: 1.22046685 | ‚àö(Val Loss) = 1.24240124 | Current Learning Rate: 0.002\n",
      "Epoch 248/1000 | Train Loss=13381.07018229 | Val Loss=1.51390155 | Data=133.78847758 | Physics=2.22116961 | Val RMSE: 1.22195959 | ‚àö(Val Loss) = 1.23040712 | Current Learning Rate: 0.002\n",
      "Epoch 249/1000 | Train Loss=13414.02128906 | Val Loss=1.53125242 | Data=134.11783854 | Physics=2.23933681 | Val RMSE: 1.22023237 | ‚àö(Val Loss) = 1.23743784 | Current Learning Rate: 0.002\n",
      "Epoch 250/1000 | Train Loss=13344.24550781 | Val Loss=1.52181264 | Data=133.42032013 | Physics=2.14898723 | Val RMSE: 1.21887064 | ‚àö(Val Loss) = 1.23361766 | Current Learning Rate: 0.002\n",
      "Epoch 251/1000 | Train Loss=13373.86022135 | Val Loss=1.52613982 | Data=133.71626282 | Physics=2.18029452 | Val RMSE: 1.21912122 | ‚àö(Val Loss) = 1.23537028 | Current Learning Rate: 0.002\n",
      "Epoch 252/1000 | Train Loss=13393.46569010 | Val Loss=1.51904007 | Data=133.91238607 | Physics=2.14112041 | Val RMSE: 1.22110236 | ‚àö(Val Loss) = 1.23249340 | Current Learning Rate: 0.002\n",
      "Epoch 253/1000 | Train Loss=13391.27747396 | Val Loss=1.48627341 | Data=133.89049377 | Physics=2.18465251 | Val RMSE: 1.22119808 | ‚àö(Val Loss) = 1.21912813 | Current Learning Rate: 0.002\n",
      "Epoch 254/1000 | Train Loss=13392.46204427 | Val Loss=1.64304638 | Data=133.90208944 | Physics=2.29580531 | Val RMSE: 1.21854103 | ‚àö(Val Loss) = 1.28181374 | Current Learning Rate: 0.002\n",
      "Epoch 255/1000 | Train Loss=13349.61516927 | Val Loss=1.52106392 | Data=133.47398071 | Physics=2.19000347 | Val RMSE: 1.21823478 | ‚àö(Val Loss) = 1.23331416 | Current Learning Rate: 0.002\n",
      "Epoch 256/1000 | Train Loss=13400.45553385 | Val Loss=1.56760351 | Data=133.98229370 | Physics=2.24741365 | Val RMSE: 1.22124362 | ‚àö(Val Loss) = 1.25203967 | Current Learning Rate: 0.002\n",
      "Epoch 257/1000 | Train Loss=13377.08815104 | Val Loss=1.60274971 | Data=133.74865926 | Physics=2.30905097 | Val RMSE: 1.21911871 | ‚àö(Val Loss) = 1.26599753 | Current Learning Rate: 0.002\n",
      "Epoch 258/1000 | Train Loss=13347.90104167 | Val Loss=1.53771623 | Data=133.45679067 | Physics=2.21038907 | Val RMSE: 1.22345948 | ‚àö(Val Loss) = 1.24004686 | Current Learning Rate: 0.002\n",
      "Epoch 259/1000 | Train Loss=13371.86289062 | Val Loss=1.56954193 | Data=133.69637095 | Physics=2.14754051 | Val RMSE: 1.22016561 | ‚àö(Val Loss) = 1.25281358 | Current Learning Rate: 0.002\n",
      "Epoch 260/1000 | Train Loss=13368.43339844 | Val Loss=1.50924687 | Data=133.66211650 | Physics=2.17158133 | Val RMSE: 1.22058296 | ‚àö(Val Loss) = 1.22851408 | Current Learning Rate: 0.002\n",
      "Epoch 261/1000 | Train Loss=13376.41992188 | Val Loss=1.55362531 | Data=133.74200745 | Physics=2.26995612 | Val RMSE: 1.21971583 | ‚àö(Val Loss) = 1.24644506 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 262/1000 | Train Loss=13374.47382812 | Val Loss=1.53731402 | Data=133.72252909 | Physics=2.15739065 | Val RMSE: 1.21951175 | ‚àö(Val Loss) = 1.23988473 | Current Learning Rate: 0.002\n",
      "Epoch 263/1000 | Train Loss=13428.55013021 | Val Loss=1.52615472 | Data=134.26323547 | Physics=2.20778158 | Val RMSE: 1.22163677 | ‚àö(Val Loss) = 1.23537636 | Current Learning Rate: 0.002\n",
      "Epoch 264/1000 | Train Loss=13383.11380208 | Val Loss=1.59696583 | Data=133.80875956 | Physics=2.26607502 | Val RMSE: 1.22037458 | ‚àö(Val Loss) = 1.26371109 | Current Learning Rate: 0.002\n",
      "Epoch 265/1000 | Train Loss=13348.05143229 | Val Loss=1.52714467 | Data=133.45834503 | Physics=2.19067483 | Val RMSE: 1.21975362 | ‚àö(Val Loss) = 1.23577690 | Current Learning Rate: 0.002\n",
      "Epoch 266/1000 | Train Loss=13367.39752604 | Val Loss=1.57417977 | Data=133.65168457 | Physics=2.27098780 | Val RMSE: 1.22321165 | ‚àö(Val Loss) = 1.25466323 | Current Learning Rate: 0.002\n",
      "Epoch 267/1000 | Train Loss=13378.71054687 | Val Loss=1.53792858 | Data=133.76486766 | Physics=2.14209059 | Val RMSE: 1.22054482 | ‚àö(Val Loss) = 1.24013245 | Current Learning Rate: 0.002\n",
      "Epoch 268/1000 | Train Loss=13385.11770833 | Val Loss=1.52466849 | Data=133.82897135 | Physics=2.26374043 | Val RMSE: 1.21922243 | ‚àö(Val Loss) = 1.23477471 | Current Learning Rate: 0.002\n",
      "Epoch 269/1000 | Train Loss=13389.90638021 | Val Loss=1.51504513 | Data=133.87678833 | Physics=2.20554077 | Val RMSE: 1.21742845 | ‚àö(Val Loss) = 1.23087168 | Current Learning Rate: 0.002\n",
      "Epoch 270/1000 | Train Loss=13376.75071615 | Val Loss=1.52438307 | Data=133.74500122 | Physics=2.25993382 | Val RMSE: 1.22107816 | ‚àö(Val Loss) = 1.23465908 | Current Learning Rate: 0.002\n",
      "Epoch 271/1000 | Train Loss=13383.56308594 | Val Loss=1.50442767 | Data=133.81343079 | Physics=2.20514784 | Val RMSE: 1.22232699 | ‚àö(Val Loss) = 1.22655118 | Current Learning Rate: 0.002\n",
      "Epoch 272/1000 | Train Loss=13392.57604167 | Val Loss=1.51332331 | Data=133.90352987 | Physics=2.20710931 | Val RMSE: 1.22151828 | ‚àö(Val Loss) = 1.23017204 | Current Learning Rate: 0.002\n",
      "Epoch 273/1000 | Train Loss=13372.88059896 | Val Loss=1.50863556 | Data=133.70665995 | Physics=2.16881581 | Val RMSE: 1.21935606 | ‚àö(Val Loss) = 1.22826529 | Current Learning Rate: 0.002\n",
      "Epoch 274/1000 | Train Loss=13376.98815104 | Val Loss=1.52747393 | Data=133.74763489 | Physics=2.17055488 | Val RMSE: 1.21971643 | ‚àö(Val Loss) = 1.23591018 | Current Learning Rate: 0.002\n",
      "Epoch 275/1000 | Train Loss=13395.16790365 | Val Loss=1.50230181 | Data=133.92947693 | Physics=2.18122269 | Val RMSE: 1.21979463 | ‚àö(Val Loss) = 1.22568417 | Current Learning Rate: 0.002\n",
      "Epoch 276/1000 | Train Loss=13395.19355469 | Val Loss=1.52735253 | Data=133.92962341 | Physics=2.11195542 | Val RMSE: 1.21991599 | ‚àö(Val Loss) = 1.23586106 | Current Learning Rate: 0.002\n",
      "Epoch 277/1000 | Train Loss=13357.08717448 | Val Loss=1.58231882 | Data=133.54860891 | Physics=2.32148223 | Val RMSE: 1.22276390 | ‚àö(Val Loss) = 1.25790250 | Current Learning Rate: 0.002\n",
      "Epoch 278/1000 | Train Loss=13386.21425781 | Val Loss=1.59584033 | Data=133.83985138 | Physics=2.22002682 | Val RMSE: 1.22026622 | ‚àö(Val Loss) = 1.26326573 | Current Learning Rate: 0.002\n",
      "Epoch 279/1000 | Train Loss=13411.77050781 | Val Loss=1.55558002 | Data=134.09548543 | Physics=2.25112843 | Val RMSE: 1.22146022 | ‚àö(Val Loss) = 1.24722898 | Current Learning Rate: 0.002\n",
      "Epoch 280/1000 | Train Loss=13396.56360677 | Val Loss=1.56918879 | Data=133.94337819 | Physics=2.26954626 | Val RMSE: 1.22084236 | ‚àö(Val Loss) = 1.25267267 | Current Learning Rate: 0.002\n",
      "Epoch 281/1000 | Train Loss=13341.89368490 | Val Loss=1.52727179 | Data=133.39672241 | Physics=2.18296792 | Val RMSE: 1.22131228 | ‚àö(Val Loss) = 1.23582840 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 282/1000 | Train Loss=13370.24947917 | Val Loss=1.50098705 | Data=133.68038279 | Physics=2.18011523 | Val RMSE: 1.22003317 | ‚àö(Val Loss) = 1.22514772 | Current Learning Rate: 0.002\n",
      "Epoch 283/1000 | Train Loss=13410.91757812 | Val Loss=1.55080382 | Data=134.08698273 | Physics=2.22515208 | Val RMSE: 1.21924233 | ‚àö(Val Loss) = 1.24531269 | Current Learning Rate: 0.002\n",
      "Epoch 284/1000 | Train Loss=13364.11920573 | Val Loss=1.48997144 | Data=133.61903737 | Physics=2.23922571 | Val RMSE: 1.22013581 | ‚àö(Val Loss) = 1.22064388 | Current Learning Rate: 0.002\n",
      "Epoch 285/1000 | Train Loss=13373.80533854 | Val Loss=1.56431258 | Data=133.71576131 | Physics=2.32912617 | Val RMSE: 1.22284865 | ‚àö(Val Loss) = 1.25072479 | Current Learning Rate: 0.002\n",
      "Epoch 286/1000 | Train Loss=13381.50312500 | Val Loss=1.57944596 | Data=133.79271444 | Physics=2.29590922 | Val RMSE: 1.22220337 | ‚àö(Val Loss) = 1.25676012 | Current Learning Rate: 0.002\n",
      "Epoch 287/1000 | Train Loss=13345.10384115 | Val Loss=1.53776896 | Data=133.42879690 | Physics=2.27028616 | Val RMSE: 1.22182131 | ‚àö(Val Loss) = 1.24006808 | Current Learning Rate: 0.002\n",
      "Epoch 288/1000 | Train Loss=13380.76132812 | Val Loss=1.54729215 | Data=133.78537140 | Physics=2.21880311 | Val RMSE: 1.22160840 | ‚àö(Val Loss) = 1.24390197 | Current Learning Rate: 0.002\n",
      "Epoch 289/1000 | Train Loss=13370.43359375 | Val Loss=1.50743083 | Data=133.68217977 | Physics=2.22549520 | Val RMSE: 1.22073722 | ‚àö(Val Loss) = 1.22777474 | Current Learning Rate: 0.002\n",
      "Epoch 290/1000 | Train Loss=13350.63183594 | Val Loss=1.63783594 | Data=133.48374227 | Physics=2.20071129 | Val RMSE: 1.21988964 | ‚àö(Val Loss) = 1.27977967 | Current Learning Rate: 0.002\n",
      "Epoch 291/1000 | Train Loss=13403.44908854 | Val Loss=1.51920577 | Data=134.01234538 | Physics=2.21933311 | Val RMSE: 1.22179770 | ‚àö(Val Loss) = 1.23256063 | Current Learning Rate: 0.002\n",
      "Epoch 292/1000 | Train Loss=13347.16529948 | Val Loss=1.51527484 | Data=133.44948018 | Physics=2.14843970 | Val RMSE: 1.22057152 | ‚àö(Val Loss) = 1.23096502 | Current Learning Rate: 0.002\n",
      "Epoch 293/1000 | Train Loss=13370.57571615 | Val Loss=1.50313524 | Data=133.68344116 | Physics=2.25401104 | Val RMSE: 1.22232199 | ‚àö(Val Loss) = 1.22602415 | Current Learning Rate: 0.002\n",
      "Epoch 294/1000 | Train Loss=13350.73261719 | Val Loss=1.47880153 | Data=133.48511709 | Physics=2.17857229 | Val RMSE: 1.21868503 | ‚àö(Val Loss) = 1.21605980 | Current Learning Rate: 0.002\n",
      "Epoch 295/1000 | Train Loss=13340.94941406 | Val Loss=1.62249744 | Data=133.38697103 | Physics=2.27326778 | Val RMSE: 1.21925247 | ‚àö(Val Loss) = 1.27377295 | Current Learning Rate: 0.002\n",
      "Epoch 296/1000 | Train Loss=13398.87714844 | Val Loss=1.61363514 | Data=133.96634827 | Physics=2.20305011 | Val RMSE: 1.22108936 | ‚àö(Val Loss) = 1.27028942 | Current Learning Rate: 0.002\n",
      "Epoch 297/1000 | Train Loss=13381.36074219 | Val Loss=1.51307805 | Data=133.79135386 | Physics=2.22409419 | Val RMSE: 1.22148764 | ‚àö(Val Loss) = 1.23007238 | Current Learning Rate: 0.002\n",
      "Epoch 298/1000 | Train Loss=13381.02740885 | Val Loss=1.47844660 | Data=133.78813731 | Physics=2.26335116 | Val RMSE: 1.22249222 | ‚àö(Val Loss) = 1.21591389 | Current Learning Rate: 0.002\n",
      "Epoch 299/1000 | Train Loss=13341.50299479 | Val Loss=1.52382700 | Data=133.39277903 | Physics=2.21640648 | Val RMSE: 1.22129190 | ‚àö(Val Loss) = 1.23443389 | Current Learning Rate: 0.002\n",
      "Epoch 300/1000 | Train Loss=13413.58645833 | Val Loss=1.52551115 | Data=134.11371053 | Physics=2.23013680 | Val RMSE: 1.22272134 | ‚àö(Val Loss) = 1.23511589 | Current Learning Rate: 0.002\n",
      "Epoch 301/1000 | Train Loss=13370.23964844 | Val Loss=1.57398820 | Data=133.68016307 | Physics=2.21606801 | Val RMSE: 1.21928632 | ‚àö(Val Loss) = 1.25458682 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 302/1000 | Train Loss=13383.36464844 | Val Loss=1.48485104 | Data=133.81150818 | Physics=2.20939756 | Val RMSE: 1.22041535 | ‚àö(Val Loss) = 1.21854460 | Current Learning Rate: 0.002\n",
      "Epoch 303/1000 | Train Loss=13372.44485677 | Val Loss=1.55796655 | Data=133.70220133 | Physics=2.21680172 | Val RMSE: 1.22162962 | ‚àö(Val Loss) = 1.24818528 | Current Learning Rate: 0.002\n",
      "Epoch 304/1000 | Train Loss=13382.39609375 | Val Loss=1.55430981 | Data=133.80162252 | Physics=2.26090680 | Val RMSE: 1.22036350 | ‚àö(Val Loss) = 1.24671960 | Current Learning Rate: 0.002\n",
      "Epoch 305/1000 | Train Loss=13356.79375000 | Val Loss=1.53805951 | Data=133.54569600 | Physics=2.18659460 | Val RMSE: 1.21967590 | ‚àö(Val Loss) = 1.24018526 | Current Learning Rate: 0.002\n",
      "Epoch 306/1000 | Train Loss=13377.58372396 | Val Loss=1.52582045 | Data=133.75364024 | Physics=2.26534630 | Val RMSE: 1.21715677 | ‚àö(Val Loss) = 1.23524106 | Current Learning Rate: 0.002\n",
      "Epoch 307/1000 | Train Loss=13348.69980469 | Val Loss=1.53850488 | Data=133.46483663 | Physics=2.23495426 | Val RMSE: 1.22104263 | ‚àö(Val Loss) = 1.24036479 | Current Learning Rate: 0.002\n",
      "Epoch 308/1000 | Train Loss=13351.44153646 | Val Loss=1.53441294 | Data=133.49220734 | Physics=2.15482756 | Val RMSE: 1.22144544 | ‚àö(Val Loss) = 1.23871422 | Current Learning Rate: 0.002\n",
      "Epoch 309/1000 | Train Loss=13363.92890625 | Val Loss=1.46679578 | Data=133.61704000 | Physics=2.20874710 | Val RMSE: 1.21826673 | ‚àö(Val Loss) = 1.21111345 | Current Learning Rate: 0.002\n",
      "Epoch 310/1000 | Train Loss=13384.09596354 | Val Loss=1.49302793 | Data=133.81862488 | Physics=2.20732088 | Val RMSE: 1.22320962 | ‚àö(Val Loss) = 1.22189522 | Current Learning Rate: 0.002\n",
      "Epoch 311/1000 | Train Loss=13383.32695313 | Val Loss=1.49903961 | Data=133.81111450 | Physics=2.24920600 | Val RMSE: 1.21794116 | ‚àö(Val Loss) = 1.22435272 | Current Learning Rate: 0.002\n",
      "Epoch 312/1000 | Train Loss=13361.15488281 | Val Loss=1.52372372 | Data=133.58908946 | Physics=2.21971630 | Val RMSE: 1.22484636 | ‚àö(Val Loss) = 1.23439205 | Current Learning Rate: 0.002\n",
      "Epoch 313/1000 | Train Loss=13390.53268229 | Val Loss=1.53316204 | Data=133.88309326 | Physics=2.22063698 | Val RMSE: 1.21182132 | ‚àö(Val Loss) = 1.23820925 | Current Learning Rate: 0.002\n",
      "Epoch 314/1000 | Train Loss=13392.45013021 | Val Loss=1.50325119 | Data=133.90232137 | Physics=2.28056788 | Val RMSE: 1.21977282 | ‚àö(Val Loss) = 1.22607148 | Current Learning Rate: 0.002\n",
      "Epoch 315/1000 | Train Loss=13382.82799479 | Val Loss=1.56697067 | Data=133.80604808 | Physics=2.20853933 | Val RMSE: 1.21894383 | ‚àö(Val Loss) = 1.25178695 | Current Learning Rate: 0.002\n",
      "Epoch 316/1000 | Train Loss=13410.30169271 | Val Loss=1.47570844 | Data=134.08085022 | Physics=2.20894997 | Val RMSE: 1.21896935 | ‚àö(Val Loss) = 1.21478736 | Current Learning Rate: 0.002\n",
      "Epoch 317/1000 | Train Loss=13391.50514323 | Val Loss=1.52888131 | Data=133.89271393 | Physics=2.15651168 | Val RMSE: 1.22041464 | ‚àö(Val Loss) = 1.23647940 | Current Learning Rate: 0.002\n",
      "Epoch 318/1000 | Train Loss=13359.77890625 | Val Loss=1.55367068 | Data=133.57550761 | Physics=2.17844718 | Val RMSE: 1.22073746 | ‚àö(Val Loss) = 1.24646330 | Current Learning Rate: 0.002\n",
      "Epoch 319/1000 | Train Loss=13375.64459635 | Val Loss=1.50570675 | Data=133.73432007 | Physics=2.16864575 | Val RMSE: 1.22045505 | ‚àö(Val Loss) = 1.22707248 | Current Learning Rate: 0.002\n",
      "Epoch 320/1000 | Train Loss=13397.12304688 | Val Loss=1.51094131 | Data=133.94900716 | Physics=2.18260696 | Val RMSE: 1.22254169 | ‚àö(Val Loss) = 1.22920346 | Current Learning Rate: 0.002\n",
      "Epoch 321/1000 | Train Loss=13372.79329427 | Val Loss=1.49252816 | Data=133.70575053 | Physics=2.19997515 | Val RMSE: 1.22253692 | ‚àö(Val Loss) = 1.22169065 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 322/1000 | Train Loss=13378.48847656 | Val Loss=1.52844568 | Data=133.76272380 | Physics=2.21015762 | Val RMSE: 1.22077966 | ‚àö(Val Loss) = 1.23630321 | Current Learning Rate: 0.002\n",
      "Epoch 323/1000 | Train Loss=13414.04544271 | Val Loss=1.54076445 | Data=134.11822561 | Physics=2.23727216 | Val RMSE: 1.21993399 | ‚àö(Val Loss) = 1.24127531 | Current Learning Rate: 0.002\n",
      "Epoch 324/1000 | Train Loss=13406.58333333 | Val Loss=1.54640877 | Data=134.04364217 | Physics=2.25571319 | Val RMSE: 1.22015607 | ‚àö(Val Loss) = 1.24354684 | Current Learning Rate: 0.002\n",
      "Epoch 325/1000 | Train Loss=13349.92799479 | Val Loss=1.50206335 | Data=133.47710876 | Physics=2.13748982 | Val RMSE: 1.22184300 | ‚àö(Val Loss) = 1.22558701 | Current Learning Rate: 0.002\n",
      "Epoch 326/1000 | Train Loss=13378.59804688 | Val Loss=1.48403172 | Data=133.76366069 | Physics=2.21390620 | Val RMSE: 1.21948218 | ‚àö(Val Loss) = 1.21820843 | Current Learning Rate: 0.002\n",
      "Epoch 327/1000 | Train Loss=13375.25631510 | Val Loss=1.46591373 | Data=133.73037974 | Physics=2.22134609 | Val RMSE: 1.21737504 | ‚àö(Val Loss) = 1.21074927 | Current Learning Rate: 0.002\n",
      "Epoch 328/1000 | Train Loss=13385.76022135 | Val Loss=1.55491479 | Data=133.83537445 | Physics=2.25445200 | Val RMSE: 1.22133493 | ‚àö(Val Loss) = 1.24696219 | Current Learning Rate: 0.002\n",
      "Epoch 329/1000 | Train Loss=13368.53216146 | Val Loss=1.54450742 | Data=133.66310221 | Physics=2.19363109 | Val RMSE: 1.22196114 | ‚àö(Val Loss) = 1.24278212 | Current Learning Rate: 0.002\n",
      "Epoch 330/1000 | Train Loss=13395.13027344 | Val Loss=1.50771022 | Data=133.92914276 | Physics=2.15406103 | Val RMSE: 1.22273099 | ‚àö(Val Loss) = 1.22788846 | Current Learning Rate: 0.002\n",
      "Epoch 331/1000 | Train Loss=13394.64166667 | Val Loss=1.48415776 | Data=133.92412974 | Physics=2.27967376 | Val RMSE: 1.22057128 | ‚àö(Val Loss) = 1.21826017 | Current Learning Rate: 0.002\n",
      "Epoch 332/1000 | Train Loss=13386.38802083 | Val Loss=1.55951118 | Data=133.84164937 | Physics=2.26012373 | Val RMSE: 1.21983004 | ‚àö(Val Loss) = 1.24880385 | Current Learning Rate: 0.002\n",
      "Epoch 333/1000 | Train Loss=13384.94596354 | Val Loss=1.47244418 | Data=133.82722422 | Physics=2.18812268 | Val RMSE: 1.21670198 | ‚àö(Val Loss) = 1.21344316 | Current Learning Rate: 0.002\n",
      "Epoch 334/1000 | Train Loss=13380.81419271 | Val Loss=1.59601998 | Data=133.78566030 | Physics=2.23006202 | Val RMSE: 1.21775603 | ‚àö(Val Loss) = 1.26333690 | Current Learning Rate: 0.002\n",
      "Epoch 335/1000 | Train Loss=13380.53535156 | Val Loss=1.48908659 | Data=133.78314921 | Physics=2.22049403 | Val RMSE: 1.22082734 | ‚àö(Val Loss) = 1.22028136 | Current Learning Rate: 0.002\n",
      "Epoch 336/1000 | Train Loss=13380.56080729 | Val Loss=1.55372763 | Data=133.78334605 | Physics=2.28005727 | Val RMSE: 1.21926761 | ‚àö(Val Loss) = 1.24648607 | Current Learning Rate: 0.002\n",
      "Epoch 337/1000 | Train Loss=13374.61569010 | Val Loss=1.50637499 | Data=133.72406006 | Physics=2.13316831 | Val RMSE: 1.22032428 | ‚àö(Val Loss) = 1.22734463 | Current Learning Rate: 0.002\n",
      "Epoch 338/1000 | Train Loss=13381.76419271 | Val Loss=1.55495818 | Data=133.79532878 | Physics=2.27064359 | Val RMSE: 1.22575378 | ‚àö(Val Loss) = 1.24697959 | Current Learning Rate: 0.002\n",
      "Epoch 339/1000 | Train Loss=13371.11764323 | Val Loss=1.53163056 | Data=133.68885396 | Physics=2.14344235 | Val RMSE: 1.21999907 | ‚àö(Val Loss) = 1.23759055 | Current Learning Rate: 0.002\n",
      "Epoch 340/1000 | Train Loss=13362.15227865 | Val Loss=1.51295841 | Data=133.59936066 | Physics=2.18575722 | Val RMSE: 1.22218561 | ‚àö(Val Loss) = 1.23002374 | Current Learning Rate: 0.002\n",
      "Epoch 341/1000 | Train Loss=13379.38860677 | Val Loss=1.51206557 | Data=133.77148132 | Physics=2.30551244 | Val RMSE: 1.22720742 | ‚àö(Val Loss) = 1.22966075 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 342/1000 | Train Loss=13368.97031250 | Val Loss=1.54759836 | Data=133.66720479 | Physics=2.13585000 | Val RMSE: 1.22009277 | ‚àö(Val Loss) = 1.24402511 | Current Learning Rate: 0.002\n",
      "Epoch 343/1000 | Train Loss=13383.70006510 | Val Loss=1.51573408 | Data=133.81469421 | Physics=2.15318192 | Val RMSE: 1.22073066 | ‚àö(Val Loss) = 1.23115158 | Current Learning Rate: 0.002\n",
      "Epoch 344/1000 | Train Loss=13374.84837240 | Val Loss=1.50943331 | Data=133.72628021 | Physics=2.24865073 | Val RMSE: 1.22277021 | ‚àö(Val Loss) = 1.22858989 | Current Learning Rate: 0.002\n",
      "Epoch 345/1000 | Train Loss=13384.13437500 | Val Loss=1.48999159 | Data=133.81897227 | Physics=2.22552768 | Val RMSE: 1.22065139 | ‚àö(Val Loss) = 1.22065210 | Current Learning Rate: 0.002\n",
      "Epoch 346/1000 | Train Loss=13385.14485677 | Val Loss=1.61778728 | Data=133.82910309 | Physics=2.28409887 | Val RMSE: 1.22291493 | ‚àö(Val Loss) = 1.27192271 | Current Learning Rate: 0.002\n",
      "Epoch 347/1000 | Train Loss=13384.04576823 | Val Loss=1.50121574 | Data=133.81820068 | Physics=2.19799025 | Val RMSE: 1.22177291 | ‚àö(Val Loss) = 1.22524107 | Current Learning Rate: 0.002\n",
      "Epoch 348/1000 | Train Loss=13370.74205729 | Val Loss=1.54802469 | Data=133.68514811 | Physics=2.34096634 | Val RMSE: 1.22095084 | ‚àö(Val Loss) = 1.24419641 | Current Learning Rate: 0.002\n",
      "Epoch 349/1000 | Train Loss=13376.79335938 | Val Loss=1.51810356 | Data=133.74565328 | Physics=2.24633207 | Val RMSE: 1.22078192 | ‚àö(Val Loss) = 1.23211348 | Current Learning Rate: 0.002\n",
      "Epoch 350/1000 | Train Loss=13324.92799479 | Val Loss=1.53461933 | Data=133.22702789 | Physics=2.21779376 | Val RMSE: 1.21942949 | ‚àö(Val Loss) = 1.23879755 | Current Learning Rate: 0.002\n",
      "Epoch 351/1000 | Train Loss=13390.37291667 | Val Loss=1.53244710 | Data=133.88152974 | Physics=2.19610170 | Val RMSE: 1.21862793 | ‚àö(Val Loss) = 1.23792052 | Current Learning Rate: 0.002\n",
      "Epoch 352/1000 | Train Loss=13369.59042969 | Val Loss=1.58749688 | Data=133.67366842 | Physics=2.24568447 | Val RMSE: 1.22338760 | ‚àö(Val Loss) = 1.25995910 | Current Learning Rate: 0.002\n",
      "Epoch 353/1000 | Train Loss=13390.84661458 | Val Loss=1.46688980 | Data=133.88623657 | Physics=2.27061403 | Val RMSE: 1.22529209 | ‚àö(Val Loss) = 1.21115232 | Current Learning Rate: 0.002\n",
      "Epoch 354/1000 | Train Loss=13382.28867187 | Val Loss=1.55495397 | Data=133.80062612 | Physics=2.22541293 | Val RMSE: 1.22204769 | ‚àö(Val Loss) = 1.24697793 | Current Learning Rate: 0.002\n",
      "Epoch 355/1000 | Train Loss=13367.12851562 | Val Loss=1.50372525 | Data=133.64907227 | Physics=2.21444803 | Val RMSE: 1.22015703 | ‚àö(Val Loss) = 1.22626472 | Current Learning Rate: 0.002\n",
      "Epoch 356/1000 | Train Loss=13379.52513021 | Val Loss=1.60512674 | Data=133.77286326 | Physics=2.30398127 | Val RMSE: 1.22232223 | ‚àö(Val Loss) = 1.26693594 | Current Learning Rate: 0.002\n",
      "Epoch 357/1000 | Train Loss=13390.48554687 | Val Loss=1.50009358 | Data=133.88266296 | Physics=2.22651558 | Val RMSE: 1.22186422 | ‚àö(Val Loss) = 1.22478306 | Current Learning Rate: 0.002\n",
      "Epoch 358/1000 | Train Loss=13369.16575521 | Val Loss=1.49670136 | Data=133.66940104 | Physics=2.27448149 | Val RMSE: 1.21990728 | ‚àö(Val Loss) = 1.22339749 | Current Learning Rate: 0.002\n",
      "Epoch 359/1000 | Train Loss=13383.53574219 | Val Loss=1.50645014 | Data=133.81297557 | Physics=2.15326680 | Val RMSE: 1.21997285 | ‚àö(Val Loss) = 1.22737527 | Current Learning Rate: 0.002\n",
      "Epoch 360/1000 | Train Loss=13342.36191406 | Val Loss=1.53488998 | Data=133.40143890 | Physics=2.18983206 | Val RMSE: 1.22149515 | ‚àö(Val Loss) = 1.23890674 | Current Learning Rate: 0.002\n",
      "Epoch 361/1000 | Train Loss=13367.82792969 | Val Loss=1.52516909 | Data=133.65596670 | Physics=2.27571421 | Val RMSE: 1.22111070 | ‚àö(Val Loss) = 1.23497736 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 362/1000 | Train Loss=13390.19361979 | Val Loss=1.52713970 | Data=133.87962850 | Physics=2.15576060 | Val RMSE: 1.21887350 | ‚àö(Val Loss) = 1.23577487 | Current Learning Rate: 0.002\n",
      "Epoch 363/1000 | Train Loss=13368.83014323 | Val Loss=1.52903648 | Data=133.66610718 | Physics=2.12222656 | Val RMSE: 1.22093666 | ‚àö(Val Loss) = 1.23654211 | Current Learning Rate: 0.002\n",
      "Epoch 364/1000 | Train Loss=13371.19479167 | Val Loss=1.54738128 | Data=133.68955536 | Physics=2.34757506 | Val RMSE: 1.22254980 | ‚àö(Val Loss) = 1.24393785 | Current Learning Rate: 0.002\n",
      "Epoch 365/1000 | Train Loss=13351.70221354 | Val Loss=1.57266978 | Data=133.49470825 | Physics=2.15093445 | Val RMSE: 1.22184873 | ‚àö(Val Loss) = 1.25406134 | Current Learning Rate: 0.002\n",
      "Epoch 366/1000 | Train Loss=13393.69570313 | Val Loss=1.54910044 | Data=133.91475372 | Physics=2.23570124 | Val RMSE: 1.22091997 | ‚àö(Val Loss) = 1.24462867 | Current Learning Rate: 0.002\n",
      "Epoch 367/1000 | Train Loss=13352.44427083 | Val Loss=1.48533400 | Data=133.50225169 | Physics=2.21708888 | Val RMSE: 1.21648443 | ‚àö(Val Loss) = 1.21874285 | Current Learning Rate: 0.002\n",
      "Epoch 368/1000 | Train Loss=13376.26529948 | Val Loss=1.57688351 | Data=133.74031118 | Physics=2.17253982 | Val RMSE: 1.22136593 | ‚àö(Val Loss) = 1.25574028 | Current Learning Rate: 0.002\n",
      "Epoch 369/1000 | Train Loss=13407.03671875 | Val Loss=1.54714215 | Data=134.04819845 | Physics=2.25000141 | Val RMSE: 1.21888828 | ‚àö(Val Loss) = 1.24384165 | Current Learning Rate: 0.002\n",
      "Epoch 370/1000 | Train Loss=13373.52558594 | Val Loss=1.52918009 | Data=133.71302948 | Physics=2.16594573 | Val RMSE: 1.22058034 | ‚àö(Val Loss) = 1.23660016 | Current Learning Rate: 0.002\n",
      "Epoch 371/1000 | Train Loss=13390.71464844 | Val Loss=1.52611725 | Data=133.88500417 | Physics=2.18281011 | Val RMSE: 1.22450900 | ‚àö(Val Loss) = 1.23536122 | Current Learning Rate: 0.002\n",
      "Epoch 372/1000 | Train Loss=13370.01028646 | Val Loss=1.52761837 | Data=133.67796427 | Physics=2.20669221 | Val RMSE: 1.21933961 | ‚àö(Val Loss) = 1.23596859 | Current Learning Rate: 0.002\n",
      "Epoch 373/1000 | Train Loss=13404.28763021 | Val Loss=1.46983691 | Data=134.02064209 | Physics=2.20372284 | Val RMSE: 1.21090412 | ‚àö(Val Loss) = 1.21236837 | Current Learning Rate: 0.002\n",
      "Epoch 374/1000 | Train Loss=13374.43886719 | Val Loss=1.51638146 | Data=133.72224325 | Physics=2.25256915 | Val RMSE: 1.22042513 | ‚àö(Val Loss) = 1.23141444 | Current Learning Rate: 0.002\n",
      "Epoch 375/1000 | Train Loss=13361.68138021 | Val Loss=1.53258681 | Data=133.59459076 | Physics=2.21956665 | Val RMSE: 1.22220373 | ‚àö(Val Loss) = 1.23797691 | Current Learning Rate: 0.002\n",
      "Epoch 376/1000 | Train Loss=13402.55305990 | Val Loss=1.48250667 | Data=134.00335999 | Physics=2.20871336 | Val RMSE: 1.22013438 | ‚àö(Val Loss) = 1.21758235 | Current Learning Rate: 0.002\n",
      "Epoch 377/1000 | Train Loss=13388.47955729 | Val Loss=1.59223127 | Data=133.86243744 | Physics=2.31846734 | Val RMSE: 1.22099161 | ‚àö(Val Loss) = 1.26183641 | Current Learning Rate: 0.002\n",
      "Epoch 378/1000 | Train Loss=13356.78867187 | Val Loss=1.51801848 | Data=133.54572856 | Physics=2.16150881 | Val RMSE: 1.21991301 | ‚àö(Val Loss) = 1.23207891 | Current Learning Rate: 0.002\n",
      "Epoch 379/1000 | Train Loss=13370.96861979 | Val Loss=1.51762009 | Data=133.68746796 | Physics=2.16579145 | Val RMSE: 1.21920621 | ‚àö(Val Loss) = 1.23191726 | Current Learning Rate: 0.002\n",
      "Epoch 380/1000 | Train Loss=13385.06621094 | Val Loss=1.52109436 | Data=133.82840474 | Physics=2.20292928 | Val RMSE: 1.21821594 | ‚àö(Val Loss) = 1.23332655 | Current Learning Rate: 0.002\n",
      "Epoch 381/1000 | Train Loss=13400.25546875 | Val Loss=1.55209053 | Data=133.98036092 | Physics=2.22144909 | Val RMSE: 1.22133839 | ‚àö(Val Loss) = 1.24582922 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 382/1000 | Train Loss=13422.02141927 | Val Loss=1.51531235 | Data=134.19804840 | Physics=2.15364996 | Val RMSE: 1.22035336 | ‚àö(Val Loss) = 1.23098028 | Current Learning Rate: 0.002\n",
      "Epoch 383/1000 | Train Loss=13395.03867187 | Val Loss=1.49255153 | Data=133.92820384 | Physics=2.20744744 | Val RMSE: 1.22204781 | ‚àö(Val Loss) = 1.22170031 | Current Learning Rate: 0.002\n",
      "Epoch 384/1000 | Train Loss=13360.75813802 | Val Loss=1.50217215 | Data=133.58544820 | Physics=2.22689197 | Val RMSE: 1.21876490 | ‚àö(Val Loss) = 1.22563136 | Current Learning Rate: 0.002\n",
      "Epoch 385/1000 | Train Loss=13357.67265625 | Val Loss=1.47688238 | Data=133.55429535 | Physics=2.18765468 | Val RMSE: 1.21860194 | ‚àö(Val Loss) = 1.21527052 | Current Learning Rate: 0.002\n",
      "Epoch 386/1000 | Train Loss=13347.82395833 | Val Loss=1.57632275 | Data=133.45592753 | Physics=2.20410531 | Val RMSE: 1.22166181 | ‚àö(Val Loss) = 1.25551689 | Current Learning Rate: 0.002\n",
      "Epoch 387/1000 | Train Loss=13384.48763021 | Val Loss=1.45371199 | Data=133.82270559 | Physics=2.22666848 | Val RMSE: 1.21688557 | ‚àö(Val Loss) = 1.20569980 | Current Learning Rate: 0.002\n",
      "Epoch 388/1000 | Train Loss=13364.55833333 | Val Loss=1.55625280 | Data=133.62338613 | Physics=2.22536278 | Val RMSE: 1.22039199 | ‚àö(Val Loss) = 1.24749863 | Current Learning Rate: 0.002\n",
      "Epoch 389/1000 | Train Loss=13387.57350260 | Val Loss=1.54722214 | Data=133.85348104 | Physics=2.26062508 | Val RMSE: 1.22041154 | ‚àö(Val Loss) = 1.24387383 | Current Learning Rate: 0.002\n",
      "Epoch 390/1000 | Train Loss=13368.52024740 | Val Loss=1.48072020 | Data=133.66287028 | Physics=2.23211977 | Val RMSE: 1.22159266 | ‚àö(Val Loss) = 1.21684849 | Current Learning Rate: 0.002\n",
      "Epoch 391/1000 | Train Loss=13361.46894531 | Val Loss=1.59835223 | Data=133.59238790 | Physics=2.22033203 | Val RMSE: 1.22017276 | ‚àö(Val Loss) = 1.26425958 | Current Learning Rate: 0.002\n",
      "Epoch 392/1000 | Train Loss=13384.25266927 | Val Loss=1.49365969 | Data=133.82034505 | Physics=2.20710351 | Val RMSE: 1.22235799 | ‚àö(Val Loss) = 1.22215378 | Current Learning Rate: 0.002\n",
      "Epoch 393/1000 | Train Loss=13383.06731771 | Val Loss=1.53212245 | Data=133.80844625 | Physics=2.19453552 | Val RMSE: 1.21846867 | ‚àö(Val Loss) = 1.23778939 | Current Learning Rate: 0.002\n",
      "Epoch 394/1000 | Train Loss=13396.01158854 | Val Loss=1.49084397 | Data=133.93795878 | Physics=2.16484200 | Val RMSE: 1.22049737 | ‚àö(Val Loss) = 1.22100127 | Current Learning Rate: 0.002\n",
      "Epoch 395/1000 | Train Loss=13369.70208333 | Val Loss=1.55728726 | Data=133.67479604 | Physics=2.30933246 | Val RMSE: 1.21963501 | ‚àö(Val Loss) = 1.24791312 | Current Learning Rate: 0.002\n",
      "Epoch 396/1000 | Train Loss=13368.99954427 | Val Loss=1.54375486 | Data=133.66779887 | Physics=2.22320169 | Val RMSE: 1.22199476 | ‚àö(Val Loss) = 1.24247932 | Current Learning Rate: 0.002\n",
      "Epoch 397/1000 | Train Loss=13384.19055990 | Val Loss=1.58476011 | Data=133.81967875 | Physics=2.25094337 | Val RMSE: 1.22107446 | ‚àö(Val Loss) = 1.25887251 | Current Learning Rate: 0.002\n",
      "Epoch 398/1000 | Train Loss=13374.24720052 | Val Loss=1.60056563 | Data=133.72011973 | Physics=2.27686271 | Val RMSE: 1.22119021 | ‚àö(Val Loss) = 1.26513469 | Current Learning Rate: 0.002\n",
      "Epoch 399/1000 | Train Loss=13379.61946615 | Val Loss=1.48637136 | Data=133.77404277 | Physics=2.21599879 | Val RMSE: 1.22415149 | ‚àö(Val Loss) = 1.21916831 | Current Learning Rate: 0.002\n",
      "Epoch 400/1000 | Train Loss=13362.08398438 | Val Loss=1.53577280 | Data=133.59860992 | Physics=2.12214129 | Val RMSE: 1.22094154 | ‚àö(Val Loss) = 1.23926306 | Current Learning Rate: 0.0002\n",
      "Epoch 401/1000 | Train Loss=13372.58313802 | Val Loss=1.45874790 | Data=133.70335795 | Physics=2.31933317 | Val RMSE: 1.22245526 | ‚àö(Val Loss) = 1.20778632 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 402/1000 | Train Loss=516.24168701 | Val Loss=1.46136900 | Data=5.14004119 | Physics=2.15786139 | Val RMSE: 1.22237921 | ‚àö(Val Loss) = 1.20887101 | Current Learning Rate: 0.0002\n",
      "Epoch 403/1000 | Train Loss=515.76789958 | Val Loss=1.46180602 | Data=5.13530347 | Physics=2.30764388 | Val RMSE: 1.22254169 | ‚àö(Val Loss) = 1.20905173 | Current Learning Rate: 0.0002\n",
      "Epoch 404/1000 | Train Loss=515.03655192 | Val Loss=1.46088640 | Data=5.12801361 | Physics=2.22824778 | Val RMSE: 1.22257686 | ‚àö(Val Loss) = 1.20867133 | Current Learning Rate: 0.0002\n",
      "Epoch 405/1000 | Train Loss=516.65435181 | Val Loss=1.45852427 | Data=5.14419982 | Physics=2.12442065 | Val RMSE: 1.22256207 | ‚àö(Val Loss) = 1.20769382 | Current Learning Rate: 0.0002\n",
      "Epoch 406/1000 | Train Loss=515.93121338 | Val Loss=1.45997586 | Data=5.13694181 | Physics=2.23653861 | Val RMSE: 1.22257650 | ‚àö(Val Loss) = 1.20829463 | Current Learning Rate: 0.0002\n",
      "Epoch 407/1000 | Train Loss=514.71805013 | Val Loss=1.45928007 | Data=5.12479054 | Physics=2.28169829 | Val RMSE: 1.22260094 | ‚àö(Val Loss) = 1.20800662 | Current Learning Rate: 0.0002\n",
      "Epoch 408/1000 | Train Loss=515.70759481 | Val Loss=1.45770504 | Data=5.13467429 | Physics=2.19675170 | Val RMSE: 1.22259617 | ‚àö(Val Loss) = 1.20735455 | Current Learning Rate: 0.0002\n",
      "Epoch 409/1000 | Train Loss=515.17893677 | Val Loss=1.45960329 | Data=5.12941284 | Physics=2.19297948 | Val RMSE: 1.22260821 | ‚àö(Val Loss) = 1.20814049 | Current Learning Rate: 0.0002\n",
      "Epoch 410/1000 | Train Loss=515.32753092 | Val Loss=1.45797765 | Data=5.13092779 | Physics=2.26927912 | Val RMSE: 1.22259843 | ‚àö(Val Loss) = 1.20746744 | Current Learning Rate: 0.0002\n",
      "Epoch 411/1000 | Train Loss=517.09224040 | Val Loss=1.46094402 | Data=5.14855137 | Physics=2.17056277 | Val RMSE: 1.22263217 | ‚àö(Val Loss) = 1.20869517 | Current Learning Rate: 0.0002\n",
      "Epoch 412/1000 | Train Loss=514.77806803 | Val Loss=1.46601021 | Data=5.12544632 | Physics=2.22510962 | Val RMSE: 1.22268200 | ‚àö(Val Loss) = 1.21078908 | Current Learning Rate: 0.0002\n",
      "Epoch 413/1000 | Train Loss=515.19541219 | Val Loss=1.45935114 | Data=5.12958336 | Physics=2.25796259 | Val RMSE: 1.22261286 | ‚àö(Val Loss) = 1.20803607 | Current Learning Rate: 0.0002\n",
      "Epoch 414/1000 | Train Loss=515.86303304 | Val Loss=1.45906977 | Data=5.13625749 | Physics=2.28164013 | Val RMSE: 1.22262633 | ‚àö(Val Loss) = 1.20791960 | Current Learning Rate: 0.0002\n",
      "Epoch 415/1000 | Train Loss=515.67870483 | Val Loss=1.45384276 | Data=5.13433297 | Physics=2.24494758 | Val RMSE: 1.22258663 | ‚àö(Val Loss) = 1.20575404 | Current Learning Rate: 0.0002\n",
      "Epoch 416/1000 | Train Loss=514.09942830 | Val Loss=1.46035341 | Data=5.11861995 | Physics=2.18527933 | Val RMSE: 1.22267723 | ‚àö(Val Loss) = 1.20845079 | Current Learning Rate: 0.0002\n",
      "Epoch 417/1000 | Train Loss=516.56763916 | Val Loss=1.45405893 | Data=5.14322726 | Physics=2.26170816 | Val RMSE: 1.22261035 | ‚àö(Val Loss) = 1.20584369 | Current Learning Rate: 0.0002\n",
      "Epoch 418/1000 | Train Loss=516.46746826 | Val Loss=1.45556200 | Data=5.14220530 | Physics=2.29524542 | Val RMSE: 1.22263646 | ‚àö(Val Loss) = 1.20646679 | Current Learning Rate: 0.0002\n",
      "Epoch 419/1000 | Train Loss=517.26217448 | Val Loss=1.45583002 | Data=5.15016870 | Physics=2.28404663 | Val RMSE: 1.22264361 | ‚àö(Val Loss) = 1.20657778 | Current Learning Rate: 0.0002\n",
      "Epoch 420/1000 | Train Loss=515.88941854 | Val Loss=1.45898267 | Data=5.13647264 | Physics=2.18730396 | Val RMSE: 1.22268486 | ‚àö(Val Loss) = 1.20788360 | Current Learning Rate: 0.0002\n",
      "Epoch 421/1000 | Train Loss=515.93110352 | Val Loss=1.45902381 | Data=5.13692624 | Physics=2.24470740 | Val RMSE: 1.22266757 | ‚àö(Val Loss) = 1.20790064 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 422/1000 | Train Loss=515.35317790 | Val Loss=1.46053255 | Data=5.13115616 | Physics=2.22423416 | Val RMSE: 1.22270370 | ‚àö(Val Loss) = 1.20852494 | Current Learning Rate: 0.0002\n",
      "Epoch 423/1000 | Train Loss=518.22916260 | Val Loss=1.45810221 | Data=5.15990858 | Physics=2.18013643 | Val RMSE: 1.22265613 | ‚àö(Val Loss) = 1.20751905 | Current Learning Rate: 0.0002\n",
      "Epoch 424/1000 | Train Loss=517.30643311 | Val Loss=1.45842944 | Data=5.15066805 | Physics=2.25289109 | Val RMSE: 1.22267616 | ‚àö(Val Loss) = 1.20765448 | Current Learning Rate: 0.0002\n",
      "Epoch 425/1000 | Train Loss=518.33920085 | Val Loss=1.45669377 | Data=5.16096443 | Physics=2.29216617 | Val RMSE: 1.22266102 | ‚àö(Val Loss) = 1.20693564 | Current Learning Rate: 0.0002\n",
      "Epoch 426/1000 | Train Loss=516.08024089 | Val Loss=1.45628111 | Data=5.13836037 | Physics=2.22486372 | Val RMSE: 1.22265601 | ‚àö(Val Loss) = 1.20676470 | Current Learning Rate: 0.0002\n",
      "Epoch 427/1000 | Train Loss=516.54883830 | Val Loss=1.45728866 | Data=5.14307105 | Physics=2.23622500 | Val RMSE: 1.22266340 | ‚àö(Val Loss) = 1.20718205 | Current Learning Rate: 0.0002\n",
      "Epoch 428/1000 | Train Loss=514.35553182 | Val Loss=1.45673851 | Data=5.12111988 | Physics=2.26974213 | Val RMSE: 1.22267079 | ‚àö(Val Loss) = 1.20695424 | Current Learning Rate: 0.0002\n",
      "Epoch 429/1000 | Train Loss=516.14511515 | Val Loss=1.45656343 | Data=5.13902534 | Physics=2.19264709 | Val RMSE: 1.22266293 | ‚àö(Val Loss) = 1.20688176 | Current Learning Rate: 0.0002\n",
      "Epoch 430/1000 | Train Loss=517.21683757 | Val Loss=1.45639980 | Data=5.14975561 | Physics=2.15203389 | Val RMSE: 1.22266328 | ‚àö(Val Loss) = 1.20681393 | Current Learning Rate: 0.0002\n",
      "Epoch 431/1000 | Train Loss=515.08941243 | Val Loss=1.45969828 | Data=5.12846858 | Physics=2.24888644 | Val RMSE: 1.22271180 | ‚àö(Val Loss) = 1.20817971 | Current Learning Rate: 0.0002\n",
      "Epoch 432/1000 | Train Loss=517.13235677 | Val Loss=1.45605781 | Data=5.14891240 | Physics=2.20041720 | Val RMSE: 1.22263956 | ‚àö(Val Loss) = 1.20667219 | Current Learning Rate: 0.0002\n",
      "Epoch 433/1000 | Train Loss=517.62482707 | Val Loss=1.45725576 | Data=5.15380421 | Physics=2.23789125 | Val RMSE: 1.22267497 | ‚àö(Val Loss) = 1.20716846 | Current Learning Rate: 0.0002\n",
      "Epoch 434/1000 | Train Loss=516.10297445 | Val Loss=1.45470126 | Data=5.13859663 | Physics=2.27695621 | Val RMSE: 1.22265112 | ‚àö(Val Loss) = 1.20611000 | Current Learning Rate: 0.0002\n",
      "Epoch 435/1000 | Train Loss=516.40487061 | Val Loss=1.45594251 | Data=5.14160973 | Physics=2.18618752 | Val RMSE: 1.22267127 | ‚àö(Val Loss) = 1.20662439 | Current Learning Rate: 0.0002\n",
      "Epoch 436/1000 | Train Loss=515.94024658 | Val Loss=1.45712717 | Data=5.13696502 | Physics=2.24966835 | Val RMSE: 1.22269177 | ‚àö(Val Loss) = 1.20711529 | Current Learning Rate: 0.0002\n",
      "Epoch 437/1000 | Train Loss=513.96447754 | Val Loss=1.45910317 | Data=5.11722972 | Physics=2.25554822 | Val RMSE: 1.22272241 | ‚àö(Val Loss) = 1.20793343 | Current Learning Rate: 0.0002\n",
      "Epoch 438/1000 | Train Loss=514.02681071 | Val Loss=1.45921143 | Data=5.11786445 | Physics=2.22107023 | Val RMSE: 1.22271240 | ‚àö(Val Loss) = 1.20797825 | Current Learning Rate: 0.0002\n",
      "Epoch 439/1000 | Train Loss=516.59872843 | Val Loss=1.45768799 | Data=5.14358190 | Physics=2.22856306 | Val RMSE: 1.22268808 | ‚àö(Val Loss) = 1.20734751 | Current Learning Rate: 0.0002\n",
      "Epoch 440/1000 | Train Loss=515.99360555 | Val Loss=1.45792556 | Data=5.13750347 | Physics=2.28179621 | Val RMSE: 1.22270906 | ‚àö(Val Loss) = 1.20744586 | Current Learning Rate: 0.0002\n",
      "Epoch 441/1000 | Train Loss=516.96168213 | Val Loss=1.45600824 | Data=5.14716946 | Physics=2.26023424 | Val RMSE: 1.22268474 | ‚àö(Val Loss) = 1.20665169 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 442/1000 | Train Loss=514.99783936 | Val Loss=1.45557717 | Data=5.12753372 | Physics=2.22501806 | Val RMSE: 1.22267938 | ‚àö(Val Loss) = 1.20647299 | Current Learning Rate: 0.0002\n",
      "Epoch 443/1000 | Train Loss=516.32612712 | Val Loss=1.45742098 | Data=5.14083579 | Physics=2.21622055 | Val RMSE: 1.22270095 | ‚àö(Val Loss) = 1.20723689 | Current Learning Rate: 0.0002\n",
      "Epoch 444/1000 | Train Loss=515.28378703 | Val Loss=1.45722544 | Data=5.13045171 | Physics=2.13377206 | Val RMSE: 1.22269487 | ‚àö(Val Loss) = 1.20715594 | Current Learning Rate: 0.0002\n",
      "Epoch 445/1000 | Train Loss=518.38747965 | Val Loss=1.45644228 | Data=5.16143471 | Physics=2.23377875 | Val RMSE: 1.22268641 | ‚àö(Val Loss) = 1.20683146 | Current Learning Rate: 0.0002\n",
      "Epoch 446/1000 | Train Loss=516.53477783 | Val Loss=1.45689940 | Data=5.14290209 | Physics=2.24406558 | Val RMSE: 1.22270954 | ‚àö(Val Loss) = 1.20702088 | Current Learning Rate: 0.0002\n",
      "Epoch 447/1000 | Train Loss=515.21404012 | Val Loss=1.45851403 | Data=5.12974733 | Physics=2.24177990 | Val RMSE: 1.22271776 | ‚àö(Val Loss) = 1.20768952 | Current Learning Rate: 0.0002\n",
      "Epoch 448/1000 | Train Loss=514.79794515 | Val Loss=1.46058184 | Data=5.12557968 | Physics=2.24068743 | Val RMSE: 1.22275913 | ‚àö(Val Loss) = 1.20854533 | Current Learning Rate: 0.0002\n",
      "Epoch 449/1000 | Train Loss=515.49776408 | Val Loss=1.45870471 | Data=5.13255428 | Physics=2.29557596 | Val RMSE: 1.22273684 | ‚àö(Val Loss) = 1.20776844 | Current Learning Rate: 0.0002\n",
      "Epoch 450/1000 | Train Loss=515.55891927 | Val Loss=1.45722687 | Data=5.13316847 | Physics=2.15588297 | Val RMSE: 1.22272480 | ‚àö(Val Loss) = 1.20715654 | Current Learning Rate: 0.0002\n",
      "Epoch 451/1000 | Train Loss=514.66256917 | Val Loss=1.45796911 | Data=5.12421182 | Physics=2.23987176 | Val RMSE: 1.22273290 | ‚àö(Val Loss) = 1.20746386 | Current Learning Rate: 0.0002\n",
      "Epoch 452/1000 | Train Loss=516.13480225 | Val Loss=1.45720096 | Data=5.13891493 | Physics=2.23473014 | Val RMSE: 1.22272253 | ‚àö(Val Loss) = 1.20714581 | Current Learning Rate: 0.0002\n",
      "Epoch 453/1000 | Train Loss=516.72086995 | Val Loss=1.45723359 | Data=5.14478979 | Physics=2.23561260 | Val RMSE: 1.22271454 | ‚àö(Val Loss) = 1.20715928 | Current Learning Rate: 0.0002\n",
      "Epoch 454/1000 | Train Loss=517.37455241 | Val Loss=1.45569094 | Data=5.15128425 | Physics=2.29399899 | Val RMSE: 1.22269964 | ‚àö(Val Loss) = 1.20652020 | Current Learning Rate: 0.0002\n",
      "Epoch 455/1000 | Train Loss=515.36672770 | Val Loss=1.45945577 | Data=5.13123840 | Physics=2.27385927 | Val RMSE: 1.22276497 | ‚àö(Val Loss) = 1.20807934 | Current Learning Rate: 0.0002\n",
      "Epoch 456/1000 | Train Loss=516.26193848 | Val Loss=1.45904275 | Data=5.14022490 | Physics=2.24250432 | Val RMSE: 1.22275114 | ‚àö(Val Loss) = 1.20790839 | Current Learning Rate: 0.0002\n",
      "Epoch 457/1000 | Train Loss=514.73305054 | Val Loss=1.45911014 | Data=5.12490635 | Physics=2.29281321 | Val RMSE: 1.22275209 | ‚àö(Val Loss) = 1.20793629 | Current Learning Rate: 0.0002\n",
      "Epoch 458/1000 | Train Loss=516.57628377 | Val Loss=1.45890647 | Data=5.14335925 | Physics=2.22094472 | Val RMSE: 1.22275472 | ‚àö(Val Loss) = 1.20785201 | Current Learning Rate: 0.0002\n",
      "Epoch 459/1000 | Train Loss=517.36572876 | Val Loss=1.45733809 | Data=5.15121196 | Physics=2.26945717 | Val RMSE: 1.22273386 | ‚àö(Val Loss) = 1.20720255 | Current Learning Rate: 0.0002\n",
      "Epoch 460/1000 | Train Loss=514.89542033 | Val Loss=1.45731342 | Data=5.12651033 | Physics=2.30083388 | Val RMSE: 1.22273862 | ‚àö(Val Loss) = 1.20719242 | Current Learning Rate: 0.0002\n",
      "Epoch 461/1000 | Train Loss=516.64621175 | Val Loss=1.45736929 | Data=5.14400085 | Physics=2.27812347 | Val RMSE: 1.22273099 | ‚àö(Val Loss) = 1.20721555 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 462/1000 | Train Loss=517.05659180 | Val Loss=1.45830941 | Data=5.14816558 | Physics=2.23174063 | Val RMSE: 1.22276402 | ‚àö(Val Loss) = 1.20760477 | Current Learning Rate: 0.0002\n",
      "Epoch 463/1000 | Train Loss=517.35961711 | Val Loss=1.45690628 | Data=5.15114711 | Physics=2.21405344 | Val RMSE: 1.22273731 | ‚àö(Val Loss) = 1.20702374 | Current Learning Rate: 0.0002\n",
      "Epoch 464/1000 | Train Loss=514.29699910 | Val Loss=1.45681234 | Data=5.12052040 | Physics=2.27586644 | Val RMSE: 1.22273862 | ‚àö(Val Loss) = 1.20698488 | Current Learning Rate: 0.0002\n",
      "Epoch 465/1000 | Train Loss=515.11245524 | Val Loss=1.45844601 | Data=5.12872470 | Physics=2.17067764 | Val RMSE: 1.22276437 | ‚àö(Val Loss) = 1.20766139 | Current Learning Rate: 0.0002\n",
      "Epoch 466/1000 | Train Loss=513.72048950 | Val Loss=1.45676327 | Data=5.11477391 | Physics=2.26177369 | Val RMSE: 1.22272861 | ‚àö(Val Loss) = 1.20696449 | Current Learning Rate: 0.0002\n",
      "Epoch 467/1000 | Train Loss=517.11054891 | Val Loss=1.45669889 | Data=5.14865573 | Physics=2.22016193 | Val RMSE: 1.22272158 | ‚àö(Val Loss) = 1.20693779 | Current Learning Rate: 0.0002\n",
      "Epoch 468/1000 | Train Loss=516.00386149 | Val Loss=1.45731048 | Data=5.13762067 | Physics=2.21002139 | Val RMSE: 1.22275352 | ‚àö(Val Loss) = 1.20719111 | Current Learning Rate: 0.0002\n",
      "Epoch 469/1000 | Train Loss=515.39610392 | Val Loss=1.45708684 | Data=5.13153226 | Physics=2.19783060 | Val RMSE: 1.22274566 | ‚àö(Val Loss) = 1.20709848 | Current Learning Rate: 0.0002\n",
      "Epoch 470/1000 | Train Loss=515.29304606 | Val Loss=1.45605399 | Data=5.13053214 | Physics=2.20166672 | Val RMSE: 1.22271228 | ‚àö(Val Loss) = 1.20667064 | Current Learning Rate: 0.0002\n",
      "Epoch 471/1000 | Train Loss=515.33099976 | Val Loss=1.45812877 | Data=5.13088195 | Physics=2.26070580 | Val RMSE: 1.22275996 | ‚àö(Val Loss) = 1.20753002 | Current Learning Rate: 0.0002\n",
      "Epoch 472/1000 | Train Loss=517.27005412 | Val Loss=1.45639018 | Data=5.15022907 | Physics=2.36798032 | Val RMSE: 1.22273326 | ‚àö(Val Loss) = 1.20680988 | Current Learning Rate: 0.0002\n",
      "Epoch 473/1000 | Train Loss=516.10699870 | Val Loss=1.45861093 | Data=5.13867356 | Physics=2.24254964 | Val RMSE: 1.22276616 | ‚àö(Val Loss) = 1.20772970 | Current Learning Rate: 0.0002\n",
      "Epoch 474/1000 | Train Loss=516.61946818 | Val Loss=1.45565828 | Data=5.14375582 | Physics=2.23931497 | Val RMSE: 1.22272301 | ‚àö(Val Loss) = 1.20650661 | Current Learning Rate: 0.0002\n",
      "Epoch 475/1000 | Train Loss=514.46901042 | Val Loss=1.45657941 | Data=5.12222961 | Physics=2.29204158 | Val RMSE: 1.22274590 | ‚àö(Val Loss) = 1.20688832 | Current Learning Rate: 0.0002\n",
      "Epoch 476/1000 | Train Loss=514.37955526 | Val Loss=1.45714410 | Data=5.12137604 | Physics=2.27003487 | Val RMSE: 1.22274709 | ‚àö(Val Loss) = 1.20712221 | Current Learning Rate: 0.0002\n",
      "Epoch 477/1000 | Train Loss=515.08739827 | Val Loss=1.45662649 | Data=5.12844241 | Physics=2.27591785 | Val RMSE: 1.22274351 | ‚àö(Val Loss) = 1.20690787 | Current Learning Rate: 0.0002\n",
      "Epoch 478/1000 | Train Loss=516.24273275 | Val Loss=1.45644108 | Data=5.13998524 | Physics=2.18062832 | Val RMSE: 1.22271085 | ‚àö(Val Loss) = 1.20683098 | Current Learning Rate: 0.0002\n",
      "Epoch 479/1000 | Train Loss=515.99848022 | Val Loss=1.45558520 | Data=5.13756199 | Physics=2.22846771 | Val RMSE: 1.22272182 | ‚àö(Val Loss) = 1.20647633 | Current Learning Rate: 0.0002\n",
      "Epoch 480/1000 | Train Loss=517.53456624 | Val Loss=1.45664914 | Data=5.15293198 | Physics=2.12124944 | Val RMSE: 1.22274649 | ‚àö(Val Loss) = 1.20691717 | Current Learning Rate: 0.0002\n",
      "Epoch 481/1000 | Train Loss=516.26872355 | Val Loss=1.45636261 | Data=5.14026543 | Physics=2.23374038 | Val RMSE: 1.22274005 | ‚àö(Val Loss) = 1.20679843 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 482/1000 | Train Loss=515.92923787 | Val Loss=1.45649560 | Data=5.13682133 | Physics=2.31628931 | Val RMSE: 1.22272909 | ‚àö(Val Loss) = 1.20685363 | Current Learning Rate: 0.0002\n",
      "Epoch 483/1000 | Train Loss=516.11680908 | Val Loss=1.45728544 | Data=5.13874985 | Physics=2.22452484 | Val RMSE: 1.22276568 | ‚àö(Val Loss) = 1.20718074 | Current Learning Rate: 0.0002\n",
      "Epoch 484/1000 | Train Loss=515.79595744 | Val Loss=1.45770248 | Data=5.13551722 | Physics=2.22932929 | Val RMSE: 1.22277808 | ‚àö(Val Loss) = 1.20735347 | Current Learning Rate: 0.0002\n",
      "Epoch 485/1000 | Train Loss=514.37416789 | Val Loss=1.45773097 | Data=5.12130651 | Physics=2.29758313 | Val RMSE: 1.22277844 | ‚àö(Val Loss) = 1.20736527 | Current Learning Rate: 0.0002\n",
      "Epoch 486/1000 | Train Loss=517.33533325 | Val Loss=1.45635657 | Data=5.15095212 | Physics=2.15423575 | Val RMSE: 1.22273493 | ‚àö(Val Loss) = 1.20679593 | Current Learning Rate: 0.0002\n",
      "Epoch 487/1000 | Train Loss=515.55423381 | Val Loss=1.45579247 | Data=5.13306208 | Physics=2.31806788 | Val RMSE: 1.22273898 | ‚àö(Val Loss) = 1.20656228 | Current Learning Rate: 0.0002\n",
      "Epoch 488/1000 | Train Loss=517.41219076 | Val Loss=1.45630340 | Data=5.15170577 | Physics=2.26209882 | Val RMSE: 1.22273529 | ‚àö(Val Loss) = 1.20677400 | Current Learning Rate: 0.0002\n",
      "Epoch 489/1000 | Train Loss=515.58029175 | Val Loss=1.45670489 | Data=5.13337059 | Physics=2.16900066 | Val RMSE: 1.22276056 | ‚àö(Val Loss) = 1.20694029 | Current Learning Rate: 0.0002\n",
      "Epoch 490/1000 | Train Loss=515.91285197 | Val Loss=1.45585907 | Data=5.13671109 | Physics=2.18090326 | Val RMSE: 1.22274733 | ‚àö(Val Loss) = 1.20658982 | Current Learning Rate: 0.0002\n",
      "Epoch 491/1000 | Train Loss=515.62440186 | Val Loss=1.45622337 | Data=5.13383811 | Physics=2.17320872 | Val RMSE: 1.22274494 | ‚àö(Val Loss) = 1.20674086 | Current Learning Rate: 0.0002\n",
      "Epoch 492/1000 | Train Loss=516.34451294 | Val Loss=1.45804707 | Data=5.14100822 | Physics=2.23784755 | Val RMSE: 1.22278142 | ‚àö(Val Loss) = 1.20749617 | Current Learning Rate: 0.0002\n",
      "Epoch 493/1000 | Train Loss=515.42141317 | Val Loss=1.45898716 | Data=5.13182888 | Physics=2.20861071 | Val RMSE: 1.22279370 | ‚àö(Val Loss) = 1.20788538 | Current Learning Rate: 0.0002\n",
      "Epoch 494/1000 | Train Loss=515.70180461 | Val Loss=1.45678159 | Data=5.13458462 | Physics=2.22328991 | Val RMSE: 1.22275209 | ‚àö(Val Loss) = 1.20697212 | Current Learning Rate: 0.0002\n",
      "Epoch 495/1000 | Train Loss=515.13818563 | Val Loss=1.45513765 | Data=5.12894764 | Physics=2.23522140 | Val RMSE: 1.22273242 | ‚àö(Val Loss) = 1.20629084 | Current Learning Rate: 0.0002\n",
      "Epoch 496/1000 | Train Loss=515.33547160 | Val Loss=1.45586034 | Data=5.13091205 | Physics=2.23059243 | Val RMSE: 1.22273779 | ‚àö(Val Loss) = 1.20659041 | Current Learning Rate: 0.0002\n",
      "Epoch 497/1000 | Train Loss=517.59288330 | Val Loss=1.45737847 | Data=5.15350962 | Physics=2.23746551 | Val RMSE: 1.22276020 | ‚àö(Val Loss) = 1.20721936 | Current Learning Rate: 0.0002\n",
      "Epoch 498/1000 | Train Loss=517.72140706 | Val Loss=1.45660992 | Data=5.15480827 | Physics=2.10503216 | Val RMSE: 1.22274780 | ‚àö(Val Loss) = 1.20690095 | Current Learning Rate: 0.0002\n",
      "Epoch 499/1000 | Train Loss=515.82978923 | Val Loss=1.45934947 | Data=5.13587920 | Physics=2.25787505 | Val RMSE: 1.22281885 | ‚àö(Val Loss) = 1.20803535 | Current Learning Rate: 0.0002\n",
      "Epoch 500/1000 | Train Loss=515.65461222 | Val Loss=1.45834106 | Data=5.13412730 | Physics=2.25915572 | Val RMSE: 1.22278237 | ‚àö(Val Loss) = 1.20761800 | Current Learning Rate: 0.0002\n",
      "Epoch 501/1000 | Train Loss=515.66381226 | Val Loss=1.45888376 | Data=5.13421564 | Physics=2.22047123 | Val RMSE: 1.22277570 | ‚àö(Val Loss) = 1.20784259 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 502/1000 | Train Loss=515.52012329 | Val Loss=1.45758651 | Data=5.13279352 | Physics=2.25911881 | Val RMSE: 1.22275639 | ‚àö(Val Loss) = 1.20730543 | Current Learning Rate: 0.0002\n",
      "Epoch 503/1000 | Train Loss=515.46237793 | Val Loss=1.45844225 | Data=5.13219496 | Physics=2.25554867 | Val RMSE: 1.22279739 | ‚àö(Val Loss) = 1.20765984 | Current Learning Rate: 0.0002\n",
      "Epoch 504/1000 | Train Loss=518.02774251 | Val Loss=1.45842602 | Data=5.15787058 | Physics=2.22625511 | Val RMSE: 1.22280681 | ‚àö(Val Loss) = 1.20765305 | Current Learning Rate: 0.0002\n",
      "Epoch 505/1000 | Train Loss=515.29362996 | Val Loss=1.45616682 | Data=5.13047962 | Physics=2.30260218 | Val RMSE: 1.22273517 | ‚àö(Val Loss) = 1.20671737 | Current Learning Rate: 0.0002\n",
      "Epoch 506/1000 | Train Loss=515.94146525 | Val Loss=1.45615236 | Data=5.13697834 | Physics=2.23812471 | Val RMSE: 1.22276139 | ‚àö(Val Loss) = 1.20671141 | Current Learning Rate: 0.0002\n",
      "Epoch 507/1000 | Train Loss=514.23080241 | Val Loss=1.45826070 | Data=5.11990458 | Physics=2.20494834 | Val RMSE: 1.22280431 | ‚àö(Val Loss) = 1.20758462 | Current Learning Rate: 0.0002\n",
      "Epoch 508/1000 | Train Loss=517.13658651 | Val Loss=1.45762753 | Data=5.14897210 | Physics=2.13288197 | Val RMSE: 1.22279000 | ‚àö(Val Loss) = 1.20732248 | Current Learning Rate: 0.0002\n",
      "Epoch 509/1000 | Train Loss=515.85168050 | Val Loss=1.45757063 | Data=5.13606510 | Physics=2.32909915 | Val RMSE: 1.22279143 | ‚àö(Val Loss) = 1.20729887 | Current Learning Rate: 0.0002\n",
      "Epoch 510/1000 | Train Loss=515.17665812 | Val Loss=1.45882185 | Data=5.12934589 | Physics=2.24242393 | Val RMSE: 1.22283304 | ‚àö(Val Loss) = 1.20781696 | Current Learning Rate: 0.0002\n",
      "Epoch 511/1000 | Train Loss=515.02656047 | Val Loss=1.45787342 | Data=5.12783279 | Physics=2.28188150 | Val RMSE: 1.22281694 | ‚àö(Val Loss) = 1.20742428 | Current Learning Rate: 0.0002\n",
      "Epoch 512/1000 | Train Loss=517.64346720 | Val Loss=1.45593870 | Data=5.15401119 | Physics=2.19424173 | Val RMSE: 1.22276521 | ‚àö(Val Loss) = 1.20662284 | Current Learning Rate: 0.0002\n",
      "Epoch 513/1000 | Train Loss=515.61307373 | Val Loss=1.45570286 | Data=5.13368041 | Physics=2.24592162 | Val RMSE: 1.22277296 | ‚àö(Val Loss) = 1.20652509 | Current Learning Rate: 0.0002\n",
      "Epoch 514/1000 | Train Loss=516.43233236 | Val Loss=1.45577017 | Data=5.14188166 | Physics=2.20635819 | Val RMSE: 1.22277474 | ‚àö(Val Loss) = 1.20655298 | Current Learning Rate: 0.0002\n",
      "Epoch 515/1000 | Train Loss=515.92312215 | Val Loss=1.45660718 | Data=5.13680010 | Physics=2.24379036 | Val RMSE: 1.22279692 | ‚àö(Val Loss) = 1.20689988 | Current Learning Rate: 0.0002\n",
      "Epoch 516/1000 | Train Loss=513.50526326 | Val Loss=1.45665371 | Data=5.11263425 | Physics=2.17904521 | Val RMSE: 1.22278190 | ‚àö(Val Loss) = 1.20691907 | Current Learning Rate: 0.0002\n",
      "Epoch 517/1000 | Train Loss=515.61099447 | Val Loss=1.45827802 | Data=5.13368994 | Physics=2.19454990 | Val RMSE: 1.22279668 | ‚àö(Val Loss) = 1.20759189 | Current Learning Rate: 0.0002\n",
      "Epoch 518/1000 | Train Loss=515.60164998 | Val Loss=1.45716019 | Data=5.13358459 | Physics=2.24195702 | Val RMSE: 1.22280872 | ‚àö(Val Loss) = 1.20712888 | Current Learning Rate: 0.0002\n",
      "Epoch 519/1000 | Train Loss=517.40808512 | Val Loss=1.45772111 | Data=5.15167284 | Physics=2.21804802 | Val RMSE: 1.22282314 | ‚àö(Val Loss) = 1.20736122 | Current Learning Rate: 0.0002\n",
      "Epoch 520/1000 | Train Loss=514.19468791 | Val Loss=1.45782371 | Data=5.11952845 | Physics=2.20937992 | Val RMSE: 1.22281146 | ‚àö(Val Loss) = 1.20740378 | Current Learning Rate: 0.0002\n",
      "Epoch 521/1000 | Train Loss=515.09476929 | Val Loss=1.45776864 | Data=5.12852615 | Physics=2.18370496 | Val RMSE: 1.22277761 | ‚àö(Val Loss) = 1.20738089 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 522/1000 | Train Loss=515.59148763 | Val Loss=1.45841245 | Data=5.13350544 | Physics=2.21504947 | Val RMSE: 1.22281361 | ‚àö(Val Loss) = 1.20764744 | Current Learning Rate: 0.0002\n",
      "Epoch 523/1000 | Train Loss=514.38979289 | Val Loss=1.45999952 | Data=5.12147611 | Physics=2.27466962 | Val RMSE: 1.22288883 | ‚àö(Val Loss) = 1.20830441 | Current Learning Rate: 0.0002\n",
      "Epoch 524/1000 | Train Loss=519.05920003 | Val Loss=1.45885475 | Data=5.16815344 | Physics=2.28024025 | Val RMSE: 1.22286844 | ‚àö(Val Loss) = 1.20783067 | Current Learning Rate: 0.0002\n",
      "Epoch 525/1000 | Train Loss=515.41343791 | Val Loss=1.45945831 | Data=5.13173304 | Physics=2.17336399 | Val RMSE: 1.22287869 | ‚àö(Val Loss) = 1.20808041 | Current Learning Rate: 0.0002\n",
      "Epoch 526/1000 | Train Loss=515.42912394 | Val Loss=1.45848477 | Data=5.13187691 | Physics=2.23299194 | Val RMSE: 1.22284877 | ‚àö(Val Loss) = 1.20767748 | Current Learning Rate: 0.0002\n",
      "Epoch 527/1000 | Train Loss=516.40941772 | Val Loss=1.45662141 | Data=5.14165729 | Physics=2.22482280 | Val RMSE: 1.22280598 | ‚àö(Val Loss) = 1.20690572 | Current Learning Rate: 0.0002\n",
      "Epoch 528/1000 | Train Loss=516.83183390 | Val Loss=1.45973885 | Data=5.14593306 | Physics=2.18813195 | Val RMSE: 1.22286654 | ‚àö(Val Loss) = 1.20819652 | Current Learning Rate: 0.0002\n",
      "Epoch 529/1000 | Train Loss=514.81587931 | Val Loss=1.45782526 | Data=5.12575518 | Physics=2.18924554 | Val RMSE: 1.22280598 | ‚àö(Val Loss) = 1.20740438 | Current Learning Rate: 0.0002\n",
      "Epoch 530/1000 | Train Loss=516.28522746 | Val Loss=1.45796152 | Data=5.14042886 | Physics=2.21842011 | Val RMSE: 1.22283387 | ‚àö(Val Loss) = 1.20746076 | Current Learning Rate: 0.0002\n",
      "Epoch 531/1000 | Train Loss=515.77798462 | Val Loss=1.45686837 | Data=5.13536339 | Physics=2.16942875 | Val RMSE: 1.22281551 | ‚àö(Val Loss) = 1.20700800 | Current Learning Rate: 0.0002\n",
      "Epoch 532/1000 | Train Loss=517.11305542 | Val Loss=1.45811860 | Data=5.14871248 | Physics=2.17571893 | Val RMSE: 1.22285247 | ‚àö(Val Loss) = 1.20752585 | Current Learning Rate: 0.0002\n",
      "Epoch 533/1000 | Train Loss=515.65437622 | Val Loss=1.45718777 | Data=5.13413035 | Physics=2.26287484 | Val RMSE: 1.22281694 | ‚àö(Val Loss) = 1.20714033 | Current Learning Rate: 0.0002\n",
      "Epoch 534/1000 | Train Loss=515.88013509 | Val Loss=1.45611429 | Data=5.13633540 | Physics=2.27436960 | Val RMSE: 1.22278559 | ‚àö(Val Loss) = 1.20669556 | Current Learning Rate: 0.0002\n",
      "Epoch 535/1000 | Train Loss=514.41511027 | Val Loss=1.45768698 | Data=5.12174225 | Physics=2.18785774 | Val RMSE: 1.22282493 | ‚àö(Val Loss) = 1.20734715 | Current Learning Rate: 0.0002\n",
      "Epoch 536/1000 | Train Loss=514.47931315 | Val Loss=1.45575909 | Data=5.12235880 | Physics=2.19345298 | Val RMSE: 1.22278559 | ‚àö(Val Loss) = 1.20654845 | Current Learning Rate: 0.0002\n",
      "Epoch 537/1000 | Train Loss=515.97260946 | Val Loss=1.45691657 | Data=5.13726126 | Physics=2.30429062 | Val RMSE: 1.22282255 | ‚àö(Val Loss) = 1.20702803 | Current Learning Rate: 0.0002\n",
      "Epoch 538/1000 | Train Loss=517.02342529 | Val Loss=1.45869621 | Data=5.14777975 | Physics=2.30873122 | Val RMSE: 1.22287560 | ‚àö(Val Loss) = 1.20776498 | Current Learning Rate: 0.0002\n",
      "Epoch 539/1000 | Train Loss=515.97960002 | Val Loss=1.46157104 | Data=5.13741481 | Physics=2.21504443 | Val RMSE: 1.22292888 | ‚àö(Val Loss) = 1.20895445 | Current Learning Rate: 0.0002\n",
      "Epoch 540/1000 | Train Loss=516.42549642 | Val Loss=1.45839850 | Data=5.14186236 | Physics=2.13616350 | Val RMSE: 1.22281587 | ‚àö(Val Loss) = 1.20764172 | Current Learning Rate: 0.0002\n",
      "Epoch 541/1000 | Train Loss=516.35218099 | Val Loss=1.46036859 | Data=5.14112050 | Physics=2.23774207 | Val RMSE: 1.22289836 | ‚àö(Val Loss) = 1.20845711 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 542/1000 | Train Loss=515.87004395 | Val Loss=1.45894214 | Data=5.13627240 | Physics=2.21434312 | Val RMSE: 1.22288752 | ‚àö(Val Loss) = 1.20786679 | Current Learning Rate: 0.0002\n",
      "Epoch 543/1000 | Train Loss=514.83716431 | Val Loss=1.46016790 | Data=5.12597386 | Physics=2.20354832 | Val RMSE: 1.22293520 | ‚àö(Val Loss) = 1.20837402 | Current Learning Rate: 0.0002\n",
      "Epoch 544/1000 | Train Loss=517.76984253 | Val Loss=1.45906214 | Data=5.15529245 | Physics=2.17895658 | Val RMSE: 1.22289419 | ‚àö(Val Loss) = 1.20791638 | Current Learning Rate: 0.0002\n",
      "Epoch 545/1000 | Train Loss=515.14636637 | Val Loss=1.45812639 | Data=5.12908300 | Physics=2.21956386 | Val RMSE: 1.22285402 | ‚àö(Val Loss) = 1.20752907 | Current Learning Rate: 0.0002\n",
      "Epoch 546/1000 | Train Loss=517.94250895 | Val Loss=1.45566499 | Data=5.15700064 | Physics=2.12775665 | Val RMSE: 1.22278595 | ‚àö(Val Loss) = 1.20650947 | Current Learning Rate: 0.0002\n",
      "Epoch 547/1000 | Train Loss=515.66901042 | Val Loss=1.45777007 | Data=5.13428510 | Physics=2.20322509 | Val RMSE: 1.22285914 | ‚àö(Val Loss) = 1.20738149 | Current Learning Rate: 0.0002\n",
      "Epoch 548/1000 | Train Loss=514.91843669 | Val Loss=1.45681508 | Data=5.12677647 | Physics=2.17799818 | Val RMSE: 1.22279787 | ‚àö(Val Loss) = 1.20698595 | Current Learning Rate: 0.0002\n",
      "Epoch 549/1000 | Train Loss=515.52265218 | Val Loss=1.45532906 | Data=5.13280347 | Physics=2.20173047 | Val RMSE: 1.22277021 | ‚àö(Val Loss) = 1.20637023 | Current Learning Rate: 0.0002\n",
      "Epoch 550/1000 | Train Loss=515.47739054 | Val Loss=1.45606490 | Data=5.13227539 | Physics=2.35204789 | Val RMSE: 1.22279739 | ‚àö(Val Loss) = 1.20667517 | Current Learning Rate: 0.0002\n",
      "Epoch 551/1000 | Train Loss=514.07979533 | Val Loss=1.45812349 | Data=5.11840445 | Physics=2.18005779 | Val RMSE: 1.22287059 | ‚àö(Val Loss) = 1.20752788 | Current Learning Rate: 0.0002\n",
      "Epoch 552/1000 | Train Loss=515.48819784 | Val Loss=1.45709113 | Data=5.13246733 | Physics=2.21520772 | Val RMSE: 1.22284436 | ‚àö(Val Loss) = 1.20710027 | Current Learning Rate: 0.0002\n",
      "Epoch 553/1000 | Train Loss=514.71137085 | Val Loss=1.45683436 | Data=5.12465582 | Physics=2.26983297 | Val RMSE: 1.22284305 | ‚àö(Val Loss) = 1.20699394 | Current Learning Rate: 0.0002\n",
      "Epoch 554/1000 | Train Loss=517.31579997 | Val Loss=1.45731819 | Data=5.15077073 | Physics=2.19287443 | Val RMSE: 1.22282588 | ‚àö(Val Loss) = 1.20719433 | Current Learning Rate: 0.0002\n",
      "Epoch 555/1000 | Train Loss=516.28967896 | Val Loss=1.45702557 | Data=5.14043090 | Physics=2.28798602 | Val RMSE: 1.22284698 | ‚àö(Val Loss) = 1.20707309 | Current Learning Rate: 0.0002\n",
      "Epoch 556/1000 | Train Loss=516.82034302 | Val Loss=1.45752239 | Data=5.14580545 | Physics=2.19315698 | Val RMSE: 1.22284865 | ‚àö(Val Loss) = 1.20727897 | Current Learning Rate: 0.0002\n",
      "Epoch 557/1000 | Train Loss=517.17318522 | Val Loss=1.45680777 | Data=5.14929072 | Physics=2.22166554 | Val RMSE: 1.22284055 | ‚àö(Val Loss) = 1.20698285 | Current Learning Rate: 0.0002\n",
      "Epoch 558/1000 | Train Loss=516.10390015 | Val Loss=1.45764999 | Data=5.13862445 | Physics=2.21190776 | Val RMSE: 1.22284222 | ‚àö(Val Loss) = 1.20733178 | Current Learning Rate: 0.0002\n",
      "Epoch 559/1000 | Train Loss=515.97615560 | Val Loss=1.45720029 | Data=5.13732945 | Physics=2.26654022 | Val RMSE: 1.22282958 | ‚àö(Val Loss) = 1.20714557 | Current Learning Rate: 0.0002\n",
      "Epoch 560/1000 | Train Loss=517.22357381 | Val Loss=1.45665924 | Data=5.14981038 | Physics=2.20666866 | Val RMSE: 1.22283602 | ‚àö(Val Loss) = 1.20692134 | Current Learning Rate: 0.0002\n",
      "Epoch 561/1000 | Train Loss=515.22829590 | Val Loss=1.45783679 | Data=5.12986517 | Physics=2.20042336 | Val RMSE: 1.22286081 | ‚àö(Val Loss) = 1.20740914 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 562/1000 | Train Loss=515.55122070 | Val Loss=1.45769815 | Data=5.13306023 | Physics=2.29043591 | Val RMSE: 1.22288263 | ‚àö(Val Loss) = 1.20735168 | Current Learning Rate: 0.0002\n",
      "Epoch 563/1000 | Train Loss=513.97571411 | Val Loss=1.46015910 | Data=5.11735910 | Physics=2.26188127 | Val RMSE: 1.22294259 | ‚àö(Val Loss) = 1.20837045 | Current Learning Rate: 0.0002\n",
      "Epoch 564/1000 | Train Loss=515.86315104 | Val Loss=1.45771142 | Data=5.13620167 | Physics=2.31151198 | Val RMSE: 1.22284794 | ‚àö(Val Loss) = 1.20735717 | Current Learning Rate: 0.0002\n",
      "Epoch 565/1000 | Train Loss=515.73369751 | Val Loss=1.45731727 | Data=5.13487504 | Physics=2.27093655 | Val RMSE: 1.22285903 | ‚àö(Val Loss) = 1.20719397 | Current Learning Rate: 0.0002\n",
      "Epoch 566/1000 | Train Loss=516.11167603 | Val Loss=1.45756718 | Data=5.13869874 | Physics=2.22270939 | Val RMSE: 1.22287595 | ‚àö(Val Loss) = 1.20729744 | Current Learning Rate: 0.0002\n",
      "Epoch 567/1000 | Train Loss=514.24201050 | Val Loss=1.45773129 | Data=5.11998501 | Physics=2.23180874 | Val RMSE: 1.22288573 | ‚àö(Val Loss) = 1.20736539 | Current Learning Rate: 0.0002\n",
      "Epoch 568/1000 | Train Loss=516.33294881 | Val Loss=1.45860557 | Data=5.14093803 | Physics=2.21385927 | Val RMSE: 1.22288299 | ‚àö(Val Loss) = 1.20772743 | Current Learning Rate: 0.0002\n",
      "Epoch 569/1000 | Train Loss=516.86678263 | Val Loss=1.45780297 | Data=5.14625142 | Physics=2.19629859 | Val RMSE: 1.22285628 | ‚àö(Val Loss) = 1.20739508 | Current Learning Rate: 0.0002\n",
      "Epoch 570/1000 | Train Loss=516.64750163 | Val Loss=1.45741061 | Data=5.14402857 | Physics=2.27014483 | Val RMSE: 1.22288775 | ‚àö(Val Loss) = 1.20723259 | Current Learning Rate: 0.0002\n",
      "Epoch 571/1000 | Train Loss=516.32728271 | Val Loss=1.45742138 | Data=5.14085846 | Physics=2.19171304 | Val RMSE: 1.22286963 | ‚àö(Val Loss) = 1.20723712 | Current Learning Rate: 0.0002\n",
      "Epoch 572/1000 | Train Loss=516.04275716 | Val Loss=1.45683936 | Data=5.13797038 | Physics=2.30616859 | Val RMSE: 1.22283483 | ‚àö(Val Loss) = 1.20699596 | Current Learning Rate: 0.0002\n",
      "Epoch 573/1000 | Train Loss=516.95225220 | Val Loss=1.45566281 | Data=5.14709568 | Physics=2.23968544 | Val RMSE: 1.22278595 | ‚àö(Val Loss) = 1.20650852 | Current Learning Rate: 0.0002\n",
      "Epoch 574/1000 | Train Loss=515.62205200 | Val Loss=1.45621363 | Data=5.13377028 | Physics=2.23580935 | Val RMSE: 1.22279906 | ‚àö(Val Loss) = 1.20673680 | Current Learning Rate: 0.0002\n",
      "Epoch 575/1000 | Train Loss=517.41320190 | Val Loss=1.45783599 | Data=5.15171235 | Physics=2.24550885 | Val RMSE: 1.22286892 | ‚àö(Val Loss) = 1.20740879 | Current Learning Rate: 0.0002\n",
      "Epoch 576/1000 | Train Loss=519.43306885 | Val Loss=1.45746481 | Data=5.17190590 | Physics=2.19881098 | Val RMSE: 1.22287178 | ‚àö(Val Loss) = 1.20725513 | Current Learning Rate: 0.0002\n",
      "Epoch 577/1000 | Train Loss=515.95612386 | Val Loss=1.45789119 | Data=5.13713716 | Physics=2.25193969 | Val RMSE: 1.22288203 | ‚àö(Val Loss) = 1.20743167 | Current Learning Rate: 0.0002\n",
      "Epoch 578/1000 | Train Loss=514.43642985 | Val Loss=1.45626827 | Data=5.12193772 | Physics=2.17289549 | Val RMSE: 1.22282779 | ‚àö(Val Loss) = 1.20675945 | Current Learning Rate: 0.0002\n",
      "Epoch 579/1000 | Train Loss=516.15367635 | Val Loss=1.45622905 | Data=5.13909877 | Physics=2.26185321 | Val RMSE: 1.22275472 | ‚àö(Val Loss) = 1.20674312 | Current Learning Rate: 0.0002\n",
      "Epoch 580/1000 | Train Loss=515.03205973 | Val Loss=1.45741034 | Data=5.12788394 | Physics=2.23316020 | Val RMSE: 1.22284651 | ‚àö(Val Loss) = 1.20723248 | Current Learning Rate: 0.0002\n",
      "Epoch 581/1000 | Train Loss=515.01036377 | Val Loss=1.45850821 | Data=5.12770198 | Physics=2.23493802 | Val RMSE: 1.22290635 | ‚àö(Val Loss) = 1.20768714 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 582/1000 | Train Loss=515.92956950 | Val Loss=1.45609939 | Data=5.13687045 | Physics=2.20618441 | Val RMSE: 1.22282314 | ‚àö(Val Loss) = 1.20668948 | Current Learning Rate: 0.0002\n",
      "Epoch 583/1000 | Train Loss=516.19967244 | Val Loss=1.45781990 | Data=5.13954220 | Physics=2.28174354 | Val RMSE: 1.22288489 | ‚àö(Val Loss) = 1.20740211 | Current Learning Rate: 0.0002\n",
      "Epoch 584/1000 | Train Loss=515.16913249 | Val Loss=1.45642829 | Data=5.12928597 | Physics=2.20573658 | Val RMSE: 1.22285032 | ‚àö(Val Loss) = 1.20682573 | Current Learning Rate: 0.0002\n",
      "Epoch 585/1000 | Train Loss=516.20966390 | Val Loss=1.45659574 | Data=5.13965467 | Physics=2.22192942 | Val RMSE: 1.22285151 | ‚àö(Val Loss) = 1.20689511 | Current Learning Rate: 0.0002\n",
      "Epoch 586/1000 | Train Loss=516.26945190 | Val Loss=1.45767975 | Data=5.14027383 | Physics=2.24652307 | Val RMSE: 1.22288954 | ‚àö(Val Loss) = 1.20734406 | Current Learning Rate: 0.0002\n",
      "Epoch 587/1000 | Train Loss=516.54209798 | Val Loss=1.45791455 | Data=5.14301977 | Physics=2.15422801 | Val RMSE: 1.22288609 | ‚àö(Val Loss) = 1.20744133 | Current Learning Rate: 0.0002\n",
      "Epoch 588/1000 | Train Loss=516.40463053 | Val Loss=1.45634699 | Data=5.14157982 | Physics=2.33080161 | Val RMSE: 1.22284484 | ‚àö(Val Loss) = 1.20679200 | Current Learning Rate: 0.0002\n",
      "Epoch 589/1000 | Train Loss=515.17150472 | Val Loss=1.45764530 | Data=5.12928343 | Physics=2.25076204 | Val RMSE: 1.22289467 | ‚àö(Val Loss) = 1.20732987 | Current Learning Rate: 0.0002\n",
      "Epoch 590/1000 | Train Loss=516.78840129 | Val Loss=1.45873491 | Data=5.14545879 | Physics=2.24526411 | Val RMSE: 1.22291160 | ‚àö(Val Loss) = 1.20778096 | Current Learning Rate: 0.0002\n",
      "Epoch 591/1000 | Train Loss=516.36079508 | Val Loss=1.45709562 | Data=5.14119342 | Physics=2.23075870 | Val RMSE: 1.22286057 | ‚àö(Val Loss) = 1.20710218 | Current Learning Rate: 0.0002\n",
      "Epoch 592/1000 | Train Loss=515.92967733 | Val Loss=1.45827746 | Data=5.13687954 | Physics=2.19225774 | Val RMSE: 1.22290778 | ‚àö(Val Loss) = 1.20759165 | Current Learning Rate: 0.0002\n",
      "Epoch 593/1000 | Train Loss=516.82750448 | Val Loss=1.45803448 | Data=5.14588817 | Physics=2.12841157 | Val RMSE: 1.22290850 | ‚àö(Val Loss) = 1.20749104 | Current Learning Rate: 0.0002\n",
      "Epoch 594/1000 | Train Loss=516.33900960 | Val Loss=1.45975920 | Data=5.14095739 | Physics=2.22419919 | Val RMSE: 1.22292638 | ‚àö(Val Loss) = 1.20820498 | Current Learning Rate: 0.0002\n",
      "Epoch 595/1000 | Train Loss=515.45234782 | Val Loss=1.45832610 | Data=5.13213072 | Physics=2.22410805 | Val RMSE: 1.22290480 | ‚àö(Val Loss) = 1.20761168 | Current Learning Rate: 0.0002\n",
      "Epoch 596/1000 | Train Loss=517.49597371 | Val Loss=1.45854370 | Data=5.15253487 | Physics=2.24152827 | Val RMSE: 1.22290587 | ‚àö(Val Loss) = 1.20770180 | Current Learning Rate: 0.0002\n",
      "Epoch 597/1000 | Train Loss=515.39517415 | Val Loss=1.45828414 | Data=5.13152332 | Physics=2.23846529 | Val RMSE: 1.22292137 | ‚àö(Val Loss) = 1.20759439 | Current Learning Rate: 0.0002\n",
      "Epoch 598/1000 | Train Loss=515.22671712 | Val Loss=1.45839226 | Data=5.12986784 | Physics=2.25342012 | Val RMSE: 1.22290301 | ‚àö(Val Loss) = 1.20763910 | Current Learning Rate: 0.0002\n",
      "Epoch 599/1000 | Train Loss=516.07519735 | Val Loss=1.45730098 | Data=5.13828204 | Physics=2.28104768 | Val RMSE: 1.22289646 | ‚àö(Val Loss) = 1.20718718 | Current Learning Rate: 0.0002\n",
      "Epoch 600/1000 | Train Loss=514.88099569 | Val Loss=1.45975606 | Data=5.12637390 | Physics=2.29758975 | Val RMSE: 1.22297370 | ‚àö(Val Loss) = 1.20820367 | Current Learning Rate: 0.0002\n",
      "Epoch 601/1000 | Train Loss=515.95390218 | Val Loss=1.45836562 | Data=5.13715795 | Physics=2.19889258 | Val RMSE: 1.22289360 | ‚àö(Val Loss) = 1.20762813 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 602/1000 | Train Loss=1715.67504069 | Val Loss=1.45581118 | Data=17.13428866 | Physics=2.17624452 | Val RMSE: 1.22279310 | ‚àö(Val Loss) = 1.20657003 | Current Learning Rate: 0.0002\n",
      "Epoch 603/1000 | Train Loss=1718.68564453 | Val Loss=1.45801592 | Data=17.16446406 | Physics=2.19914331 | Val RMSE: 1.22286689 | ‚àö(Val Loss) = 1.20748329 | Current Learning Rate: 0.0002\n",
      "Epoch 604/1000 | Train Loss=1719.23230794 | Val Loss=1.45710210 | Data=17.16989708 | Physics=2.17921157 | Val RMSE: 1.22260964 | ‚àö(Val Loss) = 1.20710480 | Current Learning Rate: 0.0002\n",
      "Epoch 605/1000 | Train Loss=1715.97194824 | Val Loss=1.45638545 | Data=17.13731550 | Physics=2.14217958 | Val RMSE: 1.22259903 | ‚àö(Val Loss) = 1.20680797 | Current Learning Rate: 0.0002\n",
      "Epoch 606/1000 | Train Loss=1717.80891927 | Val Loss=1.45703808 | Data=17.15563997 | Physics=2.23303505 | Val RMSE: 1.22273850 | ‚àö(Val Loss) = 1.20707834 | Current Learning Rate: 0.0002\n",
      "Epoch 607/1000 | Train Loss=1717.32959798 | Val Loss=1.45710047 | Data=17.15091197 | Physics=2.19340606 | Val RMSE: 1.22277415 | ‚àö(Val Loss) = 1.20710421 | Current Learning Rate: 0.0002\n",
      "Epoch 608/1000 | Train Loss=1716.53663737 | Val Loss=1.45634186 | Data=17.14292577 | Physics=2.21278201 | Val RMSE: 1.22271681 | ‚àö(Val Loss) = 1.20678985 | Current Learning Rate: 0.0002\n",
      "Epoch 609/1000 | Train Loss=1717.97327474 | Val Loss=1.45822557 | Data=17.15732835 | Physics=2.22146915 | Val RMSE: 1.22288787 | ‚àö(Val Loss) = 1.20757008 | Current Learning Rate: 0.0002\n",
      "Epoch 610/1000 | Train Loss=1719.27343750 | Val Loss=1.45614040 | Data=17.17027683 | Physics=2.29331352 | Val RMSE: 1.22272468 | ‚àö(Val Loss) = 1.20670640 | Current Learning Rate: 0.0002\n",
      "Epoch 611/1000 | Train Loss=1716.28631185 | Val Loss=1.45828044 | Data=17.14045245 | Physics=2.17574884 | Val RMSE: 1.22253072 | ‚àö(Val Loss) = 1.20759284 | Current Learning Rate: 0.0002\n",
      "Epoch 612/1000 | Train Loss=1718.42312826 | Val Loss=1.45731429 | Data=17.16182035 | Physics=2.19106343 | Val RMSE: 1.22255147 | ‚àö(Val Loss) = 1.20719266 | Current Learning Rate: 0.0002\n",
      "Epoch 613/1000 | Train Loss=1711.59820150 | Val Loss=1.45957482 | Data=17.09358463 | Physics=2.18131426 | Val RMSE: 1.22261190 | ‚àö(Val Loss) = 1.20812869 | Current Learning Rate: 0.0002\n",
      "Epoch 614/1000 | Train Loss=1718.91708171 | Val Loss=1.45747515 | Data=17.16676483 | Physics=2.21811423 | Val RMSE: 1.22251987 | ‚àö(Val Loss) = 1.20725942 | Current Learning Rate: 0.0002\n",
      "Epoch 615/1000 | Train Loss=1719.21399740 | Val Loss=1.45877377 | Data=17.16974119 | Physics=2.14071694 | Val RMSE: 1.22262406 | ‚àö(Val Loss) = 1.20779705 | Current Learning Rate: 0.0002\n",
      "Epoch 616/1000 | Train Loss=1715.04444987 | Val Loss=1.45742305 | Data=17.12804070 | Physics=2.24107767 | Val RMSE: 1.22268689 | ‚àö(Val Loss) = 1.20723784 | Current Learning Rate: 0.0002\n",
      "Epoch 617/1000 | Train Loss=1718.61063639 | Val Loss=1.45548530 | Data=17.16369120 | Physics=2.09564661 | Val RMSE: 1.22273302 | ‚àö(Val Loss) = 1.20643497 | Current Learning Rate: 0.0002\n",
      "Epoch 618/1000 | Train Loss=1718.62349447 | Val Loss=1.45621093 | Data=17.16378059 | Physics=2.28674795 | Val RMSE: 1.22269034 | ‚àö(Val Loss) = 1.20673561 | Current Learning Rate: 0.0002\n",
      "Epoch 619/1000 | Train Loss=1721.29775391 | Val Loss=1.45720625 | Data=17.19050388 | Physics=2.35928165 | Val RMSE: 1.22267449 | ‚àö(Val Loss) = 1.20714796 | Current Learning Rate: 0.0002\n",
      "Epoch 620/1000 | Train Loss=1717.84631348 | Val Loss=1.45892795 | Data=17.15605965 | Physics=2.26019040 | Val RMSE: 1.22267747 | ‚àö(Val Loss) = 1.20786095 | Current Learning Rate: 0.0002\n",
      "Epoch 621/1000 | Train Loss=1720.32192383 | Val Loss=1.45826530 | Data=17.18080438 | Physics=2.23109355 | Val RMSE: 1.22263885 | ‚àö(Val Loss) = 1.20758653 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 622/1000 | Train Loss=1718.90621745 | Val Loss=1.45942632 | Data=17.16664124 | Physics=2.22135952 | Val RMSE: 1.22271347 | ‚àö(Val Loss) = 1.20806718 | Current Learning Rate: 0.0002\n",
      "Epoch 623/1000 | Train Loss=1713.80970866 | Val Loss=1.46033732 | Data=17.11565247 | Physics=2.36015293 | Val RMSE: 1.22265840 | ‚àö(Val Loss) = 1.20844412 | Current Learning Rate: 0.0002\n",
      "Epoch 624/1000 | Train Loss=1715.78321126 | Val Loss=1.45781573 | Data=17.13541387 | Physics=2.21678799 | Val RMSE: 1.22271824 | ‚àö(Val Loss) = 1.20740044 | Current Learning Rate: 0.0002\n",
      "Epoch 625/1000 | Train Loss=1715.84970703 | Val Loss=1.45921588 | Data=17.13611266 | Physics=2.20425860 | Val RMSE: 1.22265399 | ‚àö(Val Loss) = 1.20798004 | Current Learning Rate: 0.0002\n",
      "Epoch 626/1000 | Train Loss=1717.99751790 | Val Loss=1.45796963 | Data=17.15753078 | Physics=2.31389944 | Val RMSE: 1.22267854 | ‚àö(Val Loss) = 1.20746410 | Current Learning Rate: 0.0002\n",
      "Epoch 627/1000 | Train Loss=1714.92683919 | Val Loss=1.45797598 | Data=17.12687378 | Physics=2.15397173 | Val RMSE: 1.22251022 | ‚àö(Val Loss) = 1.20746672 | Current Learning Rate: 0.0002\n",
      "Epoch 628/1000 | Train Loss=1719.25629069 | Val Loss=1.45779721 | Data=17.17015127 | Physics=2.23114609 | Val RMSE: 1.22248816 | ‚àö(Val Loss) = 1.20739269 | Current Learning Rate: 0.0002\n",
      "Epoch 629/1000 | Train Loss=1716.97853190 | Val Loss=1.45792266 | Data=17.14734968 | Physics=2.25589703 | Val RMSE: 1.22273231 | ‚àö(Val Loss) = 1.20744467 | Current Learning Rate: 0.0002\n",
      "Epoch 630/1000 | Train Loss=1713.71297201 | Val Loss=1.45876384 | Data=17.11472346 | Physics=2.17219661 | Val RMSE: 1.22270584 | ‚àö(Val Loss) = 1.20779300 | Current Learning Rate: 0.0002\n",
      "Epoch 631/1000 | Train Loss=1713.79232585 | Val Loss=1.45732526 | Data=17.11548456 | Physics=2.24687382 | Val RMSE: 1.22279048 | ‚àö(Val Loss) = 1.20719731 | Current Learning Rate: 0.0002\n",
      "Epoch 632/1000 | Train Loss=1717.97358398 | Val Loss=1.45633733 | Data=17.15731455 | Physics=2.22072264 | Val RMSE: 1.22242069 | ‚àö(Val Loss) = 1.20678806 | Current Learning Rate: 0.0002\n",
      "Epoch 633/1000 | Train Loss=1716.10959473 | Val Loss=1.45768873 | Data=17.13865922 | Physics=2.21985532 | Val RMSE: 1.22247028 | ‚àö(Val Loss) = 1.20734775 | Current Learning Rate: 0.0002\n",
      "Epoch 634/1000 | Train Loss=1713.12057292 | Val Loss=1.45721614 | Data=17.10878372 | Physics=2.26308408 | Val RMSE: 1.22255087 | ‚àö(Val Loss) = 1.20715213 | Current Learning Rate: 0.0002\n",
      "Epoch 635/1000 | Train Loss=1717.48496908 | Val Loss=1.45608521 | Data=17.15244306 | Physics=2.21700967 | Val RMSE: 1.22253883 | ‚àö(Val Loss) = 1.20668352 | Current Learning Rate: 0.0002\n",
      "Epoch 636/1000 | Train Loss=1718.03837077 | Val Loss=1.45544171 | Data=17.15791906 | Physics=2.26571028 | Val RMSE: 1.22269619 | ‚àö(Val Loss) = 1.20641685 | Current Learning Rate: 0.0002\n",
      "Epoch 637/1000 | Train Loss=1717.32920736 | Val Loss=1.45867225 | Data=17.15085615 | Physics=2.26705392 | Val RMSE: 1.22288728 | ‚àö(Val Loss) = 1.20775509 | Current Learning Rate: 0.0002\n",
      "Epoch 638/1000 | Train Loss=1722.71153971 | Val Loss=1.45841308 | Data=17.20471458 | Physics=2.16735657 | Val RMSE: 1.22266233 | ‚àö(Val Loss) = 1.20764780 | Current Learning Rate: 0.0002\n",
      "Epoch 639/1000 | Train Loss=1715.62201335 | Val Loss=1.45641239 | Data=17.13378919 | Physics=2.27408871 | Val RMSE: 1.22249484 | ‚àö(Val Loss) = 1.20681918 | Current Learning Rate: 0.0002\n",
      "Epoch 640/1000 | Train Loss=1717.67819010 | Val Loss=1.45458329 | Data=17.15432440 | Physics=2.26173993 | Val RMSE: 1.22254467 | ‚àö(Val Loss) = 1.20606101 | Current Learning Rate: 0.0002\n",
      "Epoch 641/1000 | Train Loss=1714.61972656 | Val Loss=1.45693930 | Data=17.12378216 | Physics=2.19471699 | Val RMSE: 1.22273660 | ‚àö(Val Loss) = 1.20703745 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 642/1000 | Train Loss=1720.65204264 | Val Loss=1.45806086 | Data=17.18412234 | Physics=2.16644584 | Val RMSE: 1.22258353 | ‚àö(Val Loss) = 1.20750189 | Current Learning Rate: 0.0002\n",
      "Epoch 643/1000 | Train Loss=1719.28883464 | Val Loss=1.45554757 | Data=17.17046394 | Physics=2.19364115 | Val RMSE: 1.22260416 | ‚àö(Val Loss) = 1.20646071 | Current Learning Rate: 0.0002\n",
      "Epoch 644/1000 | Train Loss=1714.94770508 | Val Loss=1.45866267 | Data=17.12706451 | Physics=2.20066233 | Val RMSE: 1.22281063 | ‚àö(Val Loss) = 1.20775104 | Current Learning Rate: 0.0002\n",
      "Epoch 645/1000 | Train Loss=1716.49042155 | Val Loss=1.45888251 | Data=17.14247799 | Physics=2.25690502 | Val RMSE: 1.22262168 | ‚àö(Val Loss) = 1.20784211 | Current Learning Rate: 0.0002\n",
      "Epoch 646/1000 | Train Loss=1713.58725586 | Val Loss=1.45905058 | Data=17.11343988 | Physics=2.27876982 | Val RMSE: 1.22271764 | ‚àö(Val Loss) = 1.20791161 | Current Learning Rate: 0.0002\n",
      "Epoch 647/1000 | Train Loss=1711.64560547 | Val Loss=1.45886898 | Data=17.09405041 | Physics=2.20022980 | Val RMSE: 1.22262514 | ‚àö(Val Loss) = 1.20783651 | Current Learning Rate: 0.0002\n",
      "Epoch 648/1000 | Train Loss=1715.78463542 | Val Loss=1.45756221 | Data=17.13545418 | Physics=2.17019126 | Val RMSE: 1.22243071 | ‚àö(Val Loss) = 1.20729542 | Current Learning Rate: 0.0002\n",
      "Epoch 649/1000 | Train Loss=1716.62744141 | Val Loss=1.45866342 | Data=17.14385452 | Physics=2.23529023 | Val RMSE: 1.22262180 | ‚àö(Val Loss) = 1.20775139 | Current Learning Rate: 0.0002\n",
      "Epoch 650/1000 | Train Loss=1716.07694499 | Val Loss=1.45729603 | Data=17.13835297 | Physics=2.21758722 | Val RMSE: 1.22267091 | ‚àö(Val Loss) = 1.20718515 | Current Learning Rate: 0.0002\n",
      "Epoch 651/1000 | Train Loss=1714.83625488 | Val Loss=1.45775453 | Data=17.12595507 | Physics=2.22620090 | Val RMSE: 1.22283161 | ‚àö(Val Loss) = 1.20737505 | Current Learning Rate: 0.0002\n",
      "Epoch 652/1000 | Train Loss=1718.60979818 | Val Loss=1.45859456 | Data=17.16367607 | Physics=2.17146916 | Val RMSE: 1.22281694 | ‚àö(Val Loss) = 1.20772290 | Current Learning Rate: 0.0002\n",
      "Epoch 653/1000 | Train Loss=1716.78761393 | Val Loss=1.45839667 | Data=17.14543126 | Physics=2.33909312 | Val RMSE: 1.22248673 | ‚àö(Val Loss) = 1.20764101 | Current Learning Rate: 0.0002\n",
      "Epoch 654/1000 | Train Loss=1715.59790039 | Val Loss=1.45766469 | Data=17.13359502 | Physics=2.15676977 | Val RMSE: 1.22237718 | ‚àö(Val Loss) = 1.20733786 | Current Learning Rate: 0.0002\n",
      "Epoch 655/1000 | Train Loss=1715.17217611 | Val Loss=1.45763652 | Data=17.12927729 | Physics=2.23622976 | Val RMSE: 1.22276258 | ‚àö(Val Loss) = 1.20732617 | Current Learning Rate: 0.0002\n",
      "Epoch 656/1000 | Train Loss=1717.26389974 | Val Loss=1.45720307 | Data=17.15022761 | Physics=2.22842910 | Val RMSE: 1.22275567 | ‚àö(Val Loss) = 1.20714664 | Current Learning Rate: 0.0002\n",
      "Epoch 657/1000 | Train Loss=1718.61590169 | Val Loss=1.45843601 | Data=17.16377195 | Physics=2.20062555 | Val RMSE: 1.22259891 | ‚àö(Val Loss) = 1.20765722 | Current Learning Rate: 0.0002\n",
      "Epoch 658/1000 | Train Loss=1714.09798991 | Val Loss=1.45658855 | Data=17.11855176 | Physics=2.18138547 | Val RMSE: 1.22273159 | ‚àö(Val Loss) = 1.20689213 | Current Learning Rate: 0.0002\n",
      "Epoch 659/1000 | Train Loss=1715.98352051 | Val Loss=1.45669246 | Data=17.13744005 | Physics=2.15534640 | Val RMSE: 1.22265220 | ‚àö(Val Loss) = 1.20693517 | Current Learning Rate: 0.0002\n",
      "Epoch 660/1000 | Train Loss=1718.29727376 | Val Loss=1.45605004 | Data=17.16053034 | Physics=2.22803960 | Val RMSE: 1.22247410 | ‚àö(Val Loss) = 1.20666897 | Current Learning Rate: 0.0002\n",
      "Epoch 661/1000 | Train Loss=1716.84069824 | Val Loss=1.45562088 | Data=17.14598173 | Physics=2.26940239 | Val RMSE: 1.22255421 | ‚àö(Val Loss) = 1.20649111 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 662/1000 | Train Loss=1718.85748698 | Val Loss=1.45580280 | Data=17.16608696 | Physics=2.32166354 | Val RMSE: 1.22247672 | ‚àö(Val Loss) = 1.20656657 | Current Learning Rate: 0.0002\n",
      "Epoch 663/1000 | Train Loss=1717.63435872 | Val Loss=1.45777774 | Data=17.15392647 | Physics=2.20449640 | Val RMSE: 1.22253799 | ‚àö(Val Loss) = 1.20738471 | Current Learning Rate: 0.0002\n",
      "Epoch 664/1000 | Train Loss=1720.08487142 | Val Loss=1.45792933 | Data=17.17844543 | Physics=2.17428164 | Val RMSE: 1.22266996 | ‚àö(Val Loss) = 1.20744741 | Current Learning Rate: 0.0002\n",
      "Epoch 665/1000 | Train Loss=1718.56114095 | Val Loss=1.45700300 | Data=17.16318378 | Physics=2.28865136 | Val RMSE: 1.22269523 | ‚àö(Val Loss) = 1.20706379 | Current Learning Rate: 0.0002\n",
      "Epoch 666/1000 | Train Loss=1715.51708171 | Val Loss=1.45636940 | Data=17.13270556 | Physics=2.24782684 | Val RMSE: 1.22270489 | ‚àö(Val Loss) = 1.20680130 | Current Learning Rate: 0.0002\n",
      "Epoch 667/1000 | Train Loss=1717.05574544 | Val Loss=1.45838320 | Data=17.14815369 | Physics=2.18061444 | Val RMSE: 1.22267580 | ‚àö(Val Loss) = 1.20763540 | Current Learning Rate: 0.0002\n",
      "Epoch 668/1000 | Train Loss=1719.01996257 | Val Loss=1.45671372 | Data=17.16778374 | Physics=2.14945079 | Val RMSE: 1.22264624 | ‚àö(Val Loss) = 1.20694399 | Current Learning Rate: 0.0002\n",
      "Epoch 669/1000 | Train Loss=1717.92793783 | Val Loss=1.45840788 | Data=17.15688985 | Physics=2.13819906 | Val RMSE: 1.22258270 | ‚àö(Val Loss) = 1.20764554 | Current Learning Rate: 0.0002\n",
      "Epoch 670/1000 | Train Loss=1719.30771484 | Val Loss=1.45894786 | Data=17.17063414 | Physics=2.31361938 | Val RMSE: 1.22276402 | ‚àö(Val Loss) = 1.20786917 | Current Learning Rate: 0.0002\n",
      "Epoch 671/1000 | Train Loss=1717.93632813 | Val Loss=1.45813493 | Data=17.15696742 | Physics=2.21083044 | Val RMSE: 1.22215676 | ‚àö(Val Loss) = 1.20753253 | Current Learning Rate: 0.0002\n",
      "Epoch 672/1000 | Train Loss=1713.84426270 | Val Loss=1.45732073 | Data=17.11602033 | Physics=2.22944874 | Val RMSE: 1.22250938 | ‚àö(Val Loss) = 1.20719540 | Current Learning Rate: 0.0002\n",
      "Epoch 673/1000 | Train Loss=1721.30805664 | Val Loss=1.45632132 | Data=17.19065832 | Physics=2.19690040 | Val RMSE: 1.22253788 | ‚àö(Val Loss) = 1.20678139 | Current Learning Rate: 0.0002\n",
      "Epoch 674/1000 | Train Loss=1722.13488770 | Val Loss=1.45682657 | Data=17.19891459 | Physics=2.22159804 | Val RMSE: 1.22273183 | ‚àö(Val Loss) = 1.20699072 | Current Learning Rate: 0.0002\n",
      "Epoch 675/1000 | Train Loss=1713.13122559 | Val Loss=1.45874202 | Data=17.10890649 | Physics=2.19492904 | Val RMSE: 1.22278786 | ‚àö(Val Loss) = 1.20778394 | Current Learning Rate: 0.0002\n",
      "Epoch 676/1000 | Train Loss=1716.21271159 | Val Loss=1.45675405 | Data=17.13971926 | Physics=2.24026712 | Val RMSE: 1.22248673 | ‚àö(Val Loss) = 1.20696068 | Current Learning Rate: 0.0002\n",
      "Epoch 677/1000 | Train Loss=1715.44175618 | Val Loss=1.45672905 | Data=17.13196856 | Physics=2.25197181 | Val RMSE: 1.22275925 | ‚àö(Val Loss) = 1.20695031 | Current Learning Rate: 0.0002\n",
      "Epoch 678/1000 | Train Loss=1715.26748861 | Val Loss=1.45726017 | Data=17.13023783 | Physics=2.29102410 | Val RMSE: 1.22284818 | ‚àö(Val Loss) = 1.20717025 | Current Learning Rate: 0.0002\n",
      "Epoch 679/1000 | Train Loss=1719.74266764 | Val Loss=1.45586268 | Data=17.17498976 | Physics=2.20079832 | Val RMSE: 1.22269881 | ‚àö(Val Loss) = 1.20659137 | Current Learning Rate: 0.0002\n",
      "Epoch 680/1000 | Train Loss=1717.95909831 | Val Loss=1.45629287 | Data=17.15717112 | Physics=2.24449120 | Val RMSE: 1.22274113 | ‚àö(Val Loss) = 1.20676959 | Current Learning Rate: 0.0002\n",
      "Epoch 681/1000 | Train Loss=1716.35008952 | Val Loss=1.45657682 | Data=17.14108213 | Physics=2.13851249 | Val RMSE: 1.22270560 | ‚àö(Val Loss) = 1.20688725 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 682/1000 | Train Loss=1718.95354818 | Val Loss=1.45683567 | Data=17.16711318 | Physics=2.22609943 | Val RMSE: 1.22280288 | ‚àö(Val Loss) = 1.20699441 | Current Learning Rate: 0.0002\n",
      "Epoch 683/1000 | Train Loss=1716.05493978 | Val Loss=1.45620962 | Data=17.13807952 | Physics=2.29579841 | Val RMSE: 1.22277749 | ‚àö(Val Loss) = 1.20673513 | Current Learning Rate: 0.0002\n",
      "Epoch 684/1000 | Train Loss=1715.23055013 | Val Loss=1.45719365 | Data=17.12987041 | Physics=2.22166122 | Val RMSE: 1.22268224 | ‚àö(Val Loss) = 1.20714271 | Current Learning Rate: 0.0002\n",
      "Epoch 685/1000 | Train Loss=1718.84436849 | Val Loss=1.45901334 | Data=17.16609389 | Physics=2.14128175 | Val RMSE: 1.22247386 | ‚àö(Val Loss) = 1.20789623 | Current Learning Rate: 0.0002\n",
      "Epoch 686/1000 | Train Loss=1710.62368164 | Val Loss=1.45571152 | Data=17.08378563 | Physics=2.19517969 | Val RMSE: 1.22242403 | ‚àö(Val Loss) = 1.20652866 | Current Learning Rate: 0.0002\n",
      "Epoch 687/1000 | Train Loss=1716.83857422 | Val Loss=1.45684946 | Data=17.14595757 | Physics=2.23738439 | Val RMSE: 1.22249699 | ‚àö(Val Loss) = 1.20700014 | Current Learning Rate: 0.0002\n",
      "Epoch 688/1000 | Train Loss=1709.95090332 | Val Loss=1.45675619 | Data=17.07708728 | Physics=2.20193331 | Val RMSE: 1.22264421 | ‚àö(Val Loss) = 1.20696151 | Current Learning Rate: 0.0002\n",
      "Epoch 689/1000 | Train Loss=1713.73331706 | Val Loss=1.45427982 | Data=17.11488902 | Physics=2.25133326 | Val RMSE: 1.22225893 | ‚àö(Val Loss) = 1.20593524 | Current Learning Rate: 0.0002\n",
      "Epoch 690/1000 | Train Loss=1720.28943685 | Val Loss=1.45678401 | Data=17.18044446 | Physics=2.28273641 | Val RMSE: 1.22248316 | ‚àö(Val Loss) = 1.20697308 | Current Learning Rate: 0.0002\n",
      "Epoch 691/1000 | Train Loss=1720.15681152 | Val Loss=1.45672631 | Data=17.17914810 | Physics=2.22226262 | Val RMSE: 1.22251272 | ‚àö(Val Loss) = 1.20694923 | Current Learning Rate: 0.0002\n",
      "Epoch 692/1000 | Train Loss=1712.02244466 | Val Loss=1.45550867 | Data=17.09777304 | Physics=2.25216071 | Val RMSE: 1.22268474 | ‚àö(Val Loss) = 1.20644462 | Current Learning Rate: 0.0002\n",
      "Epoch 693/1000 | Train Loss=1716.14061686 | Val Loss=1.45603975 | Data=17.13898398 | Physics=2.18869426 | Val RMSE: 1.22275066 | ‚àö(Val Loss) = 1.20666468 | Current Learning Rate: 0.0002\n",
      "Epoch 694/1000 | Train Loss=1720.45166829 | Val Loss=1.45829678 | Data=17.18209788 | Physics=2.22211228 | Val RMSE: 1.22271585 | ‚àö(Val Loss) = 1.20759964 | Current Learning Rate: 0.0002\n",
      "Epoch 695/1000 | Train Loss=1716.29395345 | Val Loss=1.45772080 | Data=17.14052976 | Physics=2.14295585 | Val RMSE: 1.22280240 | ‚àö(Val Loss) = 1.20736110 | Current Learning Rate: 0.0002\n",
      "Epoch 696/1000 | Train Loss=1717.16006673 | Val Loss=1.45643445 | Data=17.14915492 | Physics=2.31741985 | Val RMSE: 1.22280407 | ‚àö(Val Loss) = 1.20682824 | Current Learning Rate: 0.0002\n",
      "Epoch 697/1000 | Train Loss=1715.57188314 | Val Loss=1.45744292 | Data=17.13330485 | Physics=2.17728042 | Val RMSE: 1.22274613 | ‚àö(Val Loss) = 1.20724595 | Current Learning Rate: 0.0002\n",
      "Epoch 698/1000 | Train Loss=1717.82476400 | Val Loss=1.45826924 | Data=17.15585607 | Physics=2.19380209 | Val RMSE: 1.22239423 | ‚àö(Val Loss) = 1.20758820 | Current Learning Rate: 0.0002\n",
      "Epoch 699/1000 | Train Loss=1721.75550130 | Val Loss=1.45627328 | Data=17.19513054 | Physics=2.19176803 | Val RMSE: 1.22255993 | ‚àö(Val Loss) = 1.20676148 | Current Learning Rate: 0.0002\n",
      "Epoch 700/1000 | Train Loss=1719.08520508 | Val Loss=1.45676502 | Data=17.16843783 | Physics=2.18374162 | Val RMSE: 1.22277403 | ‚àö(Val Loss) = 1.20696521 | Current Learning Rate: 0.0002\n",
      "Epoch 701/1000 | Train Loss=1717.04270020 | Val Loss=1.45757703 | Data=17.14799423 | Physics=2.24672790 | Val RMSE: 1.22274137 | ‚àö(Val Loss) = 1.20730150 | Current Learning Rate: 0.0002\n",
      "‚úÖ Learning Rate updated to 0.001\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 702/1000 | Train Loss=1714.99779460 | Val Loss=1.48409923 | Data=17.12781448 | Physics=2.16832341 | Val RMSE: 1.22340035 | ‚àö(Val Loss) = 1.21823609 | Current Learning Rate: 0.001\n",
      "Epoch 703/1000 | Train Loss=1714.82763672 | Val Loss=1.46793507 | Data=17.12596245 | Physics=2.23557502 | Val RMSE: 1.22166395 | ‚àö(Val Loss) = 1.21158373 | Current Learning Rate: 0.001\n",
      "Epoch 704/1000 | Train Loss=1715.25925293 | Val Loss=1.47151921 | Data=17.13032595 | Physics=2.18900045 | Val RMSE: 1.22246671 | ‚àö(Val Loss) = 1.21306193 | Current Learning Rate: 0.001\n",
      "Epoch 705/1000 | Train Loss=1717.65450033 | Val Loss=1.48070381 | Data=17.15432453 | Physics=2.20678396 | Val RMSE: 1.22323549 | ‚àö(Val Loss) = 1.21684170 | Current Learning Rate: 0.001\n",
      "Epoch 706/1000 | Train Loss=1719.18011068 | Val Loss=1.46432247 | Data=17.16944097 | Physics=2.16672234 | Val RMSE: 1.22254455 | ‚àö(Val Loss) = 1.21009195 | Current Learning Rate: 0.001\n",
      "Epoch 707/1000 | Train Loss=1721.30926921 | Val Loss=1.46598369 | Data=17.19082184 | Physics=2.17489022 | Val RMSE: 1.22206593 | ‚àö(Val Loss) = 1.21077812 | Current Learning Rate: 0.001\n",
      "Epoch 708/1000 | Train Loss=1715.26844889 | Val Loss=1.47707707 | Data=17.13048147 | Physics=2.17105326 | Val RMSE: 1.22373235 | ‚àö(Val Loss) = 1.21535063 | Current Learning Rate: 0.001\n",
      "Epoch 709/1000 | Train Loss=1718.81268717 | Val Loss=1.45802172 | Data=17.16574923 | Physics=2.27072992 | Val RMSE: 1.22142053 | ‚àö(Val Loss) = 1.20748568 | Current Learning Rate: 0.001\n",
      "Epoch 710/1000 | Train Loss=1717.10804036 | Val Loss=1.46606787 | Data=17.14879004 | Physics=2.20101515 | Val RMSE: 1.22217512 | ‚àö(Val Loss) = 1.21081293 | Current Learning Rate: 0.001\n",
      "Epoch 711/1000 | Train Loss=1711.19216309 | Val Loss=1.47559293 | Data=17.08966796 | Physics=2.21595897 | Val RMSE: 1.22235489 | ‚àö(Val Loss) = 1.21473992 | Current Learning Rate: 0.001\n",
      "Epoch 712/1000 | Train Loss=1720.71745605 | Val Loss=1.46856091 | Data=17.18487727 | Physics=2.20875787 | Val RMSE: 1.22203255 | ‚àö(Val Loss) = 1.21184194 | Current Learning Rate: 0.001\n",
      "Epoch 713/1000 | Train Loss=1714.58564453 | Val Loss=1.47360792 | Data=17.12362417 | Physics=2.21099647 | Val RMSE: 1.22244000 | ‚àö(Val Loss) = 1.21392250 | Current Learning Rate: 0.001\n",
      "Epoch 714/1000 | Train Loss=1716.43891602 | Val Loss=1.47468630 | Data=17.14204591 | Physics=2.24577226 | Val RMSE: 1.22212410 | ‚àö(Val Loss) = 1.21436656 | Current Learning Rate: 0.001\n",
      "Epoch 715/1000 | Train Loss=1716.23996582 | Val Loss=1.47655225 | Data=17.14014753 | Physics=2.21547482 | Val RMSE: 1.22271097 | ‚àö(Val Loss) = 1.21513462 | Current Learning Rate: 0.001\n",
      "Epoch 716/1000 | Train Loss=1717.43640137 | Val Loss=1.47695388 | Data=17.15208054 | Physics=2.23892112 | Val RMSE: 1.22147441 | ‚àö(Val Loss) = 1.21529996 | Current Learning Rate: 0.001\n",
      "Epoch 717/1000 | Train Loss=1716.92836100 | Val Loss=1.47678326 | Data=17.14710833 | Physics=2.20983055 | Val RMSE: 1.22282779 | ‚àö(Val Loss) = 1.21522975 | Current Learning Rate: 0.001\n",
      "Epoch 718/1000 | Train Loss=1711.70470378 | Val Loss=1.46643476 | Data=17.09466788 | Physics=2.26981644 | Val RMSE: 1.22273088 | ‚àö(Val Loss) = 1.21096432 | Current Learning Rate: 0.001\n",
      "Epoch 719/1000 | Train Loss=1718.08076986 | Val Loss=1.47199349 | Data=17.15859051 | Physics=2.16487257 | Val RMSE: 1.22199106 | ‚àö(Val Loss) = 1.21325731 | Current Learning Rate: 0.001\n",
      "Epoch 720/1000 | Train Loss=1721.03567708 | Val Loss=1.46430316 | Data=17.18806979 | Physics=2.18025919 | Val RMSE: 1.22100973 | ‚àö(Val Loss) = 1.21008396 | Current Learning Rate: 0.001\n",
      "Epoch 721/1000 | Train Loss=1715.92119141 | Val Loss=1.48022672 | Data=17.13692449 | Physics=2.16941491 | Val RMSE: 1.22207928 | ‚àö(Val Loss) = 1.21664572 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 722/1000 | Train Loss=1719.09036458 | Val Loss=1.47460723 | Data=17.16870410 | Physics=2.16683288 | Val RMSE: 1.22284651 | ‚àö(Val Loss) = 1.21433401 | Current Learning Rate: 0.001\n",
      "Epoch 723/1000 | Train Loss=1720.27243652 | Val Loss=1.47606204 | Data=17.18038025 | Physics=2.23716511 | Val RMSE: 1.22257388 | ‚àö(Val Loss) = 1.21493292 | Current Learning Rate: 0.001\n",
      "Epoch 724/1000 | Train Loss=1716.64755046 | Val Loss=1.47789641 | Data=17.14424095 | Physics=2.19428797 | Val RMSE: 1.22065294 | ‚àö(Val Loss) = 1.21568763 | Current Learning Rate: 0.001\n",
      "Epoch 725/1000 | Train Loss=1712.74016113 | Val Loss=1.46562850 | Data=17.10513535 | Physics=2.19941265 | Val RMSE: 1.21975017 | ‚àö(Val Loss) = 1.21063149 | Current Learning Rate: 0.001\n",
      "Epoch 726/1000 | Train Loss=1717.87962240 | Val Loss=1.46614327 | Data=17.15646960 | Physics=2.12083878 | Val RMSE: 1.22205532 | ‚àö(Val Loss) = 1.21084404 | Current Learning Rate: 0.001\n",
      "Epoch 727/1000 | Train Loss=1717.93811849 | Val Loss=1.47659365 | Data=17.15711358 | Physics=2.18981180 | Val RMSE: 1.22316790 | ‚àö(Val Loss) = 1.21515167 | Current Learning Rate: 0.001\n",
      "Epoch 728/1000 | Train Loss=1718.28142090 | Val Loss=1.47264226 | Data=17.16050148 | Physics=2.31372152 | Val RMSE: 1.22288692 | ‚àö(Val Loss) = 1.21352470 | Current Learning Rate: 0.001\n",
      "Epoch 729/1000 | Train Loss=1714.24679362 | Val Loss=1.47592131 | Data=17.12023563 | Physics=2.14180938 | Val RMSE: 1.22253072 | ‚àö(Val Loss) = 1.21487498 | Current Learning Rate: 0.001\n",
      "Epoch 730/1000 | Train Loss=1715.80681152 | Val Loss=1.46922700 | Data=17.13576419 | Physics=2.18610567 | Val RMSE: 1.22249639 | ‚àö(Val Loss) = 1.21211672 | Current Learning Rate: 0.001\n",
      "Epoch 731/1000 | Train Loss=1714.52260742 | Val Loss=1.47017171 | Data=17.12297160 | Physics=2.20944433 | Val RMSE: 1.22175908 | ‚àö(Val Loss) = 1.21250641 | Current Learning Rate: 0.001\n",
      "Epoch 732/1000 | Train Loss=1718.11400553 | Val Loss=1.46347203 | Data=17.15877889 | Physics=2.26528207 | Val RMSE: 1.22159088 | ‚àö(Val Loss) = 1.20974052 | Current Learning Rate: 0.001\n",
      "Epoch 733/1000 | Train Loss=1718.66116536 | Val Loss=1.48085429 | Data=17.16438421 | Physics=2.08552471 | Val RMSE: 1.22207832 | ‚àö(Val Loss) = 1.21690357 | Current Learning Rate: 0.001\n",
      "Epoch 734/1000 | Train Loss=1717.26206868 | Val Loss=1.46348776 | Data=17.15036240 | Physics=2.25069836 | Val RMSE: 1.21911883 | ‚àö(Val Loss) = 1.20974696 | Current Learning Rate: 0.001\n",
      "Epoch 735/1000 | Train Loss=1719.47792155 | Val Loss=1.46848627 | Data=17.17247086 | Physics=2.20134696 | Val RMSE: 1.22015965 | ‚àö(Val Loss) = 1.21181118 | Current Learning Rate: 0.001\n",
      "Epoch 736/1000 | Train Loss=1717.65316569 | Val Loss=1.47629559 | Data=17.15429955 | Physics=2.15129146 | Val RMSE: 1.22212982 | ‚àö(Val Loss) = 1.21502900 | Current Learning Rate: 0.001\n",
      "Epoch 737/1000 | Train Loss=1715.49571126 | Val Loss=1.46997180 | Data=17.13261503 | Physics=2.18428295 | Val RMSE: 1.22209382 | ‚àö(Val Loss) = 1.21242392 | Current Learning Rate: 0.001\n",
      "Epoch 738/1000 | Train Loss=1718.10135091 | Val Loss=1.47472795 | Data=17.15878340 | Physics=2.20805332 | Val RMSE: 1.22276545 | ‚àö(Val Loss) = 1.21438384 | Current Learning Rate: 0.001\n",
      "Epoch 739/1000 | Train Loss=1719.68937174 | Val Loss=1.47312095 | Data=17.17461561 | Physics=2.17323325 | Val RMSE: 1.22179341 | ‚àö(Val Loss) = 1.21372199 | Current Learning Rate: 0.001\n",
      "Epoch 740/1000 | Train Loss=1714.44982910 | Val Loss=1.47033451 | Data=17.12223269 | Physics=2.20172253 | Val RMSE: 1.22155273 | ‚àö(Val Loss) = 1.21257353 | Current Learning Rate: 0.001\n",
      "Epoch 741/1000 | Train Loss=1717.99150391 | Val Loss=1.47061010 | Data=17.15760740 | Physics=2.21864678 | Val RMSE: 1.22233033 | ‚àö(Val Loss) = 1.21268713 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 742/1000 | Train Loss=1713.71084798 | Val Loss=1.46615958 | Data=17.11484477 | Physics=2.20340174 | Val RMSE: 1.22215390 | ‚àö(Val Loss) = 1.21085072 | Current Learning Rate: 0.001\n",
      "Epoch 743/1000 | Train Loss=1712.53489583 | Val Loss=1.46476775 | Data=17.10300020 | Physics=2.17832035 | Val RMSE: 1.22195280 | ‚àö(Val Loss) = 1.21027589 | Current Learning Rate: 0.001\n",
      "Epoch 744/1000 | Train Loss=1717.43633626 | Val Loss=1.47307855 | Data=17.15210730 | Physics=2.28391835 | Val RMSE: 1.22166681 | ‚àö(Val Loss) = 1.21370447 | Current Learning Rate: 0.001\n",
      "Epoch 745/1000 | Train Loss=1713.64165853 | Val Loss=1.47008197 | Data=17.11413905 | Physics=2.14201825 | Val RMSE: 1.21974182 | ‚àö(Val Loss) = 1.21246934 | Current Learning Rate: 0.001\n",
      "Epoch 746/1000 | Train Loss=1719.82596029 | Val Loss=1.47121859 | Data=17.17599589 | Physics=2.22762172 | Val RMSE: 1.22177219 | ‚àö(Val Loss) = 1.21293795 | Current Learning Rate: 0.001\n",
      "Epoch 747/1000 | Train Loss=1716.50900065 | Val Loss=1.47181815 | Data=17.14284197 | Physics=2.18129521 | Val RMSE: 1.22199810 | ‚àö(Val Loss) = 1.21318519 | Current Learning Rate: 0.001\n",
      "Epoch 748/1000 | Train Loss=1717.89313151 | Val Loss=1.46978641 | Data=17.15656052 | Physics=2.27194971 | Val RMSE: 1.22318339 | ‚àö(Val Loss) = 1.21234751 | Current Learning Rate: 0.001\n",
      "Epoch 749/1000 | Train Loss=1718.92562663 | Val Loss=1.46817778 | Data=17.16703733 | Physics=2.17493302 | Val RMSE: 1.22185171 | ‚àö(Val Loss) = 1.21168387 | Current Learning Rate: 0.001\n",
      "Epoch 750/1000 | Train Loss=1719.42939453 | Val Loss=1.47592872 | Data=17.17201424 | Physics=2.23139136 | Val RMSE: 1.22252822 | ‚àö(Val Loss) = 1.21487808 | Current Learning Rate: 0.001\n",
      "Epoch 751/1000 | Train Loss=1716.43281250 | Val Loss=1.47547408 | Data=17.14207452 | Physics=2.14307650 | Val RMSE: 1.22291946 | ‚àö(Val Loss) = 1.21469092 | Current Learning Rate: 0.001\n",
      "Epoch 752/1000 | Train Loss=1713.94423014 | Val Loss=1.47323541 | Data=17.11713142 | Physics=2.06933127 | Val RMSE: 1.22264385 | ‚àö(Val Loss) = 1.21376908 | Current Learning Rate: 0.001\n",
      "Epoch 753/1000 | Train Loss=1716.36984863 | Val Loss=1.47248369 | Data=17.14148026 | Physics=2.18517295 | Val RMSE: 1.22339511 | ‚àö(Val Loss) = 1.21345937 | Current Learning Rate: 0.001\n",
      "Epoch 754/1000 | Train Loss=1714.09080404 | Val Loss=1.47738262 | Data=17.11861013 | Physics=2.31062947 | Val RMSE: 1.22312450 | ‚àö(Val Loss) = 1.21547627 | Current Learning Rate: 0.001\n",
      "Epoch 755/1000 | Train Loss=1720.84605306 | Val Loss=1.46976972 | Data=17.18618024 | Physics=2.17129569 | Val RMSE: 1.22178483 | ‚àö(Val Loss) = 1.21234059 | Current Learning Rate: 0.001\n",
      "Epoch 756/1000 | Train Loss=1715.86835124 | Val Loss=1.47001104 | Data=17.13643901 | Physics=2.19761357 | Val RMSE: 1.22219658 | ‚àö(Val Loss) = 1.21244013 | Current Learning Rate: 0.001\n",
      "Epoch 757/1000 | Train Loss=1715.54553223 | Val Loss=1.47382655 | Data=17.13320859 | Physics=2.16848943 | Val RMSE: 1.22316241 | ‚àö(Val Loss) = 1.21401262 | Current Learning Rate: 0.001\n",
      "Epoch 758/1000 | Train Loss=1718.01132813 | Val Loss=1.47103421 | Data=17.15780729 | Physics=2.22818485 | Val RMSE: 1.22330427 | ‚àö(Val Loss) = 1.21286201 | Current Learning Rate: 0.001\n",
      "Epoch 759/1000 | Train Loss=1713.20407715 | Val Loss=1.46723296 | Data=17.10977904 | Physics=2.13753218 | Val RMSE: 1.22221017 | ‚àö(Val Loss) = 1.21129394 | Current Learning Rate: 0.001\n",
      "Epoch 760/1000 | Train Loss=1716.70869141 | Val Loss=1.47080541 | Data=17.14481583 | Physics=2.28676187 | Val RMSE: 1.22256660 | ‚àö(Val Loss) = 1.21276772 | Current Learning Rate: 0.001\n",
      "Epoch 761/1000 | Train Loss=1712.94120280 | Val Loss=1.47242192 | Data=17.10717990 | Physics=2.20175456 | Val RMSE: 1.22290397 | ‚àö(Val Loss) = 1.21343398 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 762/1000 | Train Loss=1719.96184082 | Val Loss=1.47228638 | Data=17.17726046 | Physics=2.17454641 | Val RMSE: 1.22243345 | ‚àö(Val Loss) = 1.21337807 | Current Learning Rate: 0.001\n",
      "Epoch 763/1000 | Train Loss=1720.84295247 | Val Loss=1.47025251 | Data=17.18625291 | Physics=2.14695759 | Val RMSE: 1.22307563 | ‚àö(Val Loss) = 1.21253967 | Current Learning Rate: 0.001\n",
      "Epoch 764/1000 | Train Loss=1720.39326172 | Val Loss=1.47488064 | Data=17.18158798 | Physics=2.16575072 | Val RMSE: 1.22373593 | ‚àö(Val Loss) = 1.21444666 | Current Learning Rate: 0.001\n",
      "Epoch 765/1000 | Train Loss=1718.17989909 | Val Loss=1.46859650 | Data=17.15961431 | Physics=2.18902931 | Val RMSE: 1.22255862 | ‚àö(Val Loss) = 1.21185660 | Current Learning Rate: 0.001\n",
      "Epoch 766/1000 | Train Loss=1713.40119629 | Val Loss=1.45571391 | Data=17.11153399 | Physics=2.24420625 | Val RMSE: 1.22114861 | ‚àö(Val Loss) = 1.20652974 | Current Learning Rate: 0.001\n",
      "Epoch 767/1000 | Train Loss=1716.02018229 | Val Loss=1.47273046 | Data=17.13796450 | Physics=2.28187671 | Val RMSE: 1.22231770 | ‚àö(Val Loss) = 1.21356106 | Current Learning Rate: 0.001\n",
      "Epoch 768/1000 | Train Loss=1717.17436523 | Val Loss=1.45599926 | Data=17.14934209 | Physics=2.28303993 | Val RMSE: 1.22089458 | ‚àö(Val Loss) = 1.20664799 | Current Learning Rate: 0.001\n",
      "Epoch 769/1000 | Train Loss=1716.39725749 | Val Loss=1.47306365 | Data=17.14165179 | Physics=2.25340044 | Val RMSE: 1.22235227 | ‚àö(Val Loss) = 1.21369839 | Current Learning Rate: 0.001\n",
      "Epoch 770/1000 | Train Loss=1714.47329102 | Val Loss=1.48142610 | Data=17.12247340 | Physics=2.25329595 | Val RMSE: 1.22366428 | ‚àö(Val Loss) = 1.21713853 | Current Learning Rate: 0.001\n",
      "Epoch 771/1000 | Train Loss=1714.38626302 | Val Loss=1.47194884 | Data=17.12160956 | Physics=2.12450616 | Val RMSE: 1.22379911 | ‚àö(Val Loss) = 1.21323895 | Current Learning Rate: 0.001\n",
      "Epoch 772/1000 | Train Loss=1715.60222982 | Val Loss=1.46635103 | Data=17.13371525 | Physics=2.15407715 | Val RMSE: 1.22190225 | ‚àö(Val Loss) = 1.21092987 | Current Learning Rate: 0.001\n",
      "Epoch 773/1000 | Train Loss=1715.83580729 | Val Loss=1.47743275 | Data=17.13608297 | Physics=2.22731544 | Val RMSE: 1.22042108 | ‚àö(Val Loss) = 1.21549690 | Current Learning Rate: 0.001\n",
      "Epoch 774/1000 | Train Loss=1712.87369792 | Val Loss=1.47451325 | Data=17.10649643 | Physics=2.13895042 | Val RMSE: 1.22148573 | ‚àö(Val Loss) = 1.21429539 | Current Learning Rate: 0.001\n",
      "Epoch 775/1000 | Train Loss=1718.03612467 | Val Loss=1.45937065 | Data=17.15808843 | Physics=2.09846583 | Val RMSE: 1.21912420 | ‚àö(Val Loss) = 1.20804417 | Current Learning Rate: 0.001\n",
      "Epoch 776/1000 | Train Loss=1710.69216309 | Val Loss=1.46978021 | Data=17.08468355 | Physics=2.25617438 | Val RMSE: 1.21972513 | ‚àö(Val Loss) = 1.21234488 | Current Learning Rate: 0.001\n",
      "Epoch 777/1000 | Train Loss=1717.87123210 | Val Loss=1.46255269 | Data=17.15639610 | Physics=2.20878065 | Val RMSE: 1.21982551 | ‚àö(Val Loss) = 1.20936048 | Current Learning Rate: 0.001\n",
      "Epoch 778/1000 | Train Loss=1716.42366536 | Val Loss=1.47292672 | Data=17.14198647 | Physics=2.27992973 | Val RMSE: 1.22137594 | ‚àö(Val Loss) = 1.21364188 | Current Learning Rate: 0.001\n",
      "Epoch 779/1000 | Train Loss=1714.96231283 | Val Loss=1.46700001 | Data=17.12731622 | Physics=2.21717726 | Val RMSE: 1.22187901 | ‚àö(Val Loss) = 1.21119773 | Current Learning Rate: 0.001\n",
      "Epoch 780/1000 | Train Loss=1720.86018066 | Val Loss=1.46914146 | Data=17.18637492 | Physics=2.16612160 | Val RMSE: 1.22200632 | ‚àö(Val Loss) = 1.21208143 | Current Learning Rate: 0.001\n",
      "Epoch 781/1000 | Train Loss=1717.11336263 | Val Loss=1.47865703 | Data=17.14889666 | Physics=2.30661376 | Val RMSE: 1.22273540 | ‚àö(Val Loss) = 1.21600044 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 782/1000 | Train Loss=1717.26388346 | Val Loss=1.46756548 | Data=17.15033760 | Physics=2.20155110 | Val RMSE: 1.22287452 | ‚àö(Val Loss) = 1.21143115 | Current Learning Rate: 0.001\n",
      "Epoch 783/1000 | Train Loss=1716.93410645 | Val Loss=1.46868821 | Data=17.14703941 | Physics=2.19724671 | Val RMSE: 1.22257984 | ‚àö(Val Loss) = 1.21189451 | Current Learning Rate: 0.001\n",
      "Epoch 784/1000 | Train Loss=1713.66105957 | Val Loss=1.46855915 | Data=17.11432953 | Physics=2.15631451 | Val RMSE: 1.22154391 | ‚àö(Val Loss) = 1.21184123 | Current Learning Rate: 0.001\n",
      "Epoch 785/1000 | Train Loss=1716.89064941 | Val Loss=1.48184923 | Data=17.14666646 | Physics=2.21248192 | Val RMSE: 1.22379875 | ‚àö(Val Loss) = 1.21731234 | Current Learning Rate: 0.001\n",
      "Epoch 786/1000 | Train Loss=1720.39703776 | Val Loss=1.47124783 | Data=17.18163325 | Physics=2.25206629 | Val RMSE: 1.22266841 | ‚àö(Val Loss) = 1.21294999 | Current Learning Rate: 0.001\n",
      "Epoch 787/1000 | Train Loss=1720.46286621 | Val Loss=1.47298080 | Data=17.18235321 | Physics=2.18231954 | Val RMSE: 1.22214937 | ‚àö(Val Loss) = 1.21366417 | Current Learning Rate: 0.001\n",
      "Epoch 788/1000 | Train Loss=1717.42731120 | Val Loss=1.46839462 | Data=17.15198860 | Physics=2.20029086 | Val RMSE: 1.22221887 | ‚àö(Val Loss) = 1.21177340 | Current Learning Rate: 0.001\n",
      "Epoch 789/1000 | Train Loss=1716.14188639 | Val Loss=1.47465642 | Data=17.13912074 | Physics=2.22146233 | Val RMSE: 1.22216725 | ‚àö(Val Loss) = 1.21435440 | Current Learning Rate: 0.001\n",
      "Epoch 790/1000 | Train Loss=1715.74578451 | Val Loss=1.46920353 | Data=17.13516960 | Physics=2.31719616 | Val RMSE: 1.22152996 | ‚àö(Val Loss) = 1.21210706 | Current Learning Rate: 0.001\n",
      "Epoch 791/1000 | Train Loss=1720.52338867 | Val Loss=1.48410626 | Data=17.18300082 | Physics=2.23261487 | Val RMSE: 1.22044587 | ‚àö(Val Loss) = 1.21823907 | Current Learning Rate: 0.001\n",
      "Epoch 792/1000 | Train Loss=1719.97731934 | Val Loss=1.47901013 | Data=17.17760124 | Physics=2.09509242 | Val RMSE: 1.22319829 | ‚àö(Val Loss) = 1.21614563 | Current Learning Rate: 0.001\n",
      "Epoch 793/1000 | Train Loss=1716.37934570 | Val Loss=1.46597942 | Data=17.14147568 | Physics=2.27011598 | Val RMSE: 1.22235537 | ‚àö(Val Loss) = 1.21077633 | Current Learning Rate: 0.001\n",
      "Epoch 794/1000 | Train Loss=1715.86259766 | Val Loss=1.47893854 | Data=17.13637365 | Physics=2.23117206 | Val RMSE: 1.22155726 | ‚àö(Val Loss) = 1.21611619 | Current Learning Rate: 0.001\n",
      "Epoch 795/1000 | Train Loss=1720.09778646 | Val Loss=1.47371221 | Data=17.17865601 | Physics=2.20778748 | Val RMSE: 1.22232234 | ‚àö(Val Loss) = 1.21396554 | Current Learning Rate: 0.001\n",
      "Epoch 796/1000 | Train Loss=1720.15922038 | Val Loss=1.47503529 | Data=17.17940470 | Physics=2.12588523 | Val RMSE: 1.22309303 | ‚àö(Val Loss) = 1.21451032 | Current Learning Rate: 0.001\n",
      "Epoch 797/1000 | Train Loss=1710.45776367 | Val Loss=1.46907459 | Data=17.08225854 | Physics=2.28224106 | Val RMSE: 1.22220182 | ‚àö(Val Loss) = 1.21205389 | Current Learning Rate: 0.001\n",
      "Epoch 798/1000 | Train Loss=1720.63234049 | Val Loss=1.47278740 | Data=17.18401985 | Physics=2.18241154 | Val RMSE: 1.22246242 | ‚àö(Val Loss) = 1.21358454 | Current Learning Rate: 0.001\n",
      "Epoch 799/1000 | Train Loss=1719.81155599 | Val Loss=1.47438379 | Data=17.17594058 | Physics=2.15360853 | Val RMSE: 1.22347128 | ‚àö(Val Loss) = 1.21424210 | Current Learning Rate: 0.001\n",
      "Epoch 800/1000 | Train Loss=1720.45089518 | Val Loss=1.46409796 | Data=17.18215459 | Physics=2.11515232 | Val RMSE: 1.22208166 | ‚àö(Val Loss) = 1.20999920 | Current Learning Rate: 0.0001\n",
      "Epoch 801/1000 | Train Loss=1711.67359212 | Val Loss=1.45300126 | Data=17.09427280 | Physics=2.17407112 | Val RMSE: 1.22257757 | ‚àö(Val Loss) = 1.20540500 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 802/1000 | Train Loss=1717.09925130 | Val Loss=1.45529759 | Data=17.14853032 | Physics=2.28052311 | Val RMSE: 1.22251260 | ‚àö(Val Loss) = 1.20635712 | Current Learning Rate: 0.0001\n",
      "Epoch 803/1000 | Train Loss=1717.52200521 | Val Loss=1.45474788 | Data=17.15278002 | Physics=2.19383593 | Val RMSE: 1.22249103 | ‚àö(Val Loss) = 1.20612931 | Current Learning Rate: 0.0001\n",
      "Epoch 804/1000 | Train Loss=1718.73250326 | Val Loss=1.45341106 | Data=17.16485856 | Physics=2.21851366 | Val RMSE: 1.22264194 | ‚àö(Val Loss) = 1.20557499 | Current Learning Rate: 0.0001\n",
      "Epoch 805/1000 | Train Loss=1717.00433757 | Val Loss=1.45404677 | Data=17.14754753 | Physics=2.32712801 | Val RMSE: 1.22274375 | ‚àö(Val Loss) = 1.20583856 | Current Learning Rate: 0.0001\n",
      "Epoch 806/1000 | Train Loss=1721.10103353 | Val Loss=1.45425912 | Data=17.18854733 | Physics=2.26792995 | Val RMSE: 1.22272563 | ‚àö(Val Loss) = 1.20592666 | Current Learning Rate: 0.0001\n",
      "Epoch 807/1000 | Train Loss=1714.67924805 | Val Loss=1.45416776 | Data=17.12430967 | Physics=2.24919582 | Val RMSE: 1.22278452 | ‚àö(Val Loss) = 1.20588875 | Current Learning Rate: 0.0001\n",
      "Epoch 808/1000 | Train Loss=1714.47367350 | Val Loss=1.45619706 | Data=17.12231032 | Physics=2.18282602 | Val RMSE: 1.22261560 | ‚àö(Val Loss) = 1.20672989 | Current Learning Rate: 0.0001\n",
      "Epoch 809/1000 | Train Loss=1709.76935221 | Val Loss=1.45512970 | Data=17.07524096 | Physics=2.26128619 | Val RMSE: 1.22244120 | ‚àö(Val Loss) = 1.20628762 | Current Learning Rate: 0.0001\n",
      "Epoch 810/1000 | Train Loss=1716.25900065 | Val Loss=1.45321608 | Data=17.14011701 | Physics=2.23195704 | Val RMSE: 1.22253454 | ‚àö(Val Loss) = 1.20549417 | Current Learning Rate: 0.0001\n",
      "Epoch 811/1000 | Train Loss=1714.51297201 | Val Loss=1.45369184 | Data=17.12267354 | Physics=2.19474181 | Val RMSE: 1.22268093 | ‚àö(Val Loss) = 1.20569146 | Current Learning Rate: 0.0001\n",
      "Epoch 812/1000 | Train Loss=1712.07739258 | Val Loss=1.45377862 | Data=17.09830964 | Physics=2.24469517 | Val RMSE: 1.22269273 | ‚àö(Val Loss) = 1.20572746 | Current Learning Rate: 0.0001\n",
      "Epoch 813/1000 | Train Loss=1717.87862142 | Val Loss=1.45396837 | Data=17.15631243 | Physics=2.29115208 | Val RMSE: 1.22258174 | ‚àö(Val Loss) = 1.20580614 | Current Learning Rate: 0.0001\n",
      "Epoch 814/1000 | Train Loss=1714.88846842 | Val Loss=1.45407800 | Data=17.12640991 | Physics=2.25601101 | Val RMSE: 1.22257566 | ‚àö(Val Loss) = 1.20585155 | Current Learning Rate: 0.0001\n",
      "Epoch 815/1000 | Train Loss=1717.29171549 | Val Loss=1.45398935 | Data=17.15045948 | Physics=2.22051202 | Val RMSE: 1.22267640 | ‚àö(Val Loss) = 1.20581484 | Current Learning Rate: 0.0001\n",
      "Epoch 816/1000 | Train Loss=1713.44432780 | Val Loss=1.45413280 | Data=17.11198603 | Physics=2.22943165 | Val RMSE: 1.22271848 | ‚àö(Val Loss) = 1.20587432 | Current Learning Rate: 0.0001\n",
      "Epoch 817/1000 | Train Loss=1716.66497396 | Val Loss=1.45414050 | Data=17.14421018 | Physics=2.18787838 | Val RMSE: 1.22261357 | ‚àö(Val Loss) = 1.20587754 | Current Learning Rate: 0.0001\n",
      "Epoch 818/1000 | Train Loss=1720.98087565 | Val Loss=1.45429564 | Data=17.18734233 | Physics=2.21525676 | Val RMSE: 1.22259712 | ‚àö(Val Loss) = 1.20594180 | Current Learning Rate: 0.0001\n",
      "Epoch 819/1000 | Train Loss=1724.36315104 | Val Loss=1.45445057 | Data=17.22119687 | Physics=2.16828316 | Val RMSE: 1.22268617 | ‚àö(Val Loss) = 1.20600605 | Current Learning Rate: 0.0001\n",
      "Epoch 820/1000 | Train Loss=1716.10345866 | Val Loss=1.45404665 | Data=17.13860486 | Physics=2.14889155 | Val RMSE: 1.22254348 | ‚àö(Val Loss) = 1.20583856 | Current Learning Rate: 0.0001\n",
      "Epoch 821/1000 | Train Loss=1715.00272624 | Val Loss=1.45362743 | Data=17.12757632 | Physics=2.15790750 | Val RMSE: 1.22263527 | ‚àö(Val Loss) = 1.20566475 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 822/1000 | Train Loss=1716.07558594 | Val Loss=1.45309329 | Data=17.13829333 | Physics=2.23207269 | Val RMSE: 1.22264493 | ‚àö(Val Loss) = 1.20544314 | Current Learning Rate: 0.0001\n",
      "Epoch 823/1000 | Train Loss=1716.02983398 | Val Loss=1.45294913 | Data=17.13784332 | Physics=2.15568540 | Val RMSE: 1.22265112 | ‚àö(Val Loss) = 1.20538342 | Current Learning Rate: 0.0001\n",
      "Epoch 824/1000 | Train Loss=1717.31390788 | Val Loss=1.45394309 | Data=17.15066503 | Physics=2.25000635 | Val RMSE: 1.22272134 | ‚àö(Val Loss) = 1.20579565 | Current Learning Rate: 0.0001\n",
      "Epoch 825/1000 | Train Loss=1717.77054036 | Val Loss=1.45448037 | Data=17.15523141 | Physics=2.26269141 | Val RMSE: 1.22274673 | ‚àö(Val Loss) = 1.20601845 | Current Learning Rate: 0.0001\n",
      "Epoch 826/1000 | Train Loss=1714.77858073 | Val Loss=1.45425530 | Data=17.12532031 | Physics=2.24631864 | Val RMSE: 1.22276723 | ‚àö(Val Loss) = 1.20592511 | Current Learning Rate: 0.0001\n",
      "Epoch 827/1000 | Train Loss=1718.03190104 | Val Loss=1.45329487 | Data=17.15784289 | Physics=2.26489320 | Val RMSE: 1.22264457 | ‚àö(Val Loss) = 1.20552683 | Current Learning Rate: 0.0001\n",
      "Epoch 828/1000 | Train Loss=1716.68085937 | Val Loss=1.45288539 | Data=17.14434973 | Physics=2.17010123 | Val RMSE: 1.22265518 | ‚àö(Val Loss) = 1.20535696 | Current Learning Rate: 0.0001\n",
      "Epoch 829/1000 | Train Loss=1712.57832031 | Val Loss=1.45467877 | Data=17.10331980 | Physics=2.22271246 | Val RMSE: 1.22276473 | ‚àö(Val Loss) = 1.20610070 | Current Learning Rate: 0.0001\n",
      "Epoch 830/1000 | Train Loss=1709.48947754 | Val Loss=1.45504415 | Data=17.07245541 | Physics=2.23621179 | Val RMSE: 1.22245932 | ‚àö(Val Loss) = 1.20625210 | Current Learning Rate: 0.0001\n",
      "Epoch 831/1000 | Train Loss=1717.36847331 | Val Loss=1.45291344 | Data=17.15121040 | Physics=2.22344054 | Val RMSE: 1.22230220 | ‚àö(Val Loss) = 1.20536852 | Current Learning Rate: 0.0001\n",
      "Epoch 832/1000 | Train Loss=1716.48542480 | Val Loss=1.45265023 | Data=17.14239902 | Physics=2.20985830 | Val RMSE: 1.22257113 | ‚àö(Val Loss) = 1.20525944 | Current Learning Rate: 0.0001\n",
      "Epoch 833/1000 | Train Loss=1718.85484212 | Val Loss=1.45289346 | Data=17.16602567 | Physics=2.32674015 | Val RMSE: 1.22273159 | ‚àö(Val Loss) = 1.20536029 | Current Learning Rate: 0.0001\n",
      "Epoch 834/1000 | Train Loss=1717.03893229 | Val Loss=1.45516233 | Data=17.14796689 | Physics=2.16649816 | Val RMSE: 1.22274292 | ‚àö(Val Loss) = 1.20630109 | Current Learning Rate: 0.0001\n",
      "Epoch 835/1000 | Train Loss=1712.68292643 | Val Loss=1.45423810 | Data=17.10435340 | Physics=2.23783311 | Val RMSE: 1.22265327 | ‚àö(Val Loss) = 1.20591795 | Current Learning Rate: 0.0001\n",
      "Epoch 836/1000 | Train Loss=1717.74810384 | Val Loss=1.45416017 | Data=17.15499013 | Physics=2.37480007 | Val RMSE: 1.22274470 | ‚àö(Val Loss) = 1.20588565 | Current Learning Rate: 0.0001\n",
      "Epoch 837/1000 | Train Loss=1721.51044108 | Val Loss=1.45367301 | Data=17.19262199 | Physics=2.25205270 | Val RMSE: 1.22264552 | ‚àö(Val Loss) = 1.20568359 | Current Learning Rate: 0.0001\n",
      "Epoch 838/1000 | Train Loss=1719.43358561 | Val Loss=1.45419594 | Data=17.17188517 | Physics=2.23223187 | Val RMSE: 1.22257662 | ‚àö(Val Loss) = 1.20590043 | Current Learning Rate: 0.0001\n",
      "Epoch 839/1000 | Train Loss=1714.36162923 | Val Loss=1.45404720 | Data=17.12113889 | Physics=2.26207426 | Val RMSE: 1.22256124 | ‚àö(Val Loss) = 1.20583880 | Current Learning Rate: 0.0001\n",
      "Epoch 840/1000 | Train Loss=1716.53689779 | Val Loss=1.45491238 | Data=17.14290797 | Physics=2.29215013 | Val RMSE: 1.22258925 | ‚àö(Val Loss) = 1.20619750 | Current Learning Rate: 0.0001\n",
      "Epoch 841/1000 | Train Loss=1720.43153483 | Val Loss=1.45408599 | Data=17.18186194 | Physics=2.19339396 | Val RMSE: 1.22261786 | ‚àö(Val Loss) = 1.20585489 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 842/1000 | Train Loss=1717.35641276 | Val Loss=1.45407244 | Data=17.15111828 | Physics=2.20727161 | Val RMSE: 1.22270048 | ‚àö(Val Loss) = 1.20584929 | Current Learning Rate: 0.0001\n",
      "Epoch 843/1000 | Train Loss=1717.01755371 | Val Loss=1.45359011 | Data=17.14771805 | Physics=2.22079692 | Val RMSE: 1.22261739 | ‚àö(Val Loss) = 1.20564926 | Current Learning Rate: 0.0001\n",
      "Epoch 844/1000 | Train Loss=1717.61131185 | Val Loss=1.45397484 | Data=17.15365321 | Physics=2.20080841 | Val RMSE: 1.22266889 | ‚àö(Val Loss) = 1.20580876 | Current Learning Rate: 0.0001\n",
      "Epoch 845/1000 | Train Loss=1714.28000488 | Val Loss=1.45415799 | Data=17.12035815 | Physics=2.16144917 | Val RMSE: 1.22266662 | ‚àö(Val Loss) = 1.20588470 | Current Learning Rate: 0.0001\n",
      "Epoch 846/1000 | Train Loss=1714.71140137 | Val Loss=1.45436259 | Data=17.12462584 | Physics=2.28442276 | Val RMSE: 1.22269642 | ‚àö(Val Loss) = 1.20596957 | Current Learning Rate: 0.0001\n",
      "Epoch 847/1000 | Train Loss=1714.70334473 | Val Loss=1.45398160 | Data=17.12461662 | Physics=2.16164119 | Val RMSE: 1.22243929 | ‚àö(Val Loss) = 1.20581162 | Current Learning Rate: 0.0001\n",
      "Epoch 848/1000 | Train Loss=1719.99435221 | Val Loss=1.45315111 | Data=17.17748528 | Physics=2.14366322 | Val RMSE: 1.22257900 | ‚àö(Val Loss) = 1.20546722 | Current Learning Rate: 0.0001\n",
      "Epoch 849/1000 | Train Loss=1718.05539551 | Val Loss=1.45413180 | Data=17.15810795 | Physics=2.22886386 | Val RMSE: 1.22252917 | ‚àö(Val Loss) = 1.20587385 | Current Learning Rate: 0.0001\n",
      "Epoch 850/1000 | Train Loss=1719.02961426 | Val Loss=1.45369053 | Data=17.16784134 | Physics=2.16357822 | Val RMSE: 1.22257972 | ‚àö(Val Loss) = 1.20569086 | Current Learning Rate: 0.0001\n",
      "Epoch 851/1000 | Train Loss=1718.74145508 | Val Loss=1.45521903 | Data=17.16499265 | Physics=2.18017077 | Val RMSE: 1.22257841 | ‚àö(Val Loss) = 1.20632458 | Current Learning Rate: 0.0001\n",
      "Epoch 852/1000 | Train Loss=1719.60392253 | Val Loss=1.45393360 | Data=17.17357546 | Physics=2.24057585 | Val RMSE: 1.22248685 | ‚àö(Val Loss) = 1.20579171 | Current Learning Rate: 0.0001\n",
      "Epoch 853/1000 | Train Loss=1718.09359538 | Val Loss=1.45403532 | Data=17.15845947 | Physics=2.28515363 | Val RMSE: 1.22258449 | ‚àö(Val Loss) = 1.20583379 | Current Learning Rate: 0.0001\n",
      "Epoch 854/1000 | Train Loss=1716.55057780 | Val Loss=1.45438051 | Data=17.14302597 | Physics=2.30858829 | Val RMSE: 1.22261834 | ‚àö(Val Loss) = 1.20597696 | Current Learning Rate: 0.0001\n",
      "Epoch 855/1000 | Train Loss=1716.36516927 | Val Loss=1.45363390 | Data=17.14120235 | Physics=2.15152945 | Val RMSE: 1.22269857 | ‚àö(Val Loss) = 1.20566738 | Current Learning Rate: 0.0001\n",
      "Epoch 856/1000 | Train Loss=1716.76739909 | Val Loss=1.45389497 | Data=17.14524689 | Physics=2.17268718 | Val RMSE: 1.22250819 | ‚àö(Val Loss) = 1.20577562 | Current Learning Rate: 0.0001\n",
      "Epoch 857/1000 | Train Loss=1715.08900553 | Val Loss=1.45261498 | Data=17.12841021 | Physics=2.25764218 | Val RMSE: 1.22247291 | ‚àö(Val Loss) = 1.20524478 | Current Learning Rate: 0.0001\n",
      "Epoch 858/1000 | Train Loss=1717.40442708 | Val Loss=1.45254358 | Data=17.15156377 | Physics=2.23143422 | Val RMSE: 1.22259593 | ‚àö(Val Loss) = 1.20521522 | Current Learning Rate: 0.0001\n",
      "Epoch 859/1000 | Train Loss=1715.96534831 | Val Loss=1.45328832 | Data=17.13718948 | Physics=2.20561151 | Val RMSE: 1.22267675 | ‚àö(Val Loss) = 1.20552409 | Current Learning Rate: 0.0001\n",
      "Epoch 860/1000 | Train Loss=1718.51901855 | Val Loss=1.45335444 | Data=17.16271515 | Physics=2.24531958 | Val RMSE: 1.22262430 | ‚àö(Val Loss) = 1.20555151 | Current Learning Rate: 0.0001\n",
      "Epoch 861/1000 | Train Loss=1716.88488770 | Val Loss=1.45352443 | Data=17.14638856 | Physics=2.21103859 | Val RMSE: 1.22265041 | ‚àö(Val Loss) = 1.20562208 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 862/1000 | Train Loss=1715.25179850 | Val Loss=1.45453922 | Data=17.13006770 | Physics=2.22054794 | Val RMSE: 1.22268915 | ‚àö(Val Loss) = 1.20604277 | Current Learning Rate: 0.0001\n",
      "Epoch 863/1000 | Train Loss=1713.03570150 | Val Loss=1.45240303 | Data=17.10787188 | Physics=2.27472615 | Val RMSE: 1.22261620 | ‚àö(Val Loss) = 1.20515692 | Current Learning Rate: 0.0001\n",
      "Epoch 864/1000 | Train Loss=1712.90668132 | Val Loss=1.45364090 | Data=17.10660362 | Physics=2.21163156 | Val RMSE: 1.22268605 | ‚àö(Val Loss) = 1.20567036 | Current Learning Rate: 0.0001\n",
      "Epoch 865/1000 | Train Loss=1721.32444661 | Val Loss=1.45373754 | Data=17.19078554 | Physics=2.24962390 | Val RMSE: 1.22262812 | ‚àö(Val Loss) = 1.20571041 | Current Learning Rate: 0.0001\n",
      "Epoch 866/1000 | Train Loss=1718.29402669 | Val Loss=1.45306762 | Data=17.16048673 | Physics=2.18246463 | Val RMSE: 1.22246122 | ‚àö(Val Loss) = 1.20543253 | Current Learning Rate: 0.0001\n",
      "Epoch 867/1000 | Train Loss=1716.08542480 | Val Loss=1.45322009 | Data=17.13838298 | Physics=2.25451397 | Val RMSE: 1.22264242 | ‚àö(Val Loss) = 1.20549583 | Current Learning Rate: 0.0001\n",
      "Epoch 868/1000 | Train Loss=1715.20896810 | Val Loss=1.45362973 | Data=17.12964338 | Physics=2.14832231 | Val RMSE: 1.22267699 | ‚àö(Val Loss) = 1.20566571 | Current Learning Rate: 0.0001\n",
      "Epoch 869/1000 | Train Loss=1717.32355143 | Val Loss=1.45331601 | Data=17.15078265 | Physics=2.20167606 | Val RMSE: 1.22257805 | ‚àö(Val Loss) = 1.20553553 | Current Learning Rate: 0.0001\n",
      "Epoch 870/1000 | Train Loss=1711.65438639 | Val Loss=1.45316752 | Data=17.09405753 | Physics=2.25526295 | Val RMSE: 1.22252071 | ‚àö(Val Loss) = 1.20547402 | Current Learning Rate: 0.0001\n",
      "Epoch 871/1000 | Train Loss=1714.25663249 | Val Loss=1.45385269 | Data=17.12008985 | Physics=2.33660060 | Val RMSE: 1.22246969 | ‚àö(Val Loss) = 1.20575809 | Current Learning Rate: 0.0001\n",
      "Epoch 872/1000 | Train Loss=1716.41946615 | Val Loss=1.45304318 | Data=17.14170659 | Physics=2.25350317 | Val RMSE: 1.22255838 | ‚àö(Val Loss) = 1.20542240 | Current Learning Rate: 0.0001\n",
      "Epoch 873/1000 | Train Loss=1718.80280762 | Val Loss=1.45491445 | Data=17.16558590 | Physics=2.23014353 | Val RMSE: 1.22265565 | ‚àö(Val Loss) = 1.20619833 | Current Learning Rate: 0.0001\n",
      "Epoch 874/1000 | Train Loss=1714.95839844 | Val Loss=1.45365930 | Data=17.12713127 | Physics=2.16348970 | Val RMSE: 1.22267151 | ‚àö(Val Loss) = 1.20567799 | Current Learning Rate: 0.0001\n",
      "Epoch 875/1000 | Train Loss=1719.22867025 | Val Loss=1.45401311 | Data=17.16979103 | Physics=2.33638517 | Val RMSE: 1.22261226 | ‚àö(Val Loss) = 1.20582461 | Current Learning Rate: 0.0001\n",
      "Epoch 876/1000 | Train Loss=1715.91749674 | Val Loss=1.45387232 | Data=17.13670502 | Physics=2.24209336 | Val RMSE: 1.22264946 | ‚àö(Val Loss) = 1.20576632 | Current Learning Rate: 0.0001\n",
      "Epoch 877/1000 | Train Loss=1715.04152832 | Val Loss=1.45355638 | Data=17.12794851 | Physics=2.22397678 | Val RMSE: 1.22262716 | ‚àö(Val Loss) = 1.20563531 | Current Learning Rate: 0.0001\n",
      "Epoch 878/1000 | Train Loss=1715.95666504 | Val Loss=1.45421859 | Data=17.13712629 | Physics=2.12415827 | Val RMSE: 1.22272205 | ‚àö(Val Loss) = 1.20590985 | Current Learning Rate: 0.0001\n",
      "Epoch 879/1000 | Train Loss=1720.67852376 | Val Loss=1.45364034 | Data=17.18434868 | Physics=2.18088700 | Val RMSE: 1.22264397 | ‚àö(Val Loss) = 1.20567012 | Current Learning Rate: 0.0001\n",
      "Epoch 880/1000 | Train Loss=1720.44158529 | Val Loss=1.45262126 | Data=17.18192565 | Physics=2.23425995 | Val RMSE: 1.22258174 | ‚àö(Val Loss) = 1.20524740 | Current Learning Rate: 0.0001\n",
      "Epoch 881/1000 | Train Loss=1712.52175293 | Val Loss=1.45414432 | Data=17.10277583 | Physics=2.18185368 | Val RMSE: 1.22275090 | ‚àö(Val Loss) = 1.20587909 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 882/1000 | Train Loss=1719.73455404 | Val Loss=1.45382675 | Data=17.17488276 | Physics=2.20877452 | Val RMSE: 1.22257924 | ‚àö(Val Loss) = 1.20574737 | Current Learning Rate: 0.0001\n",
      "Epoch 883/1000 | Train Loss=1716.97460124 | Val Loss=1.45376921 | Data=17.14729792 | Physics=2.23918268 | Val RMSE: 1.22266948 | ‚àö(Val Loss) = 1.20572352 | Current Learning Rate: 0.0001\n",
      "Epoch 884/1000 | Train Loss=1715.64532878 | Val Loss=1.45309492 | Data=17.13399150 | Physics=2.17053899 | Val RMSE: 1.22264290 | ‚àö(Val Loss) = 1.20544386 | Current Learning Rate: 0.0001\n",
      "Epoch 885/1000 | Train Loss=1713.93791504 | Val Loss=1.45394599 | Data=17.11690699 | Physics=2.27879806 | Val RMSE: 1.22266555 | ‚àö(Val Loss) = 1.20579684 | Current Learning Rate: 0.0001\n",
      "Epoch 886/1000 | Train Loss=1713.99523926 | Val Loss=1.45311097 | Data=17.11747843 | Physics=2.21019812 | Val RMSE: 1.22265232 | ‚àö(Val Loss) = 1.20545053 | Current Learning Rate: 0.0001\n",
      "Epoch 887/1000 | Train Loss=1721.51341960 | Val Loss=1.45384296 | Data=17.19269149 | Physics=2.19711021 | Val RMSE: 1.22256434 | ‚àö(Val Loss) = 1.20575416 | Current Learning Rate: 0.0001\n",
      "Epoch 888/1000 | Train Loss=1718.69152018 | Val Loss=1.45286985 | Data=17.16443888 | Physics=2.24661118 | Val RMSE: 1.22258246 | ‚àö(Val Loss) = 1.20535052 | Current Learning Rate: 0.0001\n",
      "Epoch 889/1000 | Train Loss=1715.87555339 | Val Loss=1.45388762 | Data=17.13631274 | Physics=2.19888247 | Val RMSE: 1.22247863 | ‚àö(Val Loss) = 1.20577264 | Current Learning Rate: 0.0001\n",
      "Epoch 890/1000 | Train Loss=1713.34216309 | Val Loss=1.45331673 | Data=17.11094240 | Physics=2.24354162 | Val RMSE: 1.22247744 | ‚àö(Val Loss) = 1.20553589 | Current Learning Rate: 0.0001\n",
      "Epoch 891/1000 | Train Loss=1713.13073730 | Val Loss=1.45362866 | Data=17.10886078 | Physics=2.21384798 | Val RMSE: 1.22249866 | ‚àö(Val Loss) = 1.20566523 | Current Learning Rate: 0.0001\n",
      "Epoch 892/1000 | Train Loss=1715.50704753 | Val Loss=1.45313768 | Data=17.13259494 | Physics=2.19896914 | Val RMSE: 1.22262335 | ‚àö(Val Loss) = 1.20546162 | Current Learning Rate: 0.0001\n",
      "Epoch 893/1000 | Train Loss=1717.66842448 | Val Loss=1.45418878 | Data=17.15425701 | Physics=2.20533269 | Val RMSE: 1.22258973 | ‚àö(Val Loss) = 1.20589757 | Current Learning Rate: 0.0001\n",
      "Epoch 894/1000 | Train Loss=1717.03502604 | Val Loss=1.45330954 | Data=17.14786059 | Physics=2.25057151 | Val RMSE: 1.22268987 | ‚àö(Val Loss) = 1.20553291 | Current Learning Rate: 0.0001\n",
      "Epoch 895/1000 | Train Loss=1717.45235189 | Val Loss=1.45332718 | Data=17.15208302 | Physics=2.18001133 | Val RMSE: 1.22257876 | ‚àö(Val Loss) = 1.20554018 | Current Learning Rate: 0.0001\n",
      "Epoch 896/1000 | Train Loss=1716.70148112 | Val Loss=1.45319712 | Data=17.14452858 | Physics=2.26573022 | Val RMSE: 1.22265029 | ‚àö(Val Loss) = 1.20548630 | Current Learning Rate: 0.0001\n",
      "Epoch 897/1000 | Train Loss=1718.68888346 | Val Loss=1.45427454 | Data=17.16441847 | Physics=2.27400444 | Val RMSE: 1.22256422 | ‚àö(Val Loss) = 1.20593309 | Current Learning Rate: 0.0001\n",
      "Epoch 898/1000 | Train Loss=1716.75903320 | Val Loss=1.45312190 | Data=17.14512736 | Physics=2.23250817 | Val RMSE: 1.22247982 | ‚àö(Val Loss) = 1.20545506 | Current Learning Rate: 0.0001\n",
      "Epoch 899/1000 | Train Loss=1716.35693359 | Val Loss=1.45274595 | Data=17.14109643 | Physics=2.19557575 | Val RMSE: 1.22267663 | ‚àö(Val Loss) = 1.20529914 | Current Learning Rate: 0.0001\n",
      "Epoch 900/1000 | Train Loss=1717.80802409 | Val Loss=1.45285257 | Data=17.15561574 | Physics=2.20978289 | Val RMSE: 1.22266555 | ‚àö(Val Loss) = 1.20534337 | Current Learning Rate: 0.0001\n",
      "Epoch 901/1000 | Train Loss=1713.98133138 | Val Loss=1.45292107 | Data=17.11732165 | Physics=2.23729037 | Val RMSE: 1.22272635 | ‚àö(Val Loss) = 1.20537174 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 902/1000 | Train Loss=1720.58639323 | Val Loss=1.45246613 | Data=17.18339933 | Physics=2.20362761 | Val RMSE: 1.22273207 | ‚àö(Val Loss) = 1.20518303 | Current Learning Rate: 0.0001\n",
      "Epoch 903/1000 | Train Loss=1712.54000651 | Val Loss=1.45269251 | Data=17.10292193 | Physics=2.19591535 | Val RMSE: 1.22270477 | ‚àö(Val Loss) = 1.20527697 | Current Learning Rate: 0.0001\n",
      "Epoch 904/1000 | Train Loss=1717.31437988 | Val Loss=1.45382424 | Data=17.15069466 | Physics=2.23290201 | Val RMSE: 1.22247303 | ‚àö(Val Loss) = 1.20574641 | Current Learning Rate: 0.0001\n",
      "Epoch 905/1000 | Train Loss=1721.79265137 | Val Loss=1.45325096 | Data=17.19544563 | Physics=2.21752590 | Val RMSE: 1.22264338 | ‚àö(Val Loss) = 1.20550859 | Current Learning Rate: 0.0001\n",
      "Epoch 906/1000 | Train Loss=1714.14593099 | Val Loss=1.45515315 | Data=17.11900469 | Physics=2.24510816 | Val RMSE: 1.22276568 | ‚àö(Val Loss) = 1.20629728 | Current Learning Rate: 0.0001\n",
      "Epoch 907/1000 | Train Loss=1719.62912598 | Val Loss=1.45348322 | Data=17.17379545 | Physics=2.34112215 | Val RMSE: 1.22254467 | ‚àö(Val Loss) = 1.20560491 | Current Learning Rate: 0.0001\n",
      "Epoch 908/1000 | Train Loss=1711.49667155 | Val Loss=1.45360005 | Data=17.09249897 | Physics=2.23646936 | Val RMSE: 1.22245681 | ‚àö(Val Loss) = 1.20565331 | Current Learning Rate: 0.0001\n",
      "Epoch 909/1000 | Train Loss=1720.79211426 | Val Loss=1.45441381 | Data=17.18548826 | Physics=2.21397062 | Val RMSE: 1.22252727 | ‚àö(Val Loss) = 1.20599079 | Current Learning Rate: 0.0001\n",
      "Epoch 910/1000 | Train Loss=1717.81347656 | Val Loss=1.45294853 | Data=17.15563755 | Physics=2.28610015 | Val RMSE: 1.22261894 | ‚àö(Val Loss) = 1.20538318 | Current Learning Rate: 0.0001\n",
      "Epoch 911/1000 | Train Loss=1712.71257324 | Val Loss=1.45410987 | Data=17.10466537 | Physics=2.23887435 | Val RMSE: 1.22256947 | ‚àö(Val Loss) = 1.20586479 | Current Learning Rate: 0.0001\n",
      "Epoch 912/1000 | Train Loss=1716.75337728 | Val Loss=1.45330497 | Data=17.14505781 | Physics=2.24922964 | Val RMSE: 1.22267079 | ‚àö(Val Loss) = 1.20553100 | Current Learning Rate: 0.0001\n",
      "Epoch 913/1000 | Train Loss=1719.40997721 | Val Loss=1.45327103 | Data=17.17163588 | Physics=2.19659880 | Val RMSE: 1.22261715 | ‚àö(Val Loss) = 1.20551693 | Current Learning Rate: 0.0001\n",
      "Epoch 914/1000 | Train Loss=1714.83680827 | Val Loss=1.45354672 | Data=17.12587694 | Physics=2.30975503 | Val RMSE: 1.22268999 | ‚àö(Val Loss) = 1.20563126 | Current Learning Rate: 0.0001\n",
      "Epoch 915/1000 | Train Loss=1713.12335612 | Val Loss=1.45307247 | Data=17.10873375 | Physics=2.33979648 | Val RMSE: 1.22254920 | ‚àö(Val Loss) = 1.20543456 | Current Learning Rate: 0.0001\n",
      "Epoch 916/1000 | Train Loss=1715.28736165 | Val Loss=1.45361102 | Data=17.13043633 | Physics=2.15257024 | Val RMSE: 1.22252071 | ‚àö(Val Loss) = 1.20565796 | Current Learning Rate: 0.0001\n",
      "Epoch 917/1000 | Train Loss=1717.57790527 | Val Loss=1.45314062 | Data=17.15331179 | Physics=2.20818269 | Val RMSE: 1.22260320 | ‚àö(Val Loss) = 1.20546281 | Current Learning Rate: 0.0001\n",
      "Epoch 918/1000 | Train Loss=1714.80809733 | Val Loss=1.45432548 | Data=17.12564379 | Physics=2.23499874 | Val RMSE: 1.22246671 | ‚àö(Val Loss) = 1.20595419 | Current Learning Rate: 0.0001\n",
      "Epoch 919/1000 | Train Loss=1716.57165527 | Val Loss=1.45282181 | Data=17.14321238 | Physics=2.29381067 | Val RMSE: 1.22256839 | ‚àö(Val Loss) = 1.20533061 | Current Learning Rate: 0.0001\n",
      "Epoch 920/1000 | Train Loss=1718.83378092 | Val Loss=1.45361253 | Data=17.16588567 | Physics=2.24276225 | Val RMSE: 1.22272372 | ‚àö(Val Loss) = 1.20565856 | Current Learning Rate: 0.0001\n",
      "Epoch 921/1000 | Train Loss=1718.17009277 | Val Loss=1.45239417 | Data=17.15923163 | Physics=2.17898070 | Val RMSE: 1.22264600 | ‚àö(Val Loss) = 1.20515311 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 922/1000 | Train Loss=1714.77613932 | Val Loss=1.45340931 | Data=17.12527008 | Physics=2.26337981 | Val RMSE: 1.22277761 | ‚àö(Val Loss) = 1.20557427 | Current Learning Rate: 0.0001\n",
      "Epoch 923/1000 | Train Loss=1718.57333171 | Val Loss=1.45345632 | Data=17.16325912 | Physics=2.26000695 | Val RMSE: 1.22274804 | ‚àö(Val Loss) = 1.20559371 | Current Learning Rate: 0.0001\n",
      "Epoch 924/1000 | Train Loss=1718.66765137 | Val Loss=1.45416713 | Data=17.16422564 | Physics=2.18525513 | Val RMSE: 1.22266364 | ‚àö(Val Loss) = 1.20588851 | Current Learning Rate: 0.0001\n",
      "Epoch 925/1000 | Train Loss=1722.56796875 | Val Loss=1.45417078 | Data=17.20325343 | Physics=2.16924117 | Val RMSE: 1.22255421 | ‚àö(Val Loss) = 1.20589006 | Current Learning Rate: 0.0001\n",
      "Epoch 926/1000 | Train Loss=1709.91127930 | Val Loss=1.45303190 | Data=17.07662245 | Physics=2.23705330 | Val RMSE: 1.22257304 | ‚àö(Val Loss) = 1.20541775 | Current Learning Rate: 0.0001\n",
      "Epoch 927/1000 | Train Loss=1712.97722982 | Val Loss=1.45468056 | Data=17.10735213 | Physics=2.19647061 | Val RMSE: 1.22260106 | ‚àö(Val Loss) = 1.20610142 | Current Learning Rate: 0.0001\n",
      "Epoch 928/1000 | Train Loss=1721.96870117 | Val Loss=1.45246681 | Data=17.19719798 | Physics=2.23440037 | Val RMSE: 1.22264087 | ‚àö(Val Loss) = 1.20518327 | Current Learning Rate: 0.0001\n",
      "Epoch 929/1000 | Train Loss=1714.20323079 | Val Loss=1.45407975 | Data=17.11957156 | Physics=2.23892052 | Val RMSE: 1.22267509 | ‚àö(Val Loss) = 1.20585227 | Current Learning Rate: 0.0001\n",
      "Epoch 930/1000 | Train Loss=1718.55202637 | Val Loss=1.45368195 | Data=17.16304143 | Physics=2.24991834 | Val RMSE: 1.22261953 | ‚àö(Val Loss) = 1.20568728 | Current Learning Rate: 0.0001\n",
      "Epoch 931/1000 | Train Loss=1716.82581380 | Val Loss=1.45351068 | Data=17.14577179 | Physics=2.27414542 | Val RMSE: 1.22272372 | ‚àö(Val Loss) = 1.20561624 | Current Learning Rate: 0.0001\n",
      "Epoch 932/1000 | Train Loss=1719.87430827 | Val Loss=1.45401454 | Data=17.17627284 | Physics=2.22939410 | Val RMSE: 1.22279620 | ‚àö(Val Loss) = 1.20582521 | Current Learning Rate: 0.0001\n",
      "Epoch 933/1000 | Train Loss=1717.99584961 | Val Loss=1.45383346 | Data=17.15751355 | Physics=2.17940313 | Val RMSE: 1.22271633 | ‚àö(Val Loss) = 1.20575011 | Current Learning Rate: 0.0001\n",
      "Epoch 934/1000 | Train Loss=1715.44652507 | Val Loss=1.45333064 | Data=17.13198973 | Physics=2.22921167 | Val RMSE: 1.22265995 | ‚àö(Val Loss) = 1.20554161 | Current Learning Rate: 0.0001\n",
      "Epoch 935/1000 | Train Loss=1713.23632812 | Val Loss=1.45421696 | Data=17.10992877 | Physics=2.15717307 | Val RMSE: 1.22253633 | ‚àö(Val Loss) = 1.20590913 | Current Learning Rate: 0.0001\n",
      "Epoch 936/1000 | Train Loss=1714.64343262 | Val Loss=1.45248926 | Data=17.12396596 | Physics=2.20987700 | Val RMSE: 1.22250199 | ‚àö(Val Loss) = 1.20519257 | Current Learning Rate: 0.0001\n",
      "Epoch 937/1000 | Train Loss=1713.31818034 | Val Loss=1.45456413 | Data=17.11071911 | Physics=2.19255544 | Val RMSE: 1.22277606 | ‚àö(Val Loss) = 1.20605314 | Current Learning Rate: 0.0001\n",
      "Epoch 938/1000 | Train Loss=1716.05983887 | Val Loss=1.45505540 | Data=17.13815676 | Physics=2.26551965 | Val RMSE: 1.22272813 | ‚àö(Val Loss) = 1.20625675 | Current Learning Rate: 0.0001\n",
      "Epoch 939/1000 | Train Loss=1717.40290527 | Val Loss=1.45318921 | Data=17.15154368 | Physics=2.22873702 | Val RMSE: 1.22265959 | ‚àö(Val Loss) = 1.20548296 | Current Learning Rate: 0.0001\n",
      "Epoch 940/1000 | Train Loss=1718.82337240 | Val Loss=1.45352507 | Data=17.16576983 | Physics=2.27149314 | Val RMSE: 1.22269094 | ‚àö(Val Loss) = 1.20562232 | Current Learning Rate: 0.0001\n",
      "Epoch 941/1000 | Train Loss=1721.49923503 | Val Loss=1.45312194 | Data=17.19250399 | Physics=2.26335368 | Val RMSE: 1.22260845 | ‚àö(Val Loss) = 1.20545506 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 942/1000 | Train Loss=1714.66974284 | Val Loss=1.45283854 | Data=17.12423693 | Physics=2.22912369 | Val RMSE: 1.22260976 | ‚àö(Val Loss) = 1.20533752 | Current Learning Rate: 0.0001\n",
      "Epoch 943/1000 | Train Loss=1720.70564779 | Val Loss=1.45302558 | Data=17.18454933 | Physics=2.26755402 | Val RMSE: 1.22267747 | ‚àö(Val Loss) = 1.20541513 | Current Learning Rate: 0.0001\n",
      "Epoch 944/1000 | Train Loss=1717.85177409 | Val Loss=1.45345581 | Data=17.15605812 | Physics=2.27926451 | Val RMSE: 1.22270322 | ‚àö(Val Loss) = 1.20559359 | Current Learning Rate: 0.0001\n",
      "Epoch 945/1000 | Train Loss=1714.78072917 | Val Loss=1.45317320 | Data=17.12531815 | Physics=2.23480757 | Val RMSE: 1.22262704 | ‚àö(Val Loss) = 1.20547628 | Current Learning Rate: 0.0001\n",
      "Epoch 946/1000 | Train Loss=1716.87687174 | Val Loss=1.45298040 | Data=17.14633198 | Physics=2.24195311 | Val RMSE: 1.22230339 | ‚àö(Val Loss) = 1.20539641 | Current Learning Rate: 0.0001\n",
      "Epoch 947/1000 | Train Loss=1721.04135742 | Val Loss=1.45158295 | Data=17.18789686 | Physics=2.24664243 | Val RMSE: 1.22255754 | ‚àö(Val Loss) = 1.20481658 | Current Learning Rate: 0.0001\n",
      "Epoch 948/1000 | Train Loss=1714.77622070 | Val Loss=1.45364328 | Data=17.12531249 | Physics=2.19597963 | Val RMSE: 1.22265482 | ‚àö(Val Loss) = 1.20567131 | Current Learning Rate: 0.0001\n",
      "Epoch 949/1000 | Train Loss=1716.51342773 | Val Loss=1.45337200 | Data=17.14264806 | Physics=2.29237464 | Val RMSE: 1.22252655 | ‚àö(Val Loss) = 1.20555878 | Current Learning Rate: 0.0001\n",
      "Epoch 950/1000 | Train Loss=1718.09427083 | Val Loss=1.45342680 | Data=17.15849101 | Physics=2.16407071 | Val RMSE: 1.22260451 | ‚àö(Val Loss) = 1.20558155 | Current Learning Rate: 0.0001\n",
      "Epoch 951/1000 | Train Loss=1722.54030762 | Val Loss=1.45364618 | Data=17.20293566 | Physics=2.27714909 | Val RMSE: 1.22263229 | ‚àö(Val Loss) = 1.20567250 | Current Learning Rate: 0.0001\n",
      "Epoch 952/1000 | Train Loss=1720.94363607 | Val Loss=1.45267824 | Data=17.18694763 | Physics=2.23526421 | Val RMSE: 1.22263193 | ‚àö(Val Loss) = 1.20527101 | Current Learning Rate: 0.0001\n",
      "Epoch 953/1000 | Train Loss=1715.78417155 | Val Loss=1.45434455 | Data=17.13537738 | Physics=2.25401130 | Val RMSE: 1.22266006 | ‚àö(Val Loss) = 1.20596206 | Current Learning Rate: 0.0001\n",
      "Epoch 954/1000 | Train Loss=1716.87766927 | Val Loss=1.45339835 | Data=17.14629313 | Physics=2.32444130 | Val RMSE: 1.22250354 | ‚àö(Val Loss) = 1.20556974 | Current Learning Rate: 0.0001\n",
      "Epoch 955/1000 | Train Loss=1717.69941406 | Val Loss=1.45311741 | Data=17.15451158 | Physics=2.21236211 | Val RMSE: 1.22263753 | ‚àö(Val Loss) = 1.20545316 | Current Learning Rate: 0.0001\n",
      "Epoch 956/1000 | Train Loss=1717.72360840 | Val Loss=1.45358650 | Data=17.15476341 | Physics=2.27443006 | Val RMSE: 1.22258544 | ‚àö(Val Loss) = 1.20564771 | Current Learning Rate: 0.0001\n",
      "Epoch 957/1000 | Train Loss=1717.65852051 | Val Loss=1.45387932 | Data=17.15410779 | Physics=2.30112571 | Val RMSE: 1.22248578 | ‚àö(Val Loss) = 1.20576918 | Current Learning Rate: 0.0001\n",
      "Epoch 958/1000 | Train Loss=1714.07569987 | Val Loss=1.45376503 | Data=17.11827838 | Physics=2.26770712 | Val RMSE: 1.22261333 | ‚àö(Val Loss) = 1.20572174 | Current Learning Rate: 0.0001\n",
      "Epoch 959/1000 | Train Loss=1718.44014486 | Val Loss=1.45304434 | Data=17.16194077 | Physics=2.25684294 | Val RMSE: 1.22249544 | ‚àö(Val Loss) = 1.20542288 | Current Learning Rate: 0.0001\n",
      "Epoch 960/1000 | Train Loss=1720.64003092 | Val Loss=1.45384391 | Data=17.18390948 | Physics=2.25654819 | Val RMSE: 1.22266936 | ‚àö(Val Loss) = 1.20575452 | Current Learning Rate: 0.0001\n",
      "Epoch 961/1000 | Train Loss=1716.97334798 | Val Loss=1.45408769 | Data=17.14725978 | Physics=2.34431380 | Val RMSE: 1.22252917 | ‚àö(Val Loss) = 1.20585561 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 962/1000 | Train Loss=1718.40293783 | Val Loss=1.45297388 | Data=17.16153081 | Physics=2.26267809 | Val RMSE: 1.22255921 | ‚àö(Val Loss) = 1.20539367 | Current Learning Rate: 0.0001\n",
      "Epoch 963/1000 | Train Loss=1716.57915039 | Val Loss=1.45376217 | Data=17.14333738 | Physics=2.25014450 | Val RMSE: 1.22260797 | ‚àö(Val Loss) = 1.20572054 | Current Learning Rate: 0.0001\n",
      "Epoch 964/1000 | Train Loss=1714.85681152 | Val Loss=1.45224237 | Data=17.12606277 | Physics=2.30577840 | Val RMSE: 1.22258425 | ‚àö(Val Loss) = 1.20509017 | Current Learning Rate: 0.0001\n",
      "Epoch 965/1000 | Train Loss=1720.06603190 | Val Loss=1.45269732 | Data=17.17816022 | Physics=2.34313746 | Val RMSE: 1.22257864 | ‚àö(Val Loss) = 1.20527887 | Current Learning Rate: 0.0001\n",
      "Epoch 966/1000 | Train Loss=1719.41589355 | Val Loss=1.45298485 | Data=17.17166773 | Physics=2.25577480 | Val RMSE: 1.22262192 | ‚àö(Val Loss) = 1.20539820 | Current Learning Rate: 0.0001\n",
      "Epoch 967/1000 | Train Loss=1716.94347331 | Val Loss=1.45440165 | Data=17.14698397 | Physics=2.21973497 | Val RMSE: 1.22267854 | ‚àö(Val Loss) = 1.20598578 | Current Learning Rate: 0.0001\n",
      "Epoch 968/1000 | Train Loss=1713.50589193 | Val Loss=1.45521041 | Data=17.11260897 | Physics=2.22052767 | Val RMSE: 1.22253609 | ‚àö(Val Loss) = 1.20632100 | Current Learning Rate: 0.0001\n",
      "Epoch 969/1000 | Train Loss=1718.35858561 | Val Loss=1.45336397 | Data=17.16113860 | Physics=2.20649776 | Val RMSE: 1.22236514 | ‚àö(Val Loss) = 1.20555544 | Current Learning Rate: 0.0001\n",
      "Epoch 970/1000 | Train Loss=1718.60927734 | Val Loss=1.45342525 | Data=17.16363335 | Physics=2.19378857 | Val RMSE: 1.22256708 | ‚àö(Val Loss) = 1.20558095 | Current Learning Rate: 0.0001\n",
      "Epoch 971/1000 | Train Loss=1720.30756836 | Val Loss=1.45315854 | Data=17.18060226 | Physics=2.24271001 | Val RMSE: 1.22260761 | ‚àö(Val Loss) = 1.20547020 | Current Learning Rate: 0.0001\n",
      "Epoch 972/1000 | Train Loss=1717.27585449 | Val Loss=1.45313728 | Data=17.15029640 | Physics=2.19041101 | Val RMSE: 1.22263420 | ‚àö(Val Loss) = 1.20546138 | Current Learning Rate: 0.0001\n",
      "Epoch 973/1000 | Train Loss=1720.44178874 | Val Loss=1.45373515 | Data=17.18194466 | Physics=2.25661620 | Val RMSE: 1.22262788 | ‚àö(Val Loss) = 1.20570934 | Current Learning Rate: 0.0001\n",
      "Epoch 974/1000 | Train Loss=1712.93778483 | Val Loss=1.45306842 | Data=17.10690238 | Physics=2.23783400 | Val RMSE: 1.22261143 | ‚àö(Val Loss) = 1.20543289 | Current Learning Rate: 0.0001\n",
      "Epoch 975/1000 | Train Loss=1720.32863770 | Val Loss=1.45344826 | Data=17.18082574 | Physics=2.23559096 | Val RMSE: 1.22248197 | ‚àö(Val Loss) = 1.20559049 | Current Learning Rate: 0.0001\n",
      "Epoch 976/1000 | Train Loss=1722.03369954 | Val Loss=1.45388261 | Data=17.19790529 | Physics=2.15093814 | Val RMSE: 1.22252083 | ‚àö(Val Loss) = 1.20577049 | Current Learning Rate: 0.0001\n",
      "Epoch 977/1000 | Train Loss=1713.60900879 | Val Loss=1.45379603 | Data=17.11364085 | Physics=2.20223824 | Val RMSE: 1.22259820 | ‚àö(Val Loss) = 1.20573461 | Current Learning Rate: 0.0001\n",
      "Epoch 978/1000 | Train Loss=1721.20825195 | Val Loss=1.45262047 | Data=17.18959370 | Physics=2.28426047 | Val RMSE: 1.22239816 | ‚àö(Val Loss) = 1.20524704 | Current Learning Rate: 0.0001\n",
      "Epoch 979/1000 | Train Loss=1720.58755697 | Val Loss=1.45355388 | Data=17.18341268 | Physics=2.23910451 | Val RMSE: 1.22260582 | ‚àö(Val Loss) = 1.20563424 | Current Learning Rate: 0.0001\n",
      "Epoch 980/1000 | Train Loss=1713.20805664 | Val Loss=1.45238312 | Data=17.10959524 | Physics=2.27883083 | Val RMSE: 1.22256172 | ‚àö(Val Loss) = 1.20514858 | Current Learning Rate: 0.0001\n",
      "Epoch 981/1000 | Train Loss=1714.58735352 | Val Loss=1.45258792 | Data=17.12339160 | Physics=2.23258100 | Val RMSE: 1.22261977 | ‚àö(Val Loss) = 1.20523357 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        [  0.9828,  -3.8037, -13.5150],\n",
      "        ...,\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830]]) \n",
      " Prediction :  [[  0.9755078   -3.7711146  -13.823838  ]\n",
      " [  0.9755064   -3.7711096  -13.823834  ]\n",
      " [  0.97550446  -3.7711024  -13.823829  ]\n",
      " ...\n",
      " [  0.973972    -3.7657766  -13.8196535 ]\n",
      " [  0.97396904  -3.7657657  -13.819645  ]\n",
      " [  0.9739674   -3.7657602  -13.81964   ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        [  0.9680,  -3.9820, -14.9830],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9739641   -3.7657483  -13.819632  ]\n",
      " [  0.97396207  -3.7657416  -13.819626  ]\n",
      " [  0.97396076  -3.7657375  -13.819622  ]\n",
      " ...\n",
      " [  0.9745314   -3.7677221  -13.821178  ]\n",
      " [  0.97452873  -3.767712   -13.821171  ]\n",
      " [  0.97453046  -3.7677197  -13.821177  ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        ...,\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563],\n",
      "        [  0.9841,  -3.7445, -15.6563]]) \n",
      " Prediction :  [[  0.9745282   -3.7677107  -13.82117   ]\n",
      " [  0.97452676  -3.7677057  -13.821166  ]\n",
      " [  0.9745237   -3.7676945  -13.821157  ]\n",
      " ...\n",
      " [  0.9725702   -3.760895   -13.815825  ]\n",
      " [  0.9725655   -3.760878   -13.815812  ]\n",
      " [  0.97256595  -3.7608817  -13.815814  ]] \n",
      "\n",
      "Final Test RMSE:  0.4588046669960022\n",
      "Epoch 982/1000 | Train Loss=1715.18347982 | Val Loss=1.45334248 | Data=17.12937368 | Physics=2.20287289 | Val RMSE: 1.22267461 | ‚àö(Val Loss) = 1.20554650 | Current Learning Rate: 0.0001\n",
      "Epoch 983/1000 | Train Loss=1714.12042643 | Val Loss=1.45317197 | Data=17.11870505 | Physics=2.32821355 | Val RMSE: 1.22272229 | ‚àö(Val Loss) = 1.20547581 | Current Learning Rate: 0.0001\n",
      "Epoch 984/1000 | Train Loss=1717.92362467 | Val Loss=1.45344265 | Data=17.15677668 | Physics=2.21171947 | Val RMSE: 1.22255278 | ‚àö(Val Loss) = 1.20558810 | Current Learning Rate: 0.0001\n",
      "Epoch 985/1000 | Train Loss=1715.16803385 | Val Loss=1.45277345 | Data=17.12921613 | Physics=2.23379956 | Val RMSE: 1.22249115 | ‚àö(Val Loss) = 1.20531046 | Current Learning Rate: 0.0001\n",
      "Epoch 986/1000 | Train Loss=1715.99777832 | Val Loss=1.45302665 | Data=17.13747724 | Physics=2.31055380 | Val RMSE: 1.22255588 | ‚àö(Val Loss) = 1.20541561 | Current Learning Rate: 0.0001\n",
      "Epoch 987/1000 | Train Loss=1715.66102702 | Val Loss=1.45379055 | Data=17.13415184 | Physics=2.23125351 | Val RMSE: 1.22264171 | ‚àö(Val Loss) = 1.20573235 | Current Learning Rate: 0.0001\n",
      "Epoch 988/1000 | Train Loss=1716.18910319 | Val Loss=1.45284196 | Data=17.13939832 | Physics=2.27210219 | Val RMSE: 1.22258520 | ‚àö(Val Loss) = 1.20533895 | Current Learning Rate: 0.0001\n",
      "Epoch 989/1000 | Train Loss=1719.40295410 | Val Loss=1.45426913 | Data=17.17156035 | Physics=2.27943285 | Val RMSE: 1.22261178 | ‚àö(Val Loss) = 1.20593083 | Current Learning Rate: 0.0001\n",
      "Epoch 990/1000 | Train Loss=1718.85157064 | Val Loss=1.45401593 | Data=17.16602173 | Physics=2.34467429 | Val RMSE: 1.22256827 | ‚àö(Val Loss) = 1.20582581 | Current Learning Rate: 0.0001\n",
      "Epoch 991/1000 | Train Loss=1709.87261556 | Val Loss=1.45387030 | Data=17.07627398 | Physics=2.22086916 | Val RMSE: 1.22253203 | ‚àö(Val Loss) = 1.20576549 | Current Learning Rate: 0.0001\n",
      "Epoch 992/1000 | Train Loss=1717.40562337 | Val Loss=1.45348438 | Data=17.15161591 | Physics=2.18484113 | Val RMSE: 1.22263002 | ‚àö(Val Loss) = 1.20560539 | Current Learning Rate: 0.0001\n",
      "Epoch 993/1000 | Train Loss=1719.46608887 | Val Loss=1.45320479 | Data=17.17217941 | Physics=2.25654288 | Val RMSE: 1.22256875 | ‚àö(Val Loss) = 1.20548940 | Current Learning Rate: 0.0001\n",
      "Epoch 994/1000 | Train Loss=1720.60074056 | Val Loss=1.45326479 | Data=17.18353068 | Physics=2.27695130 | Val RMSE: 1.22261322 | ‚àö(Val Loss) = 1.20551431 | Current Learning Rate: 0.0001\n",
      "Epoch 995/1000 | Train Loss=1719.10924479 | Val Loss=1.45346451 | Data=17.16863302 | Physics=2.20206111 | Val RMSE: 1.22254980 | ‚àö(Val Loss) = 1.20559716 | Current Learning Rate: 0.0001\n",
      "Epoch 996/1000 | Train Loss=1720.38851725 | Val Loss=1.45351354 | Data=17.18144760 | Physics=2.15468594 | Val RMSE: 1.22247505 | ‚àö(Val Loss) = 1.20561743 | Current Learning Rate: 0.0001\n",
      "Epoch 997/1000 | Train Loss=1714.77228190 | Val Loss=1.45399451 | Data=17.12524865 | Physics=2.24148573 | Val RMSE: 1.22264445 | ‚àö(Val Loss) = 1.20581698 | Current Learning Rate: 0.0001\n",
      "Epoch 998/1000 | Train Loss=1716.66604004 | Val Loss=1.45371242 | Data=17.14420649 | Physics=2.23657492 | Val RMSE: 1.22263110 | ‚àö(Val Loss) = 1.20570004 | Current Learning Rate: 0.0001\n",
      "Epoch 999/1000 | Train Loss=1714.93228353 | Val Loss=1.45314364 | Data=17.12683131 | Physics=2.24914479 | Val RMSE: 1.22268224 | ‚àö(Val Loss) = 1.20546401 | Current Learning Rate: 0.0001\n",
      "‚úÖ Saved last model at epoch 1000 \n",
      "Epoch 1000/1000 | Train Loss=1717.01444499 | Val Loss=1.45418596 | Data=17.14767691 | Physics=2.26877063 | Val RMSE: 1.22265315 | ‚àö(Val Loss) = 1.20589638 | Current Learning Rate: 0.0001\n",
      "‚úÖ Metrics saved successfully!\n",
      "Plot losses after training 3:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE :  1.3764140009880066\n",
      "Cutoff SoH :  0.7\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n",
      " X_['train'] shape : torch.Size([34081, 100, 1]) , y_['train'] shape : torch.Size([34081, 3]) Ôºåy_2['train'] shape: torch.Size([34081, 1])\n",
      "load : \n",
      "['train']loader lengths :  11\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n",
      " X_['val'] shape : torch.Size([9889, 100, 1]) , y_['val'] shape : torch.Size([9889, 3]) Ôºåy_2['val'] shape: torch.Size([9889, 1])\n",
      "load : \n",
      "['val']loader lengths :  4\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n",
      " X_['test'] shape : torch.Size([4452, 100, 1]) , y_['test'] shape : torch.Size([4452, 3]) Ôºåy_2['test'] shape: torch.Size([4452, 1])\n",
      "load : \n",
      "['test']loader lengths :  2\n",
      "## üß† Model\n",
      "Last model window :  last_model_window_100_model_pinn_data_high.pth\n",
      "üöÄ Initializing model output to: k=0.999761700630188, a=-3.3473141193389893, b=-9.525775909423828\n",
      "‚úÖ Model Output Parameters Initialized!\n",
      "##\n",
      "        ### üìà Gompertz Function (Physics Law)\n",
      "        \n",
      "        * `x`: Time (or cycle number)\n",
      "        \n",
      "        * `k`: Max value (e.g., max capacity)\n",
      "        \n",
      "        * `a`, `b`: Shape parameters\n",
      "## üß† Loss Functions\n",
      "\n",
      "## ‚öôÔ∏è 1. Data-Informed Loss Function\n",
      "        a data loss (what the LSTM learns from data)\n",
      "        \n",
      "        * Mean Squared Error for Training\n",
      "        * RMSE for autoregressive approximation of compound error\n",
      "        \n",
      "        ## ‚öôÔ∏è 2. Physics-Informed Loss Function\n",
      "        You combine a data loss (what the LSTM learns from data) and a physics loss (how well it conforms to Gompertz).\n",
      "        \n",
      "        * `alpha`: controls how strongly physics is enforced.\n",
      "## üõ†Ô∏è Parameter Strategy\n",
      "## üîÅ Training Loop\n",
      "‚úÖ Saved best model at epoch 1 (Val Loss = 0.72613104)\n",
      "Epoch 1/1000 | Train Loss=8462.57346413 | Val Loss=0.72613104 | Data=84.60090707 | Physics=2.47765410 | Val RMSE: 0.81093800 | ‚àö(Val Loss) = 0.85213321 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9826356 -3.330212  -9.521126 ]\n",
      " [ 0.9826366 -3.330213  -9.521126 ]\n",
      " [ 0.9826367 -3.330213  -9.521126 ]\n",
      " ...\n",
      " [ 0.9827638 -3.3303406 -9.521005 ]\n",
      " [ 0.9827629 -3.3303397 -9.521005 ]\n",
      " [ 0.9827637 -3.3303404 -9.521005 ]] \n",
      "\n",
      "\n",
      " Epoch :  0 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9827647  -3.3303416  -9.521005  ]\n",
      " [ 0.98276484 -3.3303416  -9.521005  ]\n",
      " [ 0.9827648  -3.3303416  -9.521004  ]\n",
      " ...\n",
      " [ 0.98313713 -3.3307168  -9.520643  ]\n",
      " [ 0.9831375  -3.3307173  -9.520643  ]\n",
      " [ 0.9831374  -3.330717   -9.520642  ]] \n",
      "\n",
      "Final Test RMSE:  0.5824763774871826\n",
      "‚úÖ Saved best model at epoch 2 (Val Loss = 0.63479688)\n",
      "Epoch 2/1000 | Train Loss=8472.07168857 | Val Loss=0.63479688 | Data=84.69602411 | Physics=2.44167618 | Val RMSE: 0.81874216 | ‚àö(Val Loss) = 0.79674143 | Current Learning Rate: 0.002\n",
      "Epoch 3/1000 | Train Loss=8461.74387429 | Val Loss=0.67826200 | Data=84.59303492 | Physics=2.48198281 | Val RMSE: 0.81666136 | ‚àö(Val Loss) = 0.82356662 | Current Learning Rate: 0.002\n",
      "Epoch 4/1000 | Train Loss=8469.61341442 | Val Loss=0.66549339 | Data=84.67177027 | Physics=2.42094922 | Val RMSE: 0.81236410 | ‚àö(Val Loss) = 0.81577778 | Current Learning Rate: 0.002\n",
      "Epoch 5/1000 | Train Loss=8468.91304155 | Val Loss=0.72660145 | Data=84.66460280 | Physics=2.55989317 | Val RMSE: 0.81128025 | ‚àö(Val Loss) = 0.85240918 | Current Learning Rate: 0.002\n",
      "Epoch 6/1000 | Train Loss=8461.73126776 | Val Loss=0.63634666 | Data=84.59289551 | Physics=2.35354155 | Val RMSE: 0.80813837 | ‚àö(Val Loss) = 0.79771340 | Current Learning Rate: 0.002\n",
      "Epoch 7/1000 | Train Loss=8430.85231712 | Val Loss=0.68383733 | Data=84.28400491 | Physics=2.53731488 | Val RMSE: 0.81649804 | ‚àö(Val Loss) = 0.82694459 | Current Learning Rate: 0.002\n",
      "Epoch 8/1000 | Train Loss=8461.37983842 | Val Loss=0.68844582 | Data=84.58936102 | Physics=2.45848937 | Val RMSE: 0.81898016 | ‚àö(Val Loss) = 0.82972634 | Current Learning Rate: 0.002\n",
      "Epoch 9/1000 | Train Loss=8470.88911577 | Val Loss=0.72319697 | Data=84.68444894 | Physics=2.42102426 | Val RMSE: 0.80152422 | ‚àö(Val Loss) = 0.85040987 | Current Learning Rate: 0.002\n",
      "Epoch 10/1000 | Train Loss=8457.12335760 | Val Loss=0.70355097 | Data=84.54682506 | Physics=2.49744855 | Val RMSE: 0.80943203 | ‚àö(Val Loss) = 0.83877945 | Current Learning Rate: 0.002\n",
      "Epoch 11/1000 | Train Loss=8452.02769886 | Val Loss=0.67877534 | Data=84.49588082 | Physics=2.48284671 | Val RMSE: 0.81092918 | ‚àö(Val Loss) = 0.82387823 | Current Learning Rate: 0.002\n",
      "Epoch 12/1000 | Train Loss=8456.74502841 | Val Loss=0.69160552 | Data=84.54300828 | Physics=2.50772892 | Val RMSE: 0.80510938 | ‚àö(Val Loss) = 0.83162820 | Current Learning Rate: 0.002\n",
      "Epoch 13/1000 | Train Loss=8448.42422763 | Val Loss=0.68686201 | Data=84.45975356 | Physics=2.54126352 | Val RMSE: 0.81787312 | ‚àö(Val Loss) = 0.82877135 | Current Learning Rate: 0.002\n",
      "Epoch 14/1000 | Train Loss=8482.44371449 | Val Loss=0.66985099 | Data=84.80000721 | Physics=2.50148920 | Val RMSE: 0.81319344 | ‚àö(Val Loss) = 0.81844425 | Current Learning Rate: 0.002\n",
      "Epoch 15/1000 | Train Loss=8461.74218750 | Val Loss=0.68920685 | Data=84.59304185 | Physics=2.43197730 | Val RMSE: 0.81373006 | ‚àö(Val Loss) = 0.83018482 | Current Learning Rate: 0.002\n",
      "Epoch 16/1000 | Train Loss=8460.25470526 | Val Loss=0.71607377 | Data=84.57809518 | Physics=2.47904251 | Val RMSE: 0.81099993 | ‚àö(Val Loss) = 0.84621143 | Current Learning Rate: 0.002\n",
      "Epoch 17/1000 | Train Loss=8494.02645597 | Val Loss=0.64571110 | Data=84.91583737 | Physics=2.42232622 | Val RMSE: 0.82476187 | ‚àö(Val Loss) = 0.80356151 | Current Learning Rate: 0.002\n",
      "Epoch 18/1000 | Train Loss=8432.60418146 | Val Loss=0.72913387 | Data=84.30152130 | Physics=2.52561626 | Val RMSE: 0.80981183 | ‚àö(Val Loss) = 0.85389334 | Current Learning Rate: 0.002\n",
      "Epoch 19/1000 | Train Loss=8457.85933061 | Val Loss=0.64952354 | Data=84.55423598 | Physics=2.38964773 | Val RMSE: 0.81197673 | ‚àö(Val Loss) = 0.80593026 | Current Learning Rate: 0.002\n",
      "Epoch 20/1000 | Train Loss=8448.45174893 | Val Loss=0.66461955 | Data=84.46014682 | Physics=2.45523866 | Val RMSE: 0.81237352 | ‚àö(Val Loss) = 0.81524205 | Current Learning Rate: 0.002\n",
      "Epoch 21/1000 | Train Loss=8452.63001598 | Val Loss=0.65790454 | Data=84.50190249 | Physics=2.43440573 | Val RMSE: 0.81818348 | ‚àö(Val Loss) = 0.81111318 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99286956 -3.3390133  -9.536096  ]\n",
      " [ 0.9928697  -3.3390136  -9.536093  ]\n",
      " [ 0.99286896 -3.3390129  -9.536091  ]\n",
      " ...\n",
      " [ 0.99260557 -3.338783   -9.535072  ]\n",
      " [ 0.9926049  -3.3387823  -9.535075  ]\n",
      " [ 0.9926053  -3.3387828  -9.535073  ]] \n",
      "\n",
      "\n",
      " Epoch :  20 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99260503 -3.3387828  -9.5350685 ]\n",
      " [ 0.9926041  -3.3387818  -9.535066  ]\n",
      " [ 0.99260336 -3.338781   -9.535065  ]\n",
      " ...\n",
      " [ 0.9918193  -3.3380985  -9.532072  ]\n",
      " [ 0.9918182  -3.3380976  -9.532069  ]\n",
      " [ 0.9918172  -3.3380966  -9.532067  ]] \n",
      "\n",
      "Final Test RMSE:  0.5728177726268768\n",
      "Epoch 22/1000 | Train Loss=8460.36265980 | Val Loss=0.65733868 | Data=84.57921254 | Physics=2.42449083 | Val RMSE: 0.82258344 | ‚àö(Val Loss) = 0.81076425 | Current Learning Rate: 0.002\n",
      "Epoch 23/1000 | Train Loss=8461.62264737 | Val Loss=0.67661933 | Data=84.59183225 | Physics=2.39465233 | Val RMSE: 0.81172466 | ‚àö(Val Loss) = 0.82256877 | Current Learning Rate: 0.002\n",
      "Epoch 24/1000 | Train Loss=8451.23091264 | Val Loss=8.78736019 | Data=84.48776453 | Physics=2.44425189 | Val RMSE: 0.53348875 | ‚àö(Val Loss) = 2.96434808 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 25 (Val Loss = 0.63383908)\n",
      "Epoch 25/1000 | Train Loss=8458.86598899 | Val Loss=0.63383908 | Data=84.56400022 | Physics=2.55461177 | Val RMSE: 0.80357236 | ‚àö(Val Loss) = 0.79614013 | Current Learning Rate: 0.002\n",
      "Epoch 26/1000 | Train Loss=8451.97469815 | Val Loss=0.63393309 | Data=84.49534260 | Physics=2.38431291 | Val RMSE: 0.82045513 | ‚àö(Val Loss) = 0.79619914 | Current Learning Rate: 0.002\n",
      "Epoch 27/1000 | Train Loss=8441.69109553 | Val Loss=0.66324984 | Data=84.39237005 | Physics=2.43713362 | Val RMSE: 0.81084901 | ‚àö(Val Loss) = 0.81440151 | Current Learning Rate: 0.002\n",
      "Epoch 28/1000 | Train Loss=8454.23628374 | Val Loss=0.71828075 | Data=84.51754275 | Physics=2.51917029 | Val RMSE: 0.81866521 | ‚àö(Val Loss) = 0.84751445 | Current Learning Rate: 0.002\n",
      "Epoch 29/1000 | Train Loss=8437.93159624 | Val Loss=0.66902795 | Data=84.35481193 | Physics=2.48378202 | Val RMSE: 0.80659854 | ‚àö(Val Loss) = 0.81794125 | Current Learning Rate: 0.002\n",
      "Epoch 30/1000 | Train Loss=8469.07697088 | Val Loss=0.63637067 | Data=84.66636380 | Physics=2.45729856 | Val RMSE: 0.82386130 | ‚àö(Val Loss) = 0.79772842 | Current Learning Rate: 0.002\n",
      "Epoch 31/1000 | Train Loss=8460.20689808 | Val Loss=0.68335583 | Data=84.57769983 | Physics=2.41838338 | Val RMSE: 0.80879676 | ‚àö(Val Loss) = 0.82665336 | Current Learning Rate: 0.002\n",
      "Epoch 32/1000 | Train Loss=8456.45170455 | Val Loss=0.67038117 | Data=84.54008345 | Physics=2.46910005 | Val RMSE: 0.81127119 | ‚àö(Val Loss) = 0.81876808 | Current Learning Rate: 0.002\n",
      "Epoch 33/1000 | Train Loss=8470.09548118 | Val Loss=0.65282309 | Data=84.67643599 | Physics=2.42707503 | Val RMSE: 0.81986761 | ‚àö(Val Loss) = 0.80797470 | Current Learning Rate: 0.002\n",
      "‚úÖ Saved best model at epoch 34 (Val Loss = 0.63160349)\n",
      "Epoch 34/1000 | Train Loss=8457.16774680 | Val Loss=0.63160349 | Data=84.54726410 | Physics=2.46681602 | Val RMSE: 0.81970096 | ‚àö(Val Loss) = 0.79473484 | Current Learning Rate: 0.002\n",
      "Epoch 35/1000 | Train Loss=8446.71488814 | Val Loss=0.75832225 | Data=84.44211024 | Physics=2.47146570 | Val RMSE: 0.79560232 | ‚àö(Val Loss) = 0.87081701 | Current Learning Rate: 0.002\n",
      "Epoch 36/1000 | Train Loss=8437.48171165 | Val Loss=0.65784763 | Data=84.35045693 | Physics=2.43182720 | Val RMSE: 0.81824327 | ‚àö(Val Loss) = 0.81107807 | Current Learning Rate: 0.002\n",
      "Epoch 37/1000 | Train Loss=8445.21732955 | Val Loss=0.69040772 | Data=84.42776836 | Physics=2.47084788 | Val RMSE: 0.80403090 | ‚àö(Val Loss) = 0.83090776 | Current Learning Rate: 0.002\n",
      "Epoch 38/1000 | Train Loss=8449.90696023 | Val Loss=0.65202064 | Data=84.47395186 | Physics=2.44101795 | Val RMSE: 0.81689632 | ‚àö(Val Loss) = 0.80747795 | Current Learning Rate: 0.002\n",
      "Epoch 39/1000 | Train Loss=8446.91938920 | Val Loss=0.66011228 | Data=84.44480480 | Physics=2.42237162 | Val RMSE: 0.81269032 | ‚àö(Val Loss) = 0.81247294 | Current Learning Rate: 0.002\n",
      "Epoch 40/1000 | Train Loss=8453.89528587 | Val Loss=0.66840636 | Data=84.51441262 | Physics=2.58715503 | Val RMSE: 0.84287006 | ‚àö(Val Loss) = 0.81756121 | Current Learning Rate: 0.002\n",
      "Epoch 41/1000 | Train Loss=8448.09721236 | Val Loss=0.72288630 | Data=84.45633628 | Physics=2.48339061 | Val RMSE: 0.79721373 | ‚àö(Val Loss) = 0.85022724 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  40 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 42/1000 | Train Loss=8447.44753196 | Val Loss=0.66937189 | Data=84.45008850 | Physics=2.44287468 | Val RMSE: 0.82270372 | ‚àö(Val Loss) = 0.81815153 | Current Learning Rate: 0.002\n",
      "Epoch 43/1000 | Train Loss=8456.74129972 | Val Loss=0.65269674 | Data=84.54305129 | Physics=2.37106103 | Val RMSE: 0.79588795 | ‚àö(Val Loss) = 0.80789649 | Current Learning Rate: 0.002\n",
      "Epoch 44/1000 | Train Loss=8446.36261541 | Val Loss=0.64530396 | Data=84.43900368 | Physics=2.42438310 | Val RMSE: 0.85760653 | ‚àö(Val Loss) = 0.80330813 | Current Learning Rate: 0.002\n",
      "Epoch 45/1000 | Train Loss=8440.42791193 | Val Loss=0.69079774 | Data=84.37978016 | Physics=2.42406374 | Val RMSE: 0.81640035 | ‚àö(Val Loss) = 0.83114243 | Current Learning Rate: 0.002\n",
      "Epoch 46/1000 | Train Loss=8445.67351740 | Val Loss=0.66436245 | Data=84.43236750 | Physics=2.43206985 | Val RMSE: 0.80147034 | ‚àö(Val Loss) = 0.81508428 | Current Learning Rate: 0.002\n",
      "Epoch 47/1000 | Train Loss=8455.65722656 | Val Loss=0.69098519 | Data=84.53217246 | Physics=2.35752470 | Val RMSE: 0.81045890 | ‚àö(Val Loss) = 0.83125520 | Current Learning Rate: 0.002\n",
      "Epoch 48/1000 | Train Loss=8472.80863814 | Val Loss=0.65053980 | Data=84.70325470 | Physics=2.43050278 | Val RMSE: 0.82709080 | ‚àö(Val Loss) = 0.80656046 | Current Learning Rate: 0.002\n",
      "Epoch 49/1000 | Train Loss=8458.79549893 | Val Loss=0.65896223 | Data=84.56363192 | Physics=2.34704981 | Val RMSE: 0.81340706 | ‚àö(Val Loss) = 0.81176490 | Current Learning Rate: 0.002\n",
      "Epoch 50/1000 | Train Loss=8450.41273082 | Val Loss=0.65537455 | Data=84.47972315 | Physics=2.46865700 | Val RMSE: 0.81730497 | ‚àö(Val Loss) = 0.80955207 | Current Learning Rate: 0.002\n",
      "Epoch 51/1000 | Train Loss=8457.47096946 | Val Loss=1.98955479 | Data=84.55020696 | Physics=2.45891174 | Val RMSE: 0.68179888 | ‚àö(Val Loss) = 1.41051579 | Current Learning Rate: 0.002\n",
      "Epoch 52/1000 | Train Loss=8590.36430220 | Val Loss=0.66168240 | Data=85.87318628 | Physics=2.47286760 | Val RMSE: 0.80016708 | ‚àö(Val Loss) = 0.81343865 | Current Learning Rate: 0.002\n",
      "Epoch 53/1000 | Train Loss=8513.12539950 | Val Loss=0.92043647 | Data=85.10682123 | Physics=2.55765222 | Val RMSE: 0.81282449 | ‚àö(Val Loss) = 0.95939380 | Current Learning Rate: 0.002\n",
      "Epoch 54/1000 | Train Loss=8514.48566229 | Val Loss=0.83480132 | Data=85.12049935 | Physics=2.40927802 | Val RMSE: 0.80494452 | ‚àö(Val Loss) = 0.91367459 | Current Learning Rate: 0.002\n",
      "Epoch 55/1000 | Train Loss=8503.97278942 | Val Loss=0.93503241 | Data=85.01525047 | Physics=2.48322835 | Val RMSE: 0.80534852 | ‚àö(Val Loss) = 0.96697074 | Current Learning Rate: 0.002\n",
      "Epoch 56/1000 | Train Loss=8520.09201882 | Val Loss=0.90409651 | Data=85.17648177 | Physics=2.46815944 | Val RMSE: 0.80803913 | ‚àö(Val Loss) = 0.95083988 | Current Learning Rate: 0.002\n",
      "Epoch 57/1000 | Train Loss=8522.53093928 | Val Loss=0.89492304 | Data=85.20083271 | Physics=2.41888295 | Val RMSE: 0.79031187 | ‚àö(Val Loss) = 0.94600374 | Current Learning Rate: 0.002\n",
      "Epoch 58/1000 | Train Loss=8530.72216797 | Val Loss=0.80122082 | Data=85.28231742 | Physics=2.57276970 | Val RMSE: 0.73347151 | ‚àö(Val Loss) = 0.89510942 | Current Learning Rate: 0.002\n",
      "Epoch 59/1000 | Train Loss=8517.93736683 | Val Loss=0.71670888 | Data=85.15505773 | Physics=2.35921392 | Val RMSE: 0.79380363 | ‚àö(Val Loss) = 0.84658664 | Current Learning Rate: 0.002\n",
      "Epoch 60/1000 | Train Loss=8507.95543324 | Val Loss=0.87752678 | Data=85.05508076 | Physics=2.54451071 | Val RMSE: 0.79491353 | ‚àö(Val Loss) = 0.93676400 | Current Learning Rate: 0.002\n",
      "Epoch 61/1000 | Train Loss=8508.39586293 | Val Loss=0.91957834 | Data=85.05947252 | Physics=2.46937585 | Val RMSE: 0.79469383 | ‚àö(Val Loss) = 0.95894647 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  60 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 62/1000 | Train Loss=8507.44358132 | Val Loss=1.08988005 | Data=85.04986364 | Physics=2.55386766 | Val RMSE: 0.76836777 | ‚àö(Val Loss) = 1.04397321 | Current Learning Rate: 0.002\n",
      "Epoch 63/1000 | Train Loss=8535.15522905 | Val Loss=0.83718007 | Data=85.32623568 | Physics=3.00201814 | Val RMSE: 0.73615003 | ‚àö(Val Loss) = 0.91497546 | Current Learning Rate: 0.002\n",
      "Epoch 64/1000 | Train Loss=8540.07550604 | Val Loss=0.79271479 | Data=85.37642670 | Physics=2.34726591 | Val RMSE: 0.81306034 | ‚àö(Val Loss) = 0.89034534 | Current Learning Rate: 0.002\n",
      "Epoch 65/1000 | Train Loss=8498.65806996 | Val Loss=0.81960801 | Data=84.96217762 | Physics=2.48843916 | Val RMSE: 0.80481088 | ‚àö(Val Loss) = 0.90532207 | Current Learning Rate: 0.002\n",
      "Epoch 66/1000 | Train Loss=8526.82550604 | Val Loss=1.31036800 | Data=85.24373419 | Physics=2.57104567 | Val RMSE: 0.80678177 | ‚àö(Val Loss) = 1.14471304 | Current Learning Rate: 0.002\n",
      "Epoch 67/1000 | Train Loss=8533.78910689 | Val Loss=0.72596833 | Data=85.31220662 | Physics=2.45045202 | Val RMSE: 0.79200190 | ‚àö(Val Loss) = 0.85203773 | Current Learning Rate: 0.002\n",
      "Epoch 68/1000 | Train Loss=8517.95978338 | Val Loss=0.92792411 | Data=85.15508686 | Physics=2.46286757 | Val RMSE: 0.81016535 | ‚àö(Val Loss) = 0.96328819 | Current Learning Rate: 0.002\n",
      "Epoch 69/1000 | Train Loss=8508.87415661 | Val Loss=1.10829292 | Data=85.06416182 | Physics=2.52079398 | Val RMSE: 0.79905242 | ‚àö(Val Loss) = 1.05275488 | Current Learning Rate: 0.002\n",
      "Epoch 70/1000 | Train Loss=8525.93661222 | Val Loss=0.86435885 | Data=85.23489102 | Physics=2.39306645 | Val RMSE: 0.79551172 | ‚àö(Val Loss) = 0.92970902 | Current Learning Rate: 0.002\n",
      "Epoch 71/1000 | Train Loss=8520.81778232 | Val Loss=0.87237902 | Data=85.18379697 | Physics=2.41470290 | Val RMSE: 0.78920847 | ‚àö(Val Loss) = 0.93401229 | Current Learning Rate: 0.002\n",
      "Epoch 72/1000 | Train Loss=8529.11696555 | Val Loss=0.82644646 | Data=85.26479132 | Physics=2.57841291 | Val RMSE: 0.82058352 | ‚àö(Val Loss) = 0.90909100 | Current Learning Rate: 0.002\n",
      "Epoch 73/1000 | Train Loss=8509.85711115 | Val Loss=0.83329107 | Data=85.07424927 | Physics=2.34552887 | Val RMSE: 0.79919922 | ‚àö(Val Loss) = 0.91284776 | Current Learning Rate: 0.002\n",
      "Epoch 74/1000 | Train Loss=8535.46404474 | Val Loss=0.83844765 | Data=85.33019257 | Physics=2.39839132 | Val RMSE: 0.81267047 | ‚àö(Val Loss) = 0.91566783 | Current Learning Rate: 0.002\n",
      "Epoch 75/1000 | Train Loss=8491.57204368 | Val Loss=1.33969840 | Data=84.89120414 | Physics=2.61456074 | Val RMSE: 0.80633706 | ‚àö(Val Loss) = 1.15745342 | Current Learning Rate: 0.002\n",
      "Epoch 76/1000 | Train Loss=8701.16432884 | Val Loss=3.66848516 | Data=86.98235390 | Physics=3.89157880 | Val RMSE: 0.85727400 | ‚àö(Val Loss) = 1.91532898 | Current Learning Rate: 0.002\n",
      "Epoch 77/1000 | Train Loss=8557.97660689 | Val Loss=1.02472197 | Data=85.55537623 | Physics=2.43804576 | Val RMSE: 0.82232147 | ‚àö(Val Loss) = 1.01228547 | Current Learning Rate: 0.002\n",
      "Epoch 78/1000 | Train Loss=8527.86199396 | Val Loss=0.88863619 | Data=85.25413028 | Physics=2.43518844 | Val RMSE: 0.79949176 | ‚àö(Val Loss) = 0.94267499 | Current Learning Rate: 0.002\n",
      "Epoch 79/1000 | Train Loss=8546.86732067 | Val Loss=0.85711718 | Data=85.44408555 | Physics=2.36972195 | Val RMSE: 0.80790877 | ‚àö(Val Loss) = 0.92580622 | Current Learning Rate: 0.002\n",
      "Epoch 80/1000 | Train Loss=8506.79261364 | Val Loss=0.79951434 | Data=85.04327185 | Physics=2.48692658 | Val RMSE: 0.81417108 | ‚àö(Val Loss) = 0.89415568 | Current Learning Rate: 0.002\n",
      "Epoch 81/1000 | Train Loss=8526.59179688 | Val Loss=0.81251358 | Data=85.24098136 | Physics=2.50345437 | Val RMSE: 0.81356722 | ‚àö(Val Loss) = 0.90139538 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  80 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 82/1000 | Train Loss=8528.63964844 | Val Loss=0.73684820 | Data=85.26201005 | Physics=2.42164880 | Val RMSE: 0.80979294 | ‚àö(Val Loss) = 0.85839862 | Current Learning Rate: 0.002\n",
      "Epoch 83/1000 | Train Loss=8543.97851562 | Val Loss=1.02465334 | Data=85.41514518 | Physics=2.53767414 | Val RMSE: 0.81688356 | ‚àö(Val Loss) = 1.01225162 | Current Learning Rate: 0.002\n",
      "Epoch 84/1000 | Train Loss=8528.73020241 | Val Loss=0.80784910 | Data=85.26299078 | Physics=2.37453190 | Val RMSE: 0.79865938 | ‚àö(Val Loss) = 0.89880425 | Current Learning Rate: 0.002\n",
      "Epoch 85/1000 | Train Loss=8526.76296165 | Val Loss=1.02804252 | Data=85.24309679 | Physics=2.53418763 | Val RMSE: 0.80408084 | ‚àö(Val Loss) = 1.01392436 | Current Learning Rate: 0.002\n",
      "Epoch 86/1000 | Train Loss=8520.25359553 | Val Loss=0.80993856 | Data=85.17756930 | Physics=2.88473784 | Val RMSE: 0.80089116 | ‚àö(Val Loss) = 0.89996588 | Current Learning Rate: 0.002\n",
      "Epoch 87/1000 | Train Loss=8537.24462891 | Val Loss=0.84220527 | Data=85.34783866 | Physics=2.48411160 | Val RMSE: 0.82167590 | ‚àö(Val Loss) = 0.91771746 | Current Learning Rate: 0.002\n",
      "Epoch 88/1000 | Train Loss=8539.39479759 | Val Loss=0.74281825 | Data=85.36949643 | Physics=2.41500209 | Val RMSE: 0.80229193 | ‚àö(Val Loss) = 0.86186904 | Current Learning Rate: 0.002\n",
      "Epoch 89/1000 | Train Loss=8520.70330256 | Val Loss=0.83278162 | Data=85.18267684 | Physics=2.30622381 | Val RMSE: 0.80607843 | ‚àö(Val Loss) = 0.91256869 | Current Learning Rate: 0.002\n",
      "Epoch 90/1000 | Train Loss=8512.30384411 | Val Loss=0.75964544 | Data=85.09864876 | Physics=2.45518847 | Val RMSE: 0.81183034 | ‚àö(Val Loss) = 0.87157643 | Current Learning Rate: 0.002\n",
      "Epoch 91/1000 | Train Loss=8535.22310014 | Val Loss=1.11335484 | Data=85.32763603 | Physics=2.48941965 | Val RMSE: 0.81098109 | ‚àö(Val Loss) = 1.05515635 | Current Learning Rate: 0.002\n",
      "Epoch 92/1000 | Train Loss=8529.14453125 | Val Loss=0.78334589 | Data=85.26700523 | Physics=2.48024583 | Val RMSE: 0.79650515 | ‚àö(Val Loss) = 0.88506830 | Current Learning Rate: 0.002\n",
      "Epoch 93/1000 | Train Loss=8520.31005859 | Val Loss=1.08613936 | Data=85.17862216 | Physics=2.48476544 | Val RMSE: 0.81227779 | ‚àö(Val Loss) = 1.04218006 | Current Learning Rate: 0.002\n",
      "Epoch 94/1000 | Train Loss=8515.42294034 | Val Loss=0.78744285 | Data=85.12983704 | Physics=2.53767667 | Val RMSE: 0.80205035 | ‚àö(Val Loss) = 0.88737977 | Current Learning Rate: 0.002\n",
      "Epoch 95/1000 | Train Loss=8539.33868963 | Val Loss=0.99193612 | Data=85.36875014 | Physics=2.46419745 | Val RMSE: 0.80061352 | ‚àö(Val Loss) = 0.99595994 | Current Learning Rate: 0.002\n",
      "Epoch 96/1000 | Train Loss=8541.84517045 | Val Loss=0.72505156 | Data=85.39406516 | Physics=2.44898683 | Val RMSE: 0.79519308 | ‚àö(Val Loss) = 0.85149962 | Current Learning Rate: 0.002\n",
      "Epoch 97/1000 | Train Loss=8504.64874822 | Val Loss=1.06199604 | Data=85.02179024 | Physics=2.56521088 | Val RMSE: 0.81957436 | ‚àö(Val Loss) = 1.03053188 | Current Learning Rate: 0.002\n",
      "Epoch 98/1000 | Train Loss=8530.53369141 | Val Loss=0.86245338 | Data=85.28075617 | Physics=2.41934578 | Val RMSE: 0.79650736 | ‚àö(Val Loss) = 0.92868370 | Current Learning Rate: 0.002\n",
      "Epoch 99/1000 | Train Loss=8522.40349787 | Val Loss=1.04807961 | Data=85.19929088 | Physics=2.62719429 | Val RMSE: 0.81335002 | ‚àö(Val Loss) = 1.02375758 | Current Learning Rate: 0.002\n",
      "Epoch 100/1000 | Train Loss=8536.03804155 | Val Loss=0.76870186 | Data=85.33602350 | Physics=2.49225778 | Val RMSE: 0.80676442 | ‚àö(Val Loss) = 0.87675643 | Current Learning Rate: 0.002\n",
      "Epoch 101/1000 | Train Loss=8525.46590909 | Val Loss=0.97623070 | Data=85.23021629 | Physics=2.39660769 | Val RMSE: 0.81224489 | ‚àö(Val Loss) = 0.98804384 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  100 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 102/1000 | Train Loss=8519.14497514 | Val Loss=1.34018621 | Data=85.16705322 | Physics=2.47055297 | Val RMSE: 0.76405454 | ‚àö(Val Loss) = 1.15766418 | Current Learning Rate: 0.002\n",
      "Epoch 103/1000 | Train Loss=8544.65198864 | Val Loss=0.69848993 | Data=85.41990939 | Physics=2.48915954 | Val RMSE: 0.81628954 | ‚àö(Val Loss) = 0.83575708 | Current Learning Rate: 0.002\n",
      "Epoch 104/1000 | Train Loss=8514.36745384 | Val Loss=0.92141473 | Data=85.11927449 | Physics=2.49833825 | Val RMSE: 0.80625713 | ‚àö(Val Loss) = 0.95990348 | Current Learning Rate: 0.002\n",
      "Epoch 105/1000 | Train Loss=8517.42995384 | Val Loss=0.79760244 | Data=85.14993078 | Physics=2.43950746 | Val RMSE: 0.80672872 | ‚àö(Val Loss) = 0.89308590 | Current Learning Rate: 0.002\n",
      "Epoch 106/1000 | Train Loss=8533.44775391 | Val Loss=0.76357589 | Data=85.30994554 | Physics=2.47637241 | Val RMSE: 0.80871201 | ‚àö(Val Loss) = 0.87382829 | Current Learning Rate: 0.002\n",
      "Epoch 107/1000 | Train Loss=8518.81915838 | Val Loss=1.07568809 | Data=85.16356798 | Physics=2.52874720 | Val RMSE: 0.78782618 | ‚àö(Val Loss) = 1.03715384 | Current Learning Rate: 0.002\n",
      "Epoch 108/1000 | Train Loss=8520.86052912 | Val Loss=0.73135963 | Data=85.18408411 | Physics=2.52279158 | Val RMSE: 0.79713893 | ‚àö(Val Loss) = 0.85519570 | Current Learning Rate: 0.002\n",
      "Epoch 109/1000 | Train Loss=8520.43022017 | Val Loss=1.23036427 | Data=85.17944128 | Physics=2.58758518 | Val RMSE: 0.82809705 | ‚àö(Val Loss) = 1.10921788 | Current Learning Rate: 0.002\n",
      "Epoch 110/1000 | Train Loss=8509.09996449 | Val Loss=0.71360279 | Data=85.06666149 | Physics=2.39346663 | Val RMSE: 0.78745013 | ‚àö(Val Loss) = 0.84475011 | Current Learning Rate: 0.002\n",
      "Epoch 111/1000 | Train Loss=8509.97807173 | Val Loss=0.88438906 | Data=85.07507116 | Physics=2.51973808 | Val RMSE: 0.75001466 | ‚àö(Val Loss) = 0.94041961 | Current Learning Rate: 0.002\n",
      "Epoch 112/1000 | Train Loss=8533.00368430 | Val Loss=1.28157094 | Data=85.30548512 | Physics=2.58583166 | Val RMSE: 0.72131532 | ‚àö(Val Loss) = 1.13206494 | Current Learning Rate: 0.002\n",
      "Epoch 113/1000 | Train Loss=8506.51349432 | Val Loss=1.09994683 | Data=85.04060988 | Physics=2.63742641 | Val RMSE: 0.78359216 | ‚àö(Val Loss) = 1.04878354 | Current Learning Rate: 0.002\n",
      "Epoch 114/1000 | Train Loss=8523.13822798 | Val Loss=1.02351804 | Data=85.20678434 | Physics=2.50804203 | Val RMSE: 0.81699300 | ‚àö(Val Loss) = 1.01169074 | Current Learning Rate: 0.002\n",
      "Epoch 115/1000 | Train Loss=8502.62828480 | Val Loss=0.68287353 | Data=85.00185464 | Physics=2.46898841 | Val RMSE: 0.81225920 | ‚àö(Val Loss) = 0.82636166 | Current Learning Rate: 0.002\n",
      "Epoch 116/1000 | Train Loss=8518.60182884 | Val Loss=0.87601312 | Data=85.16144076 | Physics=2.53191933 | Val RMSE: 0.81135720 | ‚àö(Val Loss) = 0.93595570 | Current Learning Rate: 0.002\n",
      "Epoch 117/1000 | Train Loss=8526.40194425 | Val Loss=0.97702006 | Data=85.23962749 | Physics=2.45050714 | Val RMSE: 0.79112315 | ‚àö(Val Loss) = 0.98844326 | Current Learning Rate: 0.002\n",
      "Epoch 118/1000 | Train Loss=8536.05828303 | Val Loss=1.29689667 | Data=85.33587716 | Physics=2.58383186 | Val RMSE: 0.71678799 | ‚àö(Val Loss) = 1.13881373 | Current Learning Rate: 0.002\n",
      "Epoch 119/1000 | Train Loss=8542.08105469 | Val Loss=0.85389525 | Data=85.39464708 | Physics=2.55886177 | Val RMSE: 0.77594495 | ‚àö(Val Loss) = 0.92406452 | Current Learning Rate: 0.002\n",
      "Epoch 120/1000 | Train Loss=8515.32284268 | Val Loss=3.63892072 | Data=85.12387917 | Physics=6.14936747 | Val RMSE: 0.72705781 | ‚àö(Val Loss) = 1.90759552 | Current Learning Rate: 0.002\n",
      "Epoch 121/1000 | Train Loss=8505.00244141 | Val Loss=0.72041478 | Data=85.02561604 | Physics=2.38121512 | Val RMSE: 0.78935075 | ‚àö(Val Loss) = 0.84877253 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  120 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 122/1000 | Train Loss=8518.39577415 | Val Loss=0.96041107 | Data=85.15946336 | Physics=2.45596633 | Val RMSE: 0.79719031 | ‚àö(Val Loss) = 0.98000562 | Current Learning Rate: 0.002\n",
      "Epoch 123/1000 | Train Loss=8531.29216974 | Val Loss=0.72425971 | Data=85.28791462 | Physics=2.38463726 | Val RMSE: 0.80505234 | ‚àö(Val Loss) = 0.85103452 | Current Learning Rate: 0.002\n",
      "Epoch 124/1000 | Train Loss=8517.60418146 | Val Loss=0.87160480 | Data=85.15168207 | Physics=2.43952734 | Val RMSE: 0.80914235 | ‚àö(Val Loss) = 0.93359774 | Current Learning Rate: 0.002\n",
      "Epoch 125/1000 | Train Loss=8518.19122869 | Val Loss=0.88845581 | Data=85.15749012 | Physics=2.41991946 | Val RMSE: 0.80705363 | ‚àö(Val Loss) = 0.94257933 | Current Learning Rate: 0.002\n",
      "Epoch 126/1000 | Train Loss=8607.72580788 | Val Loss=1.99765727 | Data=86.04300065 | Physics=7.23951859 | Val RMSE: 0.82079697 | ‚àö(Val Loss) = 1.41338503 | Current Learning Rate: 0.002\n",
      "Epoch 127/1000 | Train Loss=8526.60120739 | Val Loss=0.83879627 | Data=85.24154108 | Physics=2.44284179 | Val RMSE: 0.80267179 | ‚àö(Val Loss) = 0.91585821 | Current Learning Rate: 0.002\n",
      "Epoch 128/1000 | Train Loss=8531.35866477 | Val Loss=0.80719439 | Data=85.28904516 | Physics=2.49382403 | Val RMSE: 0.76936841 | ‚àö(Val Loss) = 0.89844000 | Current Learning Rate: 0.002\n",
      "Epoch 129/1000 | Train Loss=8517.49871271 | Val Loss=0.89455941 | Data=85.15050645 | Physics=2.38943063 | Val RMSE: 0.82377005 | ‚àö(Val Loss) = 0.94581151 | Current Learning Rate: 0.002\n",
      "Epoch 130/1000 | Train Loss=8514.06489702 | Val Loss=0.78112837 | Data=85.11631220 | Physics=2.45237436 | Val RMSE: 0.81492674 | ‚àö(Val Loss) = 0.88381463 | Current Learning Rate: 0.002\n",
      "Epoch 131/1000 | Train Loss=8556.20707564 | Val Loss=0.85281106 | Data=85.53759558 | Physics=2.45057668 | Val RMSE: 0.79926997 | ‚àö(Val Loss) = 0.92347771 | Current Learning Rate: 0.002\n",
      "Epoch 132/1000 | Train Loss=8506.84636896 | Val Loss=0.76654515 | Data=85.04406530 | Physics=2.42746272 | Val RMSE: 0.79999220 | ‚àö(Val Loss) = 0.87552565 | Current Learning Rate: 0.002\n",
      "Epoch 133/1000 | Train Loss=8527.78058416 | Val Loss=0.81528122 | Data=85.25330769 | Physics=2.41468766 | Val RMSE: 0.81576371 | ‚àö(Val Loss) = 0.90292925 | Current Learning Rate: 0.002\n",
      "Epoch 134/1000 | Train Loss=8509.04594283 | Val Loss=0.92115945 | Data=85.06600605 | Physics=2.43505371 | Val RMSE: 0.79619235 | ‚àö(Val Loss) = 0.95977050 | Current Learning Rate: 0.002\n",
      "Epoch 135/1000 | Train Loss=8512.41424006 | Val Loss=0.91963615 | Data=85.09970231 | Physics=2.49410545 | Val RMSE: 0.80972660 | ‚àö(Val Loss) = 0.95897663 | Current Learning Rate: 0.002\n",
      "Epoch 136/1000 | Train Loss=8531.11328125 | Val Loss=1.20376055 | Data=85.28657185 | Physics=2.55503393 | Val RMSE: 0.76584643 | ‚àö(Val Loss) = 1.09716022 | Current Learning Rate: 0.002\n",
      "Epoch 137/1000 | Train Loss=8530.67085405 | Val Loss=0.78468836 | Data=85.28174591 | Physics=2.56623097 | Val RMSE: 0.83087230 | ‚àö(Val Loss) = 0.88582635 | Current Learning Rate: 0.002\n",
      "Epoch 138/1000 | Train Loss=8516.67808949 | Val Loss=0.84857623 | Data=85.14241721 | Physics=2.45063199 | Val RMSE: 0.81307995 | ‚àö(Val Loss) = 0.92118198 | Current Learning Rate: 0.002\n",
      "Epoch 139/1000 | Train Loss=8522.43035334 | Val Loss=0.93602876 | Data=85.19987418 | Physics=2.41269131 | Val RMSE: 0.80712742 | ‚àö(Val Loss) = 0.96748579 | Current Learning Rate: 0.002\n",
      "Epoch 140/1000 | Train Loss=8523.53639915 | Val Loss=0.83569455 | Data=85.21072943 | Physics=2.60599122 | Val RMSE: 0.84453493 | ‚àö(Val Loss) = 0.91416329 | Current Learning Rate: 0.002\n",
      "Epoch 141/1000 | Train Loss=8524.56045810 | Val Loss=0.76683991 | Data=85.22104021 | Physics=2.44170455 | Val RMSE: 0.81445104 | ‚àö(Val Loss) = 0.87569398 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  140 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 142/1000 | Train Loss=8532.05020419 | Val Loss=0.72036583 | Data=85.29603022 | Physics=2.48862010 | Val RMSE: 0.79382366 | ‚àö(Val Loss) = 0.84874368 | Current Learning Rate: 0.002\n",
      "Epoch 143/1000 | Train Loss=8503.58158736 | Val Loss=0.84581054 | Data=85.01129775 | Physics=2.40159176 | Val RMSE: 0.81748056 | ‚àö(Val Loss) = 0.91967958 | Current Learning Rate: 0.002\n",
      "Epoch 144/1000 | Train Loss=8531.43678977 | Val Loss=0.81529364 | Data=85.29002242 | Physics=2.32851098 | Val RMSE: 0.80960149 | ‚àö(Val Loss) = 0.90293616 | Current Learning Rate: 0.002\n",
      "Epoch 145/1000 | Train Loss=8510.87073864 | Val Loss=0.82981607 | Data=85.08427915 | Physics=2.48842312 | Val RMSE: 0.80162764 | ‚àö(Val Loss) = 0.91094238 | Current Learning Rate: 0.002\n",
      "Epoch 146/1000 | Train Loss=8538.62553267 | Val Loss=0.85796822 | Data=85.36166035 | Physics=2.45376532 | Val RMSE: 0.80522561 | ‚àö(Val Loss) = 0.92626572 | Current Learning Rate: 0.002\n",
      "Epoch 147/1000 | Train Loss=8521.32705966 | Val Loss=1.28433175 | Data=85.18852511 | Physics=2.64813069 | Val RMSE: 0.72007984 | ‚àö(Val Loss) = 1.13328362 | Current Learning Rate: 0.002\n",
      "Epoch 148/1000 | Train Loss=8527.50701349 | Val Loss=0.84966811 | Data=85.25034471 | Physics=2.43504609 | Val RMSE: 0.77493334 | ‚àö(Val Loss) = 0.92177445 | Current Learning Rate: 0.002\n",
      "Epoch 149/1000 | Train Loss=8519.79927202 | Val Loss=0.82712018 | Data=85.17338909 | Physics=2.45486795 | Val RMSE: 0.82772648 | ‚àö(Val Loss) = 0.90946150 | Current Learning Rate: 0.002\n",
      "Epoch 150/1000 | Train Loss=8509.09459339 | Val Loss=0.80360127 | Data=85.06650543 | Physics=2.55016582 | Val RMSE: 0.80737931 | ‚àö(Val Loss) = 0.89643812 | Current Learning Rate: 0.002\n",
      "Epoch 151/1000 | Train Loss=8522.12531072 | Val Loss=0.79206618 | Data=85.19683838 | Physics=2.44184783 | Val RMSE: 0.80781460 | ‚àö(Val Loss) = 0.88998097 | Current Learning Rate: 0.002\n",
      "Epoch 152/1000 | Train Loss=8515.10901989 | Val Loss=1.63478065 | Data=85.12644612 | Physics=2.62697063 | Val RMSE: 0.78179878 | ‚àö(Val Loss) = 1.27858543 | Current Learning Rate: 0.002\n",
      "Epoch 153/1000 | Train Loss=8503.95405717 | Val Loss=0.91180621 | Data=85.01514504 | Physics=2.44048409 | Val RMSE: 0.73723722 | ‚àö(Val Loss) = 0.95488542 | Current Learning Rate: 0.002\n",
      "Epoch 154/1000 | Train Loss=8534.18545810 | Val Loss=0.91571448 | Data=85.31669131 | Physics=2.40387040 | Val RMSE: 0.81047785 | ‚àö(Val Loss) = 0.95692974 | Current Learning Rate: 0.002\n",
      "Epoch 155/1000 | Train Loss=8537.93084162 | Val Loss=0.82635766 | Data=85.35476546 | Physics=2.53148494 | Val RMSE: 0.81370968 | ‚àö(Val Loss) = 0.90904218 | Current Learning Rate: 0.002\n",
      "Epoch 156/1000 | Train Loss=8532.51109730 | Val Loss=0.80698556 | Data=85.30058913 | Physics=2.46785114 | Val RMSE: 0.80661768 | ‚àö(Val Loss) = 0.89832377 | Current Learning Rate: 0.002\n",
      "Epoch 157/1000 | Train Loss=8513.03591087 | Val Loss=0.86575150 | Data=85.10592166 | Physics=2.39970234 | Val RMSE: 0.81849849 | ‚àö(Val Loss) = 0.93045771 | Current Learning Rate: 0.002\n",
      "Epoch 158/1000 | Train Loss=8517.56059126 | Val Loss=0.80929784 | Data=85.15098364 | Physics=2.37093852 | Val RMSE: 0.81817859 | ‚àö(Val Loss) = 0.89960986 | Current Learning Rate: 0.002\n",
      "Epoch 159/1000 | Train Loss=8507.49227628 | Val Loss=0.70956349 | Data=85.05058011 | Physics=2.32652339 | Val RMSE: 0.80238825 | ‚àö(Val Loss) = 0.84235591 | Current Learning Rate: 0.002\n",
      "Epoch 160/1000 | Train Loss=8528.32226562 | Val Loss=0.83027717 | Data=85.25877588 | Physics=2.41075977 | Val RMSE: 0.81610465 | ‚àö(Val Loss) = 0.91119546 | Current Learning Rate: 0.002\n",
      "Epoch 161/1000 | Train Loss=8509.32257635 | Val Loss=0.77969839 | Data=85.06882546 | Physics=2.45888780 | Val RMSE: 0.80300939 | ‚àö(Val Loss) = 0.88300532 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  160 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 162/1000 | Train Loss=8546.77441406 | Val Loss=0.97567277 | Data=85.44327961 | Physics=2.48377313 | Val RMSE: 0.80448776 | ‚àö(Val Loss) = 0.98776150 | Current Learning Rate: 0.002\n",
      "Epoch 163/1000 | Train Loss=8507.42893288 | Val Loss=0.70979404 | Data=85.04967360 | Physics=2.59165714 | Val RMSE: 0.82728153 | ‚àö(Val Loss) = 0.84249276 | Current Learning Rate: 0.002\n",
      "Epoch 164/1000 | Train Loss=8525.66983310 | Val Loss=1.00930568 | Data=85.23209520 | Physics=2.56255728 | Val RMSE: 0.80460137 | ‚àö(Val Loss) = 1.00464213 | Current Learning Rate: 0.002\n",
      "Epoch 165/1000 | Train Loss=8507.16752486 | Val Loss=0.97554553 | Data=85.04720792 | Physics=2.52260823 | Val RMSE: 0.79937124 | ‚àö(Val Loss) = 0.98769706 | Current Learning Rate: 0.002\n",
      "Epoch 166/1000 | Train Loss=8504.80397727 | Val Loss=1.14968452 | Data=85.02349437 | Physics=2.53330575 | Val RMSE: 0.79226363 | ‚àö(Val Loss) = 1.07223344 | Current Learning Rate: 0.002\n",
      "Epoch 167/1000 | Train Loss=8534.19917436 | Val Loss=0.85896147 | Data=85.31745286 | Physics=2.35659924 | Val RMSE: 0.80563176 | ‚àö(Val Loss) = 0.92680174 | Current Learning Rate: 0.002\n",
      "Epoch 168/1000 | Train Loss=8539.24360795 | Val Loss=0.90355432 | Data=85.36803783 | Physics=2.42136615 | Val RMSE: 0.81286907 | ‚àö(Val Loss) = 0.95055473 | Current Learning Rate: 0.002\n",
      "Epoch 169/1000 | Train Loss=8530.94477983 | Val Loss=0.87192669 | Data=85.28508689 | Physics=2.37175458 | Val RMSE: 0.78650236 | ‚àö(Val Loss) = 0.93377012 | Current Learning Rate: 0.002\n",
      "Epoch 170/1000 | Train Loss=8522.36585582 | Val Loss=0.99851752 | Data=85.19916465 | Physics=2.42818794 | Val RMSE: 0.81516892 | ‚àö(Val Loss) = 0.99925846 | Current Learning Rate: 0.002\n",
      "Epoch 171/1000 | Train Loss=8532.14710582 | Val Loss=0.67826190 | Data=85.29697418 | Physics=2.54912399 | Val RMSE: 0.78170472 | ‚àö(Val Loss) = 0.82356656 | Current Learning Rate: 0.002\n",
      "Epoch 172/1000 | Train Loss=8545.35387074 | Val Loss=0.99800685 | Data=85.42878238 | Physics=2.48954825 | Val RMSE: 0.82329404 | ‚àö(Val Loss) = 0.99900293 | Current Learning Rate: 0.002\n",
      "Epoch 173/1000 | Train Loss=8529.06107955 | Val Loss=0.88015310 | Data=85.26617224 | Physics=2.48904460 | Val RMSE: 0.77470613 | ‚àö(Val Loss) = 0.93816477 | Current Learning Rate: 0.002\n",
      "Epoch 174/1000 | Train Loss=8530.20148260 | Val Loss=0.90886451 | Data=85.27725289 | Physics=2.42589883 | Val RMSE: 0.63474095 | ‚àö(Val Loss) = 0.95334387 | Current Learning Rate: 0.002\n",
      "Epoch 175/1000 | Train Loss=8804.53852983 | Val Loss=6.11656174 | Data=88.00844921 | Physics=5.27721821 | Val RMSE: 0.91399693 | ‚àö(Val Loss) = 2.47316837 | Current Learning Rate: 0.002\n",
      "Epoch 176/1000 | Train Loss=8521.61163885 | Val Loss=0.81516077 | Data=85.19168507 | Physics=2.47416151 | Val RMSE: 0.82399428 | ‚àö(Val Loss) = 0.90286255 | Current Learning Rate: 0.002\n",
      "Epoch 177/1000 | Train Loss=8507.45041726 | Val Loss=0.77765151 | Data=85.05008420 | Physics=2.53556576 | Val RMSE: 0.80529439 | ‚àö(Val Loss) = 0.88184547 | Current Learning Rate: 0.002\n",
      "Epoch 178/1000 | Train Loss=8542.96293501 | Val Loss=0.93508293 | Data=85.40510143 | Physics=2.41498689 | Val RMSE: 0.80906790 | ‚àö(Val Loss) = 0.96699685 | Current Learning Rate: 0.002\n",
      "Epoch 179/1000 | Train Loss=8519.88378906 | Val Loss=0.68078008 | Data=85.17437328 | Physics=2.49344577 | Val RMSE: 0.79637134 | ‚àö(Val Loss) = 0.82509398 | Current Learning Rate: 0.002\n",
      "Epoch 180/1000 | Train Loss=8518.83580433 | Val Loss=0.85923876 | Data=85.16382738 | Physics=2.42159542 | Val RMSE: 0.80685526 | ‚àö(Val Loss) = 0.92695129 | Current Learning Rate: 0.002\n",
      "Epoch 181/1000 | Train Loss=8537.80894886 | Val Loss=0.88060284 | Data=85.35376115 | Physics=2.33576787 | Val RMSE: 0.81033325 | ‚àö(Val Loss) = 0.93840438 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  180 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 182/1000 | Train Loss=8513.30672940 | Val Loss=0.99989530 | Data=85.10795871 | Physics=2.78699722 | Val RMSE: 0.83794582 | ‚àö(Val Loss) = 0.99994761 | Current Learning Rate: 0.002\n",
      "Epoch 183/1000 | Train Loss=8528.16281960 | Val Loss=1.03894472 | Data=85.25710990 | Physics=2.45013367 | Val RMSE: 0.83054513 | ‚àö(Val Loss) = 1.01928639 | Current Learning Rate: 0.002\n",
      "Epoch 184/1000 | Train Loss=8518.80189098 | Val Loss=0.79303918 | Data=85.16356173 | Physics=2.43117604 | Val RMSE: 0.80219781 | ‚àö(Val Loss) = 0.89052749 | Current Learning Rate: 0.002\n",
      "Epoch 185/1000 | Train Loss=8515.52792081 | Val Loss=1.14800006 | Data=85.13047374 | Physics=2.63081165 | Val RMSE: 0.74467361 | ‚àö(Val Loss) = 1.07144761 | Current Learning Rate: 0.002\n",
      "Epoch 186/1000 | Train Loss=8527.09929865 | Val Loss=0.93798896 | Data=85.24656816 | Physics=2.49565359 | Val RMSE: 0.80963612 | ‚àö(Val Loss) = 0.96849829 | Current Learning Rate: 0.002\n",
      "Epoch 187/1000 | Train Loss=8537.61048473 | Val Loss=0.81385555 | Data=85.35147927 | Physics=2.49456331 | Val RMSE: 0.77076113 | ‚àö(Val Loss) = 0.90213943 | Current Learning Rate: 0.002\n",
      "Epoch 188/1000 | Train Loss=8519.67720170 | Val Loss=0.91827482 | Data=85.17221416 | Physics=2.48615842 | Val RMSE: 0.82298136 | ‚àö(Val Loss) = 0.95826656 | Current Learning Rate: 0.002\n",
      "Epoch 189/1000 | Train Loss=8510.54305753 | Val Loss=0.68454477 | Data=85.08103804 | Physics=2.46346628 | Val RMSE: 0.78908563 | ‚àö(Val Loss) = 0.82737219 | Current Learning Rate: 0.002\n",
      "Epoch 190/1000 | Train Loss=8540.18017578 | Val Loss=0.74075848 | Data=85.37728396 | Physics=2.47646819 | Val RMSE: 0.81296694 | ‚àö(Val Loss) = 0.86067325 | Current Learning Rate: 0.002\n",
      "Epoch 191/1000 | Train Loss=8527.22452060 | Val Loss=0.80661751 | Data=85.24788319 | Physics=2.42797649 | Val RMSE: 0.81461507 | ‚àö(Val Loss) = 0.89811885 | Current Learning Rate: 0.002\n",
      "Epoch 192/1000 | Train Loss=8527.17848899 | Val Loss=0.84769467 | Data=85.24722013 | Physics=2.47457040 | Val RMSE: 0.81569272 | ‚àö(Val Loss) = 0.92070335 | Current Learning Rate: 0.002\n",
      "Epoch 193/1000 | Train Loss=8532.99933416 | Val Loss=0.79224737 | Data=85.30563840 | Physics=2.49136949 | Val RMSE: 0.80766916 | ‚àö(Val Loss) = 0.89008278 | Current Learning Rate: 0.002\n",
      "Epoch 194/1000 | Train Loss=8509.33149858 | Val Loss=1.08512682 | Data=85.06870270 | Physics=2.52597011 | Val RMSE: 0.78840727 | ‚àö(Val Loss) = 1.04169428 | Current Learning Rate: 0.002\n",
      "Epoch 195/1000 | Train Loss=8531.66259766 | Val Loss=0.77632183 | Data=85.29208235 | Physics=2.53487480 | Val RMSE: 0.79425955 | ‚àö(Val Loss) = 0.88109130 | Current Learning Rate: 0.002\n",
      "Epoch 196/1000 | Train Loss=8507.38791726 | Val Loss=0.86221011 | Data=85.04924427 | Physics=2.43383551 | Val RMSE: 0.82501191 | ‚àö(Val Loss) = 0.92855269 | Current Learning Rate: 0.002\n",
      "Epoch 197/1000 | Train Loss=8520.39293324 | Val Loss=0.80442341 | Data=85.17951688 | Physics=2.42989524 | Val RMSE: 0.78587323 | ‚àö(Val Loss) = 0.89689654 | Current Learning Rate: 0.002\n",
      "Epoch 198/1000 | Train Loss=8501.87566584 | Val Loss=0.89167808 | Data=84.99421761 | Physics=2.49808542 | Val RMSE: 0.77922982 | ‚àö(Val Loss) = 0.94428706 | Current Learning Rate: 0.002\n",
      "Epoch 199/1000 | Train Loss=8509.58345170 | Val Loss=0.95542915 | Data=85.07145552 | Physics=2.44255724 | Val RMSE: 0.81229639 | ‚àö(Val Loss) = 0.97746056 | Current Learning Rate: 0.002\n",
      "Epoch 200/1000 | Train Loss=8518.05140270 | Val Loss=0.90228410 | Data=85.15597534 | Physics=2.56888910 | Val RMSE: 0.82036299 | ‚àö(Val Loss) = 0.94988638 | Current Learning Rate: 0.002\n",
      "Epoch 201/1000 | Train Loss=8538.04074929 | Val Loss=0.96665392 | Data=85.35595495 | Physics=2.42400748 | Val RMSE: 0.82469499 | ‚àö(Val Loss) = 0.98318559 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  200 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 202/1000 | Train Loss=8521.18377131 | Val Loss=0.83063259 | Data=85.18738972 | Physics=2.49810696 | Val RMSE: 0.79143500 | ‚àö(Val Loss) = 0.91139048 | Current Learning Rate: 0.002\n",
      "Epoch 203/1000 | Train Loss=8517.24147727 | Val Loss=0.92869499 | Data=85.14785073 | Physics=2.43991203 | Val RMSE: 0.84020782 | ‚àö(Val Loss) = 0.96368819 | Current Learning Rate: 0.002\n",
      "Epoch 204/1000 | Train Loss=8527.82634943 | Val Loss=0.88140024 | Data=85.25369055 | Physics=2.42140431 | Val RMSE: 0.82132041 | ‚àö(Val Loss) = 0.93882918 | Current Learning Rate: 0.002\n",
      "Epoch 205/1000 | Train Loss=8519.89857067 | Val Loss=0.73879098 | Data=85.17462089 | Physics=2.45415705 | Val RMSE: 0.80163193 | ‚àö(Val Loss) = 0.85952950 | Current Learning Rate: 0.002\n",
      "Epoch 206/1000 | Train Loss=8520.43093040 | Val Loss=0.90595789 | Data=85.17978391 | Physics=2.43620499 | Val RMSE: 0.81272554 | ‚àö(Val Loss) = 0.95181817 | Current Learning Rate: 0.002\n",
      "Epoch 207/1000 | Train Loss=8542.62970526 | Val Loss=1.12603050 | Data=85.40182842 | Physics=2.51828914 | Val RMSE: 0.77104676 | ‚àö(Val Loss) = 1.06114578 | Current Learning Rate: 0.002\n",
      "Epoch 208/1000 | Train Loss=8505.98708274 | Val Loss=0.78077582 | Data=85.03517844 | Physics=2.39624316 | Val RMSE: 0.82206720 | ‚àö(Val Loss) = 0.88361520 | Current Learning Rate: 0.002\n",
      "Epoch 209/1000 | Train Loss=8534.29372337 | Val Loss=1.07553896 | Data=85.31822690 | Physics=2.53024068 | Val RMSE: 0.83409816 | ‚àö(Val Loss) = 1.03708196 | Current Learning Rate: 0.002\n",
      "Epoch 210/1000 | Train Loss=8512.78400213 | Val Loss=0.79138910 | Data=85.10347678 | Physics=2.51928574 | Val RMSE: 0.80223000 | ‚àö(Val Loss) = 0.88960052 | Current Learning Rate: 0.002\n",
      "Epoch 211/1000 | Train Loss=8535.75266335 | Val Loss=0.96430389 | Data=85.33296273 | Physics=2.55696253 | Val RMSE: 0.81296647 | ‚àö(Val Loss) = 0.98198974 | Current Learning Rate: 0.002\n",
      "Epoch 212/1000 | Train Loss=8533.74928977 | Val Loss=0.76058085 | Data=85.31314780 | Physics=2.40835097 | Val RMSE: 0.79419780 | ‚àö(Val Loss) = 0.87211287 | Current Learning Rate: 0.002\n",
      "Epoch 213/1000 | Train Loss=8562.48326527 | Val Loss=1.57175148 | Data=85.59964405 | Physics=2.46837317 | Val RMSE: 0.94450289 | ‚àö(Val Loss) = 1.25369513 | Current Learning Rate: 0.002\n",
      "Epoch 214/1000 | Train Loss=8533.87721946 | Val Loss=0.87740079 | Data=85.31430054 | Physics=2.36683044 | Val RMSE: 0.81493020 | ‚àö(Val Loss) = 0.93669677 | Current Learning Rate: 0.002\n",
      "Epoch 215/1000 | Train Loss=8537.96533203 | Val Loss=0.70116691 | Data=85.35527177 | Physics=2.40928046 | Val RMSE: 0.80675602 | ‚àö(Val Loss) = 0.83735710 | Current Learning Rate: 0.002\n",
      "Epoch 216/1000 | Train Loss=8497.72354403 | Val Loss=0.83920027 | Data=84.95274769 | Physics=2.48138407 | Val RMSE: 0.81063461 | ‚àö(Val Loss) = 0.91607875 | Current Learning Rate: 0.002\n",
      "Epoch 217/1000 | Train Loss=8511.56356534 | Val Loss=0.78428802 | Data=85.09128293 | Physics=2.37124873 | Val RMSE: 0.81513995 | ‚àö(Val Loss) = 0.88560039 | Current Learning Rate: 0.002\n",
      "Epoch 218/1000 | Train Loss=8500.29998224 | Val Loss=0.87953411 | Data=84.97845043 | Physics=2.49381682 | Val RMSE: 0.78987956 | ‚àö(Val Loss) = 0.93783480 | Current Learning Rate: 0.002\n",
      "Epoch 219/1000 | Train Loss=8559.22283381 | Val Loss=0.89920720 | Data=85.56611009 | Physics=2.38530388 | Val RMSE: 0.79821461 | ‚àö(Val Loss) = 0.94826537 | Current Learning Rate: 0.002\n",
      "Epoch 220/1000 | Train Loss=8513.09690163 | Val Loss=0.78789969 | Data=85.10660830 | Physics=2.40592462 | Val RMSE: 0.80827260 | ‚àö(Val Loss) = 0.88763714 | Current Learning Rate: 0.002\n",
      "Epoch 221/1000 | Train Loss=8514.74098899 | Val Loss=0.86415659 | Data=85.12301012 | Physics=2.50185006 | Val RMSE: 0.81352162 | ‚àö(Val Loss) = 0.92960024 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  220 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 222/1000 | Train Loss=8540.09441584 | Val Loss=0.95773653 | Data=85.37648704 | Physics=2.54349871 | Val RMSE: 0.80074477 | ‚àö(Val Loss) = 0.97864014 | Current Learning Rate: 0.002\n",
      "Epoch 223/1000 | Train Loss=8515.34517045 | Val Loss=0.75493156 | Data=85.12893330 | Physics=2.44439280 | Val RMSE: 0.82010663 | ‚àö(Val Loss) = 0.86886799 | Current Learning Rate: 0.002\n",
      "Epoch 224/1000 | Train Loss=8519.38485440 | Val Loss=1.27788974 | Data=85.16930667 | Physics=2.61157081 | Val RMSE: 0.74714464 | ‚àö(Val Loss) = 1.13043785 | Current Learning Rate: 0.002\n",
      "Epoch 225/1000 | Train Loss=8567.04296875 | Val Loss=0.74874926 | Data=85.64532609 | Physics=2.39301270 | Val RMSE: 0.82379627 | ‚àö(Val Loss) = 0.86530298 | Current Learning Rate: 0.002\n",
      "Epoch 226/1000 | Train Loss=8529.40993430 | Val Loss=0.91675311 | Data=85.26969216 | Physics=2.35003589 | Val RMSE: 0.82574010 | ‚àö(Val Loss) = 0.95747226 | Current Learning Rate: 0.002\n",
      "Epoch 227/1000 | Train Loss=8524.16468395 | Val Loss=0.89182482 | Data=85.21721025 | Physics=2.51902345 | Val RMSE: 0.81399947 | ‚àö(Val Loss) = 0.94436479 | Current Learning Rate: 0.002\n",
      "Epoch 228/1000 | Train Loss=8548.44074041 | Val Loss=0.98525468 | Data=85.46003584 | Physics=2.45277865 | Val RMSE: 0.79851097 | ‚àö(Val Loss) = 0.99259996 | Current Learning Rate: 0.002\n",
      "Epoch 229/1000 | Train Loss=8517.87979403 | Val Loss=0.91599006 | Data=85.15367612 | Physics=2.90710528 | Val RMSE: 0.79322946 | ‚àö(Val Loss) = 0.95707369 | Current Learning Rate: 0.002\n",
      "Epoch 230/1000 | Train Loss=8538.46031605 | Val Loss=0.94085662 | Data=85.35994304 | Physics=2.55490031 | Val RMSE: 0.81090343 | ‚àö(Val Loss) = 0.96997762 | Current Learning Rate: 0.002\n",
      "Epoch 231/1000 | Train Loss=8529.27667791 | Val Loss=0.76715535 | Data=85.26842013 | Physics=2.31221024 | Val RMSE: 0.79023212 | ‚àö(Val Loss) = 0.87587404 | Current Learning Rate: 0.002\n",
      "Epoch 232/1000 | Train Loss=8531.33931108 | Val Loss=0.98050009 | Data=85.28893557 | Physics=2.49420352 | Val RMSE: 0.79766005 | ‚àö(Val Loss) = 0.99020207 | Current Learning Rate: 0.002\n",
      "Epoch 233/1000 | Train Loss=8574.77450284 | Val Loss=1.32133767 | Data=85.72281161 | Physics=2.69335016 | Val RMSE: 0.70673543 | ‚àö(Val Loss) = 1.14949453 | Current Learning Rate: 0.002\n",
      "Epoch 234/1000 | Train Loss=8514.91952237 | Val Loss=0.81645912 | Data=85.12469274 | Physics=2.51564400 | Val RMSE: 0.80783629 | ‚àö(Val Loss) = 0.90358126 | Current Learning Rate: 0.002\n",
      "Epoch 235/1000 | Train Loss=8545.79305753 | Val Loss=0.90839229 | Data=85.43351538 | Physics=2.32757658 | Val RMSE: 0.78866756 | ‚àö(Val Loss) = 0.95309615 | Current Learning Rate: 0.002\n",
      "Epoch 236/1000 | Train Loss=8524.88041548 | Val Loss=1.19135781 | Data=85.22390053 | Physics=2.74410454 | Val RMSE: 0.73177326 | ‚àö(Val Loss) = 1.09149337 | Current Learning Rate: 0.002\n",
      "Epoch 237/1000 | Train Loss=8539.69659979 | Val Loss=0.79118830 | Data=85.37255859 | Physics=2.37586256 | Val RMSE: 0.80668414 | ‚àö(Val Loss) = 0.88948768 | Current Learning Rate: 0.002\n",
      "Epoch 238/1000 | Train Loss=8524.86629972 | Val Loss=0.88804604 | Data=85.22425079 | Physics=2.43420916 | Val RMSE: 0.80379808 | ‚àö(Val Loss) = 0.94236195 | Current Learning Rate: 0.002\n",
      "Epoch 239/1000 | Train Loss=8496.52055220 | Val Loss=0.77743282 | Data=84.94083196 | Physics=2.47036604 | Val RMSE: 0.79425663 | ‚àö(Val Loss) = 0.88172150 | Current Learning Rate: 0.002\n",
      "Epoch 240/1000 | Train Loss=8527.43328303 | Val Loss=0.87261126 | Data=85.24971355 | Physics=2.48099424 | Val RMSE: 0.80745476 | ‚àö(Val Loss) = 0.93413663 | Current Learning Rate: 0.002\n",
      "Epoch 241/1000 | Train Loss=8530.39359908 | Val Loss=0.77481910 | Data=85.27962910 | Physics=2.31581426 | Val RMSE: 0.78597176 | ‚àö(Val Loss) = 0.88023806 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  240 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 242/1000 | Train Loss=8530.45148260 | Val Loss=0.87698593 | Data=85.28002930 | Physics=2.38194534 | Val RMSE: 0.81820542 | ‚àö(Val Loss) = 0.93647528 | Current Learning Rate: 0.002\n",
      "Epoch 243/1000 | Train Loss=8510.05015980 | Val Loss=1.20311463 | Data=85.07571342 | Physics=2.52896156 | Val RMSE: 0.79327482 | ‚àö(Val Loss) = 1.09686577 | Current Learning Rate: 0.002\n",
      "Epoch 244/1000 | Train Loss=8566.34161932 | Val Loss=1.36203149 | Data=85.63712796 | Physics=3.89422177 | Val RMSE: 0.62497669 | ‚àö(Val Loss) = 1.16706109 | Current Learning Rate: 0.002\n",
      "Epoch 245/1000 | Train Loss=8543.15181108 | Val Loss=0.81679308 | Data=85.40717454 | Physics=2.45386244 | Val RMSE: 0.79489905 | ‚àö(Val Loss) = 0.90376604 | Current Learning Rate: 0.002\n",
      "Epoch 246/1000 | Train Loss=8521.68647905 | Val Loss=0.82488893 | Data=85.19241402 | Physics=2.40992796 | Val RMSE: 0.79528183 | ‚àö(Val Loss) = 0.90823394 | Current Learning Rate: 0.002\n",
      "Epoch 247/1000 | Train Loss=8516.96004972 | Val Loss=0.75998047 | Data=85.14491549 | Physics=2.50183121 | Val RMSE: 0.73920065 | ‚àö(Val Loss) = 0.87176859 | Current Learning Rate: 0.002\n",
      "Epoch 248/1000 | Train Loss=8509.15904652 | Val Loss=0.71031723 | Data=85.06723993 | Physics=2.39729091 | Val RMSE: 0.80006957 | ‚àö(Val Loss) = 0.84280324 | Current Learning Rate: 0.002\n",
      "Epoch 249/1000 | Train Loss=8517.48579545 | Val Loss=0.86880302 | Data=85.15042392 | Physics=2.48603657 | Val RMSE: 0.80566555 | ‚àö(Val Loss) = 0.93209606 | Current Learning Rate: 0.002\n",
      "Epoch 250/1000 | Train Loss=8524.65540661 | Val Loss=0.78926446 | Data=85.22212011 | Physics=2.40549215 | Val RMSE: 0.80307990 | ‚àö(Val Loss) = 0.88840556 | Current Learning Rate: 0.002\n",
      "Epoch 251/1000 | Train Loss=8531.86012962 | Val Loss=0.90856659 | Data=85.29420402 | Physics=2.40993004 | Val RMSE: 0.80734968 | ‚àö(Val Loss) = 0.95318758 | Current Learning Rate: 0.002\n",
      "Epoch 252/1000 | Train Loss=8508.20663175 | Val Loss=0.70909567 | Data=85.05763869 | Physics=2.50158968 | Val RMSE: 0.80508614 | ‚àö(Val Loss) = 0.84207815 | Current Learning Rate: 0.002\n",
      "Epoch 253/1000 | Train Loss=8544.39892578 | Val Loss=0.92052699 | Data=85.41944816 | Physics=2.38884759 | Val RMSE: 0.79914790 | ‚àö(Val Loss) = 0.95944095 | Current Learning Rate: 0.002\n",
      "Epoch 254/1000 | Train Loss=8518.18470348 | Val Loss=2.33589762 | Data=85.15710519 | Physics=2.82652611 | Val RMSE: 0.58864355 | ‚àö(Val Loss) = 1.52836442 | Current Learning Rate: 0.002\n",
      "Epoch 255/1000 | Train Loss=8528.37069425 | Val Loss=0.85981020 | Data=85.25921492 | Physics=2.43443326 | Val RMSE: 0.76208282 | ‚àö(Val Loss) = 0.92725950 | Current Learning Rate: 0.002\n",
      "Epoch 256/1000 | Train Loss=8525.49746982 | Val Loss=0.73367702 | Data=85.23026414 | Physics=2.42673454 | Val RMSE: 0.80631286 | ‚àö(Val Loss) = 0.85654950 | Current Learning Rate: 0.002\n",
      "Epoch 257/1000 | Train Loss=8540.08087713 | Val Loss=0.72499455 | Data=85.37649051 | Physics=2.38994379 | Val RMSE: 0.80514783 | ‚àö(Val Loss) = 0.85146612 | Current Learning Rate: 0.002\n",
      "Epoch 258/1000 | Train Loss=8534.42103161 | Val Loss=1.01748952 | Data=85.31972504 | Physics=2.48123422 | Val RMSE: 0.81727278 | ‚àö(Val Loss) = 1.00870693 | Current Learning Rate: 0.002\n",
      "Epoch 259/1000 | Train Loss=8520.34246271 | Val Loss=0.73644085 | Data=85.17905842 | Physics=2.40665091 | Val RMSE: 0.79838049 | ‚àö(Val Loss) = 0.85816133 | Current Learning Rate: 0.002\n",
      "Epoch 260/1000 | Train Loss=8519.93310547 | Val Loss=0.97104686 | Data=85.17488098 | Physics=2.50216063 | Val RMSE: 0.80609041 | ‚àö(Val Loss) = 0.98541713 | Current Learning Rate: 0.002\n",
      "Epoch 261/1000 | Train Loss=8531.17613636 | Val Loss=0.84963329 | Data=85.28736115 | Physics=2.41131394 | Val RMSE: 0.81346387 | ‚àö(Val Loss) = 0.92175555 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  260 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 262/1000 | Train Loss=8516.75439453 | Val Loss=0.78784229 | Data=85.14311912 | Physics=2.45423016 | Val RMSE: 0.76025420 | ‚àö(Val Loss) = 0.88760477 | Current Learning Rate: 0.002\n",
      "Epoch 263/1000 | Train Loss=8536.12433416 | Val Loss=0.70184444 | Data=85.33691198 | Physics=2.44563511 | Val RMSE: 0.73887074 | ‚àö(Val Loss) = 0.83776158 | Current Learning Rate: 0.002\n",
      "Epoch 264/1000 | Train Loss=8516.47545277 | Val Loss=0.88557366 | Data=85.14029486 | Physics=2.48188603 | Val RMSE: 0.80202776 | ‚àö(Val Loss) = 0.94104922 | Current Learning Rate: 0.002\n",
      "Epoch 265/1000 | Train Loss=8514.28422408 | Val Loss=1.00844314 | Data=85.11847201 | Physics=2.48380174 | Val RMSE: 0.80697882 | ‚àö(Val Loss) = 1.00421274 | Current Learning Rate: 0.002\n",
      "Epoch 266/1000 | Train Loss=8530.78395774 | Val Loss=1.04028536 | Data=85.28341328 | Physics=2.54745072 | Val RMSE: 0.77252376 | ‚àö(Val Loss) = 1.01994383 | Current Learning Rate: 0.002\n",
      "Epoch 267/1000 | Train Loss=8510.80459872 | Val Loss=0.68791292 | Data=85.08356753 | Physics=2.50003482 | Val RMSE: 0.80681783 | ‚àö(Val Loss) = 0.82940519 | Current Learning Rate: 0.002\n",
      "Epoch 268/1000 | Train Loss=8518.89257812 | Val Loss=0.87445352 | Data=85.16442386 | Physics=2.48149586 | Val RMSE: 0.82285523 | ‚àö(Val Loss) = 0.93512219 | Current Learning Rate: 0.002\n",
      "Epoch 269/1000 | Train Loss=8540.18536932 | Val Loss=0.96955709 | Data=85.37726870 | Physics=2.45567730 | Val RMSE: 0.80690646 | ‚àö(Val Loss) = 0.98466092 | Current Learning Rate: 0.002\n",
      "Epoch 270/1000 | Train Loss=8515.34042081 | Val Loss=0.75914331 | Data=85.12895133 | Physics=2.38108346 | Val RMSE: 0.81125623 | ‚àö(Val Loss) = 0.87128830 | Current Learning Rate: 0.002\n",
      "Epoch 271/1000 | Train Loss=8533.51593572 | Val Loss=0.84647069 | Data=85.31081598 | Physics=2.43262933 | Val RMSE: 0.81243908 | ‚àö(Val Loss) = 0.92003840 | Current Learning Rate: 0.002\n",
      "Epoch 272/1000 | Train Loss=8511.83065518 | Val Loss=0.89372096 | Data=85.09385404 | Physics=2.44559743 | Val RMSE: 0.81593549 | ‚àö(Val Loss) = 0.94536817 | Current Learning Rate: 0.002\n",
      "Epoch 273/1000 | Train Loss=8539.48845881 | Val Loss=0.82374369 | Data=85.37046883 | Physics=2.49944046 | Val RMSE: 0.81276083 | ‚àö(Val Loss) = 0.90760326 | Current Learning Rate: 0.002\n",
      "Epoch 274/1000 | Train Loss=8509.38560902 | Val Loss=1.54166055 | Data=85.06902036 | Physics=2.82422388 | Val RMSE: 0.79503846 | ‚àö(Val Loss) = 1.24163628 | Current Learning Rate: 0.002\n",
      "Epoch 275/1000 | Train Loss=8533.70734197 | Val Loss=0.83289065 | Data=85.31261028 | Physics=2.47013877 | Val RMSE: 0.75422674 | ‚àö(Val Loss) = 0.91262841 | Current Learning Rate: 0.002\n",
      "Epoch 276/1000 | Train Loss=8534.72505327 | Val Loss=0.82012178 | Data=85.32265056 | Physics=2.55334119 | Val RMSE: 0.81730771 | ‚àö(Val Loss) = 0.90560573 | Current Learning Rate: 0.002\n",
      "Epoch 277/1000 | Train Loss=8511.76584695 | Val Loss=0.92544046 | Data=85.09326172 | Physics=2.47022556 | Val RMSE: 0.82990682 | ‚àö(Val Loss) = 0.96199816 | Current Learning Rate: 0.002\n",
      "Epoch 278/1000 | Train Loss=8553.69957386 | Val Loss=1.05048817 | Data=85.51248377 | Physics=2.45520857 | Val RMSE: 0.77641946 | ‚àö(Val Loss) = 1.02493322 | Current Learning Rate: 0.002\n",
      "Epoch 279/1000 | Train Loss=8536.72829368 | Val Loss=0.90015215 | Data=85.34271726 | Physics=2.60836629 | Val RMSE: 0.76942754 | ‚àö(Val Loss) = 0.94876349 | Current Learning Rate: 0.002\n",
      "Epoch 280/1000 | Train Loss=8538.88485440 | Val Loss=0.91192902 | Data=85.36447906 | Physics=2.44773723 | Val RMSE: 0.81786919 | ‚àö(Val Loss) = 0.95494974 | Current Learning Rate: 0.002\n",
      "Epoch 281/1000 | Train Loss=8526.32803622 | Val Loss=0.78689373 | Data=85.23891102 | Physics=2.44974737 | Val RMSE: 0.80465257 | ‚àö(Val Loss) = 0.88707030 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  280 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 282/1000 | Train Loss=8539.14599609 | Val Loss=0.97789711 | Data=85.36702104 | Physics=2.44471686 | Val RMSE: 0.80195028 | ‚àö(Val Loss) = 0.98888677 | Current Learning Rate: 0.002\n",
      "Epoch 283/1000 | Train Loss=8505.83465021 | Val Loss=0.82998567 | Data=85.03382319 | Physics=2.42271282 | Val RMSE: 0.81140578 | ‚àö(Val Loss) = 0.91103548 | Current Learning Rate: 0.002\n",
      "Epoch 284/1000 | Train Loss=8530.69606712 | Val Loss=0.73219151 | Data=85.28260179 | Physics=2.50220331 | Val RMSE: 0.79861212 | ‚àö(Val Loss) = 0.85568190 | Current Learning Rate: 0.002\n",
      "Epoch 285/1000 | Train Loss=8528.56445312 | Val Loss=0.98413000 | Data=85.26105915 | Physics=2.52674311 | Val RMSE: 0.80031461 | ‚àö(Val Loss) = 0.99203330 | Current Learning Rate: 0.002\n",
      "Epoch 286/1000 | Train Loss=8526.06196733 | Val Loss=1.06406401 | Data=85.23521770 | Physics=2.77127282 | Val RMSE: 0.87144983 | ‚àö(Val Loss) = 1.03153479 | Current Learning Rate: 0.002\n",
      "Epoch 287/1000 | Train Loss=8509.55921520 | Val Loss=0.80863575 | Data=85.07120999 | Physics=2.47106414 | Val RMSE: 0.81779194 | ‚àö(Val Loss) = 0.89924181 | Current Learning Rate: 0.002\n",
      "Epoch 288/1000 | Train Loss=8505.27299361 | Val Loss=0.88766792 | Data=85.02827939 | Physics=2.51239933 | Val RMSE: 0.81310111 | ‚àö(Val Loss) = 0.94216126 | Current Learning Rate: 0.002\n",
      "Epoch 289/1000 | Train Loss=8542.34690163 | Val Loss=0.77984506 | Data=85.39907976 | Physics=2.48767640 | Val RMSE: 0.80866182 | ‚àö(Val Loss) = 0.88308835 | Current Learning Rate: 0.002\n",
      "Epoch 290/1000 | Train Loss=8556.99707031 | Val Loss=3.01639825 | Data=85.54415408 | Physics=3.56215183 | Val RMSE: 0.67380065 | ‚àö(Val Loss) = 1.73677814 | Current Learning Rate: 0.002\n",
      "Epoch 291/1000 | Train Loss=8504.05224609 | Val Loss=0.81856884 | Data=85.01616114 | Physics=2.44380634 | Val RMSE: 0.79646754 | ‚àö(Val Loss) = 0.90474796 | Current Learning Rate: 0.002\n",
      "Epoch 292/1000 | Train Loss=8512.08664773 | Val Loss=0.79480662 | Data=85.09628504 | Physics=2.45426656 | Val RMSE: 0.82298458 | ‚àö(Val Loss) = 0.89151925 | Current Learning Rate: 0.002\n",
      "Epoch 293/1000 | Train Loss=8529.21302379 | Val Loss=0.89229070 | Data=85.26754275 | Physics=2.44066619 | Val RMSE: 0.81123865 | ‚àö(Val Loss) = 0.94461143 | Current Learning Rate: 0.002\n",
      "Epoch 294/1000 | Train Loss=8529.74076705 | Val Loss=0.95284696 | Data=85.27282229 | Physics=2.47635080 | Val RMSE: 0.79246604 | ‚àö(Val Loss) = 0.97613877 | Current Learning Rate: 0.002\n",
      "Epoch 295/1000 | Train Loss=8530.02818714 | Val Loss=0.84672640 | Data=85.27574366 | Physics=2.57376442 | Val RMSE: 0.71850449 | ‚àö(Val Loss) = 0.92017740 | Current Learning Rate: 0.002\n",
      "Epoch 296/1000 | Train Loss=8518.15917969 | Val Loss=0.72440454 | Data=85.15717177 | Physics=2.53224307 | Val RMSE: 0.80214906 | ‚àö(Val Loss) = 0.85111958 | Current Learning Rate: 0.002\n",
      "Epoch 297/1000 | Train Loss=8536.42498224 | Val Loss=0.96575642 | Data=85.33983543 | Physics=2.39826693 | Val RMSE: 0.81794578 | ‚àö(Val Loss) = 0.98272908 | Current Learning Rate: 0.002\n",
      "Epoch 298/1000 | Train Loss=8516.70272550 | Val Loss=0.78153847 | Data=85.14260448 | Physics=2.45087363 | Val RMSE: 0.80482876 | ‚àö(Val Loss) = 0.88404667 | Current Learning Rate: 0.002\n",
      "Epoch 299/1000 | Train Loss=8511.08496094 | Val Loss=1.00227970 | Data=85.08643757 | Physics=2.41902580 | Val RMSE: 0.82095939 | ‚àö(Val Loss) = 1.00113916 | Current Learning Rate: 0.002\n",
      "Epoch 300/1000 | Train Loss=8548.35458097 | Val Loss=0.81164365 | Data=85.45916887 | Physics=2.43512249 | Val RMSE: 0.80520916 | ‚àö(Val Loss) = 0.90091270 | Current Learning Rate: 0.002\n",
      "Epoch 301/1000 | Train Loss=8527.37939453 | Val Loss=0.98239993 | Data=85.24904078 | Physics=2.50423275 | Val RMSE: 0.77545929 | ‚àö(Val Loss) = 0.99116093 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  300 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 302/1000 | Train Loss=8537.24316406 | Val Loss=0.81193410 | Data=85.34805714 | Physics=2.55409144 | Val RMSE: 0.81273717 | ‚àö(Val Loss) = 0.90107387 | Current Learning Rate: 0.002\n",
      "Epoch 303/1000 | Train Loss=8508.33083274 | Val Loss=0.83136718 | Data=85.05890656 | Physics=2.48099161 | Val RMSE: 0.80565864 | ‚àö(Val Loss) = 0.91179341 | Current Learning Rate: 0.002\n",
      "Epoch 304/1000 | Train Loss=8521.28333629 | Val Loss=0.87420623 | Data=85.18803753 | Physics=2.42501491 | Val RMSE: 0.81435788 | ‚àö(Val Loss) = 0.93498999 | Current Learning Rate: 0.002\n",
      "Epoch 305/1000 | Train Loss=8518.45716442 | Val Loss=0.96147018 | Data=85.16008967 | Physics=2.44747845 | Val RMSE: 0.82551414 | ‚àö(Val Loss) = 0.98054588 | Current Learning Rate: 0.002\n",
      "Epoch 306/1000 | Train Loss=8495.94113991 | Val Loss=0.77353781 | Data=84.93486092 | Physics=2.33439219 | Val RMSE: 0.79872692 | ‚àö(Val Loss) = 0.87950999 | Current Learning Rate: 0.002\n",
      "Epoch 307/1000 | Train Loss=8523.77978516 | Val Loss=0.96025763 | Data=85.21332550 | Physics=2.45442502 | Val RMSE: 0.81491399 | ‚àö(Val Loss) = 0.97992736 | Current Learning Rate: 0.002\n",
      "Epoch 308/1000 | Train Loss=8492.40021307 | Val Loss=0.67108248 | Data=84.89959093 | Physics=2.51924728 | Val RMSE: 0.78755492 | ‚àö(Val Loss) = 0.81919622 | Current Learning Rate: 0.002\n",
      "Epoch 309/1000 | Train Loss=8532.04776278 | Val Loss=0.89915922 | Data=85.29580758 | Physics=2.47967534 | Val RMSE: 0.82384944 | ‚àö(Val Loss) = 0.94824004 | Current Learning Rate: 0.002\n",
      "Epoch 310/1000 | Train Loss=8542.94984020 | Val Loss=0.83292673 | Data=85.40514721 | Physics=2.42588937 | Val RMSE: 0.81775129 | ‚àö(Val Loss) = 0.91264820 | Current Learning Rate: 0.002\n",
      "Epoch 311/1000 | Train Loss=8523.33877841 | Val Loss=0.82599322 | Data=85.20899062 | Physics=2.43331323 | Val RMSE: 0.79958850 | ‚àö(Val Loss) = 0.90884173 | Current Learning Rate: 0.002\n",
      "Epoch 312/1000 | Train Loss=8522.05140270 | Val Loss=0.77424100 | Data=85.19596863 | Physics=2.54637110 | Val RMSE: 0.76878011 | ‚àö(Val Loss) = 0.87990969 | Current Learning Rate: 0.002\n",
      "Epoch 313/1000 | Train Loss=8528.38245739 | Val Loss=0.96848003 | Data=85.25941190 | Physics=2.34048608 | Val RMSE: 0.81807554 | ‚àö(Val Loss) = 0.98411381 | Current Learning Rate: 0.002\n",
      "Epoch 314/1000 | Train Loss=8501.28764205 | Val Loss=0.74830334 | Data=84.98839430 | Physics=2.48191788 | Val RMSE: 0.78229159 | ‚àö(Val Loss) = 0.86504531 | Current Learning Rate: 0.002\n",
      "Epoch 315/1000 | Train Loss=8542.55393288 | Val Loss=0.81136738 | Data=85.40098988 | Physics=2.37097191 | Val RMSE: 0.81494737 | ‚àö(Val Loss) = 0.90075934 | Current Learning Rate: 0.002\n",
      "Epoch 316/1000 | Train Loss=8515.60680043 | Val Loss=0.75972318 | Data=85.13174161 | Physics=2.44088982 | Val RMSE: 0.80496544 | ‚àö(Val Loss) = 0.87162101 | Current Learning Rate: 0.002\n",
      "Epoch 317/1000 | Train Loss=8550.16890092 | Val Loss=1.09362017 | Data=85.47726579 | Physics=2.48567155 | Val RMSE: 0.86610568 | ‚àö(Val Loss) = 1.04576302 | Current Learning Rate: 0.002\n",
      "Epoch 318/1000 | Train Loss=8526.07688210 | Val Loss=0.97034009 | Data=85.23621993 | Physics=2.55841205 | Val RMSE: 0.79315531 | ‚àö(Val Loss) = 0.98505843 | Current Learning Rate: 0.002\n",
      "Epoch 319/1000 | Train Loss=8529.32049006 | Val Loss=0.72526211 | Data=85.26880715 | Physics=2.48549913 | Val RMSE: 0.81103152 | ‚àö(Val Loss) = 0.85162324 | Current Learning Rate: 0.002\n",
      "Epoch 320/1000 | Train Loss=8528.05002663 | Val Loss=0.94869748 | Data=85.25602375 | Physics=2.44862244 | Val RMSE: 0.80578154 | ‚àö(Val Loss) = 0.97401100 | Current Learning Rate: 0.002\n",
      "Epoch 321/1000 | Train Loss=8514.09672408 | Val Loss=0.70567359 | Data=85.11657784 | Physics=2.41407075 | Val RMSE: 0.80104089 | ‚àö(Val Loss) = 0.84004378 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  320 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 322/1000 | Train Loss=8518.21533203 | Val Loss=0.82989289 | Data=85.15755116 | Physics=2.38105042 | Val RMSE: 0.82460439 | ‚àö(Val Loss) = 0.91098458 | Current Learning Rate: 0.002\n",
      "Epoch 323/1000 | Train Loss=8508.37593217 | Val Loss=0.71568672 | Data=85.05943298 | Physics=2.42170117 | Val RMSE: 0.80696183 | ‚àö(Val Loss) = 0.84598273 | Current Learning Rate: 0.002\n",
      "Epoch 324/1000 | Train Loss=8542.63809482 | Val Loss=0.96050118 | Data=85.40179721 | Physics=2.43838979 | Val RMSE: 0.81209522 | ‚àö(Val Loss) = 0.98005164 | Current Learning Rate: 0.002\n",
      "Epoch 325/1000 | Train Loss=8519.77570135 | Val Loss=0.90943226 | Data=85.17334054 | Physics=2.45833758 | Val RMSE: 0.79546070 | ‚àö(Val Loss) = 0.95364159 | Current Learning Rate: 0.002\n",
      "Epoch 326/1000 | Train Loss=8545.42125355 | Val Loss=1.77280261 | Data=85.42690555 | Physics=2.52614280 | Val RMSE: 0.86010599 | ‚àö(Val Loss) = 1.33146632 | Current Learning Rate: 0.002\n",
      "Epoch 327/1000 | Train Loss=8545.61536754 | Val Loss=0.92471273 | Data=85.43170860 | Physics=2.47404399 | Val RMSE: 0.81964374 | ‚àö(Val Loss) = 0.96161985 | Current Learning Rate: 0.002\n",
      "Epoch 328/1000 | Train Loss=8513.50630327 | Val Loss=0.96779523 | Data=85.11066021 | Physics=2.38470574 | Val RMSE: 0.81938618 | ‚àö(Val Loss) = 0.98376584 | Current Learning Rate: 0.002\n",
      "Epoch 329/1000 | Train Loss=8522.91326349 | Val Loss=0.86687502 | Data=85.20471954 | Physics=2.35734076 | Val RMSE: 0.77950382 | ‚àö(Val Loss) = 0.93106121 | Current Learning Rate: 0.002\n",
      "Epoch 330/1000 | Train Loss=8531.20569957 | Val Loss=0.81535803 | Data=85.28753801 | Physics=2.49808290 | Val RMSE: 0.81877095 | ‚àö(Val Loss) = 0.90297180 | Current Learning Rate: 0.002\n",
      "Epoch 331/1000 | Train Loss=8518.24813565 | Val Loss=0.78834691 | Data=85.15800962 | Physics=2.50802999 | Val RMSE: 0.81142259 | ‚àö(Val Loss) = 0.88788903 | Current Learning Rate: 0.002\n",
      "Epoch 332/1000 | Train Loss=8516.54576527 | Val Loss=0.76712476 | Data=85.14096555 | Physics=2.46178620 | Val RMSE: 0.78462666 | ‚àö(Val Loss) = 0.87585658 | Current Learning Rate: 0.002\n",
      "Epoch 333/1000 | Train Loss=8507.19717685 | Val Loss=0.79691487 | Data=85.04757968 | Physics=2.47442018 | Val RMSE: 0.79570526 | ‚àö(Val Loss) = 0.89270091 | Current Learning Rate: 0.002\n",
      "Epoch 334/1000 | Train Loss=8514.56511896 | Val Loss=0.80453802 | Data=85.12122553 | Physics=2.42417600 | Val RMSE: 0.79513729 | ‚àö(Val Loss) = 0.89696044 | Current Learning Rate: 0.002\n",
      "Epoch 335/1000 | Train Loss=8535.35138494 | Val Loss=0.78343257 | Data=85.32902319 | Physics=2.47060999 | Val RMSE: 0.81262273 | ‚àö(Val Loss) = 0.88511729 | Current Learning Rate: 0.002\n",
      "Epoch 336/1000 | Train Loss=8492.19109553 | Val Loss=0.84563764 | Data=84.89750671 | Physics=2.42900354 | Val RMSE: 0.81884319 | ‚àö(Val Loss) = 0.91958559 | Current Learning Rate: 0.002\n",
      "Epoch 337/1000 | Train Loss=8522.48841442 | Val Loss=0.93361707 | Data=85.20047760 | Physics=2.29174666 | Val RMSE: 0.82290351 | ‚àö(Val Loss) = 0.96623862 | Current Learning Rate: 0.002\n",
      "Epoch 338/1000 | Train Loss=8533.99054510 | Val Loss=0.83002169 | Data=85.31553927 | Physics=2.52163309 | Val RMSE: 0.79887062 | ‚àö(Val Loss) = 0.91105527 | Current Learning Rate: 0.002\n",
      "Epoch 339/1000 | Train Loss=8512.39608487 | Val Loss=0.93352431 | Data=85.09937078 | Physics=2.55068453 | Val RMSE: 0.76007044 | ‚àö(Val Loss) = 0.96619064 | Current Learning Rate: 0.002\n",
      "Epoch 340/1000 | Train Loss=8511.08713601 | Val Loss=0.95118009 | Data=85.08629053 | Physics=2.51125784 | Val RMSE: 0.75083941 | ‚àö(Val Loss) = 0.97528464 | Current Learning Rate: 0.002\n",
      "Epoch 341/1000 | Train Loss=8568.22105824 | Val Loss=1.94980207 | Data=85.65470054 | Physics=5.83188196 | Val RMSE: 0.63171470 | ‚àö(Val Loss) = 1.39635313 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  340 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 342/1000 | Train Loss=8591.90403054 | Val Loss=0.72800440 | Data=85.89455344 | Physics=2.42997409 | Val RMSE: 0.78361893 | ‚àö(Val Loss) = 0.85323173 | Current Learning Rate: 0.002\n",
      "Epoch 343/1000 | Train Loss=8543.81893643 | Val Loss=0.78406468 | Data=85.41381212 | Physics=2.44818468 | Val RMSE: 0.81906176 | ‚àö(Val Loss) = 0.88547426 | Current Learning Rate: 0.002\n",
      "Epoch 344/1000 | Train Loss=8513.81218928 | Val Loss=0.89070439 | Data=85.11374248 | Physics=2.46668057 | Val RMSE: 0.80241215 | ‚àö(Val Loss) = 0.94377136 | Current Learning Rate: 0.002\n",
      "Epoch 345/1000 | Train Loss=8513.12855114 | Val Loss=0.80449165 | Data=85.10694400 | Physics=2.43554222 | Val RMSE: 0.81516433 | ‚àö(Val Loss) = 0.89693457 | Current Learning Rate: 0.002\n",
      "Epoch 346/1000 | Train Loss=8524.60236151 | Val Loss=0.92970309 | Data=85.22163877 | Physics=2.41753832 | Val RMSE: 0.81805837 | ‚àö(Val Loss) = 0.96421117 | Current Learning Rate: 0.002\n",
      "Epoch 347/1000 | Train Loss=8535.91574929 | Val Loss=0.81650728 | Data=85.33468975 | Physics=2.56298559 | Val RMSE: 0.81874382 | ‚àö(Val Loss) = 0.90360790 | Current Learning Rate: 0.002\n",
      "Epoch 348/1000 | Train Loss=8527.78440163 | Val Loss=0.78163743 | Data=85.25327717 | Physics=2.52598574 | Val RMSE: 0.79796720 | ‚àö(Val Loss) = 0.88410258 | Current Learning Rate: 0.002\n",
      "Epoch 349/1000 | Train Loss=8568.03062855 | Val Loss=1.27082609 | Data=85.65555919 | Physics=2.55783792 | Val RMSE: 0.84238654 | ‚àö(Val Loss) = 1.12730920 | Current Learning Rate: 0.002\n",
      "Epoch 350/1000 | Train Loss=8519.86505682 | Val Loss=0.73537777 | Data=85.17430947 | Physics=2.42417213 | Val RMSE: 0.80196989 | ‚àö(Val Loss) = 0.85754174 | Current Learning Rate: 0.002\n",
      "Epoch 351/1000 | Train Loss=8513.12020597 | Val Loss=0.90741056 | Data=85.10664160 | Physics=2.41593793 | Val RMSE: 0.82121253 | ‚àö(Val Loss) = 0.95258099 | Current Learning Rate: 0.002\n",
      "Epoch 352/1000 | Train Loss=8502.49804688 | Val Loss=0.83026062 | Data=85.00057290 | Physics=2.38402340 | Val RMSE: 0.80422419 | ‚àö(Val Loss) = 0.91118640 | Current Learning Rate: 0.002\n",
      "Epoch 353/1000 | Train Loss=8520.54940518 | Val Loss=0.93488055 | Data=85.18105386 | Physics=2.31070924 | Val RMSE: 0.79274297 | ‚àö(Val Loss) = 0.96689224 | Current Learning Rate: 0.002\n",
      "Epoch 354/1000 | Train Loss=8503.39204545 | Val Loss=1.11255498 | Data=85.00934393 | Physics=2.49646257 | Val RMSE: 0.79284745 | ‚àö(Val Loss) = 1.05477726 | Current Learning Rate: 0.002\n",
      "Epoch 355/1000 | Train Loss=8494.16685902 | Val Loss=0.80040581 | Data=84.91712397 | Physics=2.50093225 | Val RMSE: 0.81670195 | ‚àö(Val Loss) = 0.89465404 | Current Learning Rate: 0.002\n",
      "Epoch 356/1000 | Train Loss=8527.07164418 | Val Loss=1.06983963 | Data=85.24600844 | Physics=2.70654717 | Val RMSE: 0.79754347 | ‚àö(Val Loss) = 1.03433049 | Current Learning Rate: 0.002\n",
      "Epoch 357/1000 | Train Loss=8515.62482244 | Val Loss=0.95895611 | Data=85.13187200 | Physics=2.42054590 | Val RMSE: 0.77674401 | ‚àö(Val Loss) = 0.97926307 | Current Learning Rate: 0.002\n",
      "Epoch 358/1000 | Train Loss=8505.30184659 | Val Loss=0.75558778 | Data=85.02861300 | Physics=2.53004408 | Val RMSE: 0.80732882 | ‚àö(Val Loss) = 0.86924553 | Current Learning Rate: 0.002\n",
      "Epoch 359/1000 | Train Loss=8540.68563565 | Val Loss=0.80671218 | Data=85.38244698 | Physics=2.39828647 | Val RMSE: 0.81119609 | ‚àö(Val Loss) = 0.89817160 | Current Learning Rate: 0.002\n",
      "Epoch 360/1000 | Train Loss=8528.16188743 | Val Loss=1.42466298 | Data=85.25706343 | Physics=2.55375074 | Val RMSE: 0.73687220 | ‚àö(Val Loss) = 1.19359243 | Current Learning Rate: 0.002\n",
      "Epoch 361/1000 | Train Loss=8552.13556463 | Val Loss=0.71187097 | Data=85.49695865 | Physics=2.33591321 | Val RMSE: 0.78752756 | ‚àö(Val Loss) = 0.84372449 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  360 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 362/1000 | Train Loss=8530.49795810 | Val Loss=0.86921103 | Data=85.28063618 | Physics=2.38494892 | Val RMSE: 0.81171393 | ‚àö(Val Loss) = 0.93231487 | Current Learning Rate: 0.002\n",
      "Epoch 363/1000 | Train Loss=8536.13476562 | Val Loss=0.75019047 | Data=85.33700423 | Physics=2.38893101 | Val RMSE: 0.80028236 | ‚àö(Val Loss) = 0.86613536 | Current Learning Rate: 0.002\n",
      "Epoch 364/1000 | Train Loss=8521.62038352 | Val Loss=0.84757598 | Data=85.19171004 | Physics=2.43124518 | Val RMSE: 0.79803765 | ‚àö(Val Loss) = 0.92063886 | Current Learning Rate: 0.002\n",
      "Epoch 365/1000 | Train Loss=8528.82714844 | Val Loss=0.80661282 | Data=85.26382793 | Physics=2.41518077 | Val RMSE: 0.80178535 | ‚àö(Val Loss) = 0.89811629 | Current Learning Rate: 0.002\n",
      "Epoch 366/1000 | Train Loss=8505.82262074 | Val Loss=0.85850347 | Data=85.03383706 | Physics=2.40852854 | Val RMSE: 0.79009497 | ‚àö(Val Loss) = 0.92655462 | Current Learning Rate: 0.002\n",
      "Epoch 367/1000 | Train Loss=8506.47199041 | Val Loss=1.06486632 | Data=85.04009039 | Physics=2.51580044 | Val RMSE: 0.78020263 | ‚àö(Val Loss) = 1.03192365 | Current Learning Rate: 0.002\n",
      "Epoch 368/1000 | Train Loss=8537.44651101 | Val Loss=0.91791688 | Data=85.34994160 | Physics=2.41027933 | Val RMSE: 0.80142760 | ‚àö(Val Loss) = 0.95807981 | Current Learning Rate: 0.002\n",
      "Epoch 369/1000 | Train Loss=8518.18829901 | Val Loss=0.66957077 | Data=85.15746654 | Physics=2.45293931 | Val RMSE: 0.79583764 | ‚àö(Val Loss) = 0.81827301 | Current Learning Rate: 0.002\n",
      "Epoch 370/1000 | Train Loss=8518.01207386 | Val Loss=0.91808222 | Data=85.15558555 | Physics=2.47017485 | Val RMSE: 0.81305087 | ‚àö(Val Loss) = 0.95816606 | Current Learning Rate: 0.002\n",
      "Epoch 371/1000 | Train Loss=8516.38259055 | Val Loss=0.91154957 | Data=85.13940915 | Physics=2.44889462 | Val RMSE: 0.79340047 | ‚àö(Val Loss) = 0.95475107 | Current Learning Rate: 0.002\n",
      "Epoch 372/1000 | Train Loss=8518.81498580 | Val Loss=0.71755571 | Data=85.16355202 | Physics=2.54571740 | Val RMSE: 0.74235344 | ‚àö(Val Loss) = 0.84708661 | Current Learning Rate: 0.002\n",
      "Epoch 373/1000 | Train Loss=8559.11976207 | Val Loss=1.49876682 | Data=85.56630568 | Physics=2.77519161 | Val RMSE: 0.74536347 | ‚àö(Val Loss) = 1.22424126 | Current Learning Rate: 0.002\n",
      "Epoch 374/1000 | Train Loss=8532.32195490 | Val Loss=0.86424031 | Data=85.29884269 | Physics=2.44543924 | Val RMSE: 0.81220585 | ‚àö(Val Loss) = 0.92964524 | Current Learning Rate: 0.002\n",
      "Epoch 375/1000 | Train Loss=8522.19189453 | Val Loss=0.93842401 | Data=85.19742376 | Physics=2.47215603 | Val RMSE: 0.82112610 | ‚àö(Val Loss) = 0.96872288 | Current Learning Rate: 0.002\n",
      "Epoch 376/1000 | Train Loss=8503.54221413 | Val Loss=0.79692147 | Data=85.01107580 | Physics=2.34252948 | Val RMSE: 0.79861140 | ‚àö(Val Loss) = 0.89270461 | Current Learning Rate: 0.002\n",
      "Epoch 377/1000 | Train Loss=8517.94118430 | Val Loss=0.98915808 | Data=85.15491486 | Physics=2.45235458 | Val RMSE: 0.78295553 | ‚àö(Val Loss) = 0.99456429 | Current Learning Rate: 0.002\n",
      "Epoch 378/1000 | Train Loss=8539.60546875 | Val Loss=0.68853064 | Data=85.37161324 | Physics=2.37133976 | Val RMSE: 0.78706974 | ‚àö(Val Loss) = 0.82977748 | Current Learning Rate: 0.002\n",
      "Epoch 379/1000 | Train Loss=8530.88325639 | Val Loss=0.90805310 | Data=85.28440441 | Physics=2.43357184 | Val RMSE: 0.78319257 | ‚àö(Val Loss) = 0.95291823 | Current Learning Rate: 0.002\n",
      "Epoch 380/1000 | Train Loss=8537.49245384 | Val Loss=0.73165516 | Data=85.35051034 | Physics=2.38989825 | Val RMSE: 0.80924547 | ‚àö(Val Loss) = 0.85536844 | Current Learning Rate: 0.002\n",
      "Epoch 381/1000 | Train Loss=8520.71222479 | Val Loss=0.80164286 | Data=85.18264979 | Physics=2.53252147 | Val RMSE: 0.81435549 | ‚àö(Val Loss) = 0.89534515 | Current Learning Rate: 0.002\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  380 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 382/1000 | Train Loss=8533.48215554 | Val Loss=0.94509411 | Data=85.31027360 | Physics=2.51510925 | Val RMSE: 0.79733497 | ‚àö(Val Loss) = 0.97215950 | Current Learning Rate: 0.002\n",
      "Epoch 383/1000 | Train Loss=8527.98783736 | Val Loss=0.85197197 | Data=85.25532324 | Physics=2.50456839 | Val RMSE: 0.80195820 | ‚àö(Val Loss) = 0.92302328 | Current Learning Rate: 0.002\n",
      "Epoch 384/1000 | Train Loss=8523.40629439 | Val Loss=0.86414836 | Data=85.20952329 | Physics=2.52631302 | Val RMSE: 0.79584491 | ‚àö(Val Loss) = 0.92959583 | Current Learning Rate: 0.002\n",
      "Epoch 385/1000 | Train Loss=8528.98259943 | Val Loss=1.09271918 | Data=85.26281669 | Physics=3.84534636 | Val RMSE: 0.76446259 | ‚àö(Val Loss) = 1.04533207 | Current Learning Rate: 0.002\n",
      "Epoch 386/1000 | Train Loss=8525.32151101 | Val Loss=0.91216605 | Data=85.22876948 | Physics=2.43553381 | Val RMSE: 0.82405156 | ‚àö(Val Loss) = 0.95507383 | Current Learning Rate: 0.002\n",
      "Epoch 387/1000 | Train Loss=8537.91854581 | Val Loss=0.85731271 | Data=85.35480777 | Physics=2.49516776 | Val RMSE: 0.80685365 | ‚àö(Val Loss) = 0.92591178 | Current Learning Rate: 0.002\n",
      "Epoch 388/1000 | Train Loss=8512.74746982 | Val Loss=0.96708864 | Data=85.10295313 | Physics=2.50949787 | Val RMSE: 0.71545333 | ‚àö(Val Loss) = 0.98340666 | Current Learning Rate: 0.002\n",
      "Epoch 389/1000 | Train Loss=8699.48552912 | Val Loss=4.14624292 | Data=86.96835951 | Physics=3.40835957 | Val RMSE: 1.00556207 | ‚àö(Val Loss) = 2.03623247 | Current Learning Rate: 0.002\n",
      "Epoch 390/1000 | Train Loss=8506.48819247 | Val Loss=0.81675857 | Data=85.04038932 | Physics=2.38856586 | Val RMSE: 0.81264424 | ‚àö(Val Loss) = 0.90374696 | Current Learning Rate: 0.002\n",
      "Epoch 391/1000 | Train Loss=8527.84286222 | Val Loss=0.80808735 | Data=85.25395966 | Physics=2.40310982 | Val RMSE: 0.80711728 | ‚àö(Val Loss) = 0.89893681 | Current Learning Rate: 0.002\n",
      "Epoch 392/1000 | Train Loss=8531.09153054 | Val Loss=0.80107273 | Data=85.28651151 | Physics=2.51629680 | Val RMSE: 0.78846228 | ‚àö(Val Loss) = 0.89502668 | Current Learning Rate: 0.002\n",
      "Epoch 393/1000 | Train Loss=8532.22793857 | Val Loss=0.94395761 | Data=85.29769897 | Physics=2.46110540 | Val RMSE: 0.80912447 | ‚àö(Val Loss) = 0.97157484 | Current Learning Rate: 0.002\n",
      "Epoch 394/1000 | Train Loss=8504.57022372 | Val Loss=0.75493746 | Data=85.02131167 | Physics=2.47883506 | Val RMSE: 0.80400360 | ‚àö(Val Loss) = 0.86887139 | Current Learning Rate: 0.002\n",
      "Epoch 395/1000 | Train Loss=8517.86638849 | Val Loss=0.89099386 | Data=85.15425041 | Physics=2.42871220 | Val RMSE: 0.81025666 | ‚àö(Val Loss) = 0.94392473 | Current Learning Rate: 0.002\n",
      "Epoch 396/1000 | Train Loss=8499.61709872 | Val Loss=0.78095472 | Data=84.97183158 | Physics=2.37012275 | Val RMSE: 0.80907220 | ‚àö(Val Loss) = 0.88371640 | Current Learning Rate: 0.002\n",
      "Epoch 397/1000 | Train Loss=8506.20987216 | Val Loss=0.86489700 | Data=85.03764205 | Physics=2.44759859 | Val RMSE: 0.81403685 | ‚àö(Val Loss) = 0.92999840 | Current Learning Rate: 0.002\n",
      "Epoch 398/1000 | Train Loss=8498.27148438 | Val Loss=0.88911686 | Data=84.95829496 | Physics=2.44383635 | Val RMSE: 0.80130017 | ‚àö(Val Loss) = 0.94292992 | Current Learning Rate: 0.002\n",
      "Epoch 399/1000 | Train Loss=8543.48104581 | Val Loss=1.01797236 | Data=85.41022075 | Physics=2.49778251 | Val RMSE: 0.79640269 | ‚àö(Val Loss) = 1.00894618 | Current Learning Rate: 0.002\n",
      "Epoch 400/1000 | Train Loss=8525.34996449 | Val Loss=0.68661550 | Data=85.22911419 | Physics=2.49459021 | Val RMSE: 0.79738927 | ‚àö(Val Loss) = 0.82862264 | Current Learning Rate: 0.0002\n",
      "Epoch 401/1000 | Train Loss=8525.30730646 | Val Loss=0.65035874 | Data=85.22853643 | Physics=2.42468035 | Val RMSE: 0.81971163 | ‚àö(Val Loss) = 0.80644822 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  400 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 402/1000 | Train Loss=1560.73190030 | Val Loss=0.69042322 | Data=15.58193831 | Physics=2.62248892 | Val RMSE: 0.81980497 | ‚àö(Val Loss) = 0.83091712 | Current Learning Rate: 0.0002\n",
      "Epoch 403/1000 | Train Loss=1544.69931863 | Val Loss=0.73885680 | Data=15.42143761 | Physics=2.72526554 | Val RMSE: 0.82007742 | ‚àö(Val Loss) = 0.85956782 | Current Learning Rate: 0.0002\n",
      "Epoch 404/1000 | Train Loss=1554.33290794 | Val Loss=0.70569037 | Data=15.51804725 | Physics=2.62639172 | Val RMSE: 0.82029152 | ‚àö(Val Loss) = 0.84005380 | Current Learning Rate: 0.0002\n",
      "Epoch 405/1000 | Train Loss=1550.73571777 | Val Loss=0.72227909 | Data=15.48204231 | Physics=2.61680704 | Val RMSE: 0.82059461 | ‚àö(Val Loss) = 0.84987003 | Current Learning Rate: 0.0002\n",
      "Epoch 406/1000 | Train Loss=1548.70488947 | Val Loss=0.71968495 | Data=15.46138755 | Physics=2.72414013 | Val RMSE: 0.82056856 | ‚àö(Val Loss) = 0.84834248 | Current Learning Rate: 0.0002\n",
      "Epoch 407/1000 | Train Loss=1549.64878152 | Val Loss=0.68825558 | Data=15.47136480 | Physics=2.64432325 | Val RMSE: 0.82045245 | ‚àö(Val Loss) = 0.82961172 | Current Learning Rate: 0.0002\n",
      "Epoch 408/1000 | Train Loss=1554.06003640 | Val Loss=0.71604231 | Data=15.51516290 | Physics=2.67960435 | Val RMSE: 0.82059902 | ‚àö(Val Loss) = 0.84619284 | Current Learning Rate: 0.0002\n",
      "Epoch 409/1000 | Train Loss=1551.00256348 | Val Loss=0.68198731 | Data=15.48479427 | Physics=2.58048473 | Val RMSE: 0.82053131 | ‚àö(Val Loss) = 0.82582521 | Current Learning Rate: 0.0002\n",
      "Epoch 410/1000 | Train Loss=1553.67573686 | Val Loss=0.69199158 | Data=15.51163682 | Physics=2.59961065 | Val RMSE: 0.82063270 | ‚àö(Val Loss) = 0.83186030 | Current Learning Rate: 0.0002\n",
      "Epoch 411/1000 | Train Loss=1557.51290616 | Val Loss=0.70926929 | Data=15.54961863 | Physics=2.64603047 | Val RMSE: 0.82067233 | ‚àö(Val Loss) = 0.84218127 | Current Learning Rate: 0.0002\n",
      "Epoch 412/1000 | Train Loss=1556.00461648 | Val Loss=0.68656290 | Data=15.53500886 | Physics=2.57507467 | Val RMSE: 0.82040548 | ‚àö(Val Loss) = 0.82859093 | Current Learning Rate: 0.0002\n",
      "Epoch 413/1000 | Train Loss=1546.83032227 | Val Loss=0.70381328 | Data=15.44281344 | Physics=2.58517751 | Val RMSE: 0.82051444 | ‚àö(Val Loss) = 0.83893579 | Current Learning Rate: 0.0002\n",
      "Epoch 414/1000 | Train Loss=1555.06250000 | Val Loss=0.66727556 | Data=15.52561526 | Physics=2.62871312 | Val RMSE: 0.82042247 | ‚àö(Val Loss) = 0.81686938 | Current Learning Rate: 0.0002\n",
      "Epoch 415/1000 | Train Loss=1547.33024458 | Val Loss=0.70625212 | Data=15.44791889 | Physics=2.64475756 | Val RMSE: 0.82071930 | ‚àö(Val Loss) = 0.84038806 | Current Learning Rate: 0.0002\n",
      "Epoch 416/1000 | Train Loss=1548.13863858 | Val Loss=0.68252813 | Data=15.45630169 | Physics=2.55701550 | Val RMSE: 0.82055593 | ‚àö(Val Loss) = 0.82615262 | Current Learning Rate: 0.0002\n",
      "Epoch 417/1000 | Train Loss=1550.67662464 | Val Loss=0.69069844 | Data=15.48156105 | Physics=2.60873593 | Val RMSE: 0.82065332 | ‚àö(Val Loss) = 0.83108270 | Current Learning Rate: 0.0002\n",
      "Epoch 418/1000 | Train Loss=1547.77091841 | Val Loss=0.69132046 | Data=15.45248968 | Physics=2.62330036 | Val RMSE: 0.82068598 | ‚àö(Val Loss) = 0.83145684 | Current Learning Rate: 0.0002\n",
      "Epoch 419/1000 | Train Loss=1548.07597212 | Val Loss=0.69175037 | Data=15.45555661 | Physics=2.59389937 | Val RMSE: 0.82077157 | ‚àö(Val Loss) = 0.83171529 | Current Learning Rate: 0.0002\n",
      "Epoch 420/1000 | Train Loss=1553.71355646 | Val Loss=0.68366465 | Data=15.51189241 | Physics=2.58465517 | Val RMSE: 0.82077140 | ‚àö(Val Loss) = 0.82684016 | Current Learning Rate: 0.0002\n",
      "Epoch 421/1000 | Train Loss=1555.71290172 | Val Loss=0.67589527 | Data=15.53208620 | Physics=2.55476104 | Val RMSE: 0.82053083 | ‚àö(Val Loss) = 0.82212847 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  420 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 422/1000 | Train Loss=1547.04766291 | Val Loss=0.69945895 | Data=15.44517630 | Physics=2.71666231 | Val RMSE: 0.82079154 | ‚àö(Val Loss) = 0.83633661 | Current Learning Rate: 0.0002\n",
      "Epoch 423/1000 | Train Loss=1550.95238148 | Val Loss=0.68952342 | Data=15.48429532 | Physics=2.65244117 | Val RMSE: 0.82078862 | ‚àö(Val Loss) = 0.83037543 | Current Learning Rate: 0.0002\n",
      "Epoch 424/1000 | Train Loss=1553.64547452 | Val Loss=0.68435035 | Data=15.51127130 | Physics=2.62047591 | Val RMSE: 0.82081181 | ‚àö(Val Loss) = 0.82725471 | Current Learning Rate: 0.0002\n",
      "Epoch 425/1000 | Train Loss=1547.45537775 | Val Loss=0.69068067 | Data=15.44932357 | Physics=2.60834021 | Val RMSE: 0.82090765 | ‚àö(Val Loss) = 0.83107203 | Current Learning Rate: 0.0002\n",
      "Epoch 426/1000 | Train Loss=1549.20925071 | Val Loss=0.69069185 | Data=15.46682323 | Physics=2.65953126 | Val RMSE: 0.82094604 | ‚àö(Val Loss) = 0.83107871 | Current Learning Rate: 0.0002\n",
      "Epoch 427/1000 | Train Loss=1553.68496982 | Val Loss=0.68110420 | Data=15.51173158 | Physics=2.55112794 | Val RMSE: 0.82096899 | ‚àö(Val Loss) = 0.82529038 | Current Learning Rate: 0.0002\n",
      "Epoch 428/1000 | Train Loss=1556.39599609 | Val Loss=0.70687034 | Data=15.53853269 | Physics=2.70829533 | Val RMSE: 0.82128072 | ‚àö(Val Loss) = 0.84075582 | Current Learning Rate: 0.0002\n",
      "Epoch 429/1000 | Train Loss=1545.06243342 | Val Loss=0.68586929 | Data=15.42533164 | Physics=2.69723305 | Val RMSE: 0.82093269 | ‚àö(Val Loss) = 0.82817227 | Current Learning Rate: 0.0002\n",
      "Epoch 430/1000 | Train Loss=1561.35575728 | Val Loss=0.67173807 | Data=15.58849647 | Physics=2.58911138 | Val RMSE: 0.82087439 | ‚àö(Val Loss) = 0.81959629 | Current Learning Rate: 0.0002\n",
      "Epoch 431/1000 | Train Loss=1553.39490856 | Val Loss=0.68684301 | Data=15.50866951 | Physics=2.65973701 | Val RMSE: 0.82088816 | ‚àö(Val Loss) = 0.82875997 | Current Learning Rate: 0.0002\n",
      "Epoch 432/1000 | Train Loss=1555.67592551 | Val Loss=0.67178837 | Data=15.53172545 | Physics=2.60563470 | Val RMSE: 0.82079667 | ‚àö(Val Loss) = 0.81962699 | Current Learning Rate: 0.0002\n",
      "Epoch 433/1000 | Train Loss=1550.58654785 | Val Loss=0.68939308 | Data=15.48055137 | Physics=2.65070039 | Val RMSE: 0.82097930 | ‚àö(Val Loss) = 0.83029699 | Current Learning Rate: 0.0002\n",
      "Epoch 434/1000 | Train Loss=1554.04889471 | Val Loss=0.67024835 | Data=15.51550016 | Physics=2.53173700 | Val RMSE: 0.82083011 | ‚àö(Val Loss) = 0.81868696 | Current Learning Rate: 0.0002\n",
      "Epoch 435/1000 | Train Loss=1551.67614746 | Val Loss=0.68943139 | Data=15.49146921 | Physics=2.58697354 | Val RMSE: 0.82107604 | ‚àö(Val Loss) = 0.83032006 | Current Learning Rate: 0.0002\n",
      "Epoch 436/1000 | Train Loss=1551.67456055 | Val Loss=0.67062958 | Data=15.49173702 | Physics=2.54000772 | Val RMSE: 0.82089788 | ‚àö(Val Loss) = 0.81891978 | Current Learning Rate: 0.0002\n",
      "Epoch 437/1000 | Train Loss=1553.42813388 | Val Loss=0.69554368 | Data=15.50898864 | Physics=2.61579251 | Val RMSE: 0.82116646 | ‚àö(Val Loss) = 0.83399260 | Current Learning Rate: 0.0002\n",
      "Epoch 438/1000 | Train Loss=1550.83068848 | Val Loss=0.67568433 | Data=15.48323501 | Physics=2.52939249 | Val RMSE: 0.82093060 | ‚àö(Val Loss) = 0.82200021 | Current Learning Rate: 0.0002\n",
      "Epoch 439/1000 | Train Loss=1554.42289595 | Val Loss=0.68237811 | Data=15.51910608 | Physics=2.55600195 | Val RMSE: 0.82106274 | ‚àö(Val Loss) = 0.82606179 | Current Learning Rate: 0.0002\n",
      "Epoch 440/1000 | Train Loss=1554.84967596 | Val Loss=0.69194335 | Data=15.52322518 | Physics=2.66164390 | Val RMSE: 0.82120901 | ‚àö(Val Loss) = 0.83183134 | Current Learning Rate: 0.0002\n",
      "Epoch 441/1000 | Train Loss=1552.95046165 | Val Loss=0.69111432 | Data=15.50420804 | Physics=2.64255398 | Val RMSE: 0.82121420 | ‚àö(Val Loss) = 0.83133286 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  440 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 442/1000 | Train Loss=1547.66718084 | Val Loss=0.66858654 | Data=15.45160016 | Physics=2.59705694 | Val RMSE: 0.82097918 | ‚àö(Val Loss) = 0.81767142 | Current Learning Rate: 0.0002\n",
      "Epoch 443/1000 | Train Loss=1550.29359020 | Val Loss=0.68073689 | Data=15.47777791 | Physics=2.54007617 | Val RMSE: 0.82111388 | ‚àö(Val Loss) = 0.82506782 | Current Learning Rate: 0.0002\n",
      "Epoch 444/1000 | Train Loss=1548.17561479 | Val Loss=0.68088562 | Data=15.45660036 | Physics=2.60324128 | Val RMSE: 0.82107425 | ‚àö(Val Loss) = 0.82515794 | Current Learning Rate: 0.0002\n",
      "Epoch 445/1000 | Train Loss=1554.31815962 | Val Loss=0.68290850 | Data=15.51805115 | Physics=2.56743486 | Val RMSE: 0.82109970 | ‚àö(Val Loss) = 0.82638276 | Current Learning Rate: 0.0002\n",
      "Epoch 446/1000 | Train Loss=1552.54392312 | Val Loss=0.68483453 | Data=15.50018519 | Physics=2.59910669 | Val RMSE: 0.82115138 | ‚àö(Val Loss) = 0.82754731 | Current Learning Rate: 0.0002\n",
      "Epoch 447/1000 | Train Loss=1554.39943626 | Val Loss=0.66550704 | Data=15.51899268 | Physics=2.56523944 | Val RMSE: 0.82093799 | ‚àö(Val Loss) = 0.81578612 | Current Learning Rate: 0.0002\n",
      "Epoch 448/1000 | Train Loss=1551.10603471 | Val Loss=0.68401320 | Data=15.48584444 | Physics=2.57597619 | Val RMSE: 0.82114959 | ‚àö(Val Loss) = 0.82705092 | Current Learning Rate: 0.0002\n",
      "Epoch 449/1000 | Train Loss=1551.40495162 | Val Loss=0.66964862 | Data=15.48899668 | Physics=2.53621228 | Val RMSE: 0.82100701 | ‚àö(Val Loss) = 0.81832063 | Current Learning Rate: 0.0002\n",
      "Epoch 450/1000 | Train Loss=1557.32452947 | Val Loss=0.67911410 | Data=15.54811313 | Physics=2.57483764 | Val RMSE: 0.82120389 | ‚àö(Val Loss) = 0.82408381 | Current Learning Rate: 0.0002\n",
      "Epoch 451/1000 | Train Loss=1551.13395552 | Val Loss=0.69017126 | Data=15.48609855 | Physics=2.59068457 | Val RMSE: 0.82140809 | ‚àö(Val Loss) = 0.83076543 | Current Learning Rate: 0.0002\n",
      "Epoch 452/1000 | Train Loss=1550.32509544 | Val Loss=0.68204559 | Data=15.47806566 | Physics=2.58288748 | Val RMSE: 0.82125807 | ‚àö(Val Loss) = 0.82586050 | Current Learning Rate: 0.0002\n",
      "Epoch 453/1000 | Train Loss=1549.26832164 | Val Loss=0.67780680 | Data=15.46758218 | Physics=2.51004006 | Val RMSE: 0.82116014 | ‚àö(Val Loss) = 0.82329023 | Current Learning Rate: 0.0002\n",
      "Epoch 454/1000 | Train Loss=1549.42095392 | Val Loss=0.67341098 | Data=15.46911413 | Physics=2.52865845 | Val RMSE: 0.82112116 | ‚àö(Val Loss) = 0.82061625 | Current Learning Rate: 0.0002\n",
      "Epoch 455/1000 | Train Loss=1554.84379439 | Val Loss=0.66641850 | Data=15.52341600 | Physics=2.57915707 | Val RMSE: 0.82105416 | ‚àö(Val Loss) = 0.81634462 | Current Learning Rate: 0.0002\n",
      "Epoch 456/1000 | Train Loss=1552.56791548 | Val Loss=0.68027650 | Data=15.50052504 | Physics=2.62869302 | Val RMSE: 0.82131290 | ‚àö(Val Loss) = 0.82478875 | Current Learning Rate: 0.0002\n",
      "Epoch 457/1000 | Train Loss=1554.82239879 | Val Loss=0.68426206 | Data=15.52294558 | Physics=2.59828311 | Val RMSE: 0.82142687 | ‚àö(Val Loss) = 0.82720131 | Current Learning Rate: 0.0002\n",
      "Epoch 458/1000 | Train Loss=1549.68811035 | Val Loss=0.67268252 | Data=15.47176162 | Physics=2.57611038 | Val RMSE: 0.82121336 | ‚àö(Val Loss) = 0.82017225 | Current Learning Rate: 0.0002\n",
      "Epoch 459/1000 | Train Loss=1549.05183549 | Val Loss=0.67413246 | Data=15.46542749 | Physics=2.54276213 | Val RMSE: 0.82120460 | ‚àö(Val Loss) = 0.82105571 | Current Learning Rate: 0.0002\n",
      "Epoch 460/1000 | Train Loss=1550.74189897 | Val Loss=0.67659219 | Data=15.48227900 | Physics=2.65505470 | Val RMSE: 0.82129312 | ‚àö(Val Loss) = 0.82255220 | Current Learning Rate: 0.0002\n",
      "Epoch 461/1000 | Train Loss=1545.59343928 | Val Loss=0.67578083 | Data=15.43081197 | Physics=2.60396396 | Val RMSE: 0.82129741 | ‚àö(Val Loss) = 0.82205892 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  460 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 462/1000 | Train Loss=1552.53607733 | Val Loss=0.67435110 | Data=15.50025732 | Physics=2.53040529 | Val RMSE: 0.82124007 | ‚àö(Val Loss) = 0.82118881 | Current Learning Rate: 0.0002\n",
      "Epoch 463/1000 | Train Loss=1556.56885875 | Val Loss=0.67717066 | Data=15.54055067 | Physics=2.60034032 | Val RMSE: 0.82129848 | ‚àö(Val Loss) = 0.82290375 | Current Learning Rate: 0.0002\n",
      "Epoch 464/1000 | Train Loss=1546.58617054 | Val Loss=0.68599948 | Data=15.44065059 | Physics=2.62130235 | Val RMSE: 0.82138556 | ‚àö(Val Loss) = 0.82825089 | Current Learning Rate: 0.0002\n",
      "Epoch 465/1000 | Train Loss=1556.74714799 | Val Loss=0.68034399 | Data=15.54236846 | Physics=2.57478367 | Val RMSE: 0.82137603 | ‚àö(Val Loss) = 0.82482970 | Current Learning Rate: 0.0002\n",
      "Epoch 466/1000 | Train Loss=1548.39008123 | Val Loss=0.68488193 | Data=15.45867183 | Physics=2.58349398 | Val RMSE: 0.82145369 | ‚àö(Val Loss) = 0.82757592 | Current Learning Rate: 0.0002\n",
      "Epoch 467/1000 | Train Loss=1551.18098588 | Val Loss=0.66908186 | Data=15.48676066 | Physics=2.61254129 | Val RMSE: 0.82123327 | ‚àö(Val Loss) = 0.81797427 | Current Learning Rate: 0.0002\n",
      "Epoch 468/1000 | Train Loss=1549.05661843 | Val Loss=0.68663539 | Data=15.46533481 | Physics=2.60018223 | Val RMSE: 0.82154101 | ‚àö(Val Loss) = 0.82863462 | Current Learning Rate: 0.0002\n",
      "Epoch 469/1000 | Train Loss=1549.52207253 | Val Loss=0.67643400 | Data=15.47009225 | Physics=2.57639254 | Val RMSE: 0.82137376 | ‚àö(Val Loss) = 0.82245606 | Current Learning Rate: 0.0002\n",
      "Epoch 470/1000 | Train Loss=1550.08189808 | Val Loss=0.67581167 | Data=15.47573462 | Physics=2.55001460 | Val RMSE: 0.82140774 | ‚àö(Val Loss) = 0.82207763 | Current Learning Rate: 0.0002\n",
      "Epoch 471/1000 | Train Loss=1551.39892578 | Val Loss=0.67531423 | Data=15.48886108 | Physics=2.62303456 | Val RMSE: 0.82142645 | ‚àö(Val Loss) = 0.82177508 | Current Learning Rate: 0.0002\n",
      "Epoch 472/1000 | Train Loss=1547.98511852 | Val Loss=0.67605178 | Data=15.45467030 | Physics=2.68352022 | Val RMSE: 0.82145917 | ‚àö(Val Loss) = 0.82222366 | Current Learning Rate: 0.0002\n",
      "Epoch 473/1000 | Train Loss=1548.14166815 | Val Loss=0.67150487 | Data=15.45630507 | Physics=2.60404297 | Val RMSE: 0.82131761 | ‚àö(Val Loss) = 0.81945401 | Current Learning Rate: 0.0002\n",
      "Epoch 474/1000 | Train Loss=1550.48347612 | Val Loss=0.66733466 | Data=15.47973589 | Physics=2.64787626 | Val RMSE: 0.82125932 | ‚àö(Val Loss) = 0.81690556 | Current Learning Rate: 0.0002\n",
      "Epoch 475/1000 | Train Loss=1554.64611816 | Val Loss=0.66613296 | Data=15.52143045 | Physics=2.58047184 | Val RMSE: 0.82128799 | ‚àö(Val Loss) = 0.81616968 | Current Learning Rate: 0.0002\n",
      "Epoch 476/1000 | Train Loss=1551.47242321 | Val Loss=0.67932269 | Data=15.48956654 | Physics=2.62112508 | Val RMSE: 0.82146633 | ‚àö(Val Loss) = 0.82421035 | Current Learning Rate: 0.0002\n",
      "Epoch 477/1000 | Train Loss=1554.48293235 | Val Loss=0.67666471 | Data=15.51971834 | Physics=2.61969364 | Val RMSE: 0.82148397 | ‚àö(Val Loss) = 0.82259631 | Current Learning Rate: 0.0002\n",
      "Epoch 478/1000 | Train Loss=1547.61605558 | Val Loss=0.68609521 | Data=15.45095756 | Physics=2.62635840 | Val RMSE: 0.82166469 | ‚àö(Val Loss) = 0.82830864 | Current Learning Rate: 0.0002\n",
      "Epoch 479/1000 | Train Loss=1554.20094993 | Val Loss=0.67223874 | Data=15.51690258 | Physics=2.58396574 | Val RMSE: 0.82139945 | ‚àö(Val Loss) = 0.81990170 | Current Learning Rate: 0.0002\n",
      "Epoch 480/1000 | Train Loss=1552.10501376 | Val Loss=0.67473134 | Data=15.49594896 | Physics=2.56638715 | Val RMSE: 0.82145584 | ‚àö(Val Loss) = 0.82142031 | Current Learning Rate: 0.0002\n",
      "Epoch 481/1000 | Train Loss=1550.37401234 | Val Loss=0.67907291 | Data=15.47855707 | Physics=2.58660185 | Val RMSE: 0.82156527 | ‚àö(Val Loss) = 0.82405883 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  480 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 482/1000 | Train Loss=1554.65878018 | Val Loss=0.66332860 | Data=15.52159439 | Physics=2.52937336 | Val RMSE: 0.82124650 | ‚àö(Val Loss) = 0.81444985 | Current Learning Rate: 0.0002\n",
      "Epoch 483/1000 | Train Loss=1552.15920188 | Val Loss=0.67950769 | Data=15.49643586 | Physics=2.62681583 | Val RMSE: 0.82156837 | ‚àö(Val Loss) = 0.82432258 | Current Learning Rate: 0.0002\n",
      "Epoch 484/1000 | Train Loss=1557.49013450 | Val Loss=0.67690335 | Data=15.54974287 | Physics=2.64871876 | Val RMSE: 0.82159662 | ‚àö(Val Loss) = 0.82274139 | Current Learning Rate: 0.0002\n",
      "Epoch 485/1000 | Train Loss=1553.92254084 | Val Loss=0.67930155 | Data=15.51404546 | Physics=2.60200282 | Val RMSE: 0.82157445 | ‚àö(Val Loss) = 0.82419753 | Current Learning Rate: 0.0002\n",
      "Epoch 486/1000 | Train Loss=1548.12649814 | Val Loss=0.67230560 | Data=15.45616566 | Physics=2.51838836 | Val RMSE: 0.82147098 | ‚àö(Val Loss) = 0.81994241 | Current Learning Rate: 0.0002\n",
      "Epoch 487/1000 | Train Loss=1551.84952060 | Val Loss=0.67042914 | Data=15.49344618 | Physics=2.53235812 | Val RMSE: 0.82142758 | ‚àö(Val Loss) = 0.81879735 | Current Learning Rate: 0.0002\n",
      "Epoch 488/1000 | Train Loss=1553.99708141 | Val Loss=0.68182364 | Data=15.51478915 | Physics=2.59040175 | Val RMSE: 0.82163870 | ‚àö(Val Loss) = 0.82572615 | Current Learning Rate: 0.0002\n",
      "Epoch 489/1000 | Train Loss=1548.16748047 | Val Loss=0.67818992 | Data=15.45650526 | Physics=2.62761169 | Val RMSE: 0.82156903 | ‚àö(Val Loss) = 0.82352287 | Current Learning Rate: 0.0002\n",
      "Epoch 490/1000 | Train Loss=1556.83972168 | Val Loss=0.66584343 | Data=15.54337909 | Physics=2.54380918 | Val RMSE: 0.82140660 | ‚àö(Val Loss) = 0.81599230 | Current Learning Rate: 0.0002\n",
      "Epoch 491/1000 | Train Loss=1551.57328658 | Val Loss=0.67822545 | Data=15.49057553 | Physics=2.64725004 | Val RMSE: 0.82163495 | ‚àö(Val Loss) = 0.82354444 | Current Learning Rate: 0.0002\n",
      "Epoch 492/1000 | Train Loss=1551.79187012 | Val Loss=0.67373135 | Data=15.49278459 | Physics=2.69370882 | Val RMSE: 0.82157660 | ‚àö(Val Loss) = 0.82081139 | Current Learning Rate: 0.0002\n",
      "Epoch 493/1000 | Train Loss=1549.96416681 | Val Loss=0.68223659 | Data=15.47442696 | Physics=2.62710173 | Val RMSE: 0.82174575 | ‚àö(Val Loss) = 0.82597613 | Current Learning Rate: 0.0002\n",
      "Epoch 494/1000 | Train Loss=1556.28920676 | Val Loss=0.67294570 | Data=15.53782359 | Physics=2.50588674 | Val RMSE: 0.82156610 | ‚àö(Val Loss) = 0.82033265 | Current Learning Rate: 0.0002\n",
      "Epoch 495/1000 | Train Loss=1554.50814542 | Val Loss=0.67600562 | Data=15.51993058 | Physics=2.54862962 | Val RMSE: 0.82166481 | ‚àö(Val Loss) = 0.82219559 | Current Learning Rate: 0.0002\n",
      "Epoch 496/1000 | Train Loss=1553.11698775 | Val Loss=0.67037704 | Data=15.50609545 | Physics=2.58055188 | Val RMSE: 0.82159799 | ‚àö(Val Loss) = 0.81876552 | Current Learning Rate: 0.0002\n",
      "Epoch 497/1000 | Train Loss=1551.34216309 | Val Loss=0.68109525 | Data=15.48826088 | Physics=2.54337159 | Val RMSE: 0.82179916 | ‚àö(Val Loss) = 0.82528496 | Current Learning Rate: 0.0002\n",
      "Epoch 498/1000 | Train Loss=1555.16577148 | Val Loss=0.67221737 | Data=15.52657023 | Physics=2.59409067 | Val RMSE: 0.82163429 | ‚àö(Val Loss) = 0.81988865 | Current Learning Rate: 0.0002\n",
      "Epoch 499/1000 | Train Loss=1550.01302823 | Val Loss=0.67950990 | Data=15.47496024 | Physics=2.60629883 | Val RMSE: 0.82184339 | ‚àö(Val Loss) = 0.82432389 | Current Learning Rate: 0.0002\n",
      "Epoch 500/1000 | Train Loss=1551.41960005 | Val Loss=0.67454682 | Data=15.48905711 | Physics=2.63616255 | Val RMSE: 0.82173121 | ‚àö(Val Loss) = 0.82130802 | Current Learning Rate: 0.0002\n",
      "Epoch 501/1000 | Train Loss=1544.22929244 | Val Loss=0.67844232 | Data=15.41717165 | Physics=2.54074658 | Val RMSE: 0.82179713 | ‚àö(Val Loss) = 0.82367611 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  500 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 502/1000 | Train Loss=1546.75673606 | Val Loss=0.67152023 | Data=15.44247775 | Physics=2.57736678 | Val RMSE: 0.82166666 | ‚àö(Val Loss) = 0.81946337 | Current Learning Rate: 0.0002\n",
      "Epoch 503/1000 | Train Loss=1552.88413308 | Val Loss=0.67056140 | Data=15.50378643 | Physics=2.54094805 | Val RMSE: 0.82163537 | ‚àö(Val Loss) = 0.81887811 | Current Learning Rate: 0.0002\n",
      "Epoch 504/1000 | Train Loss=1548.82775879 | Val Loss=0.67906488 | Data=15.46311569 | Physics=2.53498741 | Val RMSE: 0.82182425 | ‚àö(Val Loss) = 0.82405394 | Current Learning Rate: 0.0002\n",
      "Epoch 505/1000 | Train Loss=1551.41596014 | Val Loss=0.67161362 | Data=15.48907358 | Physics=2.56883724 | Val RMSE: 0.82168859 | ‚àö(Val Loss) = 0.81952035 | Current Learning Rate: 0.0002\n",
      "Epoch 506/1000 | Train Loss=1551.33233088 | Val Loss=0.67761368 | Data=15.48814401 | Physics=2.67065355 | Val RMSE: 0.82181919 | ‚àö(Val Loss) = 0.82317293 | Current Learning Rate: 0.0002\n",
      "Epoch 507/1000 | Train Loss=1555.77744363 | Val Loss=0.66698394 | Data=15.53272889 | Physics=2.56477899 | Val RMSE: 0.82160598 | ‚àö(Val Loss) = 0.81669086 | Current Learning Rate: 0.0002\n",
      "Epoch 508/1000 | Train Loss=1555.56665039 | Val Loss=0.67822475 | Data=15.53056370 | Physics=2.54871803 | Val RMSE: 0.82181567 | ‚àö(Val Loss) = 0.82354403 | Current Learning Rate: 0.0002\n",
      "Epoch 509/1000 | Train Loss=1555.94524592 | Val Loss=0.68356885 | Data=15.53425286 | Physics=2.64468757 | Val RMSE: 0.82194948 | ‚àö(Val Loss) = 0.82678223 | Current Learning Rate: 0.0002\n",
      "Epoch 510/1000 | Train Loss=1548.52650036 | Val Loss=0.68143309 | Data=15.46009688 | Physics=2.60440987 | Val RMSE: 0.82191318 | ‚àö(Val Loss) = 0.82548958 | Current Learning Rate: 0.0002\n",
      "Epoch 511/1000 | Train Loss=1552.04906117 | Val Loss=0.67270095 | Data=15.49538994 | Physics=2.56839223 | Val RMSE: 0.82176149 | ‚àö(Val Loss) = 0.82018346 | Current Learning Rate: 0.0002\n",
      "Epoch 512/1000 | Train Loss=1554.32276500 | Val Loss=0.68304484 | Data=15.51808851 | Physics=2.57062810 | Val RMSE: 0.82203156 | ‚àö(Val Loss) = 0.82646525 | Current Learning Rate: 0.0002\n",
      "Epoch 513/1000 | Train Loss=1546.88264604 | Val Loss=0.68100710 | Data=15.44358540 | Physics=2.66080841 | Val RMSE: 0.82196462 | ‚àö(Val Loss) = 0.82523155 | Current Learning Rate: 0.0002\n",
      "Epoch 514/1000 | Train Loss=1550.33841220 | Val Loss=0.66980597 | Data=15.47828458 | Physics=2.62791235 | Val RMSE: 0.82169390 | ‚àö(Val Loss) = 0.81841671 | Current Learning Rate: 0.0002\n",
      "Epoch 515/1000 | Train Loss=1550.67670233 | Val Loss=0.67076791 | Data=15.48171078 | Physics=2.55717387 | Val RMSE: 0.82176065 | ‚àö(Val Loss) = 0.81900424 | Current Learning Rate: 0.0002\n",
      "Epoch 516/1000 | Train Loss=1555.51427113 | Val Loss=0.68061705 | Data=15.52997043 | Physics=2.65523674 | Val RMSE: 0.82201993 | ‚àö(Val Loss) = 0.82499516 | Current Learning Rate: 0.0002\n",
      "Epoch 517/1000 | Train Loss=1550.86813077 | Val Loss=0.68435120 | Data=15.48347586 | Physics=2.61615101 | Val RMSE: 0.82213867 | ‚àö(Val Loss) = 0.82725525 | Current Learning Rate: 0.0002\n",
      "Epoch 518/1000 | Train Loss=1549.78739790 | Val Loss=0.67320734 | Data=15.47272648 | Physics=2.64468767 | Val RMSE: 0.82186973 | ‚àö(Val Loss) = 0.82049215 | Current Learning Rate: 0.0002\n",
      "Epoch 519/1000 | Train Loss=1553.41158780 | Val Loss=0.67247920 | Data=15.50899289 | Physics=2.67373253 | Val RMSE: 0.82186472 | ‚àö(Val Loss) = 0.82004827 | Current Learning Rate: 0.0002\n",
      "Epoch 520/1000 | Train Loss=1551.10381525 | Val Loss=0.67576797 | Data=15.48590643 | Physics=2.53094782 | Val RMSE: 0.82193494 | ‚àö(Val Loss) = 0.82205105 | Current Learning Rate: 0.0002\n",
      "Epoch 521/1000 | Train Loss=1550.99967818 | Val Loss=0.66381299 | Data=15.48499402 | Physics=2.55545505 | Val RMSE: 0.82166910 | ‚àö(Val Loss) = 0.81474721 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  520 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 522/1000 | Train Loss=1551.85815430 | Val Loss=0.67483070 | Data=15.49344661 | Physics=2.56686436 | Val RMSE: 0.82196498 | ‚àö(Val Loss) = 0.82148081 | Current Learning Rate: 0.0002\n",
      "Epoch 523/1000 | Train Loss=1555.64362127 | Val Loss=0.66493887 | Data=15.53140996 | Physics=2.56617619 | Val RMSE: 0.82172537 | ‚àö(Val Loss) = 0.81543785 | Current Learning Rate: 0.0002\n",
      "Epoch 524/1000 | Train Loss=1551.38514293 | Val Loss=0.67270490 | Data=15.48872974 | Physics=2.64176230 | Val RMSE: 0.82189071 | ‚àö(Val Loss) = 0.82018590 | Current Learning Rate: 0.0002\n",
      "Epoch 525/1000 | Train Loss=1550.47037021 | Val Loss=0.67188668 | Data=15.47956207 | Physics=2.63686280 | Val RMSE: 0.82197416 | ‚àö(Val Loss) = 0.81968695 | Current Learning Rate: 0.0002\n",
      "Epoch 526/1000 | Train Loss=1550.18463690 | Val Loss=0.67114335 | Data=15.47673009 | Physics=2.62229083 | Val RMSE: 0.82197028 | ‚àö(Val Loss) = 0.81923342 | Current Learning Rate: 0.0002\n",
      "Epoch 527/1000 | Train Loss=1548.57591664 | Val Loss=0.67654695 | Data=15.46057701 | Physics=2.61297400 | Val RMSE: 0.82206035 | ‚àö(Val Loss) = 0.82252473 | Current Learning Rate: 0.0002\n",
      "Epoch 528/1000 | Train Loss=1550.41725852 | Val Loss=0.66534486 | Data=15.47909000 | Physics=2.66221383 | Val RMSE: 0.82178634 | ‚àö(Val Loss) = 0.81568670 | Current Learning Rate: 0.0002\n",
      "Epoch 529/1000 | Train Loss=1556.13724032 | Val Loss=0.67724467 | Data=15.53622185 | Physics=2.60577586 | Val RMSE: 0.82206035 | ‚àö(Val Loss) = 0.82294875 | Current Learning Rate: 0.0002\n",
      "Epoch 530/1000 | Train Loss=1549.64377663 | Val Loss=0.67809209 | Data=15.47128192 | Physics=2.62294934 | Val RMSE: 0.82205039 | ‚àö(Val Loss) = 0.82346350 | Current Learning Rate: 0.0002\n",
      "Epoch 531/1000 | Train Loss=1552.67648038 | Val Loss=0.67060618 | Data=15.50169910 | Physics=2.62528195 | Val RMSE: 0.82191569 | ‚àö(Val Loss) = 0.81890547 | Current Learning Rate: 0.0002\n",
      "Epoch 532/1000 | Train Loss=1552.11302601 | Val Loss=0.68076891 | Data=15.49596422 | Physics=2.58683284 | Val RMSE: 0.82217479 | ‚àö(Val Loss) = 0.82508719 | Current Learning Rate: 0.0002\n",
      "Epoch 533/1000 | Train Loss=1549.12213690 | Val Loss=0.67863148 | Data=15.46608682 | Physics=2.56612080 | Val RMSE: 0.82213706 | ‚àö(Val Loss) = 0.82379091 | Current Learning Rate: 0.0002\n",
      "Epoch 534/1000 | Train Loss=1548.98700506 | Val Loss=0.67073921 | Data=15.46480682 | Physics=2.53596480 | Val RMSE: 0.82195681 | ‚àö(Val Loss) = 0.81898671 | Current Learning Rate: 0.0002\n",
      "Epoch 535/1000 | Train Loss=1558.64885920 | Val Loss=0.67376167 | Data=15.56137935 | Physics=2.64674360 | Val RMSE: 0.82206279 | ‚àö(Val Loss) = 0.82082987 | Current Learning Rate: 0.0002\n",
      "Epoch 536/1000 | Train Loss=1555.50032182 | Val Loss=0.68130801 | Data=15.52983891 | Physics=2.66989130 | Val RMSE: 0.82221276 | ‚àö(Val Loss) = 0.82541388 | Current Learning Rate: 0.0002\n",
      "Epoch 537/1000 | Train Loss=1553.68511408 | Val Loss=0.67905683 | Data=15.51171060 | Physics=2.54869413 | Val RMSE: 0.82220721 | ‚àö(Val Loss) = 0.82404906 | Current Learning Rate: 0.0002\n",
      "Epoch 538/1000 | Train Loss=1549.12837358 | Val Loss=0.66905232 | Data=15.46620430 | Physics=2.52879592 | Val RMSE: 0.82195866 | ‚àö(Val Loss) = 0.81795615 | Current Learning Rate: 0.0002\n",
      "Epoch 539/1000 | Train Loss=1556.55780584 | Val Loss=0.66468312 | Data=15.54058769 | Physics=2.50307832 | Val RMSE: 0.82188380 | ‚àö(Val Loss) = 0.81528097 | Current Learning Rate: 0.0002\n",
      "Epoch 540/1000 | Train Loss=1557.63267933 | Val Loss=0.67693587 | Data=15.55117720 | Physics=2.53529729 | Val RMSE: 0.82221609 | ‚àö(Val Loss) = 0.82276112 | Current Learning Rate: 0.0002\n",
      "Epoch 541/1000 | Train Loss=1552.83138761 | Val Loss=0.68114029 | Data=15.50311886 | Physics=2.65423034 | Val RMSE: 0.82229495 | ‚àö(Val Loss) = 0.82531226 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  540 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 542/1000 | Train Loss=1552.99113326 | Val Loss=0.68336866 | Data=15.50470985 | Physics=2.67527100 | Val RMSE: 0.82236624 | ‚àö(Val Loss) = 0.82666117 | Current Learning Rate: 0.0002\n",
      "Epoch 543/1000 | Train Loss=1554.58255282 | Val Loss=0.67992881 | Data=15.52064800 | Physics=2.62152923 | Val RMSE: 0.82231063 | ‚àö(Val Loss) = 0.82457799 | Current Learning Rate: 0.0002\n",
      "Epoch 544/1000 | Train Loss=1549.65765936 | Val Loss=0.67062206 | Data=15.47148947 | Physics=2.59319581 | Val RMSE: 0.82203621 | ‚àö(Val Loss) = 0.81891519 | Current Learning Rate: 0.0002\n",
      "Epoch 545/1000 | Train Loss=1550.19728782 | Val Loss=0.66839064 | Data=15.47691206 | Physics=2.53317716 | Val RMSE: 0.82198882 | ‚àö(Val Loss) = 0.81755161 | Current Learning Rate: 0.0002\n",
      "Epoch 546/1000 | Train Loss=1554.07539506 | Val Loss=0.67398855 | Data=15.51569176 | Physics=2.57465949 | Val RMSE: 0.82211089 | ‚àö(Val Loss) = 0.82096809 | Current Learning Rate: 0.0002\n",
      "Epoch 547/1000 | Train Loss=1557.06722745 | Val Loss=0.67381006 | Data=15.54554445 | Physics=2.58263557 | Val RMSE: 0.82213980 | ‚àö(Val Loss) = 0.82085937 | Current Learning Rate: 0.0002\n",
      "Epoch 548/1000 | Train Loss=1551.89622914 | Val Loss=0.66612675 | Data=15.49391365 | Physics=2.58091688 | Val RMSE: 0.82196772 | ‚àö(Val Loss) = 0.81616586 | Current Learning Rate: 0.0002\n",
      "Epoch 549/1000 | Train Loss=1552.43681197 | Val Loss=0.67948687 | Data=15.49919649 | Physics=2.65206795 | Val RMSE: 0.82234108 | ‚àö(Val Loss) = 0.82430995 | Current Learning Rate: 0.0002\n",
      "Epoch 550/1000 | Train Loss=1552.72683993 | Val Loss=0.68448095 | Data=15.50207069 | Physics=2.56362747 | Val RMSE: 0.82255286 | ‚àö(Val Loss) = 0.82733363 | Current Learning Rate: 0.0002\n",
      "Epoch 551/1000 | Train Loss=1555.89337713 | Val Loss=0.67270133 | Data=15.53383272 | Physics=2.61695961 | Val RMSE: 0.82218951 | ‚àö(Val Loss) = 0.82018375 | Current Learning Rate: 0.0002\n",
      "Epoch 552/1000 | Train Loss=1552.29388983 | Val Loss=0.67734294 | Data=15.49779762 | Physics=2.54818830 | Val RMSE: 0.82237959 | ‚àö(Val Loss) = 0.82300848 | Current Learning Rate: 0.0002\n",
      "Epoch 553/1000 | Train Loss=1555.84922097 | Val Loss=0.66914877 | Data=15.53342464 | Physics=2.58199864 | Val RMSE: 0.82215822 | ‚àö(Val Loss) = 0.81801516 | Current Learning Rate: 0.0002\n",
      "Epoch 554/1000 | Train Loss=1550.14915882 | Val Loss=0.67417511 | Data=15.47637168 | Physics=2.63637734 | Val RMSE: 0.82218206 | ‚àö(Val Loss) = 0.82108170 | Current Learning Rate: 0.0002\n",
      "Epoch 555/1000 | Train Loss=1550.38768422 | Val Loss=0.67019209 | Data=15.47879965 | Physics=2.62258520 | Val RMSE: 0.82213843 | ‚àö(Val Loss) = 0.81865257 | Current Learning Rate: 0.0002\n",
      "Epoch 556/1000 | Train Loss=1550.68665661 | Val Loss=0.67774172 | Data=15.48170723 | Physics=2.62318054 | Val RMSE: 0.82235432 | ‚àö(Val Loss) = 0.82325071 | Current Learning Rate: 0.0002\n",
      "Epoch 557/1000 | Train Loss=1548.76400479 | Val Loss=0.66978826 | Data=15.46256117 | Physics=2.60365278 | Val RMSE: 0.82217151 | ‚àö(Val Loss) = 0.81840593 | Current Learning Rate: 0.0002\n",
      "Epoch 558/1000 | Train Loss=1555.16028942 | Val Loss=0.66933433 | Data=15.52651336 | Physics=2.63441011 | Val RMSE: 0.82217503 | ‚àö(Val Loss) = 0.81812859 | Current Learning Rate: 0.0002\n",
      "Epoch 559/1000 | Train Loss=1546.37908381 | Val Loss=0.67498440 | Data=15.43867692 | Physics=2.50338107 | Val RMSE: 0.82238334 | ‚àö(Val Loss) = 0.82157433 | Current Learning Rate: 0.0002\n",
      "Epoch 560/1000 | Train Loss=1550.15889116 | Val Loss=0.66612571 | Data=15.47657039 | Physics=2.58304909 | Val RMSE: 0.82213026 | ‚àö(Val Loss) = 0.81616527 | Current Learning Rate: 0.0002\n",
      "Epoch 561/1000 | Train Loss=1552.84042081 | Val Loss=0.67571506 | Data=15.50329503 | Physics=2.56417371 | Val RMSE: 0.82233757 | ‚àö(Val Loss) = 0.82201886 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  560 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 562/1000 | Train Loss=1551.10293857 | Val Loss=0.67068758 | Data=15.48598662 | Physics=2.52866715 | Val RMSE: 0.82223028 | ‚àö(Val Loss) = 0.81895518 | Current Learning Rate: 0.0002\n",
      "Epoch 563/1000 | Train Loss=1554.11924050 | Val Loss=0.68058239 | Data=15.51605736 | Physics=2.55771066 | Val RMSE: 0.82251561 | ‚àö(Val Loss) = 0.82497418 | Current Learning Rate: 0.0002\n",
      "Epoch 564/1000 | Train Loss=1549.16967773 | Val Loss=0.68231705 | Data=15.46652412 | Physics=2.54442357 | Val RMSE: 0.82258803 | ‚àö(Val Loss) = 0.82602489 | Current Learning Rate: 0.0002\n",
      "Epoch 565/1000 | Train Loss=1551.32657138 | Val Loss=0.67221170 | Data=15.48819681 | Physics=2.57446510 | Val RMSE: 0.82231539 | ‚àö(Val Loss) = 0.81988519 | Current Learning Rate: 0.0002\n",
      "Epoch 566/1000 | Train Loss=1548.72541948 | Val Loss=0.67482521 | Data=15.46212665 | Physics=2.65014806 | Val RMSE: 0.82239306 | ‚àö(Val Loss) = 0.82147747 | Current Learning Rate: 0.0002\n",
      "Epoch 567/1000 | Train Loss=1549.86548961 | Val Loss=0.66394470 | Data=15.47363255 | Physics=2.60268218 | Val RMSE: 0.82208389 | ‚àö(Val Loss) = 0.81482804 | Current Learning Rate: 0.0002\n",
      "Epoch 568/1000 | Train Loss=1546.00821200 | Val Loss=0.67580861 | Data=15.43494467 | Physics=2.62203062 | Val RMSE: 0.82247633 | ‚àö(Val Loss) = 0.82207578 | Current Learning Rate: 0.0002\n",
      "Epoch 569/1000 | Train Loss=1551.92600320 | Val Loss=0.67063131 | Data=15.49419074 | Physics=2.58351807 | Val RMSE: 0.82231140 | ‚àö(Val Loss) = 0.81892079 | Current Learning Rate: 0.0002\n",
      "Epoch 570/1000 | Train Loss=1548.34217418 | Val Loss=0.67714334 | Data=15.45823886 | Physics=2.61532621 | Val RMSE: 0.82254523 | ‚àö(Val Loss) = 0.82288718 | Current Learning Rate: 0.0002\n",
      "Epoch 571/1000 | Train Loss=1550.16388494 | Val Loss=0.66487946 | Data=15.47662345 | Physics=2.54114337 | Val RMSE: 0.82216412 | ‚àö(Val Loss) = 0.81540138 | Current Learning Rate: 0.0002\n",
      "Epoch 572/1000 | Train Loss=1556.87264737 | Val Loss=0.67538271 | Data=15.54358647 | Physics=2.63326024 | Val RMSE: 0.82251000 | ‚àö(Val Loss) = 0.82181674 | Current Learning Rate: 0.0002\n",
      "Epoch 573/1000 | Train Loss=1551.67592551 | Val Loss=0.67807962 | Data=15.49163116 | Physics=2.56011839 | Val RMSE: 0.82251978 | ‚àö(Val Loss) = 0.82345587 | Current Learning Rate: 0.0002\n",
      "Epoch 574/1000 | Train Loss=1555.56074663 | Val Loss=0.67237199 | Data=15.53052035 | Physics=2.60216505 | Val RMSE: 0.82240260 | ‚àö(Val Loss) = 0.81998289 | Current Learning Rate: 0.0002\n",
      "Epoch 575/1000 | Train Loss=1546.03515625 | Val Loss=0.67650516 | Data=15.43525280 | Physics=2.52351270 | Val RMSE: 0.82255226 | ‚àö(Val Loss) = 0.82249933 | Current Learning Rate: 0.0002\n",
      "Epoch 576/1000 | Train Loss=1561.76609109 | Val Loss=0.66773391 | Data=15.59260004 | Physics=2.57351680 | Val RMSE: 0.82232517 | ‚àö(Val Loss) = 0.81714988 | Current Learning Rate: 0.0002\n",
      "Epoch 577/1000 | Train Loss=1551.51222923 | Val Loss=0.67623743 | Data=15.49001555 | Physics=2.52826347 | Val RMSE: 0.82253641 | ‚àö(Val Loss) = 0.82233661 | Current Learning Rate: 0.0002\n",
      "Epoch 578/1000 | Train Loss=1548.05246804 | Val Loss=0.67077315 | Data=15.45546315 | Physics=2.55091284 | Val RMSE: 0.82241976 | ‚àö(Val Loss) = 0.81900740 | Current Learning Rate: 0.0002\n",
      "Epoch 579/1000 | Train Loss=1552.99856845 | Val Loss=0.66907153 | Data=15.50492460 | Physics=2.61070563 | Val RMSE: 0.82235658 | ‚àö(Val Loss) = 0.81796795 | Current Learning Rate: 0.0002\n",
      "Epoch 580/1000 | Train Loss=1543.12698642 | Val Loss=0.67700727 | Data=15.40611787 | Physics=2.62148963 | Val RMSE: 0.82261455 | ‚àö(Val Loss) = 0.82280451 | Current Learning Rate: 0.0002\n",
      "Epoch 581/1000 | Train Loss=1554.03786399 | Val Loss=0.67010662 | Data=15.51532589 | Physics=2.56179073 | Val RMSE: 0.82244891 | ‚àö(Val Loss) = 0.81860042 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  580 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 582/1000 | Train Loss=1547.32053445 | Val Loss=0.67824203 | Data=15.44803689 | Physics=2.60882063 | Val RMSE: 0.82264638 | ‚àö(Val Loss) = 0.82355452 | Current Learning Rate: 0.0002\n",
      "Epoch 583/1000 | Train Loss=1552.05665172 | Val Loss=0.67126388 | Data=15.49551227 | Physics=2.55534957 | Val RMSE: 0.82245708 | ‚àö(Val Loss) = 0.81930697 | Current Learning Rate: 0.0002\n",
      "Epoch 584/1000 | Train Loss=1550.59418279 | Val Loss=0.67811667 | Data=15.48079447 | Physics=2.54332238 | Val RMSE: 0.82268119 | ‚àö(Val Loss) = 0.82347840 | Current Learning Rate: 0.0002\n",
      "Epoch 585/1000 | Train Loss=1552.21280185 | Val Loss=0.66861220 | Data=15.49707005 | Physics=2.56187189 | Val RMSE: 0.82242107 | ‚àö(Val Loss) = 0.81768709 | Current Learning Rate: 0.0002\n",
      "Epoch 586/1000 | Train Loss=1545.13221325 | Val Loss=0.67726570 | Data=15.42619584 | Physics=2.57541317 | Val RMSE: 0.82264400 | ‚àö(Val Loss) = 0.82296157 | Current Learning Rate: 0.0002\n",
      "Epoch 587/1000 | Train Loss=1550.65932395 | Val Loss=0.67023364 | Data=15.48151762 | Physics=2.63255519 | Val RMSE: 0.82246339 | ‚àö(Val Loss) = 0.81867796 | Current Learning Rate: 0.0002\n",
      "Epoch 588/1000 | Train Loss=1549.17616966 | Val Loss=0.67311835 | Data=15.46663328 | Physics=2.65563582 | Val RMSE: 0.82258368 | ‚àö(Val Loss) = 0.82043791 | Current Learning Rate: 0.0002\n",
      "Epoch 589/1000 | Train Loss=1553.15822532 | Val Loss=0.68397795 | Data=15.50641745 | Physics=2.59502950 | Val RMSE: 0.82291025 | ‚àö(Val Loss) = 0.82702959 | Current Learning Rate: 0.0002\n",
      "Epoch 590/1000 | Train Loss=1551.05236816 | Val Loss=0.68435656 | Data=15.48534125 | Physics=2.60053858 | Val RMSE: 0.82294726 | ‚àö(Val Loss) = 0.82725847 | Current Learning Rate: 0.0002\n",
      "Epoch 591/1000 | Train Loss=1553.50216397 | Val Loss=0.67733591 | Data=15.50987955 | Physics=2.61416997 | Val RMSE: 0.82272220 | ‚àö(Val Loss) = 0.82300419 | Current Learning Rate: 0.0002\n",
      "Epoch 592/1000 | Train Loss=1550.06501909 | Val Loss=0.67561199 | Data=15.47548606 | Physics=2.62184179 | Val RMSE: 0.82261825 | ‚àö(Val Loss) = 0.82195616 | Current Learning Rate: 0.0002\n",
      "Epoch 593/1000 | Train Loss=1553.89344371 | Val Loss=0.66473137 | Data=15.51395607 | Physics=2.52000145 | Val RMSE: 0.82230127 | ‚àö(Val Loss) = 0.81531060 | Current Learning Rate: 0.0002\n",
      "Epoch 594/1000 | Train Loss=1546.66054466 | Val Loss=0.67532887 | Data=15.44151228 | Physics=2.56536583 | Val RMSE: 0.82259971 | ‚àö(Val Loss) = 0.82178396 | Current Learning Rate: 0.0002\n",
      "Epoch 595/1000 | Train Loss=1548.89317738 | Val Loss=0.67005509 | Data=15.46385704 | Physics=2.64539711 | Val RMSE: 0.82251841 | ‚àö(Val Loss) = 0.81856894 | Current Learning Rate: 0.0002\n",
      "Epoch 596/1000 | Train Loss=1554.21245783 | Val Loss=0.67251759 | Data=15.51702959 | Physics=2.54291165 | Val RMSE: 0.82263440 | ‚àö(Val Loss) = 0.82007170 | Current Learning Rate: 0.0002\n",
      "Epoch 597/1000 | Train Loss=1552.79129306 | Val Loss=0.68161602 | Data=15.50275699 | Physics=2.63412714 | Val RMSE: 0.82291067 | ‚àö(Val Loss) = 0.82560039 | Current Learning Rate: 0.0002\n",
      "Epoch 598/1000 | Train Loss=1545.94186124 | Val Loss=0.68196098 | Data=15.43424199 | Physics=2.59212728 | Val RMSE: 0.82295442 | ‚àö(Val Loss) = 0.82580930 | Current Learning Rate: 0.0002\n",
      "Epoch 599/1000 | Train Loss=1552.35715554 | Val Loss=0.66896451 | Data=15.49849588 | Physics=2.55015666 | Val RMSE: 0.82251132 | ‚àö(Val Loss) = 0.81790251 | Current Learning Rate: 0.0002\n",
      "Epoch 600/1000 | Train Loss=1551.45336914 | Val Loss=0.67294323 | Data=15.48944933 | Physics=2.56623299 | Val RMSE: 0.82266372 | ‚àö(Val Loss) = 0.82033116 | Current Learning Rate: 0.0002\n",
      "Epoch 601/1000 | Train Loss=1557.42514870 | Val Loss=0.67894223 | Data=15.54908518 | Physics=2.57310173 | Val RMSE: 0.82288444 | ‚àö(Val Loss) = 0.82397950 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  600 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 602/1000 | Train Loss=2308.21899414 | Val Loss=0.67111955 | Data=23.05708469 | Physics=2.48888696 | Val RMSE: 0.82273257 | ‚àö(Val Loss) = 0.81921887 | Current Learning Rate: 0.0002\n",
      "Epoch 603/1000 | Train Loss=2309.28864080 | Val Loss=0.66275391 | Data=23.06790092 | Physics=2.59342355 | Val RMSE: 0.82247072 | ‚àö(Val Loss) = 0.81409699 | Current Learning Rate: 0.0002\n",
      "Epoch 604/1000 | Train Loss=2311.79543235 | Val Loss=0.67826301 | Data=23.09283239 | Physics=2.53701050 | Val RMSE: 0.82290173 | ‚àö(Val Loss) = 0.82356727 | Current Learning Rate: 0.0002\n",
      "Epoch 605/1000 | Train Loss=2316.51955344 | Val Loss=0.67285646 | Data=23.14010950 | Physics=2.56587537 | Val RMSE: 0.82260263 | ‚àö(Val Loss) = 0.82027829 | Current Learning Rate: 0.0002\n",
      "Epoch 606/1000 | Train Loss=2311.45596591 | Val Loss=0.67944596 | Data=23.08942431 | Physics=2.58324155 | Val RMSE: 0.82277709 | ‚àö(Val Loss) = 0.82428515 | Current Learning Rate: 0.0002\n",
      "Epoch 607/1000 | Train Loss=2320.66084428 | Val Loss=0.67156192 | Data=23.18150746 | Physics=2.62992135 | Val RMSE: 0.82245016 | ‚àö(Val Loss) = 0.81948882 | Current Learning Rate: 0.0002\n",
      "Epoch 608/1000 | Train Loss=2315.12293590 | Val Loss=0.67784228 | Data=23.12609985 | Physics=2.58931699 | Val RMSE: 0.82273090 | ‚àö(Val Loss) = 0.82331175 | Current Learning Rate: 0.0002\n",
      "Epoch 609/1000 | Train Loss=2313.20913974 | Val Loss=0.67345950 | Data=23.10699203 | Physics=2.60597237 | Val RMSE: 0.82250738 | ‚àö(Val Loss) = 0.82064575 | Current Learning Rate: 0.0002\n",
      "Epoch 610/1000 | Train Loss=2315.57308683 | Val Loss=0.67717955 | Data=23.13060674 | Physics=2.53182110 | Val RMSE: 0.82263482 | ‚àö(Val Loss) = 0.82290924 | Current Learning Rate: 0.0002\n",
      "Epoch 611/1000 | Train Loss=2312.56574041 | Val Loss=0.66951608 | Data=23.10061472 | Physics=2.57185261 | Val RMSE: 0.82228428 | ‚àö(Val Loss) = 0.81823963 | Current Learning Rate: 0.0002\n",
      "Epoch 612/1000 | Train Loss=2310.53466797 | Val Loss=0.67672151 | Data=23.08020349 | Physics=2.64264667 | Val RMSE: 0.82264316 | ‚àö(Val Loss) = 0.82263082 | Current Learning Rate: 0.0002\n",
      "Epoch 613/1000 | Train Loss=2313.62309126 | Val Loss=0.66750888 | Data=23.11122270 | Physics=2.52624680 | Val RMSE: 0.82251149 | ‚àö(Val Loss) = 0.81701219 | Current Learning Rate: 0.0002\n",
      "Epoch 614/1000 | Train Loss=2310.69637784 | Val Loss=0.67826426 | Data=23.08181780 | Physics=2.61638964 | Val RMSE: 0.82282925 | ‚àö(Val Loss) = 0.82356799 | Current Learning Rate: 0.0002\n",
      "Epoch 615/1000 | Train Loss=2316.01482599 | Val Loss=0.67475255 | Data=23.13504549 | Physics=2.59442480 | Val RMSE: 0.82284069 | ‚àö(Val Loss) = 0.82143325 | Current Learning Rate: 0.0002\n",
      "Epoch 616/1000 | Train Loss=2316.99602717 | Val Loss=0.68250010 | Data=23.14480036 | Physics=2.56374146 | Val RMSE: 0.82295299 | ‚àö(Val Loss) = 0.82613564 | Current Learning Rate: 0.0002\n",
      "Epoch 617/1000 | Train Loss=2307.94682173 | Val Loss=0.68001105 | Data=23.05434158 | Physics=2.53435284 | Val RMSE: 0.82281774 | ‚àö(Val Loss) = 0.82462782 | Current Learning Rate: 0.0002\n",
      "Epoch 618/1000 | Train Loss=2315.98291016 | Val Loss=0.66662575 | Data=23.13481019 | Physics=2.57054581 | Val RMSE: 0.82239366 | ‚àö(Val Loss) = 0.81647152 | Current Learning Rate: 0.0002\n",
      "Epoch 619/1000 | Train Loss=2315.87142667 | Val Loss=0.67654892 | Data=23.13359434 | Physics=2.58349066 | Val RMSE: 0.82276845 | ‚àö(Val Loss) = 0.82252592 | Current Learning Rate: 0.0002\n",
      "Epoch 620/1000 | Train Loss=2320.49476207 | Val Loss=0.67368328 | Data=23.17983194 | Physics=2.67303624 | Val RMSE: 0.82259727 | ‚àö(Val Loss) = 0.82078212 | Current Learning Rate: 0.0002\n",
      "Epoch 621/1000 | Train Loss=2307.02192827 | Val Loss=0.67639693 | Data=23.04508348 | Physics=2.58731369 | Val RMSE: 0.82276303 | ‚àö(Val Loss) = 0.82243353 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  620 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 622/1000 | Train Loss=2307.52921919 | Val Loss=0.66270903 | Data=23.05028343 | Physics=2.60351340 | Val RMSE: 0.82244784 | ‚àö(Val Loss) = 0.81406945 | Current Learning Rate: 0.0002\n",
      "Epoch 623/1000 | Train Loss=2317.72039240 | Val Loss=0.67510530 | Data=23.15207187 | Physics=2.60532337 | Val RMSE: 0.82308501 | ‚àö(Val Loss) = 0.82164794 | Current Learning Rate: 0.0002\n",
      "Epoch 624/1000 | Train Loss=2312.89852628 | Val Loss=0.67568560 | Data=23.10384872 | Physics=2.52706100 | Val RMSE: 0.82293010 | ‚àö(Val Loss) = 0.82200098 | Current Learning Rate: 0.0002\n",
      "Epoch 625/1000 | Train Loss=2318.70587713 | Val Loss=0.66479843 | Data=23.16205129 | Physics=2.50906947 | Val RMSE: 0.82247972 | ‚àö(Val Loss) = 0.81535172 | Current Learning Rate: 0.0002\n",
      "Epoch 626/1000 | Train Loss=2313.11472390 | Val Loss=0.67601967 | Data=23.10602327 | Physics=2.56576666 | Val RMSE: 0.82291365 | ‚àö(Val Loss) = 0.82220417 | Current Learning Rate: 0.0002\n",
      "Epoch 627/1000 | Train Loss=2318.56309925 | Val Loss=0.66984840 | Data=23.16057639 | Physics=2.58345155 | Val RMSE: 0.82288855 | ‚àö(Val Loss) = 0.81844264 | Current Learning Rate: 0.0002\n",
      "Epoch 628/1000 | Train Loss=2323.87413441 | Val Loss=0.67560212 | Data=23.21365859 | Physics=2.51014529 | Val RMSE: 0.82298321 | ‚àö(Val Loss) = 0.82195020 | Current Learning Rate: 0.0002\n",
      "Epoch 629/1000 | Train Loss=2303.86636630 | Val Loss=0.67711687 | Data=23.01354582 | Physics=2.52946384 | Val RMSE: 0.82301909 | ‚àö(Val Loss) = 0.82287109 | Current Learning Rate: 0.0002\n",
      "Epoch 630/1000 | Train Loss=2308.24032315 | Val Loss=0.66832325 | Data=23.05734999 | Physics=2.53430999 | Val RMSE: 0.82278675 | ‚àö(Val Loss) = 0.81751037 | Current Learning Rate: 0.0002\n",
      "Epoch 631/1000 | Train Loss=2318.85031960 | Val Loss=0.67301693 | Data=23.16342128 | Physics=2.60127309 | Val RMSE: 0.82303071 | ‚àö(Val Loss) = 0.82037610 | Current Learning Rate: 0.0002\n",
      "Epoch 632/1000 | Train Loss=2311.12038352 | Val Loss=0.67826794 | Data=23.08605177 | Physics=2.55724393 | Val RMSE: 0.82307661 | ‚àö(Val Loss) = 0.82357025 | Current Learning Rate: 0.0002\n",
      "Epoch 633/1000 | Train Loss=2310.74192116 | Val Loss=0.66408720 | Data=23.08243283 | Physics=2.52241978 | Val RMSE: 0.82247353 | ‚àö(Val Loss) = 0.81491542 | Current Learning Rate: 0.0002\n",
      "Epoch 634/1000 | Train Loss=2313.68812145 | Val Loss=0.68108574 | Data=23.11171514 | Physics=2.66136530 | Val RMSE: 0.82319325 | ‚àö(Val Loss) = 0.82527918 | Current Learning Rate: 0.0002\n",
      "Epoch 635/1000 | Train Loss=2316.34752308 | Val Loss=0.67617437 | Data=23.13835265 | Physics=2.61591952 | Val RMSE: 0.82300478 | ‚àö(Val Loss) = 0.82229823 | Current Learning Rate: 0.0002\n",
      "Epoch 636/1000 | Train Loss=2315.67762340 | Val Loss=0.68580490 | Data=23.13167295 | Physics=2.48472361 | Val RMSE: 0.82333827 | ‚àö(Val Loss) = 0.82813340 | Current Learning Rate: 0.0002\n",
      "Epoch 637/1000 | Train Loss=2313.89546342 | Val Loss=0.68434704 | Data=23.11375843 | Physics=2.64486507 | Val RMSE: 0.82318848 | ‚àö(Val Loss) = 0.82725269 | Current Learning Rate: 0.0002\n",
      "Epoch 638/1000 | Train Loss=2310.33562678 | Val Loss=0.67506268 | Data=23.07828886 | Physics=2.53136588 | Val RMSE: 0.82283807 | ‚àö(Val Loss) = 0.82162195 | Current Learning Rate: 0.0002\n",
      "Epoch 639/1000 | Train Loss=2311.90498491 | Val Loss=0.67550707 | Data=23.09395547 | Physics=2.57836214 | Val RMSE: 0.82278883 | ‚àö(Val Loss) = 0.82189238 | Current Learning Rate: 0.0002\n",
      "Epoch 640/1000 | Train Loss=2308.18801048 | Val Loss=0.68666059 | Data=23.05667114 | Physics=2.56872155 | Val RMSE: 0.82321399 | ‚àö(Val Loss) = 0.82864988 | Current Learning Rate: 0.0002\n",
      "Epoch 641/1000 | Train Loss=2309.38647461 | Val Loss=0.67442648 | Data=23.06878211 | Physics=2.52582923 | Val RMSE: 0.82293981 | ‚àö(Val Loss) = 0.82123476 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  640 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 642/1000 | Train Loss=2309.87921697 | Val Loss=0.67182242 | Data=23.07372596 | Physics=2.51441022 | Val RMSE: 0.82289279 | ‚àö(Val Loss) = 0.81964773 | Current Learning Rate: 0.0002\n",
      "Epoch 643/1000 | Train Loss=2310.40875799 | Val Loss=0.67175429 | Data=23.07898157 | Physics=2.68497129 | Val RMSE: 0.82295370 | ‚àö(Val Loss) = 0.81960618 | Current Learning Rate: 0.0002\n",
      "Epoch 644/1000 | Train Loss=2315.59069824 | Val Loss=0.67491546 | Data=23.13081637 | Physics=2.62174531 | Val RMSE: 0.82315284 | ‚àö(Val Loss) = 0.82153237 | Current Learning Rate: 0.0002\n",
      "Epoch 645/1000 | Train Loss=2308.95871804 | Val Loss=0.68227793 | Data=23.06443683 | Physics=2.60196664 | Val RMSE: 0.82321507 | ‚àö(Val Loss) = 0.82600117 | Current Learning Rate: 0.0002\n",
      "Epoch 646/1000 | Train Loss=2309.52889737 | Val Loss=0.67466557 | Data=23.07016650 | Physics=2.59051552 | Val RMSE: 0.82298613 | ‚àö(Val Loss) = 0.82138026 | Current Learning Rate: 0.0002\n",
      "Epoch 647/1000 | Train Loss=2304.63927113 | Val Loss=0.67177665 | Data=23.02132468 | Physics=2.56135355 | Val RMSE: 0.82297045 | ‚àö(Val Loss) = 0.81961983 | Current Learning Rate: 0.0002\n",
      "Epoch 648/1000 | Train Loss=2318.06145685 | Val Loss=0.67359516 | Data=23.15552555 | Physics=2.59424638 | Val RMSE: 0.82308733 | ‚àö(Val Loss) = 0.82072842 | Current Learning Rate: 0.0002\n",
      "Epoch 649/1000 | Train Loss=2309.29869496 | Val Loss=0.68026135 | Data=23.06783433 | Physics=2.54835172 | Val RMSE: 0.82332432 | ‚àö(Val Loss) = 0.82477957 | Current Learning Rate: 0.0002\n",
      "Epoch 650/1000 | Train Loss=2301.80192427 | Val Loss=0.66348691 | Data=22.99297680 | Physics=2.66577635 | Val RMSE: 0.82266724 | ‚àö(Val Loss) = 0.81454706 | Current Learning Rate: 0.0002\n",
      "Epoch 651/1000 | Train Loss=2314.42842241 | Val Loss=0.67125120 | Data=23.11921258 | Physics=2.58906437 | Val RMSE: 0.82306296 | ‚àö(Val Loss) = 0.81929922 | Current Learning Rate: 0.0002\n",
      "Epoch 652/1000 | Train Loss=2311.64046964 | Val Loss=0.68302748 | Data=23.09125103 | Physics=2.58511854 | Val RMSE: 0.82339329 | ‚àö(Val Loss) = 0.82645476 | Current Learning Rate: 0.0002\n",
      "Epoch 653/1000 | Train Loss=2313.37455611 | Val Loss=0.67252839 | Data=23.10866477 | Physics=2.55122768 | Val RMSE: 0.82296717 | ‚àö(Val Loss) = 0.82007825 | Current Learning Rate: 0.0002\n",
      "Epoch 654/1000 | Train Loss=2310.21810636 | Val Loss=0.67475791 | Data=23.07707388 | Physics=2.59173835 | Val RMSE: 0.82305551 | ‚àö(Val Loss) = 0.82143646 | Current Learning Rate: 0.0002\n",
      "Epoch 655/1000 | Train Loss=2317.06203391 | Val Loss=0.67639549 | Data=23.14548094 | Physics=2.59763697 | Val RMSE: 0.82290393 | ‚àö(Val Loss) = 0.82243264 | Current Learning Rate: 0.0002\n",
      "Epoch 656/1000 | Train Loss=2307.23086825 | Val Loss=0.67688749 | Data=23.04719821 | Physics=2.56307867 | Val RMSE: 0.82308418 | ‚àö(Val Loss) = 0.82273173 | Current Learning Rate: 0.0002\n",
      "Epoch 657/1000 | Train Loss=2312.56403143 | Val Loss=0.67573793 | Data=23.10055455 | Physics=2.56816008 | Val RMSE: 0.82307529 | ‚àö(Val Loss) = 0.82203281 | Current Learning Rate: 0.0002\n",
      "Epoch 658/1000 | Train Loss=2317.42240767 | Val Loss=0.68250979 | Data=23.14907455 | Physics=2.53757660 | Val RMSE: 0.82302821 | ‚àö(Val Loss) = 0.82614148 | Current Learning Rate: 0.0002\n",
      "Epoch 659/1000 | Train Loss=2307.56964666 | Val Loss=0.68520298 | Data=23.05053208 | Physics=2.63975285 | Val RMSE: 0.82324308 | ‚àö(Val Loss) = 0.82776988 | Current Learning Rate: 0.0002\n",
      "Epoch 660/1000 | Train Loss=2311.17709073 | Val Loss=0.67649851 | Data=23.08662779 | Physics=2.62100659 | Val RMSE: 0.82315916 | ‚àö(Val Loss) = 0.82249528 | Current Learning Rate: 0.0002\n",
      "Epoch 661/1000 | Train Loss=2317.84801136 | Val Loss=0.67560004 | Data=23.15339175 | Physics=2.58317666 | Val RMSE: 0.82319987 | ‚àö(Val Loss) = 0.82194895 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  660 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 662/1000 | Train Loss=2314.64322177 | Val Loss=0.68278285 | Data=23.12124599 | Physics=2.66168194 | Val RMSE: 0.82344043 | ‚àö(Val Loss) = 0.82630676 | Current Learning Rate: 0.0002\n",
      "Epoch 663/1000 | Train Loss=2307.58573775 | Val Loss=0.67538049 | Data=23.05072680 | Physics=2.63848075 | Val RMSE: 0.82331300 | ‚àö(Val Loss) = 0.82181537 | Current Learning Rate: 0.0002\n",
      "Epoch 664/1000 | Train Loss=2312.30075906 | Val Loss=0.67691788 | Data=23.09788513 | Physics=2.63391437 | Val RMSE: 0.82339072 | ‚àö(Val Loss) = 0.82275021 | Current Learning Rate: 0.0002\n",
      "Epoch 665/1000 | Train Loss=2309.25565962 | Val Loss=0.68667533 | Data=23.06732889 | Physics=2.61046617 | Val RMSE: 0.82362467 | ‚àö(Val Loss) = 0.82865876 | Current Learning Rate: 0.0002\n",
      "Epoch 666/1000 | Train Loss=2311.86534535 | Val Loss=0.67601794 | Data=23.09355961 | Physics=2.58168516 | Val RMSE: 0.82317042 | ‚àö(Val Loss) = 0.82220310 | Current Learning Rate: 0.0002\n",
      "Epoch 667/1000 | Train Loss=2315.49083363 | Val Loss=0.68398874 | Data=23.12972728 | Physics=2.59847328 | Val RMSE: 0.82325792 | ‚àö(Val Loss) = 0.82703614 | Current Learning Rate: 0.0002\n",
      "Epoch 668/1000 | Train Loss=2310.17416104 | Val Loss=0.68036414 | Data=23.07654953 | Physics=2.60605315 | Val RMSE: 0.82310545 | ‚àö(Val Loss) = 0.82484186 | Current Learning Rate: 0.0002\n",
      "Epoch 669/1000 | Train Loss=2313.57350852 | Val Loss=0.67164325 | Data=23.11067182 | Physics=2.57911075 | Val RMSE: 0.82304126 | ‚àö(Val Loss) = 0.81953841 | Current Learning Rate: 0.0002\n",
      "Epoch 670/1000 | Train Loss=2311.92977628 | Val Loss=0.68644000 | Data=23.09406281 | Physics=2.63397528 | Val RMSE: 0.82383370 | ‚àö(Val Loss) = 0.82851672 | Current Learning Rate: 0.0002\n",
      "Epoch 671/1000 | Train Loss=2319.11618874 | Val Loss=0.67739259 | Data=23.16607926 | Physics=2.54617725 | Val RMSE: 0.82327056 | ‚àö(Val Loss) = 0.82303864 | Current Learning Rate: 0.0002\n",
      "Epoch 672/1000 | Train Loss=2314.23936879 | Val Loss=0.68024557 | Data=23.11723883 | Physics=2.56330012 | Val RMSE: 0.82329035 | ‚àö(Val Loss) = 0.82477003 | Current Learning Rate: 0.0002\n",
      "Epoch 673/1000 | Train Loss=2311.19200550 | Val Loss=0.66848140 | Data=23.08691788 | Physics=2.53109777 | Val RMSE: 0.82271880 | ‚àö(Val Loss) = 0.81760710 | Current Learning Rate: 0.0002\n",
      "Epoch 674/1000 | Train Loss=2308.54478871 | Val Loss=0.68337228 | Data=23.06027360 | Physics=2.61203702 | Val RMSE: 0.82337362 | ‚àö(Val Loss) = 0.82666332 | Current Learning Rate: 0.0002\n",
      "Epoch 675/1000 | Train Loss=2311.71297940 | Val Loss=0.67513101 | Data=23.09201535 | Physics=2.57124284 | Val RMSE: 0.82318044 | ‚àö(Val Loss) = 0.82166356 | Current Learning Rate: 0.0002\n",
      "Epoch 676/1000 | Train Loss=2318.16961115 | Val Loss=0.67816743 | Data=23.15660962 | Physics=2.47985837 | Val RMSE: 0.82347530 | ‚àö(Val Loss) = 0.82350922 | Current Learning Rate: 0.0002\n",
      "Epoch 677/1000 | Train Loss=2314.86410245 | Val Loss=0.68256732 | Data=23.12343545 | Physics=2.65121975 | Val RMSE: 0.82352149 | ‚àö(Val Loss) = 0.82617629 | Current Learning Rate: 0.0002\n",
      "Epoch 678/1000 | Train Loss=2317.49873491 | Val Loss=0.66682447 | Data=23.14994292 | Physics=2.59031259 | Val RMSE: 0.82298130 | ‚àö(Val Loss) = 0.81659323 | Current Learning Rate: 0.0002\n",
      "Epoch 679/1000 | Train Loss=2307.44795366 | Val Loss=0.67860379 | Data=23.04932820 | Physics=2.59517227 | Val RMSE: 0.82341802 | ‚àö(Val Loss) = 0.82377410 | Current Learning Rate: 0.0002\n",
      "Epoch 680/1000 | Train Loss=2311.34410511 | Val Loss=0.66727993 | Data=23.08842121 | Physics=2.60317883 | Val RMSE: 0.82302982 | ‚àö(Val Loss) = 0.81687206 | Current Learning Rate: 0.0002\n",
      "Epoch 681/1000 | Train Loss=2309.68421520 | Val Loss=0.68070315 | Data=23.07167834 | Physics=2.58482426 | Val RMSE: 0.82356071 | ‚àö(Val Loss) = 0.82504737 | Current Learning Rate: 0.0002\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  680 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 682/1000 | Train Loss=2318.84044300 | Val Loss=0.66840712 | Data=23.16338435 | Physics=2.55684786 | Val RMSE: 0.82314831 | ‚àö(Val Loss) = 0.81756169 | Current Learning Rate: 0.0002\n",
      "Epoch 683/1000 | Train Loss=2312.52294922 | Val Loss=0.68336336 | Data=23.10005379 | Physics=2.57945534 | Val RMSE: 0.82358170 | ‚àö(Val Loss) = 0.82665795 | Current Learning Rate: 0.0002\n",
      "Epoch 684/1000 | Train Loss=2304.47837136 | Val Loss=0.67231421 | Data=23.01969216 | Physics=2.62931859 | Val RMSE: 0.82296145 | ‚àö(Val Loss) = 0.81994772 | Current Learning Rate: 0.0002\n",
      "Epoch 685/1000 | Train Loss=2317.55820534 | Val Loss=0.67653750 | Data=23.15050333 | Physics=2.54686662 | Val RMSE: 0.82337773 | ‚àö(Val Loss) = 0.82251900 | Current Learning Rate: 0.0002\n",
      "Epoch 686/1000 | Train Loss=2311.42702415 | Val Loss=0.68691308 | Data=23.08907994 | Physics=2.60248237 | Val RMSE: 0.82360715 | ‚àö(Val Loss) = 0.82880217 | Current Learning Rate: 0.0002\n",
      "Epoch 687/1000 | Train Loss=2320.26997514 | Val Loss=0.67603738 | Data=23.17762531 | Physics=2.54178438 | Val RMSE: 0.82286870 | ‚àö(Val Loss) = 0.82221490 | Current Learning Rate: 0.0002\n",
      "Epoch 688/1000 | Train Loss=2312.29600941 | Val Loss=0.68508395 | Data=23.09778664 | Physics=2.55137418 | Val RMSE: 0.82337147 | ‚àö(Val Loss) = 0.82769799 | Current Learning Rate: 0.0002\n",
      "Epoch 689/1000 | Train Loss=2310.36163885 | Val Loss=0.66690612 | Data=23.07860635 | Physics=2.44273277 | Val RMSE: 0.82205343 | ‚àö(Val Loss) = 0.81664318 | Current Learning Rate: 0.0002\n",
      "Epoch 690/1000 | Train Loss=2308.45083896 | Val Loss=0.66788892 | Data=23.05948223 | Physics=2.58097478 | Val RMSE: 0.82285243 | ‚àö(Val Loss) = 0.81724471 | Current Learning Rate: 0.0002\n",
      "Epoch 691/1000 | Train Loss=2315.54059393 | Val Loss=0.68652658 | Data=23.13020463 | Physics=2.63945124 | Val RMSE: 0.82382488 | ‚àö(Val Loss) = 0.82856899 | Current Learning Rate: 0.0002\n",
      "Epoch 692/1000 | Train Loss=2315.16530540 | Val Loss=0.68668802 | Data=23.12650091 | Physics=2.56118082 | Val RMSE: 0.82359469 | ‚àö(Val Loss) = 0.82866639 | Current Learning Rate: 0.0002\n",
      "Epoch 693/1000 | Train Loss=2312.73510742 | Val Loss=0.68643473 | Data=23.10213020 | Physics=2.70939771 | Val RMSE: 0.82369304 | ‚àö(Val Loss) = 0.82851356 | Current Learning Rate: 0.0002\n",
      "Epoch 694/1000 | Train Loss=2310.11636630 | Val Loss=0.67674003 | Data=23.07602778 | Physics=2.63633388 | Val RMSE: 0.82338578 | ‚àö(Val Loss) = 0.82264209 | Current Learning Rate: 0.0002\n",
      "Epoch 695/1000 | Train Loss=2308.78275923 | Val Loss=0.67674351 | Data=23.06269230 | Physics=2.67898852 | Val RMSE: 0.82351863 | ‚àö(Val Loss) = 0.82264423 | Current Learning Rate: 0.0002\n",
      "Epoch 696/1000 | Train Loss=2310.83833452 | Val Loss=0.67827991 | Data=23.08324311 | Physics=2.60054270 | Val RMSE: 0.82367098 | ‚àö(Val Loss) = 0.82357752 | Current Learning Rate: 0.0002\n",
      "Epoch 697/1000 | Train Loss=2317.78056197 | Val Loss=0.67778451 | Data=23.15268933 | Physics=2.65874863 | Val RMSE: 0.82344466 | ‚àö(Val Loss) = 0.82327670 | Current Learning Rate: 0.0002\n",
      "Epoch 698/1000 | Train Loss=2309.64837092 | Val Loss=0.67790513 | Data=23.07131178 | Physics=2.59506180 | Val RMSE: 0.82348752 | ‚àö(Val Loss) = 0.82334995 | Current Learning Rate: 0.0002\n",
      "Epoch 699/1000 | Train Loss=2306.53879616 | Val Loss=0.65879144 | Data=23.04044307 | Physics=2.55366539 | Val RMSE: 0.82262266 | ‚àö(Val Loss) = 0.81165969 | Current Learning Rate: 0.0002\n",
      "Epoch 700/1000 | Train Loss=2313.92106490 | Val Loss=0.67737186 | Data=23.11412083 | Physics=2.53068197 | Val RMSE: 0.82367837 | ‚àö(Val Loss) = 0.82302606 | Current Learning Rate: 0.0002\n",
      "Epoch 701/1000 | Train Loss=2320.72183505 | Val Loss=0.68429796 | Data=23.18202712 | Physics=2.65556873 | Val RMSE: 0.82383192 | ‚àö(Val Loss) = 0.82722306 | Current Learning Rate: 0.0002\n",
      "‚úÖ Learning Rate updated to 0.001\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  700 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 702/1000 | Train Loss=2314.28262607 | Val Loss=0.72451776 | Data=23.11576705 | Physics=2.76257365 | Val RMSE: 0.82623035 | ‚àö(Val Loss) = 0.85118610 | Current Learning Rate: 0.001\n",
      "Epoch 703/1000 | Train Loss=2315.39120206 | Val Loss=0.70532501 | Data=23.12890764 | Physics=2.63478553 | Val RMSE: 0.82447064 | ‚àö(Val Loss) = 0.83983630 | Current Learning Rate: 0.001\n",
      "Epoch 704/1000 | Train Loss=2311.05664062 | Val Loss=0.71969214 | Data=23.08426562 | Physics=2.76190309 | Val RMSE: 0.82433754 | ‚àö(Val Loss) = 0.84834671 | Current Learning Rate: 0.001\n",
      "Epoch 705/1000 | Train Loss=2315.63425515 | Val Loss=0.72314989 | Data=23.13107733 | Physics=2.76588192 | Val RMSE: 0.82453549 | ‚àö(Val Loss) = 0.85038221 | Current Learning Rate: 0.001\n",
      "Epoch 706/1000 | Train Loss=2315.99752530 | Val Loss=0.70889979 | Data=23.13398118 | Physics=2.64350206 | Val RMSE: 0.82414174 | ‚àö(Val Loss) = 0.84196186 | Current Learning Rate: 0.001\n",
      "Epoch 707/1000 | Train Loss=2309.36177202 | Val Loss=0.72887206 | Data=23.06839804 | Physics=2.71783747 | Val RMSE: 0.82461989 | ‚àö(Val Loss) = 0.85374004 | Current Learning Rate: 0.001\n",
      "Epoch 708/1000 | Train Loss=2304.15751509 | Val Loss=0.72227820 | Data=23.01544606 | Physics=2.67791553 | Val RMSE: 0.82506710 | ‚àö(Val Loss) = 0.84986949 | Current Learning Rate: 0.001\n",
      "Epoch 709/1000 | Train Loss=2309.85147372 | Val Loss=0.73625160 | Data=23.07319641 | Physics=2.68804661 | Val RMSE: 0.82521254 | ‚àö(Val Loss) = 0.85805106 | Current Learning Rate: 0.001\n",
      "Epoch 710/1000 | Train Loss=2309.78688743 | Val Loss=0.72157194 | Data=23.07172151 | Physics=2.73212256 | Val RMSE: 0.82526112 | ‚àö(Val Loss) = 0.84945393 | Current Learning Rate: 0.001\n",
      "Epoch 711/1000 | Train Loss=2318.91903409 | Val Loss=0.69979576 | Data=23.16394893 | Physics=2.62739821 | Val RMSE: 0.82306254 | ‚àö(Val Loss) = 0.83653796 | Current Learning Rate: 0.001\n",
      "Epoch 712/1000 | Train Loss=2308.98015803 | Val Loss=0.72698978 | Data=23.06413460 | Physics=2.77185931 | Val RMSE: 0.81937444 | ‚àö(Val Loss) = 0.85263699 | Current Learning Rate: 0.001\n",
      "Epoch 713/1000 | Train Loss=2309.52878640 | Val Loss=0.70962612 | Data=23.06955754 | Physics=2.76543185 | Val RMSE: 0.82540703 | ‚àö(Val Loss) = 0.84239310 | Current Learning Rate: 0.001\n",
      "Epoch 714/1000 | Train Loss=2323.67733487 | Val Loss=0.74972510 | Data=23.21110171 | Physics=2.73605436 | Val RMSE: 0.82763988 | ‚àö(Val Loss) = 0.86586666 | Current Learning Rate: 0.001\n",
      "Epoch 715/1000 | Train Loss=2306.89524148 | Val Loss=0.71455450 | Data=23.04302805 | Physics=2.72850263 | Val RMSE: 0.82344455 | ‚àö(Val Loss) = 0.84531325 | Current Learning Rate: 0.001\n",
      "Epoch 716/1000 | Train Loss=2309.41242010 | Val Loss=0.73248084 | Data=23.06881645 | Physics=2.67675099 | Val RMSE: 0.82477570 | ‚àö(Val Loss) = 0.85585093 | Current Learning Rate: 0.001\n",
      "Epoch 717/1000 | Train Loss=2308.34104226 | Val Loss=0.71297224 | Data=23.05745558 | Physics=2.67811304 | Val RMSE: 0.82446027 | ‚àö(Val Loss) = 0.84437680 | Current Learning Rate: 0.001\n",
      "Epoch 718/1000 | Train Loss=2308.47467596 | Val Loss=0.70745759 | Data=23.05947234 | Physics=2.69201864 | Val RMSE: 0.82469976 | ‚àö(Val Loss) = 0.84110498 | Current Learning Rate: 0.001\n",
      "Epoch 719/1000 | Train Loss=2315.08420632 | Val Loss=0.72273187 | Data=23.12498127 | Physics=2.68945181 | Val RMSE: 0.82526505 | ‚àö(Val Loss) = 0.85013640 | Current Learning Rate: 0.001\n",
      "Epoch 720/1000 | Train Loss=2313.90793679 | Val Loss=0.73673307 | Data=23.11350874 | Physics=2.77939942 | Val RMSE: 0.82521528 | ‚àö(Val Loss) = 0.85833156 | Current Learning Rate: 0.001\n",
      "Epoch 721/1000 | Train Loss=2309.73501864 | Val Loss=0.68311377 | Data=23.07162285 | Physics=2.63365263 | Val RMSE: 0.82342416 | ‚àö(Val Loss) = 0.82650697 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  720 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 722/1000 | Train Loss=2314.50632546 | Val Loss=0.73174275 | Data=23.11986611 | Physics=2.65951270 | Val RMSE: 0.82370925 | ‚àö(Val Loss) = 0.85541964 | Current Learning Rate: 0.001\n",
      "Epoch 723/1000 | Train Loss=2313.87480025 | Val Loss=0.70000242 | Data=23.11276141 | Physics=2.64617319 | Val RMSE: 0.82252824 | ‚àö(Val Loss) = 0.83666146 | Current Learning Rate: 0.001\n",
      "Epoch 724/1000 | Train Loss=2315.23872514 | Val Loss=0.73820439 | Data=23.12719987 | Physics=2.65815481 | Val RMSE: 0.82500529 | ‚àö(Val Loss) = 0.85918826 | Current Learning Rate: 0.001\n",
      "Epoch 725/1000 | Train Loss=2308.84348366 | Val Loss=0.66832622 | Data=23.06259988 | Physics=2.56558273 | Val RMSE: 0.82086885 | ‚àö(Val Loss) = 0.81751221 | Current Learning Rate: 0.001\n",
      "Epoch 726/1000 | Train Loss=2313.88438832 | Val Loss=0.74299809 | Data=23.11364885 | Physics=2.67852433 | Val RMSE: 0.81934297 | ‚àö(Val Loss) = 0.86197335 | Current Learning Rate: 0.001\n",
      "Epoch 727/1000 | Train Loss=2307.83438388 | Val Loss=0.68560074 | Data=23.05237129 | Physics=2.63873265 | Val RMSE: 0.82229298 | ‚àö(Val Loss) = 0.82801014 | Current Learning Rate: 0.001\n",
      "Epoch 728/1000 | Train Loss=2326.25470526 | Val Loss=0.72066243 | Data=23.23738185 | Physics=2.64077431 | Val RMSE: 0.82471657 | ‚àö(Val Loss) = 0.84891838 | Current Learning Rate: 0.001\n",
      "Epoch 729/1000 | Train Loss=2315.30881570 | Val Loss=0.70170639 | Data=23.12700636 | Physics=2.63628713 | Val RMSE: 0.82319480 | ‚àö(Val Loss) = 0.83767921 | Current Learning Rate: 0.001\n",
      "Epoch 730/1000 | Train Loss=2313.91776900 | Val Loss=0.71114376 | Data=23.11405719 | Physics=2.64974002 | Val RMSE: 0.82467854 | ‚àö(Val Loss) = 0.84329337 | Current Learning Rate: 0.001\n",
      "Epoch 731/1000 | Train Loss=2306.25175337 | Val Loss=0.69819465 | Data=23.03646903 | Physics=2.62536404 | Val RMSE: 0.82336485 | ‚àö(Val Loss) = 0.83558041 | Current Learning Rate: 0.001\n",
      "Epoch 732/1000 | Train Loss=2308.73465243 | Val Loss=0.72406849 | Data=23.06225204 | Physics=2.60417075 | Val RMSE: 0.82225269 | ‚àö(Val Loss) = 0.85092211 | Current Learning Rate: 0.001\n",
      "Epoch 733/1000 | Train Loss=2300.65552868 | Val Loss=0.69047384 | Data=22.98061596 | Physics=2.70934294 | Val RMSE: 0.82010520 | ‚àö(Val Loss) = 0.83094758 | Current Learning Rate: 0.001\n",
      "Epoch 734/1000 | Train Loss=2311.99955611 | Val Loss=0.71868666 | Data=23.09483181 | Physics=2.61584768 | Val RMSE: 0.82427084 | ‚àö(Val Loss) = 0.84775388 | Current Learning Rate: 0.001\n",
      "Epoch 735/1000 | Train Loss=2312.82388583 | Val Loss=0.72181765 | Data=23.10210991 | Physics=2.81044384 | Val RMSE: 0.82549918 | ‚àö(Val Loss) = 0.84959853 | Current Learning Rate: 0.001\n",
      "Epoch 736/1000 | Train Loss=2315.12586559 | Val Loss=0.70454879 | Data=23.12595836 | Physics=2.66386052 | Val RMSE: 0.82459205 | ‚àö(Val Loss) = 0.83937407 | Current Learning Rate: 0.001\n",
      "Epoch 737/1000 | Train Loss=2311.54157049 | Val Loss=0.70932580 | Data=23.08974249 | Physics=2.68658517 | Val RMSE: 0.82449329 | ‚àö(Val Loss) = 0.84221482 | Current Learning Rate: 0.001\n",
      "Epoch 738/1000 | Train Loss=2310.80866033 | Val Loss=0.70430146 | Data=23.08264230 | Physics=2.62094119 | Val RMSE: 0.82546568 | ‚àö(Val Loss) = 0.83922672 | Current Learning Rate: 0.001\n",
      "Epoch 739/1000 | Train Loss=2311.59905451 | Val Loss=0.72606942 | Data=23.09029926 | Physics=2.68751559 | Val RMSE: 0.82574147 | ‚àö(Val Loss) = 0.85209709 | Current Learning Rate: 0.001\n",
      "Epoch 740/1000 | Train Loss=2311.14321067 | Val Loss=0.69296886 | Data=23.08588271 | Physics=2.69806302 | Val RMSE: 0.82501113 | ‚àö(Val Loss) = 0.83244753 | Current Learning Rate: 0.001\n",
      "Epoch 741/1000 | Train Loss=2317.90498491 | Val Loss=0.72635922 | Data=23.15358734 | Physics=2.68952195 | Val RMSE: 0.82546675 | ‚àö(Val Loss) = 0.85226715 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  740 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 742/1000 | Train Loss=2312.72154652 | Val Loss=0.70795489 | Data=23.10141719 | Physics=2.67331966 | Val RMSE: 0.82615721 | ‚àö(Val Loss) = 0.84140056 | Current Learning Rate: 0.001\n",
      "Epoch 743/1000 | Train Loss=2321.25341797 | Val Loss=0.72120478 | Data=23.18715980 | Physics=2.67224731 | Val RMSE: 0.82522929 | ‚àö(Val Loss) = 0.84923774 | Current Learning Rate: 0.001\n",
      "Epoch 744/1000 | Train Loss=2315.01471502 | Val Loss=0.67989243 | Data=23.12456894 | Physics=2.60908743 | Val RMSE: 0.82102382 | ‚àö(Val Loss) = 0.82455587 | Current Learning Rate: 0.001\n",
      "Epoch 745/1000 | Train Loss=2314.09275124 | Val Loss=0.70665915 | Data=23.11541419 | Physics=2.66081463 | Val RMSE: 0.82248765 | ‚àö(Val Loss) = 0.84063017 | Current Learning Rate: 0.001\n",
      "Epoch 746/1000 | Train Loss=2310.35480291 | Val Loss=0.75601782 | Data=23.07775029 | Physics=2.81510315 | Val RMSE: 0.82690150 | ‚àö(Val Loss) = 0.86949283 | Current Learning Rate: 0.001\n",
      "Epoch 747/1000 | Train Loss=2302.64873713 | Val Loss=0.73187264 | Data=23.00039395 | Physics=2.70906088 | Val RMSE: 0.82790279 | ‚àö(Val Loss) = 0.85549551 | Current Learning Rate: 0.001\n",
      "Epoch 748/1000 | Train Loss=2305.54394531 | Val Loss=0.69269237 | Data=23.02999531 | Physics=2.58663271 | Val RMSE: 0.82389820 | ‚àö(Val Loss) = 0.83228147 | Current Learning Rate: 0.001\n",
      "Epoch 749/1000 | Train Loss=2307.58420632 | Val Loss=0.71108325 | Data=23.05042804 | Physics=2.63415395 | Val RMSE: 0.82231879 | ‚àö(Val Loss) = 0.84325749 | Current Learning Rate: 0.001\n",
      "Epoch 750/1000 | Train Loss=2310.68852095 | Val Loss=0.73499206 | Data=23.08118040 | Physics=2.72180649 | Val RMSE: 0.82566780 | ‚àö(Val Loss) = 0.85731673 | Current Learning Rate: 0.001\n",
      "Epoch 751/1000 | Train Loss=2311.34590288 | Val Loss=0.69505533 | Data=23.08792409 | Physics=2.59027797 | Val RMSE: 0.82501149 | ‚àö(Val Loss) = 0.83369976 | Current Learning Rate: 0.001\n",
      "Epoch 752/1000 | Train Loss=2312.48592862 | Val Loss=0.71636218 | Data=23.09942800 | Physics=2.61615505 | Val RMSE: 0.82425570 | ‚àö(Val Loss) = 0.84638184 | Current Learning Rate: 0.001\n",
      "Epoch 753/1000 | Train Loss=2307.18328303 | Val Loss=0.70928039 | Data=23.04623309 | Physics=2.65151805 | Val RMSE: 0.82368356 | ‚àö(Val Loss) = 0.84218782 | Current Learning Rate: 0.001\n",
      "Epoch 754/1000 | Train Loss=2311.11232688 | Val Loss=0.71118778 | Data=23.08563995 | Physics=2.67583005 | Val RMSE: 0.82620203 | ‚àö(Val Loss) = 0.84331948 | Current Learning Rate: 0.001\n",
      "Epoch 755/1000 | Train Loss=2304.55670721 | Val Loss=0.73082154 | Data=23.01989954 | Physics=2.69552399 | Val RMSE: 0.82614148 | ‚àö(Val Loss) = 0.85488099 | Current Learning Rate: 0.001\n",
      "Epoch 756/1000 | Train Loss=2306.85844283 | Val Loss=0.73829667 | Data=23.04275357 | Physics=2.74282633 | Val RMSE: 0.82639074 | ‚àö(Val Loss) = 0.85924190 | Current Learning Rate: 0.001\n",
      "Epoch 757/1000 | Train Loss=2302.37758567 | Val Loss=0.69889094 | Data=22.99812473 | Physics=2.70939458 | Val RMSE: 0.82524973 | ‚àö(Val Loss) = 0.83599699 | Current Learning Rate: 0.001\n",
      "Epoch 758/1000 | Train Loss=2308.42988725 | Val Loss=0.72015156 | Data=23.05876108 | Physics=2.76349367 | Val RMSE: 0.82525933 | ‚àö(Val Loss) = 0.84861743 | Current Learning Rate: 0.001\n",
      "Epoch 759/1000 | Train Loss=2308.53384677 | Val Loss=0.71732197 | Data=23.05957135 | Physics=2.65167636 | Val RMSE: 0.82428849 | ‚àö(Val Loss) = 0.84694862 | Current Learning Rate: 0.001\n",
      "Epoch 760/1000 | Train Loss=2310.52838690 | Val Loss=0.70419953 | Data=23.07980607 | Physics=2.73974806 | Val RMSE: 0.82440013 | ‚àö(Val Loss) = 0.83916599 | Current Learning Rate: 0.001\n",
      "Epoch 761/1000 | Train Loss=2308.73286577 | Val Loss=0.73508881 | Data=23.06154511 | Physics=2.74983620 | Val RMSE: 0.82537770 | ‚àö(Val Loss) = 0.85737324 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  760 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 762/1000 | Train Loss=2308.73033558 | Val Loss=0.73980217 | Data=23.06150402 | Physics=2.74489082 | Val RMSE: 0.82581657 | ‚àö(Val Loss) = 0.86011755 | Current Learning Rate: 0.001\n",
      "Epoch 763/1000 | Train Loss=2309.46677468 | Val Loss=0.73705647 | Data=23.06886361 | Physics=2.73710434 | Val RMSE: 0.82545042 | ‚àö(Val Loss) = 0.85851997 | Current Learning Rate: 0.001\n",
      "Epoch 764/1000 | Train Loss=2310.99374112 | Val Loss=0.69797605 | Data=23.08441596 | Physics=2.58955275 | Val RMSE: 0.82463670 | ‚àö(Val Loss) = 0.83544964 | Current Learning Rate: 0.001\n",
      "Epoch 765/1000 | Train Loss=2315.40545099 | Val Loss=0.70444271 | Data=23.12851108 | Physics=2.61684205 | Val RMSE: 0.82532895 | ‚àö(Val Loss) = 0.83931082 | Current Learning Rate: 0.001\n",
      "Epoch 766/1000 | Train Loss=2313.04552113 | Val Loss=0.71452830 | Data=23.10498324 | Physics=2.70866337 | Val RMSE: 0.82407719 | ‚àö(Val Loss) = 0.84529775 | Current Learning Rate: 0.001\n",
      "Epoch 767/1000 | Train Loss=2313.40485174 | Val Loss=0.73055270 | Data=23.10816643 | Physics=2.70386088 | Val RMSE: 0.82793581 | ‚àö(Val Loss) = 0.85472375 | Current Learning Rate: 0.001\n",
      "Epoch 768/1000 | Train Loss=2303.12842907 | Val Loss=0.74015729 | Data=23.00569326 | Physics=2.74144318 | Val RMSE: 0.82564569 | ‚àö(Val Loss) = 0.86032397 | Current Learning Rate: 0.001\n",
      "Epoch 769/1000 | Train Loss=2311.57954545 | Val Loss=0.73522674 | Data=23.08982190 | Physics=2.69327487 | Val RMSE: 0.82608527 | ‚àö(Val Loss) = 0.85745364 | Current Learning Rate: 0.001\n",
      "Epoch 770/1000 | Train Loss=2309.28313654 | Val Loss=0.73218600 | Data=23.06727479 | Physics=2.66638381 | Val RMSE: 0.82535326 | ‚àö(Val Loss) = 0.85567868 | Current Learning Rate: 0.001\n",
      "Epoch 771/1000 | Train Loss=2309.84050959 | Val Loss=0.71942534 | Data=23.07249485 | Physics=2.76277968 | Val RMSE: 0.82480633 | ‚àö(Val Loss) = 0.84818941 | Current Learning Rate: 0.001\n",
      "Epoch 772/1000 | Train Loss=2314.69906339 | Val Loss=0.73366934 | Data=23.12145441 | Physics=2.72864602 | Val RMSE: 0.82521749 | ‚àö(Val Loss) = 0.85654503 | Current Learning Rate: 0.001\n",
      "Epoch 773/1000 | Train Loss=2313.96828391 | Val Loss=0.72680964 | Data=23.11381236 | Physics=2.69415503 | Val RMSE: 0.82479477 | ‚àö(Val Loss) = 0.85253131 | Current Learning Rate: 0.001\n",
      "Epoch 774/1000 | Train Loss=2313.12167081 | Val Loss=0.71053359 | Data=23.10569503 | Physics=2.72750386 | Val RMSE: 0.82557422 | ‚àö(Val Loss) = 0.84293157 | Current Learning Rate: 0.001\n",
      "Epoch 775/1000 | Train Loss=2306.84099787 | Val Loss=0.69627136 | Data=23.04283246 | Physics=2.64571070 | Val RMSE: 0.82572395 | ‚àö(Val Loss) = 0.83442879 | Current Learning Rate: 0.001\n",
      "Epoch 776/1000 | Train Loss=2311.80752841 | Val Loss=0.73579109 | Data=23.09250918 | Physics=2.81019843 | Val RMSE: 0.82629555 | ‚àö(Val Loss) = 0.85778266 | Current Learning Rate: 0.001\n",
      "Epoch 777/1000 | Train Loss=2306.31556286 | Val Loss=0.68402816 | Data=23.03740033 | Physics=2.60153296 | Val RMSE: 0.82436013 | ‚àö(Val Loss) = 0.82705992 | Current Learning Rate: 0.001\n",
      "Epoch 778/1000 | Train Loss=2314.31704989 | Val Loss=0.74573478 | Data=23.11789062 | Physics=2.76140788 | Val RMSE: 0.82483679 | ‚àö(Val Loss) = 0.86355937 | Current Learning Rate: 0.001\n",
      "Epoch 779/1000 | Train Loss=2313.87553267 | Val Loss=0.72515103 | Data=23.11223446 | Physics=2.74145839 | Val RMSE: 0.82596433 | ‚àö(Val Loss) = 0.85155797 | Current Learning Rate: 0.001\n",
      "Epoch 780/1000 | Train Loss=2312.57461825 | Val Loss=0.73125912 | Data=23.10052091 | Physics=2.71053493 | Val RMSE: 0.82549942 | ‚àö(Val Loss) = 0.85513687 | Current Learning Rate: 0.001\n",
      "Epoch 781/1000 | Train Loss=2310.16137695 | Val Loss=0.69504369 | Data=23.07555077 | Physics=2.58981330 | Val RMSE: 0.82167548 | ‚àö(Val Loss) = 0.83369279 | Current Learning Rate: 0.001\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  780 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 782/1000 | Train Loss=2308.87091619 | Val Loss=0.71482864 | Data=23.06353777 | Physics=2.69815555 | Val RMSE: 0.82413095 | ‚àö(Val Loss) = 0.84547538 | Current Learning Rate: 0.001\n",
      "Epoch 783/1000 | Train Loss=2307.92422763 | Val Loss=0.70995450 | Data=23.05307094 | Physics=2.66491722 | Val RMSE: 0.82388866 | ‚àö(Val Loss) = 0.84258795 | Current Learning Rate: 0.001\n",
      "Epoch 784/1000 | Train Loss=2314.54705256 | Val Loss=0.69822059 | Data=23.12025001 | Physics=2.61477917 | Val RMSE: 0.82565814 | ‚àö(Val Loss) = 0.83559597 | Current Learning Rate: 0.001\n",
      "Epoch 785/1000 | Train Loss=2307.91215376 | Val Loss=0.71537997 | Data=23.05339154 | Physics=2.62652963 | Val RMSE: 0.82230723 | ‚àö(Val Loss) = 0.84580135 | Current Learning Rate: 0.001\n",
      "Epoch 786/1000 | Train Loss=2308.41552734 | Val Loss=0.72697522 | Data=23.05848122 | Physics=2.72969220 | Val RMSE: 0.82551116 | ‚àö(Val Loss) = 0.85262841 | Current Learning Rate: 0.001\n",
      "Epoch 787/1000 | Train Loss=2305.69830877 | Val Loss=0.69383340 | Data=23.03143050 | Physics=2.66815416 | Val RMSE: 0.82517439 | ‚àö(Val Loss) = 0.83296663 | Current Learning Rate: 0.001\n",
      "Epoch 788/1000 | Train Loss=2313.65962358 | Val Loss=0.71132140 | Data=23.11102884 | Physics=2.62104377 | Val RMSE: 0.82712793 | ‚àö(Val Loss) = 0.84339875 | Current Learning Rate: 0.001\n",
      "Epoch 789/1000 | Train Loss=2315.28588867 | Val Loss=0.69528684 | Data=23.12732384 | Physics=2.66274245 | Val RMSE: 0.82569784 | ‚àö(Val Loss) = 0.83383864 | Current Learning Rate: 0.001\n",
      "Epoch 790/1000 | Train Loss=2307.37941673 | Val Loss=0.72564292 | Data=23.04829563 | Physics=2.64037108 | Val RMSE: 0.82322001 | ‚àö(Val Loss) = 0.85184675 | Current Learning Rate: 0.001\n",
      "Epoch 791/1000 | Train Loss=2302.25389515 | Val Loss=0.69368293 | Data=22.99693871 | Physics=2.65033611 | Val RMSE: 0.82454664 | ‚àö(Val Loss) = 0.83287627 | Current Learning Rate: 0.001\n",
      "Epoch 792/1000 | Train Loss=2309.20594371 | Val Loss=0.73821462 | Data=23.06651688 | Physics=2.71554854 | Val RMSE: 0.82467270 | ‚àö(Val Loss) = 0.85919416 | Current Learning Rate: 0.001\n",
      "Epoch 793/1000 | Train Loss=2308.85737749 | Val Loss=0.72596174 | Data=23.06259589 | Physics=2.70730055 | Val RMSE: 0.82374954 | ‚àö(Val Loss) = 0.85203391 | Current Learning Rate: 0.001\n",
      "Epoch 794/1000 | Train Loss=2307.45410156 | Val Loss=0.74507850 | Data=23.04886159 | Physics=2.71182227 | Val RMSE: 0.82615304 | ‚àö(Val Loss) = 0.86317933 | Current Learning Rate: 0.001\n",
      "Epoch 795/1000 | Train Loss=2312.09490412 | Val Loss=0.68232360 | Data=23.09529391 | Physics=2.59899950 | Val RMSE: 0.82208681 | ‚àö(Val Loss) = 0.82602882 | Current Learning Rate: 0.001\n",
      "Epoch 796/1000 | Train Loss=2313.36421342 | Val Loss=0.72018036 | Data=23.10818568 | Physics=2.68708190 | Val RMSE: 0.82335520 | ‚àö(Val Loss) = 0.84863442 | Current Learning Rate: 0.001\n",
      "Epoch 797/1000 | Train Loss=2308.25727983 | Val Loss=0.72144547 | Data=23.05657803 | Physics=2.77825578 | Val RMSE: 0.82607591 | ‚àö(Val Loss) = 0.84937948 | Current Learning Rate: 0.001\n",
      "Epoch 798/1000 | Train Loss=2317.91044478 | Val Loss=0.73948233 | Data=23.15358908 | Physics=2.63184675 | Val RMSE: 0.82633096 | ‚àö(Val Loss) = 0.85993159 | Current Learning Rate: 0.001\n",
      "Epoch 799/1000 | Train Loss=2315.35870916 | Val Loss=0.72689478 | Data=23.12767514 | Physics=2.68094881 | Val RMSE: 0.82492989 | ‚àö(Val Loss) = 0.85258126 | Current Learning Rate: 0.001\n",
      "Epoch 800/1000 | Train Loss=2310.46644176 | Val Loss=0.70829892 | Data=23.07920560 | Physics=2.70382885 | Val RMSE: 0.82425088 | ‚àö(Val Loss) = 0.84160495 | Current Learning Rate: 0.0001\n",
      "Epoch 801/1000 | Train Loss=2323.76271751 | Val Loss=0.63381051 | Data=23.21299380 | Physics=2.39845110 | Val RMSE: 0.82106578 | ‚àö(Val Loss) = 0.79612219 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  800 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 802/1000 | Train Loss=2315.62246982 | Val Loss=0.64187782 | Data=23.13142482 | Physics=2.54533433 | Val RMSE: 0.82225537 | ‚àö(Val Loss) = 0.80117279 | Current Learning Rate: 0.0001\n",
      "Epoch 803/1000 | Train Loss=2316.50865589 | Val Loss=0.64614888 | Data=23.14023625 | Physics=2.52233138 | Val RMSE: 0.82264066 | ‚àö(Val Loss) = 0.80383384 | Current Learning Rate: 0.0001\n",
      "Epoch 804/1000 | Train Loss=2319.82530629 | Val Loss=0.64864160 | Data=23.17337938 | Physics=2.51904019 | Val RMSE: 0.82287163 | ‚àö(Val Loss) = 0.80538291 | Current Learning Rate: 0.0001\n",
      "Epoch 805/1000 | Train Loss=2308.75049938 | Val Loss=0.64891631 | Data=23.06265103 | Physics=2.45299977 | Val RMSE: 0.82288170 | ‚àö(Val Loss) = 0.80555344 | Current Learning Rate: 0.0001\n",
      "Epoch 806/1000 | Train Loss=2312.48357599 | Val Loss=0.65070313 | Data=23.09991524 | Physics=2.57712893 | Val RMSE: 0.82300127 | ‚àö(Val Loss) = 0.80666173 | Current Learning Rate: 0.0001\n",
      "Epoch 807/1000 | Train Loss=2311.58735795 | Val Loss=0.64982523 | Data=23.09099215 | Physics=2.47883770 | Val RMSE: 0.82295913 | ‚àö(Val Loss) = 0.80611736 | Current Learning Rate: 0.0001\n",
      "Epoch 808/1000 | Train Loss=2317.67471591 | Val Loss=0.64941450 | Data=23.15186570 | Physics=2.50021845 | Val RMSE: 0.82292455 | ‚àö(Val Loss) = 0.80586255 | Current Learning Rate: 0.0001\n",
      "Epoch 809/1000 | Train Loss=2310.46249112 | Val Loss=0.64918805 | Data=23.07973602 | Physics=2.55370389 | Val RMSE: 0.82290429 | ‚àö(Val Loss) = 0.80572206 | Current Learning Rate: 0.0001\n",
      "Epoch 810/1000 | Train Loss=2320.83766868 | Val Loss=0.65021306 | Data=23.18347844 | Physics=2.51227624 | Val RMSE: 0.82300651 | ‚àö(Val Loss) = 0.80635792 | Current Learning Rate: 0.0001\n",
      "Epoch 811/1000 | Train Loss=2320.46262429 | Val Loss=0.64913260 | Data=23.17974195 | Physics=2.53124575 | Val RMSE: 0.82290858 | ‚àö(Val Loss) = 0.80568767 | Current Learning Rate: 0.0001\n",
      "Epoch 812/1000 | Train Loss=2308.30197976 | Val Loss=0.64886456 | Data=23.05815419 | Physics=2.53496814 | Val RMSE: 0.82287961 | ‚àö(Val Loss) = 0.80552131 | Current Learning Rate: 0.0001\n",
      "Epoch 813/1000 | Train Loss=2311.17837802 | Val Loss=0.64968538 | Data=23.08686274 | Physics=2.64228862 | Val RMSE: 0.82290357 | ‚àö(Val Loss) = 0.80603063 | Current Learning Rate: 0.0001\n",
      "Epoch 814/1000 | Train Loss=2309.32776989 | Val Loss=0.65024373 | Data=23.06837585 | Physics=2.55609128 | Val RMSE: 0.82300037 | ‚àö(Val Loss) = 0.80637687 | Current Learning Rate: 0.0001\n",
      "Epoch 815/1000 | Train Loss=2313.78471236 | Val Loss=0.65065815 | Data=23.11292527 | Physics=2.56251786 | Val RMSE: 0.82305503 | ‚àö(Val Loss) = 0.80663383 | Current Learning Rate: 0.0001\n",
      "Epoch 816/1000 | Train Loss=2312.31354315 | Val Loss=0.64942160 | Data=23.09824753 | Physics=2.50462195 | Val RMSE: 0.82293844 | ‚àö(Val Loss) = 0.80586696 | Current Learning Rate: 0.0001\n",
      "Epoch 817/1000 | Train Loss=2320.59403853 | Val Loss=0.64944464 | Data=23.18107067 | Physics=2.48926837 | Val RMSE: 0.82293987 | ‚àö(Val Loss) = 0.80588126 | Current Learning Rate: 0.0001\n",
      "Epoch 818/1000 | Train Loss=2307.89366566 | Val Loss=0.64923264 | Data=23.05406328 | Physics=2.50378558 | Val RMSE: 0.82291436 | ‚àö(Val Loss) = 0.80574971 | Current Learning Rate: 0.0001\n",
      "Epoch 819/1000 | Train Loss=2310.59932085 | Val Loss=0.65042420 | Data=23.08109075 | Physics=2.55750297 | Val RMSE: 0.82296824 | ‚àö(Val Loss) = 0.80648881 | Current Learning Rate: 0.0001\n",
      "Epoch 820/1000 | Train Loss=2306.93568004 | Val Loss=0.64981025 | Data=23.04448613 | Physics=2.50633400 | Val RMSE: 0.82296008 | ‚àö(Val Loss) = 0.80610812 | Current Learning Rate: 0.0001\n",
      "Epoch 821/1000 | Train Loss=2310.99667081 | Val Loss=0.65045781 | Data=23.08508370 | Physics=2.46722324 | Val RMSE: 0.82300508 | ‚àö(Val Loss) = 0.80650961 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  820 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 822/1000 | Train Loss=2314.08640359 | Val Loss=0.65026921 | Data=23.11592570 | Physics=2.66641184 | Val RMSE: 0.82302386 | ‚àö(Val Loss) = 0.80639273 | Current Learning Rate: 0.0001\n",
      "Epoch 823/1000 | Train Loss=2310.96821733 | Val Loss=0.65024149 | Data=23.08477384 | Physics=2.57179529 | Val RMSE: 0.82302326 | ‚àö(Val Loss) = 0.80637550 | Current Learning Rate: 0.0001\n",
      "Epoch 824/1000 | Train Loss=2308.62832919 | Val Loss=0.65031361 | Data=23.06139946 | Physics=2.54738668 | Val RMSE: 0.82304263 | ‚àö(Val Loss) = 0.80642027 | Current Learning Rate: 0.0001\n",
      "Epoch 825/1000 | Train Loss=2312.68419300 | Val Loss=0.65047679 | Data=23.10193686 | Physics=2.53842876 | Val RMSE: 0.82302088 | ‚àö(Val Loss) = 0.80652142 | Current Learning Rate: 0.0001\n",
      "Epoch 826/1000 | Train Loss=2323.18228427 | Val Loss=0.65073363 | Data=23.20692877 | Physics=2.54579807 | Val RMSE: 0.82307386 | ‚àö(Val Loss) = 0.80668062 | Current Learning Rate: 0.0001\n",
      "Epoch 827/1000 | Train Loss=2311.77268288 | Val Loss=0.64954699 | Data=23.09281731 | Physics=2.59961227 | Val RMSE: 0.82296181 | ‚àö(Val Loss) = 0.80594480 | Current Learning Rate: 0.0001\n",
      "Epoch 828/1000 | Train Loss=2314.93778853 | Val Loss=0.64974137 | Data=23.12448900 | Physics=2.53068243 | Val RMSE: 0.82298768 | ‚àö(Val Loss) = 0.80606538 | Current Learning Rate: 0.0001\n",
      "Epoch 829/1000 | Train Loss=2309.77479137 | Val Loss=0.64974603 | Data=23.07284979 | Physics=2.55717209 | Val RMSE: 0.82298881 | ‚àö(Val Loss) = 0.80606830 | Current Learning Rate: 0.0001\n",
      "Epoch 830/1000 | Train Loss=2312.49806907 | Val Loss=0.65095591 | Data=23.10008240 | Physics=2.54226935 | Val RMSE: 0.82313591 | ‚àö(Val Loss) = 0.80681837 | Current Learning Rate: 0.0001\n",
      "Epoch 831/1000 | Train Loss=2315.00412820 | Val Loss=0.64979970 | Data=23.12516212 | Physics=2.49335738 | Val RMSE: 0.82299304 | ‚àö(Val Loss) = 0.80610156 | Current Learning Rate: 0.0001\n",
      "Epoch 832/1000 | Train Loss=2306.94611151 | Val Loss=0.65034395 | Data=23.04458549 | Physics=2.48947727 | Val RMSE: 0.82304698 | ‚àö(Val Loss) = 0.80643904 | Current Learning Rate: 0.0001\n",
      "Epoch 833/1000 | Train Loss=2315.98923562 | Val Loss=0.65284796 | Data=23.13496642 | Physics=2.52064004 | Val RMSE: 0.82323182 | ‚àö(Val Loss) = 0.80799007 | Current Learning Rate: 0.0001\n",
      "Epoch 834/1000 | Train Loss=2324.09541460 | Val Loss=0.65052070 | Data=23.21606081 | Physics=2.53284562 | Val RMSE: 0.82306629 | ‚àö(Val Loss) = 0.80654860 | Current Learning Rate: 0.0001\n",
      "Epoch 835/1000 | Train Loss=2321.62886186 | Val Loss=0.64978765 | Data=23.19138145 | Physics=2.56908759 | Val RMSE: 0.82300937 | ‚àö(Val Loss) = 0.80609405 | Current Learning Rate: 0.0001\n",
      "Epoch 836/1000 | Train Loss=2312.48950195 | Val Loss=0.64944395 | Data=23.10001009 | Physics=2.50781413 | Val RMSE: 0.82297057 | ‚àö(Val Loss) = 0.80588084 | Current Learning Rate: 0.0001\n",
      "Epoch 837/1000 | Train Loss=2318.98657227 | Val Loss=0.65005682 | Data=23.16497699 | Physics=2.49776284 | Val RMSE: 0.82301986 | ‚àö(Val Loss) = 0.80626100 | Current Learning Rate: 0.0001\n",
      "Epoch 838/1000 | Train Loss=2312.53928445 | Val Loss=0.65016415 | Data=23.10046768 | Physics=2.62458959 | Val RMSE: 0.82304919 | ‚àö(Val Loss) = 0.80632758 | Current Learning Rate: 0.0001\n",
      "Epoch 839/1000 | Train Loss=2324.36858576 | Val Loss=0.65046727 | Data=23.21878607 | Physics=2.54469656 | Val RMSE: 0.82306010 | ‚àö(Val Loss) = 0.80651551 | Current Learning Rate: 0.0001\n",
      "Epoch 840/1000 | Train Loss=2312.46968217 | Val Loss=0.65025051 | Data=23.09980566 | Physics=2.52648772 | Val RMSE: 0.82305080 | ‚àö(Val Loss) = 0.80638111 | Current Learning Rate: 0.0001\n",
      "Epoch 841/1000 | Train Loss=2317.67456055 | Val Loss=0.65030709 | Data=23.15187992 | Physics=2.47257151 | Val RMSE: 0.82305634 | ‚àö(Val Loss) = 0.80641621 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  840 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 842/1000 | Train Loss=2319.16148793 | Val Loss=0.64982167 | Data=23.16673643 | Physics=2.51217422 | Val RMSE: 0.82300425 | ‚àö(Val Loss) = 0.80611521 | Current Learning Rate: 0.0001\n",
      "Epoch 843/1000 | Train Loss=2315.27996271 | Val Loss=0.64975017 | Data=23.12788773 | Physics=2.58977801 | Val RMSE: 0.82299840 | ‚àö(Val Loss) = 0.80607080 | Current Learning Rate: 0.0001\n",
      "Epoch 844/1000 | Train Loss=2312.60651190 | Val Loss=0.64988928 | Data=23.10115571 | Physics=2.64597328 | Val RMSE: 0.82301486 | ‚àö(Val Loss) = 0.80615711 | Current Learning Rate: 0.0001\n",
      "Epoch 845/1000 | Train Loss=2324.33240856 | Val Loss=0.65044622 | Data=23.21842176 | Physics=2.52186189 | Val RMSE: 0.82303846 | ‚àö(Val Loss) = 0.80650246 | Current Learning Rate: 0.0001\n",
      "Epoch 846/1000 | Train Loss=2313.59463778 | Val Loss=0.64962833 | Data=23.11106717 | Physics=2.50298938 | Val RMSE: 0.82298750 | ‚àö(Val Loss) = 0.80599523 | Current Learning Rate: 0.0001\n",
      "Epoch 847/1000 | Train Loss=2317.85023082 | Val Loss=0.64967672 | Data=23.15359861 | Physics=2.55819523 | Val RMSE: 0.82299197 | ‚àö(Val Loss) = 0.80602527 | Current Learning Rate: 0.0001\n",
      "Epoch 848/1000 | Train Loss=2314.48160067 | Val Loss=0.64979447 | Data=23.11993946 | Physics=2.47250504 | Val RMSE: 0.82300270 | ‚àö(Val Loss) = 0.80609828 | Current Learning Rate: 0.0001\n",
      "Epoch 849/1000 | Train Loss=2313.14075817 | Val Loss=0.64990209 | Data=23.10651103 | Physics=2.51101599 | Val RMSE: 0.82300979 | ‚àö(Val Loss) = 0.80616504 | Current Learning Rate: 0.0001\n",
      "Epoch 850/1000 | Train Loss=2317.97085849 | Val Loss=0.65034933 | Data=23.15481862 | Physics=2.56293135 | Val RMSE: 0.82306111 | ‚àö(Val Loss) = 0.80644238 | Current Learning Rate: 0.0001\n",
      "Epoch 851/1000 | Train Loss=2312.78060636 | Val Loss=0.65108957 | Data=23.10290562 | Physics=2.48357709 | Val RMSE: 0.82312441 | ‚àö(Val Loss) = 0.80690122 | Current Learning Rate: 0.0001\n",
      "Epoch 852/1000 | Train Loss=2319.45703125 | Val Loss=0.65041773 | Data=23.16965294 | Physics=2.57580686 | Val RMSE: 0.82307369 | ‚àö(Val Loss) = 0.80648482 | Current Learning Rate: 0.0001\n",
      "Epoch 853/1000 | Train Loss=2311.58471680 | Val Loss=0.65020292 | Data=23.09095452 | Physics=2.51588351 | Val RMSE: 0.82306373 | ‚àö(Val Loss) = 0.80635160 | Current Learning Rate: 0.0001\n",
      "Epoch 854/1000 | Train Loss=2312.91839045 | Val Loss=0.65080744 | Data=23.10426937 | Physics=2.56903425 | Val RMSE: 0.82312381 | ‚àö(Val Loss) = 0.80672640 | Current Learning Rate: 0.0001\n",
      "Epoch 855/1000 | Train Loss=2309.21388938 | Val Loss=0.65017980 | Data=23.06724150 | Physics=2.54262663 | Val RMSE: 0.82303357 | ‚àö(Val Loss) = 0.80633730 | Current Learning Rate: 0.0001\n",
      "Epoch 856/1000 | Train Loss=2306.84494851 | Val Loss=0.65198641 | Data=23.04355309 | Physics=2.54759612 | Val RMSE: 0.82319367 | ‚àö(Val Loss) = 0.80745673 | Current Learning Rate: 0.0001\n",
      "Epoch 857/1000 | Train Loss=2313.74340820 | Val Loss=0.65458585 | Data=23.11247895 | Physics=2.54634514 | Val RMSE: 0.82344961 | ‚àö(Val Loss) = 0.80906481 | Current Learning Rate: 0.0001\n",
      "Epoch 858/1000 | Train Loss=2315.12060547 | Val Loss=0.65143556 | Data=23.12631676 | Physics=2.50885112 | Val RMSE: 0.82317442 | ‚àö(Val Loss) = 0.80711555 | Current Learning Rate: 0.0001\n",
      "Epoch 859/1000 | Train Loss=2318.30308949 | Val Loss=0.65077280 | Data=23.15812562 | Physics=2.52145727 | Val RMSE: 0.82309484 | ‚àö(Val Loss) = 0.80670488 | Current Learning Rate: 0.0001\n",
      "Epoch 860/1000 | Train Loss=2307.53604403 | Val Loss=0.65064417 | Data=23.05049255 | Physics=2.47029112 | Val RMSE: 0.82308233 | ‚àö(Val Loss) = 0.80662519 | Current Learning Rate: 0.0001\n",
      "Epoch 861/1000 | Train Loss=2321.16978871 | Val Loss=0.65210301 | Data=23.18679255 | Physics=2.51760929 | Val RMSE: 0.82315654 | ‚àö(Val Loss) = 0.80752897 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  860 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 862/1000 | Train Loss=2308.86905185 | Val Loss=0.65079291 | Data=23.06377307 | Physics=2.51371392 | Val RMSE: 0.82309616 | ‚àö(Val Loss) = 0.80671734 | Current Learning Rate: 0.0001\n",
      "Epoch 863/1000 | Train Loss=2313.84454901 | Val Loss=0.65212889 | Data=23.11354290 | Physics=2.51617772 | Val RMSE: 0.82330656 | ‚àö(Val Loss) = 0.80754495 | Current Learning Rate: 0.0001\n",
      "Epoch 864/1000 | Train Loss=2314.37253640 | Val Loss=0.65057533 | Data=23.11882296 | Physics=2.55241402 | Val RMSE: 0.82310539 | ‚àö(Val Loss) = 0.80658251 | Current Learning Rate: 0.0001\n",
      "Epoch 865/1000 | Train Loss=2306.84277344 | Val Loss=0.65065634 | Data=23.04352934 | Physics=2.54009262 | Val RMSE: 0.82309580 | ‚àö(Val Loss) = 0.80663270 | Current Learning Rate: 0.0001\n",
      "Epoch 866/1000 | Train Loss=2316.70920632 | Val Loss=0.65167736 | Data=23.14216527 | Physics=2.55183045 | Val RMSE: 0.82320344 | ‚àö(Val Loss) = 0.80726534 | Current Learning Rate: 0.0001\n",
      "Epoch 867/1000 | Train Loss=2318.15081232 | Val Loss=0.65062272 | Data=23.15660078 | Physics=2.52587188 | Val RMSE: 0.82310683 | ‚àö(Val Loss) = 0.80661190 | Current Learning Rate: 0.0001\n",
      "Epoch 868/1000 | Train Loss=2305.81474165 | Val Loss=0.65049178 | Data=23.03321440 | Physics=2.60076596 | Val RMSE: 0.82311422 | ‚àö(Val Loss) = 0.80653071 | Current Learning Rate: 0.0001\n",
      "Epoch 869/1000 | Train Loss=2315.55104759 | Val Loss=0.65382514 | Data=23.13058662 | Physics=2.55527623 | Val RMSE: 0.82349432 | ‚àö(Val Loss) = 0.80859458 | Current Learning Rate: 0.0001\n",
      "Epoch 870/1000 | Train Loss=2315.17196378 | Val Loss=0.65137753 | Data=23.12685429 | Physics=2.43575399 | Val RMSE: 0.82318461 | ‚àö(Val Loss) = 0.80707967 | Current Learning Rate: 0.0001\n",
      "Epoch 871/1000 | Train Loss=2310.96235795 | Val Loss=0.65086782 | Data=23.08471472 | Physics=2.55584338 | Val RMSE: 0.82312751 | ‚àö(Val Loss) = 0.80676377 | Current Learning Rate: 0.0001\n",
      "Epoch 872/1000 | Train Loss=2311.18652344 | Val Loss=0.65097683 | Data=23.08694753 | Physics=2.58972093 | Val RMSE: 0.82314891 | ‚àö(Val Loss) = 0.80683136 | Current Learning Rate: 0.0001\n",
      "Epoch 873/1000 | Train Loss=2318.65145597 | Val Loss=0.65224196 | Data=23.16160982 | Physics=2.55478709 | Val RMSE: 0.82328761 | ‚àö(Val Loss) = 0.80761498 | Current Learning Rate: 0.0001\n",
      "Epoch 874/1000 | Train Loss=2312.48830344 | Val Loss=0.65103169 | Data=23.09999882 | Physics=2.47514910 | Val RMSE: 0.82314593 | ‚àö(Val Loss) = 0.80686533 | Current Learning Rate: 0.0001\n",
      "Epoch 875/1000 | Train Loss=2317.63347834 | Val Loss=0.65079783 | Data=23.15142978 | Physics=2.57332793 | Val RMSE: 0.82312083 | ‚àö(Val Loss) = 0.80672044 | Current Learning Rate: 0.0001\n",
      "Epoch 876/1000 | Train Loss=2321.65074574 | Val Loss=0.65058570 | Data=23.19161034 | Physics=2.46480302 | Val RMSE: 0.82309490 | ‚àö(Val Loss) = 0.80658895 | Current Learning Rate: 0.0001\n",
      "Epoch 877/1000 | Train Loss=2314.55178001 | Val Loss=0.65006047 | Data=23.12062506 | Physics=2.51732727 | Val RMSE: 0.82306170 | ‚àö(Val Loss) = 0.80626327 | Current Learning Rate: 0.0001\n",
      "Epoch 878/1000 | Train Loss=2305.20290305 | Val Loss=0.65116373 | Data=23.02715197 | Physics=2.50234509 | Val RMSE: 0.82316542 | ‚àö(Val Loss) = 0.80694717 | Current Learning Rate: 0.0001\n",
      "Epoch 879/1000 | Train Loss=2320.02832031 | Val Loss=0.65394959 | Data=23.17536805 | Physics=2.50500452 | Val RMSE: 0.82338220 | ‚àö(Val Loss) = 0.80867153 | Current Learning Rate: 0.0001\n",
      "Epoch 880/1000 | Train Loss=2305.75089888 | Val Loss=0.65098019 | Data=23.03261636 | Physics=2.50246698 | Val RMSE: 0.82314241 | ‚àö(Val Loss) = 0.80683345 | Current Learning Rate: 0.0001\n",
      "Epoch 881/1000 | Train Loss=2312.44655540 | Val Loss=0.64941967 | Data=23.09957712 | Physics=2.53680853 | Val RMSE: 0.82306707 | ‚àö(Val Loss) = 0.80586576 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  880 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 882/1000 | Train Loss=2317.36878551 | Val Loss=0.64989638 | Data=23.14879435 | Physics=2.59101721 | Val RMSE: 0.82308257 | ‚àö(Val Loss) = 0.80616152 | Current Learning Rate: 0.0001\n",
      "Epoch 883/1000 | Train Loss=2311.64275568 | Val Loss=0.65055758 | Data=23.09152690 | Physics=2.54483893 | Val RMSE: 0.82310951 | ‚àö(Val Loss) = 0.80657148 | Current Learning Rate: 0.0001\n",
      "Epoch 884/1000 | Train Loss=2318.44919656 | Val Loss=0.65100312 | Data=23.15958370 | Physics=2.54372110 | Val RMSE: 0.82313949 | ‚àö(Val Loss) = 0.80684763 | Current Learning Rate: 0.0001\n",
      "Epoch 885/1000 | Train Loss=2318.05843839 | Val Loss=0.65093313 | Data=23.15569982 | Physics=2.50261906 | Val RMSE: 0.82314372 | ‚àö(Val Loss) = 0.80680430 | Current Learning Rate: 0.0001\n",
      "Epoch 886/1000 | Train Loss=2321.53484553 | Val Loss=0.65044504 | Data=23.19044460 | Physics=2.52916820 | Val RMSE: 0.82309902 | ‚àö(Val Loss) = 0.80650175 | Current Learning Rate: 0.0001\n",
      "Epoch 887/1000 | Train Loss=2310.37431197 | Val Loss=0.65052196 | Data=23.07883436 | Physics=2.60578258 | Val RMSE: 0.82311612 | ‚àö(Val Loss) = 0.80654943 | Current Learning Rate: 0.0001\n",
      "Epoch 888/1000 | Train Loss=2313.80502042 | Val Loss=0.65131414 | Data=23.11314357 | Physics=2.55245744 | Val RMSE: 0.82316941 | ‚àö(Val Loss) = 0.80704033 | Current Learning Rate: 0.0001\n",
      "Epoch 889/1000 | Train Loss=2312.35127397 | Val Loss=0.65116992 | Data=23.09864027 | Physics=2.47255578 | Val RMSE: 0.82316899 | ‚àö(Val Loss) = 0.80695099 | Current Learning Rate: 0.0001\n",
      "Epoch 890/1000 | Train Loss=2310.62229226 | Val Loss=0.65077640 | Data=23.08129519 | Physics=2.60952297 | Val RMSE: 0.82311267 | ‚àö(Val Loss) = 0.80670714 | Current Learning Rate: 0.0001\n",
      "Epoch 891/1000 | Train Loss=2313.50272994 | Val Loss=0.65095281 | Data=23.11017245 | Physics=2.40179649 | Val RMSE: 0.82314712 | ‚àö(Val Loss) = 0.80681646 | Current Learning Rate: 0.0001\n",
      "Epoch 892/1000 | Train Loss=2315.01740057 | Val Loss=0.65049190 | Data=23.12527362 | Physics=2.51372284 | Val RMSE: 0.82308042 | ‚àö(Val Loss) = 0.80653077 | Current Learning Rate: 0.0001\n",
      "Epoch 893/1000 | Train Loss=2314.37599876 | Val Loss=0.65112581 | Data=23.11885175 | Physics=2.56313667 | Val RMSE: 0.82317752 | ‚àö(Val Loss) = 0.80692363 | Current Learning Rate: 0.0001\n",
      "Epoch 894/1000 | Train Loss=2306.43522505 | Val Loss=0.65159646 | Data=23.03947379 | Physics=2.46127058 | Val RMSE: 0.82319707 | ‚àö(Val Loss) = 0.80721527 | Current Learning Rate: 0.0001\n",
      "Epoch 895/1000 | Train Loss=2323.26569158 | Val Loss=0.65417225 | Data=23.20773558 | Physics=2.55573977 | Val RMSE: 0.82337093 | ‚àö(Val Loss) = 0.80880916 | Current Learning Rate: 0.0001\n",
      "Epoch 896/1000 | Train Loss=2315.80464311 | Val Loss=0.65188892 | Data=23.13313831 | Physics=2.55112439 | Val RMSE: 0.82320780 | ‚àö(Val Loss) = 0.80739635 | Current Learning Rate: 0.0001\n",
      "Epoch 897/1000 | Train Loss=2318.45862926 | Val Loss=0.65155862 | Data=23.15967196 | Physics=2.50772678 | Val RMSE: 0.82320303 | ‚àö(Val Loss) = 0.80719185 | Current Learning Rate: 0.0001\n",
      "Epoch 898/1000 | Train Loss=2321.66574929 | Val Loss=0.65071769 | Data=23.19179483 | Physics=2.42049211 | Val RMSE: 0.82314074 | ‚àö(Val Loss) = 0.80667073 | Current Learning Rate: 0.0001\n",
      "Epoch 899/1000 | Train Loss=2315.48698287 | Val Loss=0.65117261 | Data=23.12996639 | Physics=2.57687101 | Val RMSE: 0.82317781 | ‚àö(Val Loss) = 0.80695271 | Current Learning Rate: 0.0001\n",
      "Epoch 900/1000 | Train Loss=2313.49567205 | Val Loss=0.65105936 | Data=23.11002853 | Physics=2.58217518 | Val RMSE: 0.82315588 | ‚àö(Val Loss) = 0.80688250 | Current Learning Rate: 0.0001\n",
      "Epoch 901/1000 | Train Loss=2314.57883523 | Val Loss=0.65104487 | Data=23.12088446 | Physics=2.52625897 | Val RMSE: 0.82316494 | ‚àö(Val Loss) = 0.80687350 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  900 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 902/1000 | Train Loss=2318.87943892 | Val Loss=0.65102542 | Data=23.16391061 | Physics=2.50928034 | Val RMSE: 0.82317066 | ‚àö(Val Loss) = 0.80686146 | Current Learning Rate: 0.0001\n",
      "Epoch 903/1000 | Train Loss=2308.63749556 | Val Loss=0.65069288 | Data=23.06145235 | Physics=2.57118960 | Val RMSE: 0.82311600 | ‚àö(Val Loss) = 0.80665535 | Current Learning Rate: 0.0001\n",
      "Epoch 904/1000 | Train Loss=2312.91998846 | Val Loss=0.65164263 | Data=23.10431532 | Physics=2.46359687 | Val RMSE: 0.82322109 | ‚àö(Val Loss) = 0.80724382 | Current Learning Rate: 0.0001\n",
      "Epoch 905/1000 | Train Loss=2311.43829901 | Val Loss=0.65161117 | Data=23.08949592 | Physics=2.51700205 | Val RMSE: 0.82322258 | ‚àö(Val Loss) = 0.80722433 | Current Learning Rate: 0.0001\n",
      "Epoch 906/1000 | Train Loss=2315.55579723 | Val Loss=0.65118407 | Data=23.13067662 | Physics=2.46861076 | Val RMSE: 0.82315361 | ‚àö(Val Loss) = 0.80695975 | Current Learning Rate: 0.0001\n",
      "Epoch 907/1000 | Train Loss=2308.90394176 | Val Loss=0.65067818 | Data=23.06414830 | Physics=2.51604578 | Val RMSE: 0.82311165 | ‚àö(Val Loss) = 0.80664623 | Current Learning Rate: 0.0001\n",
      "Epoch 908/1000 | Train Loss=2314.17990945 | Val Loss=0.65232128 | Data=23.11688666 | Physics=2.55243769 | Val RMSE: 0.82328397 | ‚àö(Val Loss) = 0.80766410 | Current Learning Rate: 0.0001\n",
      "Epoch 909/1000 | Train Loss=2313.67099831 | Val Loss=0.65160153 | Data=23.11183132 | Physics=2.46238586 | Val RMSE: 0.82320297 | ‚àö(Val Loss) = 0.80721843 | Current Learning Rate: 0.0001\n",
      "Epoch 910/1000 | Train Loss=2317.48621715 | Val Loss=0.65137681 | Data=23.14996060 | Physics=2.54052146 | Val RMSE: 0.82318747 | ‚àö(Val Loss) = 0.80707920 | Current Learning Rate: 0.0001\n",
      "Epoch 911/1000 | Train Loss=2312.61408026 | Val Loss=0.65225028 | Data=23.10124345 | Physics=2.49159901 | Val RMSE: 0.82327843 | ‚àö(Val Loss) = 0.80762017 | Current Learning Rate: 0.0001\n",
      "Epoch 912/1000 | Train Loss=2315.31871449 | Val Loss=0.65227806 | Data=23.12827769 | Physics=2.54625426 | Val RMSE: 0.82328892 | ‚àö(Val Loss) = 0.80763733 | Current Learning Rate: 0.0001\n",
      "Epoch 913/1000 | Train Loss=2306.27370384 | Val Loss=0.65111153 | Data=23.03780140 | Physics=2.63059762 | Val RMSE: 0.82320631 | ‚àö(Val Loss) = 0.80691481 | Current Learning Rate: 0.0001\n",
      "Epoch 914/1000 | Train Loss=2314.63494318 | Val Loss=0.65157188 | Data=23.12144696 | Physics=2.55508080 | Val RMSE: 0.82335871 | ‚àö(Val Loss) = 0.80720001 | Current Learning Rate: 0.0001\n",
      "Epoch 915/1000 | Train Loss=2316.08948864 | Val Loss=0.65184917 | Data=23.13598737 | Physics=2.51210541 | Val RMSE: 0.82330942 | ‚àö(Val Loss) = 0.80737174 | Current Learning Rate: 0.0001\n",
      "Epoch 916/1000 | Train Loss=2316.69409180 | Val Loss=0.65112815 | Data=23.14202222 | Physics=2.54518534 | Val RMSE: 0.82319653 | ‚àö(Val Loss) = 0.80692512 | Current Learning Rate: 0.0001\n",
      "Epoch 917/1000 | Train Loss=2321.05126953 | Val Loss=0.65167263 | Data=23.18560808 | Physics=2.56009631 | Val RMSE: 0.82326341 | ‚àö(Val Loss) = 0.80726242 | Current Learning Rate: 0.0001\n",
      "Epoch 918/1000 | Train Loss=2312.43010920 | Val Loss=0.65132436 | Data=23.09940772 | Physics=2.51939231 | Val RMSE: 0.82319844 | ‚àö(Val Loss) = 0.80704671 | Current Learning Rate: 0.0001\n",
      "Epoch 919/1000 | Train Loss=2310.61186080 | Val Loss=0.65154002 | Data=23.08123346 | Physics=2.46637523 | Val RMSE: 0.82323033 | ‚àö(Val Loss) = 0.80718029 | Current Learning Rate: 0.0001\n",
      "Epoch 920/1000 | Train Loss=2329.01922053 | Val Loss=0.65176052 | Data=23.26531115 | Physics=2.45569666 | Val RMSE: 0.82325608 | ‚àö(Val Loss) = 0.80731684 | Current Learning Rate: 0.0001\n",
      "Epoch 921/1000 | Train Loss=2325.14473100 | Val Loss=0.65107115 | Data=23.22652903 | Physics=2.58748500 | Val RMSE: 0.82316804 | ‚àö(Val Loss) = 0.80688977 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  920 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 922/1000 | Train Loss=2324.07182173 | Val Loss=0.65108675 | Data=23.21580505 | Physics=2.56794809 | Val RMSE: 0.82317829 | ‚àö(Val Loss) = 0.80689949 | Current Learning Rate: 0.0001\n",
      "Epoch 923/1000 | Train Loss=2316.93059748 | Val Loss=0.65098855 | Data=23.14438525 | Physics=2.53878058 | Val RMSE: 0.82316214 | ‚àö(Val Loss) = 0.80683857 | Current Learning Rate: 0.0001\n",
      "Epoch 924/1000 | Train Loss=2317.30468750 | Val Loss=0.65154141 | Data=23.14815435 | Physics=2.49024761 | Val RMSE: 0.82325697 | ‚àö(Val Loss) = 0.80718118 | Current Learning Rate: 0.0001\n",
      "Epoch 925/1000 | Train Loss=2313.67924361 | Val Loss=0.65145663 | Data=23.11190553 | Physics=2.54695073 | Val RMSE: 0.82321382 | ‚àö(Val Loss) = 0.80712867 | Current Learning Rate: 0.0001\n",
      "Epoch 926/1000 | Train Loss=2308.42005504 | Val Loss=0.65176077 | Data=23.05930658 | Physics=2.47214850 | Val RMSE: 0.82320309 | ‚àö(Val Loss) = 0.80731702 | Current Learning Rate: 0.0001\n",
      "Epoch 927/1000 | Train Loss=2314.95296964 | Val Loss=0.65242366 | Data=23.12460587 | Physics=2.54797623 | Val RMSE: 0.82325619 | ‚àö(Val Loss) = 0.80772746 | Current Learning Rate: 0.0001\n",
      "Epoch 928/1000 | Train Loss=2311.91967773 | Val Loss=0.65160085 | Data=23.09430278 | Physics=2.49873343 | Val RMSE: 0.82320702 | ‚àö(Val Loss) = 0.80721796 | Current Learning Rate: 0.0001\n",
      "Epoch 929/1000 | Train Loss=2308.11783114 | Val Loss=0.65183129 | Data=23.05628326 | Physics=2.47583706 | Val RMSE: 0.82325596 | ‚àö(Val Loss) = 0.80736071 | Current Learning Rate: 0.0001\n",
      "Epoch 930/1000 | Train Loss=2315.24813565 | Val Loss=0.65226268 | Data=23.12757735 | Physics=2.58320252 | Val RMSE: 0.82328939 | ‚àö(Val Loss) = 0.80762780 | Current Learning Rate: 0.0001\n",
      "Epoch 931/1000 | Train Loss=2303.44830877 | Val Loss=0.65155716 | Data=23.00957472 | Physics=2.52265489 | Val RMSE: 0.82321662 | ‚àö(Val Loss) = 0.80719090 | Current Learning Rate: 0.0001\n",
      "Epoch 932/1000 | Train Loss=2312.82366388 | Val Loss=0.65062791 | Data=23.10333894 | Physics=2.53826178 | Val RMSE: 0.82315516 | ‚àö(Val Loss) = 0.80661511 | Current Learning Rate: 0.0001\n",
      "Epoch 933/1000 | Train Loss=2318.26708984 | Val Loss=0.65118348 | Data=23.15779530 | Physics=2.51785815 | Val RMSE: 0.82321680 | ‚àö(Val Loss) = 0.80695939 | Current Learning Rate: 0.0001\n",
      "Epoch 934/1000 | Train Loss=2308.21666371 | Val Loss=0.65172736 | Data=23.05724265 | Physics=2.58996043 | Val RMSE: 0.82324553 | ‚àö(Val Loss) = 0.80729634 | Current Learning Rate: 0.0001\n",
      "Epoch 935/1000 | Train Loss=2309.87178178 | Val Loss=0.65409319 | Data=23.07380035 | Physics=2.50912816 | Val RMSE: 0.82351536 | ‚àö(Val Loss) = 0.80876029 | Current Learning Rate: 0.0001\n",
      "Epoch 936/1000 | Train Loss=2317.13299006 | Val Loss=0.65301237 | Data=23.14640115 | Physics=2.56521402 | Val RMSE: 0.82335782 | ‚àö(Val Loss) = 0.80809182 | Current Learning Rate: 0.0001\n",
      "Epoch 937/1000 | Train Loss=2321.10748846 | Val Loss=0.65206932 | Data=23.18616069 | Physics=2.55470814 | Val RMSE: 0.82327455 | ‚àö(Val Loss) = 0.80750811 | Current Learning Rate: 0.0001\n",
      "Epoch 938/1000 | Train Loss=2309.31838157 | Val Loss=0.65132162 | Data=23.06829574 | Physics=2.48744458 | Val RMSE: 0.82317734 | ‚àö(Val Loss) = 0.80704498 | Current Learning Rate: 0.0001\n",
      "Epoch 939/1000 | Train Loss=2307.97733931 | Val Loss=0.65301503 | Data=23.05483038 | Physics=2.59649197 | Val RMSE: 0.82332635 | ‚àö(Val Loss) = 0.80809343 | Current Learning Rate: 0.0001\n",
      "Epoch 940/1000 | Train Loss=2313.66739169 | Val Loss=0.65265697 | Data=23.11174029 | Physics=2.59305837 | Val RMSE: 0.82329834 | ‚àö(Val Loss) = 0.80787188 | Current Learning Rate: 0.0001\n",
      "Epoch 941/1000 | Train Loss=2316.27317116 | Val Loss=0.65234412 | Data=23.13780039 | Physics=2.59949992 | Val RMSE: 0.82329261 | ‚àö(Val Loss) = 0.80767822 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  940 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 942/1000 | Train Loss=2316.05723988 | Val Loss=0.65234306 | Data=23.13564249 | Physics=2.56854424 | Val RMSE: 0.82330310 | ‚àö(Val Loss) = 0.80767757 | Current Learning Rate: 0.0001\n",
      "Epoch 943/1000 | Train Loss=2312.82976740 | Val Loss=0.65159777 | Data=23.10342529 | Physics=2.45216619 | Val RMSE: 0.82323503 | ‚àö(Val Loss) = 0.80721611 | Current Learning Rate: 0.0001\n",
      "Epoch 944/1000 | Train Loss=2317.54730779 | Val Loss=0.65210160 | Data=23.15059818 | Physics=2.43326968 | Val RMSE: 0.82326436 | ‚àö(Val Loss) = 0.80752808 | Current Learning Rate: 0.0001\n",
      "Epoch 945/1000 | Train Loss=2311.93903143 | Val Loss=0.65193915 | Data=23.09447410 | Physics=2.52000135 | Val RMSE: 0.82322812 | ‚àö(Val Loss) = 0.80742747 | Current Learning Rate: 0.0001\n",
      "Epoch 946/1000 | Train Loss=2320.41255327 | Val Loss=0.65201968 | Data=23.17921413 | Physics=2.48598812 | Val RMSE: 0.82325476 | ‚àö(Val Loss) = 0.80747736 | Current Learning Rate: 0.0001\n",
      "Epoch 947/1000 | Train Loss=2316.79263583 | Val Loss=0.65173634 | Data=23.14301907 | Physics=2.49785090 | Val RMSE: 0.82324857 | ‚àö(Val Loss) = 0.80730188 | Current Learning Rate: 0.0001\n",
      "Epoch 948/1000 | Train Loss=2303.02085183 | Val Loss=0.65150274 | Data=23.00529723 | Physics=2.56898197 | Val RMSE: 0.82321954 | ‚àö(Val Loss) = 0.80715722 | Current Learning Rate: 0.0001\n",
      "Epoch 949/1000 | Train Loss=2309.59684615 | Val Loss=0.65220863 | Data=23.07106902 | Physics=2.49943836 | Val RMSE: 0.82336152 | ‚àö(Val Loss) = 0.80759436 | Current Learning Rate: 0.0001\n",
      "Epoch 950/1000 | Train Loss=2312.97991388 | Val Loss=0.65277072 | Data=23.10487765 | Physics=2.55963669 | Val RMSE: 0.82340008 | ‚àö(Val Loss) = 0.80794227 | Current Learning Rate: 0.0001\n",
      "Epoch 951/1000 | Train Loss=2308.66890092 | Val Loss=0.65184064 | Data=23.06177486 | Physics=2.56356099 | Val RMSE: 0.82325792 | ‚àö(Val Loss) = 0.80736649 | Current Learning Rate: 0.0001\n",
      "Epoch 952/1000 | Train Loss=2318.58618164 | Val Loss=0.65311673 | Data=23.16096046 | Physics=2.46300194 | Val RMSE: 0.82336110 | ‚àö(Val Loss) = 0.80815637 | Current Learning Rate: 0.0001\n",
      "Epoch 953/1000 | Train Loss=2307.52800959 | Val Loss=0.65224635 | Data=23.05036978 | Physics=2.55756285 | Val RMSE: 0.82327157 | ‚àö(Val Loss) = 0.80761772 | Current Learning Rate: 0.0001\n",
      "Epoch 954/1000 | Train Loss=2310.78127219 | Val Loss=0.65717292 | Data=23.08281621 | Physics=2.63834836 | Val RMSE: 0.82380420 | ‚àö(Val Loss) = 0.81066203 | Current Learning Rate: 0.0001\n",
      "Epoch 955/1000 | Train Loss=2314.39923651 | Val Loss=0.65433796 | Data=23.11908913 | Physics=2.46609430 | Val RMSE: 0.82350564 | ‚àö(Val Loss) = 0.80891156 | Current Learning Rate: 0.0001\n",
      "Epoch 956/1000 | Train Loss=2316.37988281 | Val Loss=0.65236146 | Data=23.13891203 | Physics=2.48439913 | Val RMSE: 0.82327306 | ‚àö(Val Loss) = 0.80768895 | Current Learning Rate: 0.0001\n",
      "Epoch 957/1000 | Train Loss=2318.36894087 | Val Loss=0.65215212 | Data=23.15875730 | Physics=2.52954748 | Val RMSE: 0.82327294 | ‚àö(Val Loss) = 0.80755937 | Current Learning Rate: 0.0001\n",
      "Epoch 958/1000 | Train Loss=2319.19315962 | Val Loss=0.65214603 | Data=23.16700693 | Physics=2.55796234 | Val RMSE: 0.82331121 | ‚àö(Val Loss) = 0.80755562 | Current Learning Rate: 0.0001\n",
      "Epoch 959/1000 | Train Loss=2309.79372337 | Val Loss=0.65196634 | Data=23.07301365 | Physics=2.58329638 | Val RMSE: 0.82329261 | ‚àö(Val Loss) = 0.80744433 | Current Learning Rate: 0.0001\n",
      "Epoch 960/1000 | Train Loss=2319.05346680 | Val Loss=0.65263556 | Data=23.16564473 | Physics=2.44904295 | Val RMSE: 0.82337058 | ‚àö(Val Loss) = 0.80785865 | Current Learning Rate: 0.0001\n",
      "Epoch 961/1000 | Train Loss=2327.52117365 | Val Loss=0.65205864 | Data=23.25032581 | Physics=2.43445441 | Val RMSE: 0.82329744 | ‚àö(Val Loss) = 0.80750149 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  960 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 962/1000 | Train Loss=2314.51455966 | Val Loss=0.65154455 | Data=23.12023579 | Physics=2.52931044 | Val RMSE: 0.82323766 | ‚àö(Val Loss) = 0.80718309 | Current Learning Rate: 0.0001\n",
      "Epoch 963/1000 | Train Loss=2309.36265980 | Val Loss=0.65223822 | Data=23.06868414 | Physics=2.65692122 | Val RMSE: 0.82331836 | ‚àö(Val Loss) = 0.80761266 | Current Learning Rate: 0.0001\n",
      "Epoch 964/1000 | Train Loss=2316.88545366 | Val Loss=0.65500198 | Data=23.14392679 | Physics=2.50724682 | Val RMSE: 0.82358652 | ‚àö(Val Loss) = 0.80932194 | Current Learning Rate: 0.0001\n",
      "Epoch 965/1000 | Train Loss=2311.83817915 | Val Loss=0.65303245 | Data=23.09346771 | Physics=2.53437733 | Val RMSE: 0.82339400 | ‚àö(Val Loss) = 0.80810428 | Current Learning Rate: 0.0001\n",
      "Epoch 966/1000 | Train Loss=2313.69802024 | Val Loss=0.65263593 | Data=23.11207893 | Physics=2.52951596 | Val RMSE: 0.82334179 | ‚àö(Val Loss) = 0.80785888 | Current Learning Rate: 0.0001\n",
      "Epoch 967/1000 | Train Loss=2313.93177379 | Val Loss=0.65191180 | Data=23.11444074 | Physics=2.44306270 | Val RMSE: 0.82325727 | ‚àö(Val Loss) = 0.80741054 | Current Learning Rate: 0.0001\n",
      "Epoch 968/1000 | Train Loss=2311.71584251 | Val Loss=0.65172889 | Data=23.09226140 | Physics=2.54449513 | Val RMSE: 0.82321626 | ‚àö(Val Loss) = 0.80729729 | Current Learning Rate: 0.0001\n",
      "Epoch 969/1000 | Train Loss=2316.47476474 | Val Loss=0.65286118 | Data=23.13985651 | Physics=2.46612245 | Val RMSE: 0.82332921 | ‚àö(Val Loss) = 0.80799824 | Current Learning Rate: 0.0001\n",
      "Epoch 970/1000 | Train Loss=2310.45185991 | Val Loss=0.65269029 | Data=23.07961481 | Physics=2.52953016 | Val RMSE: 0.82332516 | ‚àö(Val Loss) = 0.80789250 | Current Learning Rate: 0.0001\n",
      "Epoch 971/1000 | Train Loss=2312.65487393 | Val Loss=0.65312974 | Data=23.10164677 | Physics=2.50224832 | Val RMSE: 0.82330072 | ‚àö(Val Loss) = 0.80816442 | Current Learning Rate: 0.0001\n",
      "Epoch 972/1000 | Train Loss=2312.32131126 | Val Loss=0.65297551 | Data=23.09830128 | Physics=2.52125810 | Val RMSE: 0.82332468 | ‚àö(Val Loss) = 0.80806899 | Current Learning Rate: 0.0001\n",
      "Epoch 973/1000 | Train Loss=2314.65522905 | Val Loss=0.65288229 | Data=23.12164341 | Physics=2.52393499 | Val RMSE: 0.82333982 | ‚àö(Val Loss) = 0.80801129 | Current Learning Rate: 0.0001\n",
      "Epoch 974/1000 | Train Loss=2309.30508700 | Val Loss=0.65231687 | Data=23.06812893 | Physics=2.61243944 | Val RMSE: 0.82329804 | ‚àö(Val Loss) = 0.80766135 | Current Learning Rate: 0.0001\n",
      "Epoch 975/1000 | Train Loss=2320.52618963 | Val Loss=0.65321863 | Data=23.18037675 | Physics=2.45478080 | Val RMSE: 0.82336152 | ‚àö(Val Loss) = 0.80821943 | Current Learning Rate: 0.0001\n",
      "Epoch 976/1000 | Train Loss=2316.82231001 | Val Loss=0.65274257 | Data=23.14329234 | Physics=2.57023743 | Val RMSE: 0.82332087 | ‚àö(Val Loss) = 0.80792487 | Current Learning Rate: 0.0001\n",
      "Epoch 977/1000 | Train Loss=2316.20567738 | Val Loss=0.65218251 | Data=23.13717634 | Physics=2.47316004 | Val RMSE: 0.82327831 | ‚àö(Val Loss) = 0.80757821 | Current Learning Rate: 0.0001\n",
      "Epoch 978/1000 | Train Loss=2317.80801669 | Val Loss=0.65277999 | Data=23.15318819 | Physics=2.47503080 | Val RMSE: 0.82332617 | ‚àö(Val Loss) = 0.80794799 | Current Learning Rate: 0.0001\n",
      "Epoch 979/1000 | Train Loss=2322.62260298 | Val Loss=0.65246508 | Data=23.20132030 | Physics=2.51778315 | Val RMSE: 0.82329971 | ‚àö(Val Loss) = 0.80775315 | Current Learning Rate: 0.0001\n",
      "Epoch 980/1000 | Train Loss=2320.61605558 | Val Loss=0.65223436 | Data=23.18122083 | Physics=2.60992914 | Val RMSE: 0.82326829 | ‚àö(Val Loss) = 0.80761027 | Current Learning Rate: 0.0001\n",
      "Epoch 981/1000 | Train Loss=2326.29639782 | Val Loss=0.65258454 | Data=23.23806000 | Physics=2.49576322 | Val RMSE: 0.82333434 | ‚àö(Val Loss) = 0.80782706 | Current Learning Rate: 0.0001\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        [  1.0019,  -3.5995, -11.3073],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.99498504 -3.3404703  -9.547409  ]\n",
      " [ 0.99498343 -3.340469   -9.547397  ]\n",
      " [ 0.9949811  -3.3404667  -9.547388  ]\n",
      " ...\n",
      " [ 0.9935515  -3.3390877  -9.541965  ]\n",
      " [ 0.99355054 -3.3390868  -9.541968  ]\n",
      " [ 0.99354964 -3.339086   -9.54196   ]] \n",
      "\n",
      "\n",
      " Epoch :  980 \n",
      " Target :  tensor([[  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        ...,\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017],\n",
      "        [  0.9908,  -3.6061, -10.8017]]) \n",
      " Prediction :  [[ 0.9935469  -3.3390834  -9.541944  ]\n",
      " [ 0.99354357 -3.33908    -9.541932  ]\n",
      " [ 0.99354076 -3.3390775  -9.5419235 ]\n",
      " ...\n",
      " [ 0.9891998  -3.334892   -9.52562   ]\n",
      " [ 0.9891954  -3.3348877  -9.525604  ]\n",
      " [ 0.9891915  -3.334884   -9.525592  ]] \n",
      "\n",
      "Final Test RMSE:  0.5717771053314209\n",
      "Epoch 982/1000 | Train Loss=2313.42460494 | Val Loss=0.65327066 | Data=23.10933217 | Physics=2.53723207 | Val RMSE: 0.82341200 | ‚àö(Val Loss) = 0.80825162 | Current Learning Rate: 0.0001\n",
      "Epoch 983/1000 | Train Loss=2311.36885210 | Val Loss=0.65340140 | Data=23.08877338 | Physics=2.57991732 | Val RMSE: 0.82337797 | ‚àö(Val Loss) = 0.80833244 | Current Learning Rate: 0.0001\n",
      "Epoch 984/1000 | Train Loss=2319.32062322 | Val Loss=0.65354151 | Data=23.16828520 | Physics=2.53802017 | Val RMSE: 0.82339078 | ‚àö(Val Loss) = 0.80841917 | Current Learning Rate: 0.0001\n",
      "Epoch 985/1000 | Train Loss=2314.90400835 | Val Loss=0.65235428 | Data=23.12412799 | Physics=2.53854728 | Val RMSE: 0.82328135 | ‚àö(Val Loss) = 0.80768454 | Current Learning Rate: 0.0001\n",
      "Epoch 986/1000 | Train Loss=2317.91397372 | Val Loss=0.65230857 | Data=23.15422925 | Physics=2.50802127 | Val RMSE: 0.82327414 | ‚àö(Val Loss) = 0.80765623 | Current Learning Rate: 0.0001\n",
      "Epoch 987/1000 | Train Loss=2316.36687678 | Val Loss=0.65267319 | Data=23.13877296 | Physics=2.48177979 | Val RMSE: 0.82334089 | ‚àö(Val Loss) = 0.80788189 | Current Learning Rate: 0.0001\n",
      "Epoch 988/1000 | Train Loss=2313.64459783 | Val Loss=0.65298950 | Data=23.11149129 | Physics=2.61263877 | Val RMSE: 0.82336146 | ‚àö(Val Loss) = 0.80807763 | Current Learning Rate: 0.0001\n",
      "Epoch 989/1000 | Train Loss=2310.57208807 | Val Loss=0.65286471 | Data=23.08082823 | Physics=2.48960917 | Val RMSE: 0.82333565 | ‚àö(Val Loss) = 0.80800045 | Current Learning Rate: 0.0001\n",
      "Epoch 990/1000 | Train Loss=2316.35874245 | Val Loss=0.65395742 | Data=23.13867968 | Physics=2.51215434 | Val RMSE: 0.82344037 | ‚àö(Val Loss) = 0.80867636 | Current Learning Rate: 0.0001\n",
      "Epoch 991/1000 | Train Loss=2311.15735973 | Val Loss=0.65334349 | Data=23.08668466 | Physics=2.50638824 | Val RMSE: 0.82336843 | ‚àö(Val Loss) = 0.80829668 | Current Learning Rate: 0.0001\n",
      "Epoch 992/1000 | Train Loss=2310.64681729 | Val Loss=0.65326831 | Data=23.08155094 | Physics=2.52102073 | Val RMSE: 0.82329291 | ‚àö(Val Loss) = 0.80825019 | Current Learning Rate: 0.0001\n",
      "Epoch 993/1000 | Train Loss=2311.81043590 | Val Loss=0.65423864 | Data=23.09317918 | Physics=2.54282796 | Val RMSE: 0.82352775 | ‚àö(Val Loss) = 0.80885017 | Current Learning Rate: 0.0001\n",
      "Epoch 994/1000 | Train Loss=2309.82430753 | Val Loss=0.65341920 | Data=23.07331865 | Physics=2.56202187 | Val RMSE: 0.82338184 | ‚àö(Val Loss) = 0.80834347 | Current Learning Rate: 0.0001\n",
      "Epoch 995/1000 | Train Loss=2311.09952060 | Val Loss=0.65349966 | Data=23.08606841 | Physics=2.53509954 | Val RMSE: 0.82335824 | ‚àö(Val Loss) = 0.80839324 | Current Learning Rate: 0.0001\n",
      "Epoch 996/1000 | Train Loss=2304.82205478 | Val Loss=0.65372787 | Data=23.02326237 | Physics=2.61864685 | Val RMSE: 0.82348698 | ‚àö(Val Loss) = 0.80853438 | Current Learning Rate: 0.0001\n",
      "Epoch 997/1000 | Train Loss=2316.65036843 | Val Loss=0.65036518 | Data=23.14161370 | Physics=2.53127149 | Val RMSE: 0.82329327 | ‚àö(Val Loss) = 0.80645221 | Current Learning Rate: 0.0001\n",
      "Epoch 998/1000 | Train Loss=2311.87983842 | Val Loss=0.65152618 | Data=23.09391022 | Physics=2.51930043 | Val RMSE: 0.82330394 | ‚àö(Val Loss) = 0.80717176 | Current Learning Rate: 0.0001\n",
      "Epoch 999/1000 | Train Loss=2314.07393022 | Val Loss=0.65151786 | Data=23.11584559 | Physics=2.50140088 | Val RMSE: 0.82319123 | ‚àö(Val Loss) = 0.80716658 | Current Learning Rate: 0.0001\n",
      "‚úÖ Saved last model at epoch 1000 \n",
      "Epoch 1000/1000 | Train Loss=2304.78485662 | Val Loss=0.65175915 | Data=23.02290778 | Physics=2.62189247 | Val RMSE: 0.82323873 | ‚àö(Val Loss) = 0.80731601 | Current Learning Rate: 0.0001\n",
      "‚úÖ Metrics saved successfully!\n",
      "Plot losses after training 3:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE :  1.1435542106628418\n"
     ]
    }
   ],
   "source": [
    "data_ranges = ['all','low','mid','high']\n",
    "for data_range in data_ranges:\n",
    "    train_model(WINDOW_SIZES,*datasets[data_range],data_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb8787f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:26.438044Z",
     "iopub.status.busy": "2026-02-08T15:20:26.437728Z",
     "iopub.status.idle": "2026-02-08T15:20:26.440958Z",
     "shell.execute_reply": "2026-02-08T15:20:26.440393Z"
    },
    "papermill": {
     "duration": 0.21702,
     "end_time": "2026-02-08T15:20:26.442334",
     "exception": false,
     "start_time": "2026-02-08T15:20:26.225314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# help(TensorDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd22986a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:26.957994Z",
     "iopub.status.busy": "2026-02-08T15:20:26.957724Z",
     "iopub.status.idle": "2026-02-08T15:20:26.960997Z",
     "shell.execute_reply": "2026-02-08T15:20:26.960443Z"
    },
    "papermill": {
     "duration": 0.220307,
     "end_time": "2026-02-08T15:20:26.962400",
     "exception": false,
     "start_time": "2026-02-08T15:20:26.742093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Use X_batch.shape[1] to dynamically get the correct window size (e.g., 6)\n",
    "# current_batch_size = X_batch.size(0)\n",
    "# current_window_size = X_batch.size(1) \n",
    "\n",
    "# x_seq = torch.arange(current_window_size, dtype=torch.float32, device=X_batch.device)\n",
    "# x_seq = x_seq.view(1, -1, 1).repeat(current_batch_size, 1, 1) # Shape: [32, 6, 1]\n",
    "\n",
    "# loss, data_loss, phys_loss = criterion(y_pred, y_batch, x_seq, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e052c13c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:27.391927Z",
     "iopub.status.busy": "2026-02-08T15:20:27.391670Z",
     "iopub.status.idle": "2026-02-08T15:20:27.401743Z",
     "shell.execute_reply": "2026-02-08T15:20:27.401137Z"
    },
    "papermill": {
     "duration": 0.226793,
     "end_time": "2026-02-08T15:20:27.403092",
     "exception": false,
     "start_time": "2026-02-08T15:20:27.176299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_to_X_y_tensor(df, window_size=5,output_size=5):\n",
    "    '''\n",
    "    Converts a time series into (X, y) tensors for LSTM training.\n",
    "    \n",
    "    X shape: (num_samples, window_size, 1)\n",
    "    y shape: (num_samples, 1)\n",
    "    '''\n",
    "    # if isinstance(df, (pd.DataFrame, pd.Series)):\n",
    "    #     df_as_np = df.to_numpy()\n",
    "    # else:\n",
    "    #     df_as_np = df  # Assume already numpy\n",
    "\n",
    "    X, y , y2 = [], [], []\n",
    "    #for i in range(len(df_as_np) - window_size):\n",
    "    X.append(list(df['SoH'])[:window_size+1])\n",
    "    #y.append([df_as_np[i + window_size:i + window_size+output_size]])\n",
    "    y.append([list(df['k'])[-1],list(df['a'])[-1],list(df['b'])[-1]])\n",
    "    #append([[val] for val in df_as_np[i + window_size:i + window_size+1]]) #next cycle\n",
    "    y2.append(list(df['rul'])[:1])\n",
    "    X,y,y2 = np.array(X),np.array(y), np.array(y2)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)#.squeeze()\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)#.squeeze()\n",
    "    y_2_tensor = torch.tensor(y2, dtype=torch.float32)  #bug was here written y instead of y2\n",
    "    return X_tensor, y_tensor, y_2_tensor\n",
    "\n",
    "def get_x_y_lists(paths):\n",
    "    X_list,y_list,y_target = [],[],[]\n",
    "    for path in paths:\n",
    "        print(path)\n",
    "        df = pd.read_csv(path)\n",
    "        df['Cycle number'] = df['Cycle number']/10000\n",
    "        df['rul'] = df['rul']/10000\n",
    "        #normalize SoH\n",
    "        df['SoH'] =  df['SoH']/soh_normalization_constant\n",
    "        df.index = df['Cycle number']\n",
    "        #SoH = df[model_columns]\n",
    "        X, y , y1 = df_to_X_y_tensor(df, window_size=WINDOW_SIZE,output_size=OUTPUT_SIZE)\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "        y_target.append(y1) #RUL\n",
    "    return X_list,y_list, y_target\n",
    "\n",
    "\n",
    "\n",
    "def give_paths_get_loaders(paths,data_type,shuffle=False):\n",
    "    X_list, y_list, y_target = get_x_y_lists(paths)\n",
    "\n",
    "    batch_size = torch.cat(X_list, dim=0).shape[0]\n",
    "    \n",
    "    if INPUT_SIZE == 1:\n",
    "        # Concatenate all X and y\n",
    "        X_1,y_1,y_2 = torch.cat(X_list, dim=0).unsqueeze(-1),torch.cat(y_list, dim=0).view(batch_size,-1),torch.cat(y_target, dim=0).view(batch_size,-1)\n",
    "    else:\n",
    "        X_1,y_1,y_2 = torch.cat(X_list, dim=0).squeeze(2),torch.cat(y_list, dim=0).view(-1,INPUT_SIZE),torch.cat(y_target, dim=0).view(-1,INPUT_SIZE)\n",
    "    \n",
    "    print(f\" X_{data_type} shape : {X_1.shape} , y_{data_type} shape : {y_1.shape} Ôºåy_2{data_type} shape: {y_2.shape}\" )\n",
    "    \n",
    "    #DataLoader\n",
    "    print(\"load : \")\n",
    "    loader = DataLoader(TensorDataset(X_1, y_1, y_2), batch_size=32, shuffle=shuffle)\n",
    "    print(f\"{data_type}loader lengths : \",loader.__len__())\n",
    "    return loader,X_1,y_1, y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba1e899b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:27.842004Z",
     "iopub.status.busy": "2026-02-08T15:20:27.841688Z",
     "iopub.status.idle": "2026-02-08T15:20:29.119193Z",
     "shell.execute_reply": "2026-02-08T15:20:29.118251Z"
    },
    "papermill": {
     "duration": 1.500358,
     "end_time": "2026-02-08T15:20:29.120762",
     "exception": false,
     "start_time": "2026-02-08T15:20:27.620404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\n",
      " X_all shape : torch.Size([55, 100, 1]) , y_all shape : torch.Size([55, 3]) Ôºåy_2all shape: torch.Size([55, 1])\n",
      "load : \n",
      "allloader lengths :  2\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n",
      " X_all shape : torch.Size([11, 100, 1]) , y_all shape : torch.Size([11, 3]) Ôºåy_2all shape: torch.Size([11, 1])\n",
      "load : \n",
      "allloader lengths :  1\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n",
      " X_all shape : torch.Size([11, 100, 1]) , y_all shape : torch.Size([11, 3]) Ôºåy_2all shape: torch.Size([11, 1])\n",
      "load : \n",
      "allloader lengths :  1\n",
      "Test RMSE :  2.145491361618042\n",
      "0.99287224 -3.456695 -10.95133\n",
      "RUL : 2197 cycles vs Target RUL : 1975.0000536441803\n",
      "0.99584293 -3.418214 -10.754566\n",
      "RUL : 2209 cycles vs Target RUL : 2284.9999368190765\n",
      "0.9910672 -3.4899518 -11.147712\n",
      "RUL : 2183 cycles vs Target RUL : 1875.0\n",
      "0.99190056 -3.472927 -11.043456\n",
      "RUL : 2190 cycles vs Target RUL : 1938.0000233650208\n",
      "0.99216986 -3.4683056 -11.01672\n",
      "RUL : 2192 cycles vs Target RUL : 1678.9999604225159\n",
      "0.99047476 -3.5058022 -11.253524\n",
      "RUL : 2175 cycles vs Target RUL : 1561.0000491142273\n",
      "0.9878433 -3.6117408 -12.320762\n",
      "RUL : 2066 cycles vs Target RUL : 1307.9999387264252\n",
      "0.993387 -3.4485435 -10.906904\n",
      "RUL : 2199 cycles vs Target RUL : 2364.9999499320984\n",
      "0.9872573 -3.6166937 -12.383966\n",
      "RUL : 2058 cycles vs Target RUL : 1419.0000295639038\n",
      "0.9905944 -3.5295973 -11.446668\n",
      "RUL : 2159 cycles vs Target RUL : 1386.0000669956207\n",
      "0.9845619 -3.6346729 -12.633575\n",
      "RUL : 2026 cycles vs Target RUL : 1503.9999783039093\n",
      "------------------------------\n",
      "\n",
      "\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-8-hust_gompertz_params.csv\n",
      " X_low shape : torch.Size([14, 100, 1]) , y_low shape : torch.Size([14, 3]) Ôºåy_2low shape: torch.Size([14, 1])\n",
      "load : \n",
      "lowloader lengths :  1\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-5-hust_gompertz_params.csv\n",
      " X_low shape : torch.Size([2, 100, 1]) , y_low shape : torch.Size([2, 3]) Ôºåy_2low shape: torch.Size([2, 1])\n",
      "load : \n",
      "lowloader lengths :  1\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n",
      " X_low shape : torch.Size([5, 100, 1]) , y_low shape : torch.Size([5, 3]) Ôºåy_2low shape: torch.Size([5, 1])\n",
      "load : \n",
      "lowloader lengths :  1\n",
      "Test RMSE :  1.2774525880813599\n",
      "0.95371073 -3.9243014 -19.244375\n",
      "RUL : 1429 cycles vs Target RUL : 1561.0000491142273\n",
      "0.9537601 -3.924331 -19.244612\n",
      "RUL : 1429 cycles vs Target RUL : 1307.9999387264252\n",
      "0.95376253 -3.9243324 -19.244623\n",
      "RUL : 1429 cycles vs Target RUL : 1419.0000295639038\n",
      "0.95371175 -3.9243019 -19.24438\n",
      "RUL : 1429 cycles vs Target RUL : 1386.0000669956207\n",
      "0.9537689 -3.9243362 -19.244652\n",
      "RUL : 1430 cycles vs Target RUL : 1503.9999783039093\n",
      "------------------------------\n",
      "\n",
      "\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-5-hust_gompertz_params.csv\n",
      " X_mid shape : torch.Size([26, 100, 1]) , y_mid shape : torch.Size([26, 3]) Ôºåy_2mid shape: torch.Size([26, 1])\n",
      "load : \n",
      "midloader lengths :  1\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-1-hust_gompertz_params.csv\n",
      " X_mid shape : torch.Size([5, 100, 1]) , y_mid shape : torch.Size([5, 3]) Ôºåy_2mid shape: torch.Size([5, 1])\n",
      "load : \n",
      "midloader lengths :  1\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n",
      " X_mid shape : torch.Size([4, 100, 1]) , y_mid shape : torch.Size([4, 3]) Ôºåy_2mid shape: torch.Size([4, 1])\n",
      "load : \n",
      "midloader lengths :  1\n",
      "Test RMSE :  0.4280466139316559\n",
      "0.9755078 -3.7711146 -13.823838\n",
      "RUL : 1930 cycles vs Target RUL : 1975.0000536441803\n",
      "0.97538406 -3.770685 -13.823502\n",
      "RUL : 1930 cycles vs Target RUL : 1875.0\n",
      "0.9754595 -3.770947 -13.823707\n",
      "RUL : 1930 cycles vs Target RUL : 1938.0000233650208\n",
      "0.9754784 -3.7710123 -13.823758\n",
      "RUL : 1930 cycles vs Target RUL : 1678.9999604225159\n",
      "------------------------------\n",
      "\n",
      "\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-2-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-7-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-8-hust_gompertz_params.csv\n",
      " X_high shape : torch.Size([15, 100, 1]) , y_high shape : torch.Size([15, 3]) Ôºåy_2high shape: torch.Size([15, 1])\n",
      "load : \n",
      "highloader lengths :  1\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/5-3-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/6-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-2-hust_gompertz_params.csv\n",
      " X_high shape : torch.Size([4, 100, 1]) , y_high shape : torch.Size([4, 3]) Ôºåy_2high shape: torch.Size([4, 1])\n",
      "load : \n",
      "highloader lengths :  1\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n",
      " X_high shape : torch.Size([2, 100, 1]) , y_high shape : torch.Size([2, 3]) Ôºåy_2high shape: torch.Size([2, 1])\n",
      "load : \n",
      "highloader lengths :  1\n",
      "Test RMSE :  0.5988370180130005\n",
      "0.99498504 -3.3404703 -9.547409\n",
      "RUL : 2404 cycles vs Target RUL : 2284.9999368190765\n",
      "0.9948007 -3.3402925 -9.546707\n",
      "RUL : 2404 cycles vs Target RUL : 2364.9999499320984\n",
      "------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inverse_gompertz_func(y,k,a,b):\n",
    "    return (a - np.log(np.log(k/y)))/b\n",
    "\n",
    "for data_range in data_ranges:\n",
    "    for WINDOW_SIZE in WINDOW_SIZES:\n",
    "        WINDOW_SIZE = WINDOW_SIZE - 1\n",
    "        train_paths,val_paths,test_paths = datasets[data_range]\n",
    "        train_loader,X_train,y_train,y_train_target= give_paths_get_loaders(train_paths,data_range,shuffle=True)\n",
    "        val_loader,X_val,y_val,y_val_target= give_paths_get_loaders(val_paths,data_range)\n",
    "        test_loader,X_test,y_test,y_test_target = give_paths_get_loaders(test_paths,data_range)\n",
    "    # print(float(torch.mean(y_train, dim=0)[0]),float(torch.mean(y_train, dim=0)[1]),float(torch.mean(y_train, dim=0)[2]))\n",
    "    # print()\n",
    "    # print()\n",
    "    model = PhysicsInformedLSTM(input_size=INPUT_SIZE).to(device)\n",
    "    model.load_state_dict(torch.load(f'/kaggle/working/best_lstm_model-window-100_model_pinn_data_{data_range}.pth'))\n",
    "    test_rmse = 0 \n",
    "    for X_batch, y_batch,y_target in test_loader:\n",
    "        # Calculate RMSE directly\n",
    "        \n",
    "        y_pred = model(X_batch.to(device)).cpu().detach().numpy()\n",
    "        test_rmse += root_mean_squared_error(y_batch, y_pred)\n",
    "    print('Test RMSE : ',test_rmse)    \n",
    "    \n",
    "\n",
    "    for i, pred in enumerate(y_pred):\n",
    "        k,a,b,target_rul = y_pred[i][0],y_pred[i][1],y_pred[i][2],y_target[i].item()*10000\n",
    "        \n",
    "        print(k,a,b)\n",
    "        \n",
    "        print(f'RUL : {inverse_gompertz_func(y=0.7,k=k,a=a,b=b)*10000:.0f} cycles vs Target RUL : {target_rul}')\n",
    "    print('-'*30)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c66cfad6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:29.557347Z",
     "iopub.status.busy": "2026-02-08T15:20:29.557068Z",
     "iopub.status.idle": "2026-02-08T15:20:29.560185Z",
     "shell.execute_reply": "2026-02-08T15:20:29.559541Z"
    },
    "papermill": {
     "duration": 0.223207,
     "end_time": "2026-02-08T15:20:29.561564",
     "exception": false,
     "start_time": "2026-02-08T15:20:29.338357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for sample in train_loader:\n",
    "#     print(sample)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7dc7c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:29.998571Z",
     "iopub.status.busy": "2026-02-08T15:20:29.998270Z",
     "iopub.status.idle": "2026-02-08T15:20:30.001691Z",
     "shell.execute_reply": "2026-02-08T15:20:30.001032Z"
    },
    "papermill": {
     "duration": 0.225879,
     "end_time": "2026-02-08T15:20:30.003137",
     "exception": false,
     "start_time": "2026-02-08T15:20:29.777258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b733c5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:30.439081Z",
     "iopub.status.busy": "2026-02-08T15:20:30.438416Z",
     "iopub.status.idle": "2026-02-08T15:20:30.441634Z",
     "shell.execute_reply": "2026-02-08T15:20:30.441055Z"
    },
    "papermill": {
     "duration": 0.22385,
     "end_time": "2026-02-08T15:20:30.442893",
     "exception": false,
     "start_time": "2026-02-08T15:20:30.219043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # help(model)\n",
    "# for X_batch, y_batch, y_target in train_loader:\n",
    "#     #optimizer.zero_grad()\n",
    "#     #Set computing environment\n",
    "#     X_batch, y_batch = X_batch.to(device), y_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b05e4b04",
   "metadata": {
    "_cell_guid": "35554e79-62f0-483e-95f8-aab488cb1af6",
    "_uuid": "ab16f4e0-1acc-499b-8d8e-43e88dc79030",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:30.961766Z",
     "iopub.status.busy": "2026-02-08T15:20:30.961474Z",
     "iopub.status.idle": "2026-02-08T15:20:30.964585Z",
     "shell.execute_reply": "2026-02-08T15:20:30.964036Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.22333,
     "end_time": "2026-02-08T15:20:30.965922",
     "exception": false,
     "start_time": "2026-02-08T15:20:30.742592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "756ac182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:31.398437Z",
     "iopub.status.busy": "2026-02-08T15:20:31.397945Z",
     "iopub.status.idle": "2026-02-08T15:20:31.401151Z",
     "shell.execute_reply": "2026-02-08T15:20:31.400547Z"
    },
    "papermill": {
     "duration": 0.218865,
     "end_time": "2026-02-08T15:20:31.402537",
     "exception": false,
     "start_time": "2026-02-08T15:20:31.183672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71a6c12f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:31.833345Z",
     "iopub.status.busy": "2026-02-08T15:20:31.833068Z",
     "iopub.status.idle": "2026-02-08T15:20:31.836220Z",
     "shell.execute_reply": "2026-02-08T15:20:31.835628Z"
    },
    "papermill": {
     "duration": 0.220662,
     "end_time": "2026-02-08T15:20:31.837519",
     "exception": false,
     "start_time": "2026-02-08T15:20:31.616857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f097ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:32.272032Z",
     "iopub.status.busy": "2026-02-08T15:20:32.271722Z",
     "iopub.status.idle": "2026-02-08T15:20:32.274831Z",
     "shell.execute_reply": "2026-02-08T15:20:32.274272Z"
    },
    "papermill": {
     "duration": 0.221664,
     "end_time": "2026-02-08T15:20:32.276137",
     "exception": false,
     "start_time": "2026-02-08T15:20:32.054473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2446151a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:32.705956Z",
     "iopub.status.busy": "2026-02-08T15:20:32.705677Z",
     "iopub.status.idle": "2026-02-08T15:20:32.708832Z",
     "shell.execute_reply": "2026-02-08T15:20:32.708293Z"
    },
    "papermill": {
     "duration": 0.220432,
     "end_time": "2026-02-08T15:20:32.710165",
     "exception": false,
     "start_time": "2026-02-08T15:20:32.489733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for X_batch, y_batch, y_target in test_loader:\n",
    "#     print(model(X_batch.to(device)))\n",
    "#     print(y_batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da6ba258",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:33.141447Z",
     "iopub.status.busy": "2026-02-08T15:20:33.141165Z",
     "iopub.status.idle": "2026-02-08T15:20:33.144383Z",
     "shell.execute_reply": "2026-02-08T15:20:33.143845Z"
    },
    "papermill": {
     "duration": 0.223544,
     "end_time": "2026-02-08T15:20:33.145788",
     "exception": false,
     "start_time": "2026-02-08T15:20:32.922244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_rmse = 0 \n",
    "\n",
    "# for X_batch, y_batch,y_target in test_loader:\n",
    "#     # Calculate RMSE directly\n",
    "#     y_pred = model(X_batch.to(device)).cpu().detach().numpy()\n",
    "#     test_rmse += root_mean_squared_error(y_batch, y_pred)\n",
    "# print('Test RMSE : ',test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f82b79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:33.664357Z",
     "iopub.status.busy": "2026-02-08T15:20:33.664074Z",
     "iopub.status.idle": "2026-02-08T15:20:33.667197Z",
     "shell.execute_reply": "2026-02-08T15:20:33.666563Z"
    },
    "papermill": {
     "duration": 0.224672,
     "end_time": "2026-02-08T15:20:33.668584",
     "exception": false,
     "start_time": "2026-02-08T15:20:33.443912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "811d9fa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:34.100788Z",
     "iopub.status.busy": "2026-02-08T15:20:34.100492Z",
     "iopub.status.idle": "2026-02-08T15:20:34.103664Z",
     "shell.execute_reply": "2026-02-08T15:20:34.103062Z"
    },
    "papermill": {
     "duration": 0.222039,
     "end_time": "2026-02-08T15:20:34.104929",
     "exception": false,
     "start_time": "2026-02-08T15:20:33.882890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #['6-6', '8-6', '10-1', '6-8', '8-8', '10-7', '10-6', '7-6', '4-5', '10-4', '3-1', '8-1', '9-6', '1-2', '6-1', '6-2', '8-5', '5-3', '2-5', '9-4', '7-5', '1-1']\n",
    "# y_batch, y_pred, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5867d779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:34.536483Z",
     "iopub.status.busy": "2026-02-08T15:20:34.535865Z",
     "iopub.status.idle": "2026-02-08T15:20:34.539311Z",
     "shell.execute_reply": "2026-02-08T15:20:34.538746Z"
    },
    "papermill": {
     "duration": 0.218696,
     "end_time": "2026-02-08T15:20:34.540634",
     "exception": false,
     "start_time": "2026-02-08T15:20:34.321938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def inverse_gompertz_func(y,k,a,b):\n",
    "#     return (a - np.log(np.log(k/y)))/b\n",
    "\n",
    "# for i, pred in enumerate(y_pred):\n",
    "#     k,a,b,target_rul = y_pred[i][0],y_pred[i][1],y_pred[i][2],y_target[i].item()*10000\n",
    "    \n",
    "#     print(k,a,b)\n",
    "    \n",
    "#     print(f'RUL : {inverse_gompertz_func(y=0.7,k=k,a=a,b=b)*10000:.0f} cycles vs Target RUL : {target_rul}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98da8530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:34.966959Z",
     "iopub.status.busy": "2026-02-08T15:20:34.966666Z",
     "iopub.status.idle": "2026-02-08T15:20:34.970628Z",
     "shell.execute_reply": "2026-02-08T15:20:34.970046Z"
    },
    "papermill": {
     "duration": 0.219138,
     "end_time": "2026-02-08T15:20:34.971900",
     "exception": false,
     "start_time": "2026-02-08T15:20:34.752762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def gompertz_func(x,k,a,b):\n",
    "    return k*np.exp(-np.exp(a-(b*x)))\n",
    "\n",
    "def gompertz_exponent_func(x,k,a,b):\n",
    "    return a-(b*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28b723b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:35.400141Z",
     "iopub.status.busy": "2026-02-08T15:20:35.399854Z",
     "iopub.status.idle": "2026-02-08T15:20:35.874838Z",
     "shell.execute_reply": "2026-02-08T15:20:35.874028Z"
    },
    "papermill": {
     "duration": 0.690514,
     "end_time": "2026-02-08T15:20:35.876292",
     "exception": false,
     "start_time": "2026-02-08T15:20:35.185778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('6-6', 0.999946950696718, -3.0463019901673665, -8.058702298233401, -3.045496119937543)\n",
      "('8-7', 0.960536968518218, -3.8912779384551865, -13.448726573064285, -3.88993306579788)\n",
      "('8-6', 0.9907509429027922, -3.6060510716281655, -10.801653050097192, -3.604970906323156)\n",
      "('9-1', 0.9891766657708968, -3.6505840630566007, -12.21416655437278, -3.6493626464011633)\n",
      "('10-', 0.9481215769904175, -4.211693705003083, -17.547137063751773, -4.209938991296708)\n",
      "('6-8', 1.0107420560063778, -2.9030997786747967, -7.691465017103098, -2.9023306321730864)\n",
      "('8-8', 0.98413059165122, -3.7445201053751354, -15.65627371654888, -3.7429544780034805)\n",
      "('10-', 0.9786268880932516, -3.700592087534084, -14.445176742202348, -3.6991475698598637)\n",
      "('3-5', 1.0099262656709476, -3.2992647017070773, -8.637530260413152, -3.298400948681036)\n",
      "('5-1', 0.9967531041266844, -3.1470462327694158, -8.325162160953383, -3.1462137165533206)\n",
      "('5-5', 0.9897916363777954, -3.3170924362716403, -14.18101023291364, -3.315674335248349)\n",
      "('7-1', 0.9711572804774884, -3.575039239519331, -14.258499516057528, -3.573613389567725)\n",
      "('2-7', 0.9816085817779644, -3.690469938888635, -11.465814604260702, -3.689323357428209)\n",
      "('10-', 1.001915470696929, -3.599474816014408, -11.307329465009976, -3.5983440830679068)\n",
      "('4-7', 1.0052259003082558, -3.407105896520339, -10.876646876210822, -3.406018231832718)\n",
      "('7-7', 0.946147991876142, -4.208655935983245, -17.71497672411127, -4.2068844383108335)\n",
      "('7-6', 0.9417780156098636, -4.459192427373631, -22.763029699141367, -4.456916124403717)\n",
      "('4-5', 0.9645748300540335, -4.011222592503926, -18.31586703008384, -4.009391005800918)\n",
      "('9-2', 0.9868162660323028, -3.5523454627640505, -11.455413817936458, -3.5511999213822567)\n",
      "('10-', 0.9660291957197452, -4.116644983133188, -16.335190867850276, -4.115011464046403)\n",
      "('3-1', 0.9821717134588616, -3.672732167031856, -13.070265487410865, -3.6714251404831146)\n",
      "('9-7', 0.9775381856439505, -3.744888811894645, -12.948923061859276, -3.743593919588459)\n",
      "('8-1', 0.9456276972788872, -4.333303161056529, -23.960673470163748, -4.330907093709513)\n",
      "('10-', 0.9593479646405142, -3.935482671348799, -19.795025034420167, -3.9335031688453572)\n",
      "('8-4', 0.951491728160938, -4.194247424452098, -15.876148349374354, -4.192659809617161)\n",
      "('4-6', 0.9556436039053372, -3.675950935516932, -17.921465786644312, -3.674158788938268)\n",
      "('4-4', 0.9446541576167672, -4.116515985488998, -19.43706976660556, -4.1145722785123375)\n",
      "('3-8', 0.998790862672299, -3.489105881736752, -10.105225673784416, -3.4880953591693733)\n",
      "('5-4', 0.9575242339770756, -3.869757189316574, -13.903639295285016, -3.8683668253870453)\n",
      "('9-6', 0.9894867250978762, -3.514648337342772, -13.6811165735078, -3.5132802256854214)\n",
      "('10-', 0.9748932220917949, -3.8520124764630586, -13.306799932569524, -3.8506817964698015)\n",
      "('7-8', 0.97426043797264, -3.848156231659844, -14.075337896306312, -3.8467486978702135)\n",
      "('5-2', 0.9737189970425392, -3.786715934788834, -13.759517077949551, -3.785339983081039)\n",
      "('9-8', 0.9793520149404712, -3.5667178792469256, -10.5803751100897, -3.5656598417359167)\n",
      "('1-2', 0.9878393112287064, -3.2905567616226774, -8.286660537755745, -3.289728095568902)\n",
      "('5-6', 0.992199632791357, -3.234753679404763, -8.78836361122989, -3.23387484304364)\n",
      "('10-', 0.9810927453141476, -3.696831703548425, -14.90488840010583, -3.695341214708414)\n",
      "('2-6', 0.9741816586297092, -3.929944284676785, -17.607317697644717, -3.928183552907021)\n",
      "('6-1', 0.9588366998743444, -3.640233365160378, -15.379144993044608, -3.6386954506610736)\n",
      "('2-4', 0.9666667211568614, -3.6660744576841258, -16.745222017238643, -3.6643999354824017)\n",
      "('1-4', 0.9673059424220348, -3.925555364526829, -18.654933380454832, -3.9236898711887833)\n",
      "('4-1', 0.974821967446186, -3.372626606036675, -9.707446711479887, -3.371655861365527)\n",
      "('1-6', 0.9929696191801812, -3.072267260322839, -17.486425834693904, -3.0705186177393697)\n",
      "('6-2', 0.996469739189045, -3.4288210477352727, -12.41892394491297, -3.4275791553407813)\n",
      "('8-5', 0.9443346786607972, -4.248933227509226, -22.64083448113504, -4.246669144061112)\n",
      "('5-7', 0.9509452700127012, -4.168676257617839, -20.346791664402623, -4.166641578451398)\n",
      "('1-5', 1.0269961764888282, -2.5394838913108764, -7.762704719391589, -2.538707620838937)\n",
      "('1-8', 1.176959439132687, -1.76136905371235, -4.594320481289704, -1.760909621664221)\n",
      "('5-3', 0.9844119628138948, -3.370905329648145, -8.364134928392389, -3.3700689161553057)\n",
      "('6-5', 0.9712489726304112, -3.576215979830769, -11.063467716199494, -3.575109633059149)\n",
      "('9-5', 0.9814640731936904, -3.618466236145764, -11.45437482678623, -3.6173207986630853)\n",
      "('4-8', 0.9758667820481126, -3.952479028448344, -16.603497593198004, -3.950818678689024)\n",
      "('7-2', 0.9830362000868677, -3.6081140523844697, -11.879995409053606, -3.6069260528435643)\n",
      "('2-5', 0.9918741565499856, -3.1225846814739864, -14.754183927612011, -3.1211092630812254)\n",
      "('7-3', 0.9380190201332748, -4.421884133200824, -24.77026606455475, -4.4194071065943685)\n",
      "('9-3', 0.9829966800119196, -3.780097805501982, -13.953569509172684, -3.7787024485510647)\n",
      "('9-4', 0.9828479237657588, -3.803723861433833, -13.514995536394917, -3.8023723618801935)\n",
      "('8-2', 0.9700145507747884, -3.770245304757344, -12.93207578012185, -3.7689520971793318)\n",
      "('10-', 0.9587643506561674, -3.966746560065031, -14.856415967509578, -3.96526091846828)\n",
      "('6-3', 0.9806756263864168, -3.726280764417439, -14.527732773997478, -3.7248279911400393)\n",
      "('3-2', 1.0002698989405732, -3.217233095172596, -9.342928016212378, -3.2162988023709747)\n",
      "('7-5', 0.9679687433672456, -3.982004648141131, -14.98298186355295, -3.9805063499547755)\n",
      "('3-7', 0.9668157454051522, -3.434256467160316, -9.241872331646816, -3.4333322799271513)\n",
      "('2-3', 0.9668372715267, -3.95717090564515, -16.188588649938964, -3.955552046780156)\n",
      "('1-3', 0.9665676417676108, -3.831582591469573, -14.469462795195676, -3.8301356451900532)\n",
      "('8-3', 0.9778512025135716, -3.686571756642753, -11.118306872408995, -3.685459925955512)\n",
      "('2-8', 0.950967890890836, -4.177833850384779, -20.089537392760224, -4.175824896645503)\n",
      "('7-4', 0.9387359424428108, -4.519553698685354, -23.65230300554172, -4.5171884683848)\n",
      "('4-2', 0.9682300546367292, -3.931617464161364, -15.636234694434467, -3.9300538406919205)\n",
      "('6-4', 0.966222823987164, -3.5907074328107025, -14.105482777800818, -3.5892968845329225)\n",
      "('1-1', 0.9476435264638574, -4.022110744135286, -18.819087759290067, -4.020228835359357)\n",
      "('3-3', 0.9740832906865424, -3.754155073650343, -15.56499408085347, -3.7525985742422576)\n",
      "('4-3', 0.943336599803412, -4.355165630424821, -27.431058619993472, -4.352422524562821)\n",
      "('3-4', 0.9616245947685256, -3.6340887875536914, -13.679658656239406, -3.6327208216880673)\n",
      "('2-2', 0.9833277323259622, -3.811998089978892, -10.046908660312472, -3.810993399112861)\n",
      "('1-7', 0.9584982828776633, -3.978633021713848, -16.753315677650253, -3.976957690146083)\n",
      "('3-6', 0.9851126867563308, -3.474965978645603, -9.437832325931804, -3.4740221954130095)\n"
     ]
    }
   ],
   "source": [
    "# Check if k,a and b are the correct k, a and b\n",
    "path = \"../input/generate-hust-data-gompertz-k-a-b/\"\n",
    "files = os.listdir(path)\n",
    "k_s, a_s, b_s ,e_s = [], [],[],[]\n",
    "#print(files)\n",
    "files = [f for f in files if re.match(r'^\\d', f) and f.endswith('-hust_gompertz_params.csv')]\n",
    "for file in files:\n",
    "    df = pd.read_csv(path+file)\n",
    "    #print(df.head())\n",
    "    df['exponent'] =  gompertz_exponent_func(x=df['rul']/10000,k=df['k'],a=df['a'],b=df['b'])\n",
    "    \n",
    "    answers = file[:3],list(df['k'])[-1], list(df['a'])[-1], list(df['b'])[-1], list(df['exponent'])[-1]\n",
    "    k_s.append(answers[1]), a_s.append(answers[2]), b_s.append(answers[3]) , e_s.append(answers[4])\n",
    "    print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77c7824e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:36.307784Z",
     "iopub.status.busy": "2026-02-08T15:20:36.307293Z",
     "iopub.status.idle": "2026-02-08T15:20:36.312383Z",
     "shell.execute_reply": "2026-02-08T15:20:36.311685Z"
    },
    "papermill": {
     "duration": 0.221287,
     "end_time": "2026-02-08T15:20:36.313743",
     "exception": false,
     "start_time": "2026-02-08T15:20:36.092456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9380190201332748,\n",
       " 1.176959439132687,\n",
       " -4.519553698685354,\n",
       " -1.76136905371235,\n",
       " -27.431058619993472,\n",
       " -4.594320481289704,\n",
       " -4.5171884683848,\n",
       " -1.760909621664221)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(k_s), max(k_s), min(a_s), max(a_s), min(b_s), max(b_s), min(e_s), max(e_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dfaa978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:36.819536Z",
     "iopub.status.busy": "2026-02-08T15:20:36.818854Z",
     "iopub.status.idle": "2026-02-08T15:20:36.857077Z",
     "shell.execute_reply": "2026-02-08T15:20:36.856209Z"
    },
    "papermill": {
     "duration": 0.253592,
     "end_time": "2026-02-08T15:20:36.858768",
     "exception": false,
     "start_time": "2026-02-08T15:20:36.605176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        file         k         a          b   rul\n",
      "0    6-6.csv  0.999947 -3.046302  -8.058702  2468\n",
      "1    7-8.csv  0.974260 -3.848156 -14.075338  1938\n",
      "2    5-3.csv  0.984412 -3.370905  -8.364135  2689\n",
      "3    6-8.csv  1.010742 -2.903100  -7.691465  2450\n",
      "4    5-5.csv  0.989792 -3.317092 -14.181010  1583\n",
      "5    2-4.csv  0.966667 -3.666074 -16.745222  1499\n",
      "6   10-3.csv  0.958764 -3.966747 -14.856416  1848\n",
      "7    1-4.csv  0.967306 -3.925555 -18.654933  1500\n",
      "8    6-3.csv  0.980676 -3.726281 -14.527733  1804\n",
      "9    5-6.csv  0.992200 -3.234754  -8.788364  2460\n",
      "10   8-3.csv  0.977851 -3.686572 -11.118307  2290\n",
      "11   3-6.csv  0.985113 -3.474966  -9.437832  2491\n",
      "12  10-4.csv  0.966029 -4.116645 -16.335191  1811\n",
      "13   9-6.csv  0.989487 -3.514648 -13.681117  1742\n",
      "14   3-2.csv  1.000270 -3.217233  -9.342928  2283\n",
      "15   9-2.csv  0.986816 -3.552345 -11.455414  2143\n",
      "16   9-7.csv  0.977538 -3.744889 -12.948923  2012\n",
      "17   6-5.csv  0.971249 -3.576216 -11.063468  2178\n",
      "18   4-8.csv  0.975867 -3.952479 -16.603498  1706\n",
      "19   5-7.csv  0.950945 -4.168676 -20.346792  1448\n",
      "20   1-5.csv  1.026996 -2.539484  -7.762705  1971\n",
      "21   1-7.csv  0.958498 -3.978633 -16.753316  1678\n",
      "22   1-8.csv  1.176959 -1.761369  -4.594320  2285\n",
      "23   8-7.csv  0.960537 -3.891278 -13.448727  2047\n",
      "24   6-2.csv  0.996470 -3.428821 -12.418924  1908\n",
      "25   3-3.csv  0.974083 -3.754155 -15.564994  1649\n",
      "26   6-1.csv  0.958837 -3.640233 -15.379145  1609\n",
      "27   2-7.csv  0.981609 -3.690470 -11.465815  2202\n",
      "28   1-2.csv  0.987839 -3.290557  -8.286661  2678\n",
      "29  10-7.csv  0.978627 -3.700592 -14.445177  1783\n",
      "30  10-1.csv  0.948122 -4.211694 -17.547137  1702\n",
      "31   9-1.csv  0.989177 -3.650584 -12.214167  2057\n",
      "32   8-5.csv  0.944335 -4.248933 -22.640834  1348\n",
      "33   9-4.csv  0.982848 -3.803724 -13.514996  1975\n",
      "34   4-4.csv  0.944654 -4.116516 -19.437070  1491\n",
      "35   3-4.csv  0.961625 -3.634089 -13.679659  1766\n",
      "36  10-6.csv  1.001915 -3.599475 -11.307329  2285\n",
      "37   5-2.csv  0.973719 -3.786716 -13.759517  1926\n",
      "38   7-5.csv  0.967969 -3.982005 -14.982982  1875\n",
      "39   4-3.csv  0.943337 -4.355166 -27.431059  1142\n",
      "40   5-4.csv  0.957524 -3.869757 -13.903639  1962\n",
      "41   3-5.csv  1.009926 -3.299265  -8.637530  2657\n",
      "42   7-2.csv  0.983036 -3.608114 -11.879995  2030\n",
      "43   3-7.csv  0.966816 -3.434256  -9.241872  2479\n",
      "44   1-3.csv  0.966568 -3.831583 -14.469463  1858\n",
      "45   9-5.csv  0.981464 -3.618466 -11.454375  2168\n",
      "46   3-1.csv  0.982172 -3.672732 -13.070265  1938\n",
      "47   4-1.csv  0.974822 -3.372627  -9.707447  2217\n",
      "48   9-3.csv  0.982997 -3.780098 -13.953570  1905\n",
      "49   7-1.csv  0.971157 -3.575039 -14.258500  1690\n",
      "        file         k         a          b   rul\n",
      "50   8-8.csv  0.984131 -3.744520 -15.656274  1679\n",
      "51   6-4.csv  0.966223 -3.590707 -14.105483  1717\n",
      "52   4-2.csv  0.968230 -3.931617 -15.636235  1782\n",
      "53   5-1.csv  0.996753 -3.147046  -8.325162  2507\n",
      "54   2-3.csv  0.966837 -3.957171 -16.188589  1751\n",
      "55   4-5.csv  0.964575 -4.011223 -18.315867  1561\n",
      "56   4-6.csv  0.955644 -3.675951 -17.921466  1380\n",
      "57   2-8.csv  0.950968 -4.177834 -20.089537  1481\n",
      "58   8-1.csv  0.945628 -4.333303 -23.960673  1308\n",
      "59  10-8.csv  0.959348 -3.935483 -19.795025  1400\n",
      "60   9-8.csv  0.979352 -3.566718 -10.580375  2308\n",
      "61   3-8.csv  0.998791 -3.489106 -10.105226  2342\n",
      "62   7-3.csv  0.938019 -4.421884 -24.770266  1295\n",
      "63   8-2.csv  0.970015 -3.770245 -12.932076  2041\n",
      "64   8-6.csv  0.990751 -3.606051 -10.801653  2365\n",
      "65   7-6.csv  0.941778 -4.459192 -22.763030  1419\n",
      "66  10-5.csv  0.974893 -3.852012 -13.306800  2030\n",
      "67   7-7.csv  0.946148 -4.208656 -17.714977  1685\n",
      "68   7-4.csv  0.938736 -4.519554 -23.652303  1393\n",
      "69   4-7.csv  1.005226 -3.407106 -10.876647  2216\n",
      "70   2-6.csv  0.974182 -3.929944 -17.607318  1572\n",
      "71  10-2.csv  0.981093 -3.696832 -14.904888  1697\n",
      "72   1-6.csv  0.992970 -3.072267 -17.486426  1143\n",
      "73   8-4.csv  0.951492 -4.194247 -15.876148  1885\n",
      "74   2-5.csv  0.991874 -3.122585 -14.754184  1386\n",
      "75   1-1.csv  0.947644 -4.022111 -18.819088  1504\n",
      "76   2-2.csv  0.983328 -3.811998 -10.046909  2651\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/generate-hust-data-gompertz-k-a-b/hust_gompertz_params.csv')\n",
    "df['exponent'] = df['a'] - df['b']\n",
    "print(df[['file','k','a','b','rul']].head(50))\n",
    "print(df[['file','k','a','b','rul']].tail(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946f231",
   "metadata": {
    "papermill": {
     "duration": 0.212908,
     "end_time": "2026-02-08T15:20:37.290547",
     "exception": false,
     "start_time": "2026-02-08T15:20:37.077639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mixture of Experts Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e975b53d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:37.726537Z",
     "iopub.status.busy": "2026-02-08T15:20:37.725925Z",
     "iopub.status.idle": "2026-02-08T15:20:37.732105Z",
     "shell.execute_reply": "2026-02-08T15:20:37.731363Z"
    },
    "papermill": {
     "duration": 0.229012,
     "end_time": "2026-02-08T15:20:37.733563",
     "exception": false,
     "start_time": "2026-02-08T15:20:37.504551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GompertzPINN(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model =  base_model\n",
    "        \n",
    "        # 1. Freeze the base model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.base_model.eval()\n",
    "        \n",
    "    def inverse_gompertz_layer(self, y, k, a, b):\n",
    "        # Formula: (a - ln(ln(k/y))) / b\n",
    "        # Safety: Ensure k/y > 1 to avoid NaN in first log, and ln(k/y) > 0 for second log\n",
    "        # We clamp ratio to be slightly > 1.0 + epsilon\n",
    "        ratio = torch.clamp(k / y, min=1.00001)\n",
    "        \n",
    "        term1 = torch.log(ratio)\n",
    "        term2 = torch.log(term1 + 1e-8) # Safety epsilon\n",
    "        \n",
    "        rul = (a - term2) / b\n",
    "        return rul\n",
    "\n",
    "    def forward(self, x, current_soh=None):\n",
    "        # 1. Get Parameters (k, a, b) from frozen model\n",
    "        # Assuming model output is shape (Batch, 3) -> [k, a, b]\n",
    "        params = self.base_model(x)\n",
    "        k, a, b = params[:, 0:1], params[:, 1:2], params[:, 2:3]\n",
    "        \n",
    "        # # 2. Get Current SoH (y)\n",
    "        # # If not provided, grab the last value from the input window\n",
    "        # # Assuming x shape is (Batch, Window, Features) and SoH is Feature 0\n",
    "        # if current_soh is None:\n",
    "        #     current_soh = x[:, -1, 0].unsqueeze(1)\n",
    "\n",
    "        # 3. Predict RUL\n",
    "        rul = self.inverse_gompertz_layer(0.7, k, a, b)*10000\n",
    "        return rul, k, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed039b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:38.199814Z",
     "iopub.status.busy": "2026-02-08T15:20:38.199114Z",
     "iopub.status.idle": "2026-02-08T15:20:38.442111Z",
     "shell.execute_reply": "2026-02-08T15:20:38.441341Z"
    },
    "papermill": {
     "duration": 0.462444,
     "end_time": "2026-02-08T15:20:38.443645",
     "exception": false,
     "start_time": "2026-02-08T15:20:37.981201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models loaded and wrapped successfully!\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/9-4-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/10-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/3-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-8-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/4-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-1-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/8-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/7-6-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/2-5-hust_gompertz_params.csv\n",
      "/kaggle/input/generate-hust-data-gompertz-k-a-b/1-1-hust_gompertz_params.csv\n",
      " X_all shape : torch.Size([11, 100, 1]) , y_all shape : torch.Size([11, 3]) Ôºåy_2all shape: torch.Size([11, 1])\n",
      "load : \n",
      "allloader lengths :  1\n",
      "tensor([  0.9828,  -3.8037, -13.5150]) tensor([0.1975])\n",
      "Low Model says: 1429.29--------------(tensor([[0.9536]]), tensor([[-3.9243]]), tensor([[-19.2441]]))\n",
      "Mid Model says: 1930.09--------------(tensor([[0.9755]]), tensor([[-3.7711]]), tensor([[-13.8238]]))\n",
      "High Model says: 2403.58--------------(tensor([[0.9948]]), tensor([[-3.3403]]), tensor([[-9.5467]]))\n"
     ]
    }
   ],
   "source": [
    "# # Create 3 independent PINNs\n",
    "# pinn_low  = GompertzPINN(f'best_lstm_model-window-100_model_pinn_data_{data_ranges[1]}.pth')\n",
    "# pinn_mid  = GompertzPINN(f'best_lstm_model-window-100_model_pinn_data_{data_ranges[2]}.pth')\n",
    "# pinn_high = GompertzPINN(f'best_lstm_model-window-100_model_pinn_data_{data_ranges[3]}.pth')\n",
    "WINDOW_SIZE = 99\n",
    "def load_and_wrap_model(weight_path):\n",
    "    # 1. Initialize the empty architecture\n",
    "    # Make sure INPUT_SIZE matches what you used during training\n",
    "    model = PhysicsInformedLSTM(input_size=INPUT_SIZE).to('cpu')\n",
    "    \n",
    "    # 2. Load the weights from the file\n",
    "    # We use map_location='cpu' to be safe, then move to device later\n",
    "    model.load_state_dict(torch.load(weight_path, map_location='cpu'))\n",
    "    \n",
    "    # 3. Wrap it in the Physics/Gompertz layer\n",
    "    pinn_wrapper = GompertzPINN(base_model=model)\n",
    "    \n",
    "    return pinn_wrapper.to('cpu')\n",
    "\n",
    "# --- Usage ---\n",
    "\n",
    "# 1. Define your paths\n",
    "path_low  = f'best_lstm_model-window-100_model_pinn_data_{data_ranges[1]}.pth'\n",
    "path_mid  = f'best_lstm_model-window-100_model_pinn_data_{data_ranges[2]}.pth'\n",
    "path_high = f'best_lstm_model-window-100_model_pinn_data_{data_ranges[3]}.pth'\n",
    "\n",
    "# 2. Create the 3 independent PINNs correctly\n",
    "pinn_low  = load_and_wrap_model(path_low)\n",
    "pinn_mid  = load_and_wrap_model(path_mid)\n",
    "pinn_high = load_and_wrap_model(path_high)\n",
    "\n",
    "print(\"‚úÖ Models loaded and wrapped successfully!\")\n",
    "\n",
    "train_paths,val_paths,test_paths = datasets[data_ranges[0]]\n",
    "test_loader,X_test,y_test,y_test_target = give_paths_get_loaders(test_paths,data_ranges[0])\n",
    "\n",
    "for X_batch, y_batch,y_target in test_loader:\n",
    "    break\n",
    "\n",
    "# Usage\n",
    "with torch.no_grad():\n",
    "    rul_l, k_l, a_l, b_l = pinn_low(X_batch[0].unsqueeze(0))\n",
    "    rul_m, k_m, a_m, b_m = pinn_mid(X_batch[0].unsqueeze(0))\n",
    "    rul_h, k_h, a_h, b_h = pinn_high(X_batch[0].unsqueeze(0))\n",
    "print(y_batch[0],y_target[0])\n",
    "print(f\"Low Model says: {rul_l.item():.2f}--------------{k_l, a_l, b_l}\")\n",
    "print(f\"Mid Model says: {rul_m.item():.2f}--------------{k_m, a_m, b_m}\")\n",
    "print(f\"High Model says: {rul_h.item():.2f}--------------{k_h, a_h, b_h}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d14f11c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:38.878691Z",
     "iopub.status.busy": "2026-02-08T15:20:38.878092Z",
     "iopub.status.idle": "2026-02-08T15:20:39.210033Z",
     "shell.execute_reply": "2026-02-08T15:20:39.209288Z"
    },
    "papermill": {
     "duration": 0.550932,
     "end_time": "2026-02-08T15:20:39.211605",
     "exception": false,
     "start_time": "2026-02-08T15:20:38.660673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.9536,  -3.9243, -19.2441],\n",
       "        [  0.9536,  -3.9242, -19.2439],\n",
       "        [  0.9537,  -3.9243, -19.2442],\n",
       "        [  0.9537,  -3.9243, -19.2441],\n",
       "        [  0.9537,  -3.9243, -19.2441],\n",
       "        [  0.9537,  -3.9243, -19.2444],\n",
       "        [  0.9538,  -3.9243, -19.2446],\n",
       "        [  0.9536,  -3.9243, -19.2441],\n",
       "        [  0.9538,  -3.9243, -19.2446],\n",
       "        [  0.9537,  -3.9243, -19.2444],\n",
       "        [  0.9538,  -3.9243, -19.2447]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PhysicsInformedLSTM(input_size=INPUT_SIZE).to(device)\n",
    "model.load_state_dict(torch.load(f'/kaggle/working/best_lstm_model-window-100_model_pinn_data_{data_ranges[1]}.pth'))\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()\n",
    "model(X_batch.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05a966",
   "metadata": {
    "papermill": {
     "duration": 0.21619,
     "end_time": "2026-02-08T15:20:39.645933",
     "exception": false,
     "start_time": "2026-02-08T15:20:39.429743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a104bd",
   "metadata": {
    "papermill": {
     "duration": 0.220219,
     "end_time": "2026-02-08T15:20:40.171168",
     "exception": false,
     "start_time": "2026-02-08T15:20:39.950949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Mixture of Experts Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4ef81ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:40.613452Z",
     "iopub.status.busy": "2026-02-08T15:20:40.612747Z",
     "iopub.status.idle": "2026-02-08T15:20:40.625068Z",
     "shell.execute_reply": "2026-02-08T15:20:40.624468Z"
    },
    "papermill": {
     "duration": 0.232095,
     "end_time": "2026-02-08T15:20:40.626423",
     "exception": false,
     "start_time": "2026-02-08T15:20:40.394328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. The MoE Model Architecture ---\n",
    "class TrainableGompertzMoE(nn.Module):\n",
    "    def __init__(self, model_low, model_mid, model_high, input_dim=1, window_size=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load Experts\n",
    "        self.expert_low = model_low\n",
    "        self.expert_mid = model_mid\n",
    "        self.expert_high = model_high\n",
    "        \n",
    "        # # Freeze Experts (We only train the Gate)\n",
    "        # self.freeze_expert(self.expert_low)\n",
    "        # self.freeze_expert(self.expert_mid)\n",
    "        # self.freeze_expert(self.expert_high)\n",
    "        # Partially Freeze Experts\n",
    "        # We freeze the LSTM (the \"Brain\") but train the FC (the \"Mouth\")\n",
    "        self.partial_freeze(self.expert_low)\n",
    "        self.partial_freeze(self.expert_mid)\n",
    "        self.partial_freeze(self.expert_high)\n",
    "        \n",
    "        # Gating Network (The Router)\n",
    "        # Decides which expert to trust based on the input curve\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(window_size * input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3), \n",
    "        )\n",
    "        \n",
    "    def freeze_expert(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.eval()\n",
    "\n",
    "    def partial_freeze(self, model):\n",
    "        # 1. Freeze EVERYTHING first\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 2. Unfreeze ONLY the Final Layer (self.fc)\n",
    "        # This assumes your expert class has a layer named 'fc'\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        # 3. Set mode (Keep LSTM in eval mode to disable Dropout randomness in the feature extractor)\n",
    "        model.eval()\n",
    "\n",
    "    def inverse_gompertz_layer(self, y, k, a, b):\n",
    "        # RUL = (a - ln(ln(k/y))) / b\n",
    "        # Safety Clamping\n",
    "        ratio = torch.clamp(k / y, min=1.0001)\n",
    "        term1 = torch.log(ratio)\n",
    "        term2 = torch.log(torch.clamp(term1, min=1e-6)) \n",
    "        rul = (a - term2) / (b - 1e-6)\n",
    "        return rul\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Window, Features]\n",
    "        \n",
    "        # 1. Get Expert Predictions\n",
    "        out_l = self.expert_low(x) # [Batch, 3]\n",
    "        out_m = self.expert_mid(x)\n",
    "        out_h = self.expert_high(x)\n",
    "        \n",
    "        expert_outputs = torch.stack([out_l, out_m, out_h], dim=1) # [Batch, 3, 3]\n",
    "        \n",
    "        # 2. Get Gate Weights\n",
    "        logits = self.gate(x) # [Batch, 3]\n",
    "        temperature = 5.0 # <--- The Magic Number. Start with 2.0 or 5.0\n",
    "        weights = F.softmax(logits * temperature, dim=1)\n",
    "        # 3. Weighted Average of Parameters\n",
    "        # bmm: Batch Matrix Multiply\n",
    "        weighted_params = torch.bmm(weights.unsqueeze(1), expert_outputs).squeeze(1)\n",
    "        \n",
    "        k_pred = weighted_params[:, 0:1]\n",
    "        a_pred = weighted_params[:, 1:2]\n",
    "        b_pred = weighted_params[:, 2:3]\n",
    "\n",
    "        # 4. Calculate RUL\n",
    "        rul_pred = self.inverse_gompertz_layer(0.7, k_pred, a_pred, b_pred)\n",
    "        \n",
    "        return rul_pred, k_pred, a_pred, b_pred, weights\n",
    "\n",
    "# --- 2. The MoE Loss Function ---\n",
    "def direct_rul_loss(pred_rul, true_rul, weights, pred_params, true_params):\n",
    "    \"\"\"\n",
    "    Optimizes for RUL accuracy while keeping parameters sane.\n",
    "    \"\"\"\n",
    "    # 1. RUL Error (Primary Goal) - Robust Huber Loss\n",
    "    rul_loss = F.smooth_l1_loss(pred_rul, true_rul)\n",
    "    \n",
    "    # 2. Parameter Consistency (Secondary Goal)\n",
    "    # Ensures the weighted k,a,b aren't wildly different from the single-model targets\n",
    "    param_loss = F.mse_loss(pred_params, true_params)\n",
    "    \n",
    "    # 3. Entropy Regularization\n",
    "    # Encourages the gate to be decisive (pick one expert) rather than averaging all 3\n",
    "    entropy_loss = -torch.mean(torch.sum(weights * torch.log(weights + 1e-8), dim=1))\n",
    "    \n",
    "    # Weights: RUL is king (1.0), Params help guide (0.1), Entropy for sharpness (0.01)\n",
    "    total_loss = (10000.0 * rul_loss) + (0.01 * param_loss) + (0.001 * entropy_loss)\n",
    "    \n",
    "    return total_loss, rul_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ec4418e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:41.061397Z",
     "iopub.status.busy": "2026-02-08T15:20:41.061101Z",
     "iopub.status.idle": "2026-02-08T15:20:41.078890Z",
     "shell.execute_reply": "2026-02-08T15:20:41.078260Z"
    },
    "papermill": {
     "duration": 0.237022,
     "end_time": "2026-02-08T15:20:41.080277",
     "exception": false,
     "start_time": "2026-02-08T15:20:40.843255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_moe_model(WINDOW_SIZES, train_paths, val_paths, test_paths, path_low, path_mid, path_high):\n",
    "    \n",
    "    # --- Helper: Data Preparation (Your Original Code) ---\n",
    "    def df_to_X_y_tensor(df, window_size=5, output_size=5):\n",
    "        X, y , y2 = [], [], []\n",
    "        # Create sequences\n",
    "        # Note: Ideally iterate carefully to avoid index errors, assuming your data is pre-padded or sufficient\n",
    "        df_vals = df['SoH'].values\n",
    "        df_k = df['k'].values\n",
    "        df_a = df['a'].values\n",
    "        df_b = df['b'].values\n",
    "        df_rul = df['rul'].values\n",
    "        \n",
    "        # Optimized loop\n",
    "        # for i in range(len(df) - window_size):\n",
    "        X.append(df_vals[i : i + window_size])\n",
    "        # Target params at the END of the window\n",
    "        y.append([df_k[-1], df_a[-1], df_b[-1]])\n",
    "        # Target RUL at the END of the window\n",
    "        y2.append(df_rul[:1]) #list(df['rul'])[:1]\n",
    "\n",
    "        X = np.array(X)[..., np.newaxis] # Add feature dim\n",
    "        y = np.array(y)\n",
    "        y2 = np.array(y2)\n",
    "        \n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), torch.tensor(y2, dtype=torch.float32)\n",
    "\n",
    "    def get_x_y_lists(paths, w_size):\n",
    "        X_list, y_list, y_target = [], [], []\n",
    "        for path in paths:\n",
    "            df = pd.read_csv(path)\n",
    "            # Normalization (ensure these match your previous training!)\n",
    "            df['Cycle number'] = df['Cycle number'] / 10000\n",
    "            df['rul'] = df['rul'] / 10000 \n",
    "            df['SoH'] = df['SoH'] # Assuming SoH is already 0-1 or normalized externally\n",
    "            \n",
    "            X, y, y1 = df_to_X_y_tensor(df, window_size=w_size)\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            y_target.append(y1)\n",
    "        return X_list, y_list, y_target\n",
    "\n",
    "    def give_paths_get_loaders(paths, w_size, shuffle=False):\n",
    "        X_list, y_list, y_target = get_x_y_lists(paths, w_size)\n",
    "        \n",
    "        # Concatenate\n",
    "        X_cat = torch.cat(X_list, dim=0)\n",
    "        y_cat = torch.cat(y_list, dim=0)\n",
    "        y_tar_cat = torch.cat(y_target, dim=0)\n",
    "        \n",
    "        print(f\"Dataset Shape: X={X_cat.shape}, Param_Y={y_cat.shape}, RUL_Y={y_tar_cat.shape}\")\n",
    "        \n",
    "        loader = DataLoader(TensorDataset(X_cat, y_cat, y_tar_cat), batch_size=32, shuffle=shuffle)\n",
    "        return loader\n",
    "\n",
    "    # --- Helper: Load Base Model ---\n",
    "    def load_base_model(path, input_size):\n",
    "        # Ensure this matches your generic PhysicsInformedLSTM class definition\n",
    "        model = PhysicsInformedLSTM(input_size=input_size) \n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        return model\n",
    "\n",
    "    # --- MAIN LOOP ---\n",
    "    for WINDOW_SIZE in WINDOW_SIZES:\n",
    "        #WINDOW_SIZE = WINDOW_SIZE + 1 \n",
    "        print(f\"\\nüöÄ STARTING TRAINING FOR WINDOW SIZE: {WINDOW_SIZE}\")\n",
    "        \n",
    "        # 1. Prepare Data\n",
    "        train_loader = give_paths_get_loaders(train_paths, WINDOW_SIZE, shuffle=True)\n",
    "        val_loader   = give_paths_get_loaders(val_paths, WINDOW_SIZE, shuffle=False)\n",
    "        test_loader  = give_paths_get_loaders(test_paths, WINDOW_SIZE, shuffle=False)\n",
    "        \n",
    "        # 2. Initialize Experts & MoE\n",
    "        print(\"Loading Experts...\")\n",
    "        INPUT_SIZE = 1 # Assuming SoH only\n",
    "        expert_low  = load_base_model(path_low, INPUT_SIZE)\n",
    "        expert_mid  = load_base_model(path_mid, INPUT_SIZE)\n",
    "        expert_high = load_base_model(path_high, INPUT_SIZE)\n",
    "        \n",
    "        model = TrainableGompertzMoE(\n",
    "            expert_low, expert_mid, expert_high, \n",
    "            input_dim=INPUT_SIZE, window_size=WINDOW_SIZE\n",
    "        ).to(device)\n",
    "        \n",
    "        # 3. Optimizer (Only train the Gate!)\n",
    "        optimizer = torch.optim.Adam(model.gate.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n",
    "        \n",
    "        # 4. Metrics Setup\n",
    "        best_val_loss = float('inf')\n",
    "        train_history = []\n",
    "        val_history = []\n",
    "        \n",
    "        # 5. Training Loop\n",
    "        num_epochs = 1000 # MoE converges faster than raw LSTM\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            total_rul_error = 0\n",
    "            \n",
    "            for X_batch, y_params_batch, y_rul_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_params_batch = y_params_batch.to(device)\n",
    "                y_rul_batch = y_rul_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward Pass\n",
    "                pred_rul, pred_k, pred_a, pred_b, gate_weights = model(X_batch)\n",
    "                \n",
    "                # Combined Params for loss calculation\n",
    "                pred_params = torch.cat([pred_k, pred_a, pred_b], dim=1)\n",
    "                \n",
    "                # Loss Calculation\n",
    "                loss, rul_l1 = direct_rul_loss(pred_rul, y_rul_batch, gate_weights, pred_params, y_params_batch)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "                total_rul_error += rul_l1\n",
    "                \n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            avg_rul_error = total_rul_error / len(train_loader)\n",
    "            \n",
    "            # --- Validation ---\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            val_rmse_accum = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_val, y_params_val, y_rul_val in val_loader:\n",
    "                    X_val = X_val.to(device)\n",
    "                    y_params_val = y_params_val.to(device)\n",
    "                    y_rul_val = y_rul_val.to(device)\n",
    "                    \n",
    "                    pred_rul, k, a, b, w = model(X_val)\n",
    "                    pred_params = torch.cat([k, a, b], dim=1)\n",
    "                    \n",
    "                    loss, _ = direct_rul_loss(pred_rul, y_rul_val, w, pred_params, y_params_val)\n",
    "                    \n",
    "                    # RMSE Calculation\n",
    "                    mse = F.mse_loss(pred_rul, y_rul_val)\n",
    "                    val_rmse_accum += torch.sqrt(mse).item()\n",
    "                    total_val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            avg_val_rmse = val_rmse_accum / len(val_loader)\n",
    "            \n",
    "            # Adjust LR\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | RUL L1: {avg_rul_error:.4f} | Val Loss: {avg_val_loss:.4f} | Val RMSE: {avg_val_rmse:.4f}\")\n",
    "            \n",
    "            # Save Best\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), f\"best_moe_model_window_{WINDOW_SIZE}.pth\")\n",
    "                print(\"‚úÖ Saved Best MoE Model\")\n",
    "                \n",
    "        # 6. Final Test\n",
    "        print(\"\\n--- Running Final Test ---\")\n",
    "        model.load_state_dict(torch.load(f\"best_moe_model_window_{WINDOW_SIZE}.pth\"))\n",
    "        model.eval()\n",
    "        test_rmse_total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_test, _, y_rul_test in test_loader:\n",
    "                X_test = X_test.to(device)\n",
    "                y_rul_test = y_rul_test.to(device)\n",
    "                \n",
    "                pred_rul, _, _, _, _ = model(X_test)\n",
    "                mse = F.mse_loss(pred_rul, y_rul_test)\n",
    "                test_rmse_total += torch.sqrt(mse).item()\n",
    "        \n",
    "        print(f\"üèÜ Final Test RMSE for Window {WINDOW_SIZE}: {test_rmse_total / len(test_loader):.4f}\")\n",
    "        # return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddf60d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:41.514729Z",
     "iopub.status.busy": "2026-02-08T15:20:41.514071Z",
     "iopub.status.idle": "2026-02-08T15:20:55.023763Z",
     "shell.execute_reply": "2026-02-08T15:20:55.022926Z"
    },
    "papermill": {
     "duration": 13.728315,
     "end_time": "2026-02-08T15:20:55.025278",
     "exception": false,
     "start_time": "2026-02-08T15:20:41.296963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ STARTING TRAINING FOR WINDOW SIZE: 100\n",
      "Dataset Shape: X=torch.Size([55, 100, 1]), Param_Y=torch.Size([55, 3]), RUL_Y=torch.Size([55, 1])\n",
      "Dataset Shape: X=torch.Size([11, 100, 1]), Param_Y=torch.Size([11, 3]), RUL_Y=torch.Size([11, 1])\n",
      "Dataset Shape: X=torch.Size([11, 100, 1]), Param_Y=torch.Size([11, 3]), RUL_Y=torch.Size([11, 1])\n",
      "Loading Experts...\n",
      "Epoch 1/1000 | Train Loss: 7.1404 | RUL L1: 0.0007 | Val Loss: 12.5243 | Val RMSE: 0.0499\n",
      "‚úÖ Saved Best MoE Model\n",
      "Epoch 2/1000 | Train Loss: 7.2429 | RUL L1: 0.0007 | Val Loss: 12.9315 | Val RMSE: 0.0507\n",
      "Epoch 3/1000 | Train Loss: 6.8409 | RUL L1: 0.0007 | Val Loss: 12.1277 | Val RMSE: 0.0491\n",
      "‚úÖ Saved Best MoE Model\n",
      "Epoch 4/1000 | Train Loss: 6.8844 | RUL L1: 0.0007 | Val Loss: 11.2576 | Val RMSE: 0.0473\n",
      "‚úÖ Saved Best MoE Model\n",
      "Epoch 5/1000 | Train Loss: 7.0949 | RUL L1: 0.0007 | Val Loss: 10.7518 | Val RMSE: 0.0462\n",
      "‚úÖ Saved Best MoE Model\n",
      "Epoch 6/1000 | Train Loss: 7.1126 | RUL L1: 0.0007 | Val Loss: 10.5488 | Val RMSE: 0.0458\n",
      "‚úÖ Saved Best MoE Model\n",
      "Epoch 7/1000 | Train Loss: 7.4261 | RUL L1: 0.0007 | Val Loss: 10.5271 | Val RMSE: 0.0457\n",
      "‚úÖ Saved Best MoE Model\n",
      "Epoch 8/1000 | Train Loss: 7.9679 | RUL L1: 0.0008 | Val Loss: 10.5575 | Val RMSE: 0.0458\n",
      "Epoch 9/1000 | Train Loss: 7.0034 | RUL L1: 0.0007 | Val Loss: 10.6686 | Val RMSE: 0.0460\n",
      "Epoch 10/1000 | Train Loss: 7.2569 | RUL L1: 0.0007 | Val Loss: 10.7801 | Val RMSE: 0.0463\n",
      "Epoch 11/1000 | Train Loss: 7.1902 | RUL L1: 0.0007 | Val Loss: 10.8377 | Val RMSE: 0.0464\n",
      "Epoch 12/1000 | Train Loss: 7.0247 | RUL L1: 0.0007 | Val Loss: 10.8252 | Val RMSE: 0.0464\n",
      "Epoch 13/1000 | Train Loss: 7.2532 | RUL L1: 0.0007 | Val Loss: 10.7563 | Val RMSE: 0.0462\n",
      "Epoch 14/1000 | Train Loss: 6.9444 | RUL L1: 0.0007 | Val Loss: 10.7002 | Val RMSE: 0.0461\n",
      "Epoch 15/1000 | Train Loss: 6.9953 | RUL L1: 0.0007 | Val Loss: 10.6606 | Val RMSE: 0.0460\n",
      "Epoch 16/1000 | Train Loss: 6.9502 | RUL L1: 0.0007 | Val Loss: 10.6394 | Val RMSE: 0.0460\n",
      "Epoch 17/1000 | Train Loss: 6.8827 | RUL L1: 0.0007 | Val Loss: 10.6364 | Val RMSE: 0.0460\n",
      "Epoch 18/1000 | Train Loss: 7.0034 | RUL L1: 0.0007 | Val Loss: 10.6541 | Val RMSE: 0.0460\n",
      "Epoch 19/1000 | Train Loss: 6.9317 | RUL L1: 0.0007 | Val Loss: 10.6593 | Val RMSE: 0.0460\n",
      "Epoch 20/1000 | Train Loss: 7.4449 | RUL L1: 0.0007 | Val Loss: 10.6402 | Val RMSE: 0.0460\n",
      "Epoch 21/1000 | Train Loss: 6.9099 | RUL L1: 0.0007 | Val Loss: 10.6285 | Val RMSE: 0.0459\n",
      "Epoch 22/1000 | Train Loss: 7.0599 | RUL L1: 0.0007 | Val Loss: 10.6242 | Val RMSE: 0.0459\n",
      "Epoch 23/1000 | Train Loss: 7.1471 | RUL L1: 0.0007 | Val Loss: 10.6220 | Val RMSE: 0.0459\n",
      "Epoch 24/1000 | Train Loss: 6.6059 | RUL L1: 0.0007 | Val Loss: 10.6278 | Val RMSE: 0.0459\n",
      "Epoch 25/1000 | Train Loss: 7.2373 | RUL L1: 0.0007 | Val Loss: 10.6280 | Val RMSE: 0.0459\n",
      "Epoch 26/1000 | Train Loss: 7.4207 | RUL L1: 0.0007 | Val Loss: 10.6386 | Val RMSE: 0.0460\n",
      "Epoch 27/1000 | Train Loss: 7.1654 | RUL L1: 0.0007 | Val Loss: 10.6515 | Val RMSE: 0.0460\n",
      "Epoch 28/1000 | Train Loss: 7.0587 | RUL L1: 0.0007 | Val Loss: 10.6568 | Val RMSE: 0.0460\n",
      "Epoch 29/1000 | Train Loss: 6.9335 | RUL L1: 0.0007 | Val Loss: 10.6707 | Val RMSE: 0.0460\n",
      "Epoch 30/1000 | Train Loss: 7.1435 | RUL L1: 0.0007 | Val Loss: 10.6787 | Val RMSE: 0.0461\n",
      "Epoch 31/1000 | Train Loss: 6.7426 | RUL L1: 0.0007 | Val Loss: 10.6715 | Val RMSE: 0.0460\n",
      "Epoch 32/1000 | Train Loss: 7.0009 | RUL L1: 0.0007 | Val Loss: 10.6670 | Val RMSE: 0.0460\n",
      "Epoch 33/1000 | Train Loss: 6.9529 | RUL L1: 0.0007 | Val Loss: 10.6651 | Val RMSE: 0.0460\n",
      "Epoch 34/1000 | Train Loss: 7.2385 | RUL L1: 0.0007 | Val Loss: 10.6649 | Val RMSE: 0.0460\n",
      "Epoch 35/1000 | Train Loss: 7.1625 | RUL L1: 0.0007 | Val Loss: 10.6597 | Val RMSE: 0.0460\n",
      "Epoch 36/1000 | Train Loss: 7.2223 | RUL L1: 0.0007 | Val Loss: 10.6544 | Val RMSE: 0.0460\n",
      "Epoch 37/1000 | Train Loss: 7.1472 | RUL L1: 0.0007 | Val Loss: 10.6460 | Val RMSE: 0.0460\n",
      "Epoch 38/1000 | Train Loss: 6.9954 | RUL L1: 0.0007 | Val Loss: 10.6411 | Val RMSE: 0.0460\n",
      "Epoch 39/1000 | Train Loss: 6.8840 | RUL L1: 0.0007 | Val Loss: 10.6329 | Val RMSE: 0.0460\n",
      "Epoch 40/1000 | Train Loss: 6.9791 | RUL L1: 0.0007 | Val Loss: 10.6253 | Val RMSE: 0.0459\n",
      "Epoch 41/1000 | Train Loss: 7.6247 | RUL L1: 0.0008 | Val Loss: 10.6229 | Val RMSE: 0.0459\n",
      "Epoch 42/1000 | Train Loss: 7.1782 | RUL L1: 0.0007 | Val Loss: 10.6176 | Val RMSE: 0.0459\n",
      "Epoch 43/1000 | Train Loss: 6.7567 | RUL L1: 0.0007 | Val Loss: 10.6128 | Val RMSE: 0.0459\n",
      "Epoch 44/1000 | Train Loss: 7.1260 | RUL L1: 0.0007 | Val Loss: 10.6112 | Val RMSE: 0.0459\n",
      "Epoch 45/1000 | Train Loss: 7.1613 | RUL L1: 0.0007 | Val Loss: 10.6101 | Val RMSE: 0.0459\n",
      "Epoch 46/1000 | Train Loss: 7.0291 | RUL L1: 0.0007 | Val Loss: 10.6088 | Val RMSE: 0.0459\n",
      "Epoch 47/1000 | Train Loss: 7.0720 | RUL L1: 0.0007 | Val Loss: 10.6068 | Val RMSE: 0.0459\n",
      "Epoch 48/1000 | Train Loss: 7.2647 | RUL L1: 0.0007 | Val Loss: 10.6067 | Val RMSE: 0.0459\n",
      "Epoch 49/1000 | Train Loss: 6.8182 | RUL L1: 0.0007 | Val Loss: 10.6062 | Val RMSE: 0.0459\n",
      "Epoch 50/1000 | Train Loss: 6.9336 | RUL L1: 0.0007 | Val Loss: 10.6049 | Val RMSE: 0.0459\n",
      "Epoch 51/1000 | Train Loss: 7.2925 | RUL L1: 0.0007 | Val Loss: 10.6003 | Val RMSE: 0.0459\n",
      "Epoch 52/1000 | Train Loss: 6.9698 | RUL L1: 0.0007 | Val Loss: 10.5982 | Val RMSE: 0.0459\n",
      "Epoch 53/1000 | Train Loss: 7.0268 | RUL L1: 0.0007 | Val Loss: 10.5971 | Val RMSE: 0.0459\n",
      "Epoch 54/1000 | Train Loss: 7.0516 | RUL L1: 0.0007 | Val Loss: 10.5977 | Val RMSE: 0.0459\n",
      "Epoch 55/1000 | Train Loss: 6.8095 | RUL L1: 0.0007 | Val Loss: 10.5970 | Val RMSE: 0.0459\n",
      "Epoch 56/1000 | Train Loss: 6.9301 | RUL L1: 0.0007 | Val Loss: 10.5967 | Val RMSE: 0.0459\n",
      "Epoch 57/1000 | Train Loss: 6.7513 | RUL L1: 0.0007 | Val Loss: 10.5968 | Val RMSE: 0.0459\n",
      "Epoch 58/1000 | Train Loss: 6.9323 | RUL L1: 0.0007 | Val Loss: 10.5972 | Val RMSE: 0.0459\n",
      "Epoch 59/1000 | Train Loss: 6.6198 | RUL L1: 0.0007 | Val Loss: 10.5972 | Val RMSE: 0.0459\n",
      "Epoch 60/1000 | Train Loss: 7.1212 | RUL L1: 0.0007 | Val Loss: 10.5974 | Val RMSE: 0.0459\n",
      "Epoch 61/1000 | Train Loss: 6.7904 | RUL L1: 0.0007 | Val Loss: 10.5983 | Val RMSE: 0.0459\n",
      "Epoch 62/1000 | Train Loss: 7.4658 | RUL L1: 0.0007 | Val Loss: 10.5978 | Val RMSE: 0.0459\n",
      "Epoch 63/1000 | Train Loss: 7.3638 | RUL L1: 0.0007 | Val Loss: 10.5972 | Val RMSE: 0.0459\n",
      "Epoch 64/1000 | Train Loss: 6.9143 | RUL L1: 0.0007 | Val Loss: 10.5971 | Val RMSE: 0.0459\n",
      "Epoch 65/1000 | Train Loss: 7.3808 | RUL L1: 0.0007 | Val Loss: 10.5962 | Val RMSE: 0.0459\n",
      "Epoch 66/1000 | Train Loss: 6.8225 | RUL L1: 0.0007 | Val Loss: 10.5949 | Val RMSE: 0.0459\n",
      "Epoch 67/1000 | Train Loss: 6.8904 | RUL L1: 0.0007 | Val Loss: 10.5930 | Val RMSE: 0.0459\n",
      "Epoch 68/1000 | Train Loss: 6.7582 | RUL L1: 0.0007 | Val Loss: 10.5921 | Val RMSE: 0.0459\n",
      "Epoch 69/1000 | Train Loss: 6.9077 | RUL L1: 0.0007 | Val Loss: 10.5921 | Val RMSE: 0.0459\n",
      "Epoch 70/1000 | Train Loss: 7.4923 | RUL L1: 0.0007 | Val Loss: 10.5917 | Val RMSE: 0.0459\n",
      "Epoch 71/1000 | Train Loss: 7.0639 | RUL L1: 0.0007 | Val Loss: 10.5909 | Val RMSE: 0.0459\n",
      "Epoch 72/1000 | Train Loss: 7.3371 | RUL L1: 0.0007 | Val Loss: 10.5905 | Val RMSE: 0.0459\n",
      "Epoch 73/1000 | Train Loss: 7.2051 | RUL L1: 0.0007 | Val Loss: 10.5901 | Val RMSE: 0.0459\n",
      "Epoch 74/1000 | Train Loss: 7.3312 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 75/1000 | Train Loss: 7.2366 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 76/1000 | Train Loss: 6.9355 | RUL L1: 0.0007 | Val Loss: 10.5890 | Val RMSE: 0.0459\n",
      "Epoch 77/1000 | Train Loss: 6.8286 | RUL L1: 0.0007 | Val Loss: 10.5886 | Val RMSE: 0.0459\n",
      "Epoch 78/1000 | Train Loss: 7.1711 | RUL L1: 0.0007 | Val Loss: 10.5883 | Val RMSE: 0.0459\n",
      "Epoch 79/1000 | Train Loss: 7.1054 | RUL L1: 0.0007 | Val Loss: 10.5878 | Val RMSE: 0.0459\n",
      "Epoch 80/1000 | Train Loss: 7.4102 | RUL L1: 0.0007 | Val Loss: 10.5876 | Val RMSE: 0.0459\n",
      "Epoch 81/1000 | Train Loss: 6.8278 | RUL L1: 0.0007 | Val Loss: 10.5876 | Val RMSE: 0.0459\n",
      "Epoch 82/1000 | Train Loss: 6.9152 | RUL L1: 0.0007 | Val Loss: 10.5878 | Val RMSE: 0.0459\n",
      "Epoch 83/1000 | Train Loss: 6.9527 | RUL L1: 0.0007 | Val Loss: 10.5883 | Val RMSE: 0.0459\n",
      "Epoch 84/1000 | Train Loss: 7.1158 | RUL L1: 0.0007 | Val Loss: 10.5888 | Val RMSE: 0.0459\n",
      "Epoch 85/1000 | Train Loss: 6.8947 | RUL L1: 0.0007 | Val Loss: 10.5890 | Val RMSE: 0.0459\n",
      "Epoch 86/1000 | Train Loss: 6.9433 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 87/1000 | Train Loss: 7.0869 | RUL L1: 0.0007 | Val Loss: 10.5893 | Val RMSE: 0.0459\n",
      "Epoch 88/1000 | Train Loss: 7.0309 | RUL L1: 0.0007 | Val Loss: 10.5893 | Val RMSE: 0.0459\n",
      "Epoch 89/1000 | Train Loss: 7.0813 | RUL L1: 0.0007 | Val Loss: 10.5893 | Val RMSE: 0.0459\n",
      "Epoch 90/1000 | Train Loss: 7.1412 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 91/1000 | Train Loss: 6.9071 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 92/1000 | Train Loss: 6.9942 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 93/1000 | Train Loss: 7.3327 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 94/1000 | Train Loss: 6.9840 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 95/1000 | Train Loss: 7.4076 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 96/1000 | Train Loss: 7.0902 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 97/1000 | Train Loss: 7.0754 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 98/1000 | Train Loss: 7.0482 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 99/1000 | Train Loss: 7.0163 | RUL L1: 0.0007 | Val Loss: 10.5893 | Val RMSE: 0.0459\n",
      "Epoch 100/1000 | Train Loss: 7.1957 | RUL L1: 0.0007 | Val Loss: 10.5892 | Val RMSE: 0.0459\n",
      "Epoch 101/1000 | Train Loss: 6.8826 | RUL L1: 0.0007 | Val Loss: 10.5892 | Val RMSE: 0.0459\n",
      "Epoch 102/1000 | Train Loss: 7.3043 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 103/1000 | Train Loss: 6.9022 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 104/1000 | Train Loss: 6.9994 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 105/1000 | Train Loss: 7.1595 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 106/1000 | Train Loss: 6.9645 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 107/1000 | Train Loss: 7.2410 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 108/1000 | Train Loss: 6.9203 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 109/1000 | Train Loss: 6.9946 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 110/1000 | Train Loss: 7.2733 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 111/1000 | Train Loss: 7.1222 | RUL L1: 0.0007 | Val Loss: 10.5890 | Val RMSE: 0.0459\n",
      "Epoch 112/1000 | Train Loss: 6.7664 | RUL L1: 0.0007 | Val Loss: 10.5890 | Val RMSE: 0.0459\n",
      "Epoch 113/1000 | Train Loss: 6.5632 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 114/1000 | Train Loss: 6.7780 | RUL L1: 0.0007 | Val Loss: 10.5891 | Val RMSE: 0.0459\n",
      "Epoch 115/1000 | Train Loss: 6.9780 | RUL L1: 0.0007 | Val Loss: 10.5892 | Val RMSE: 0.0459\n",
      "Epoch 116/1000 | Train Loss: 7.0328 | RUL L1: 0.0007 | Val Loss: 10.5892 | Val RMSE: 0.0459\n",
      "Epoch 117/1000 | Train Loss: 6.8419 | RUL L1: 0.0007 | Val Loss: 10.5892 | Val RMSE: 0.0459\n",
      "Epoch 118/1000 | Train Loss: 7.4799 | RUL L1: 0.0007 | Val Loss: 10.5892 | Val RMSE: 0.0459\n",
      "Epoch 119/1000 | Train Loss: 6.8990 | RUL L1: 0.0007 | Val Loss: 10.5893 | Val RMSE: 0.0459\n",
      "Epoch 120/1000 | Train Loss: 6.6680 | RUL L1: 0.0007 | Val Loss: 10.5893 | Val RMSE: 0.0459\n",
      "Epoch 121/1000 | Train Loss: 7.0416 | RUL L1: 0.0007 | Val Loss: 10.5893 | Val RMSE: 0.0459\n",
      "Epoch 122/1000 | Train Loss: 7.3509 | RUL L1: 0.0007 | Val Loss: 10.5893 | Val RMSE: 0.0459\n",
      "Epoch 123/1000 | Train Loss: 7.0869 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 124/1000 | Train Loss: 7.1550 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 125/1000 | Train Loss: 7.2471 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 126/1000 | Train Loss: 7.0010 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 127/1000 | Train Loss: 7.1264 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 128/1000 | Train Loss: 6.7787 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 129/1000 | Train Loss: 6.8543 | RUL L1: 0.0007 | Val Loss: 10.5894 | Val RMSE: 0.0459\n",
      "Epoch 130/1000 | Train Loss: 7.1872 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 131/1000 | Train Loss: 7.2296 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 132/1000 | Train Loss: 6.9416 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 133/1000 | Train Loss: 7.0221 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 134/1000 | Train Loss: 7.1589 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 135/1000 | Train Loss: 6.9966 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 136/1000 | Train Loss: 7.1764 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 137/1000 | Train Loss: 6.9909 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 138/1000 | Train Loss: 6.7129 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 139/1000 | Train Loss: 7.1223 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 140/1000 | Train Loss: 7.2896 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 141/1000 | Train Loss: 6.9908 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 142/1000 | Train Loss: 7.0232 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 143/1000 | Train Loss: 6.8837 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 144/1000 | Train Loss: 7.4725 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 145/1000 | Train Loss: 6.7426 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 146/1000 | Train Loss: 6.7528 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 147/1000 | Train Loss: 7.1370 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 148/1000 | Train Loss: 6.9561 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 149/1000 | Train Loss: 7.0347 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 150/1000 | Train Loss: 7.0604 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 151/1000 | Train Loss: 7.1269 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 152/1000 | Train Loss: 7.1694 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 153/1000 | Train Loss: 6.4904 | RUL L1: 0.0006 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 154/1000 | Train Loss: 7.3370 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 155/1000 | Train Loss: 6.8999 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 156/1000 | Train Loss: 7.2198 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 157/1000 | Train Loss: 7.0583 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 158/1000 | Train Loss: 7.0822 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 159/1000 | Train Loss: 6.9110 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 160/1000 | Train Loss: 7.0940 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 161/1000 | Train Loss: 7.1803 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 162/1000 | Train Loss: 7.0023 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 163/1000 | Train Loss: 6.9638 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 164/1000 | Train Loss: 7.2945 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 165/1000 | Train Loss: 7.1788 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 166/1000 | Train Loss: 7.0379 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 167/1000 | Train Loss: 6.8997 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 168/1000 | Train Loss: 6.9683 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 169/1000 | Train Loss: 6.9659 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 170/1000 | Train Loss: 6.8982 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 171/1000 | Train Loss: 6.9294 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 172/1000 | Train Loss: 7.1166 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 173/1000 | Train Loss: 7.2392 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 174/1000 | Train Loss: 6.6196 | RUL L1: 0.0007 | Val Loss: 10.5895 | Val RMSE: 0.0459\n",
      "Epoch 175/1000 | Train Loss: 7.2106 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 176/1000 | Train Loss: 7.4788 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 177/1000 | Train Loss: 7.0847 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 178/1000 | Train Loss: 7.2456 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 179/1000 | Train Loss: 7.2367 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 180/1000 | Train Loss: 7.0746 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 181/1000 | Train Loss: 6.7740 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 182/1000 | Train Loss: 7.3166 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 183/1000 | Train Loss: 6.8837 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 184/1000 | Train Loss: 7.1101 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 185/1000 | Train Loss: 7.0574 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 186/1000 | Train Loss: 6.8425 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 187/1000 | Train Loss: 7.1001 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 188/1000 | Train Loss: 7.3932 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 189/1000 | Train Loss: 7.0124 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 190/1000 | Train Loss: 6.9256 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 191/1000 | Train Loss: 7.0363 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 192/1000 | Train Loss: 6.9219 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 193/1000 | Train Loss: 6.9815 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 194/1000 | Train Loss: 7.1170 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 195/1000 | Train Loss: 7.1633 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 196/1000 | Train Loss: 6.9973 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 197/1000 | Train Loss: 6.9214 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 198/1000 | Train Loss: 7.1832 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 199/1000 | Train Loss: 7.3556 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 200/1000 | Train Loss: 7.1060 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 201/1000 | Train Loss: 7.4712 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 202/1000 | Train Loss: 6.8919 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 203/1000 | Train Loss: 7.3368 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 204/1000 | Train Loss: 7.2458 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 205/1000 | Train Loss: 7.0352 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 206/1000 | Train Loss: 7.0667 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 207/1000 | Train Loss: 7.0204 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 208/1000 | Train Loss: 7.3763 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 209/1000 | Train Loss: 7.0507 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 210/1000 | Train Loss: 7.0909 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 211/1000 | Train Loss: 7.0806 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 212/1000 | Train Loss: 7.0743 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 213/1000 | Train Loss: 7.4029 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 214/1000 | Train Loss: 7.0741 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 215/1000 | Train Loss: 6.9592 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 216/1000 | Train Loss: 7.0825 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 217/1000 | Train Loss: 6.8299 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 218/1000 | Train Loss: 6.9651 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 219/1000 | Train Loss: 6.9660 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 220/1000 | Train Loss: 6.7677 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 221/1000 | Train Loss: 6.9154 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 222/1000 | Train Loss: 7.0654 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 223/1000 | Train Loss: 7.1826 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 224/1000 | Train Loss: 7.0703 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 225/1000 | Train Loss: 7.2634 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 226/1000 | Train Loss: 7.0420 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 227/1000 | Train Loss: 7.0292 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 228/1000 | Train Loss: 7.2744 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 229/1000 | Train Loss: 7.3186 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 230/1000 | Train Loss: 6.8136 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 231/1000 | Train Loss: 7.2994 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 232/1000 | Train Loss: 6.9275 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 233/1000 | Train Loss: 7.3012 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 234/1000 | Train Loss: 7.2651 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 235/1000 | Train Loss: 7.2477 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 236/1000 | Train Loss: 6.9780 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 237/1000 | Train Loss: 6.8315 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 238/1000 | Train Loss: 6.8818 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 239/1000 | Train Loss: 6.9932 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 240/1000 | Train Loss: 7.4537 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 241/1000 | Train Loss: 7.1635 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 242/1000 | Train Loss: 7.1411 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 243/1000 | Train Loss: 6.9204 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 244/1000 | Train Loss: 7.4801 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 245/1000 | Train Loss: 7.1036 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 246/1000 | Train Loss: 7.2495 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 247/1000 | Train Loss: 7.1356 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 248/1000 | Train Loss: 7.2170 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 249/1000 | Train Loss: 6.9785 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 250/1000 | Train Loss: 7.1350 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 251/1000 | Train Loss: 6.9083 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 252/1000 | Train Loss: 7.5347 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 253/1000 | Train Loss: 7.0231 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 254/1000 | Train Loss: 6.9385 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 255/1000 | Train Loss: 7.1742 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 256/1000 | Train Loss: 7.1401 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 257/1000 | Train Loss: 6.8957 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 258/1000 | Train Loss: 7.1069 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 259/1000 | Train Loss: 7.0374 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 260/1000 | Train Loss: 6.9584 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 261/1000 | Train Loss: 7.0753 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 262/1000 | Train Loss: 6.8904 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 263/1000 | Train Loss: 7.0105 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 264/1000 | Train Loss: 7.0285 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 265/1000 | Train Loss: 7.2878 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 266/1000 | Train Loss: 7.0537 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 267/1000 | Train Loss: 6.7067 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 268/1000 | Train Loss: 7.0942 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 269/1000 | Train Loss: 6.6674 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 270/1000 | Train Loss: 7.0439 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 271/1000 | Train Loss: 6.8564 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 272/1000 | Train Loss: 6.7398 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 273/1000 | Train Loss: 6.9317 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 274/1000 | Train Loss: 6.6569 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 275/1000 | Train Loss: 7.0055 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 276/1000 | Train Loss: 7.0212 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 277/1000 | Train Loss: 7.2775 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 278/1000 | Train Loss: 7.1238 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 279/1000 | Train Loss: 7.3412 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 280/1000 | Train Loss: 7.1355 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 281/1000 | Train Loss: 6.7904 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 282/1000 | Train Loss: 7.0303 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 283/1000 | Train Loss: 7.6327 | RUL L1: 0.0008 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 284/1000 | Train Loss: 6.7144 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 285/1000 | Train Loss: 7.2002 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 286/1000 | Train Loss: 6.7585 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 287/1000 | Train Loss: 7.0718 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 288/1000 | Train Loss: 7.0355 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 289/1000 | Train Loss: 6.9186 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 290/1000 | Train Loss: 7.0233 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 291/1000 | Train Loss: 7.1825 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 292/1000 | Train Loss: 7.0942 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 293/1000 | Train Loss: 6.9695 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 294/1000 | Train Loss: 7.0537 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 295/1000 | Train Loss: 6.8085 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 296/1000 | Train Loss: 6.9441 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 297/1000 | Train Loss: 7.1806 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 298/1000 | Train Loss: 6.4501 | RUL L1: 0.0006 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 299/1000 | Train Loss: 7.2817 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 300/1000 | Train Loss: 7.3247 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 301/1000 | Train Loss: 7.2852 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 302/1000 | Train Loss: 7.0414 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 303/1000 | Train Loss: 7.3653 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 304/1000 | Train Loss: 7.0067 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 305/1000 | Train Loss: 7.2020 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 306/1000 | Train Loss: 6.9501 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 307/1000 | Train Loss: 7.0854 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 308/1000 | Train Loss: 7.1292 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 309/1000 | Train Loss: 7.3070 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 310/1000 | Train Loss: 7.3927 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 311/1000 | Train Loss: 7.2032 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 312/1000 | Train Loss: 7.2186 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 313/1000 | Train Loss: 7.1823 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 314/1000 | Train Loss: 7.1197 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 315/1000 | Train Loss: 7.1184 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 316/1000 | Train Loss: 6.8314 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 317/1000 | Train Loss: 6.8478 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 318/1000 | Train Loss: 7.1981 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 319/1000 | Train Loss: 6.9808 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 320/1000 | Train Loss: 7.2607 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 321/1000 | Train Loss: 6.6928 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 322/1000 | Train Loss: 7.1897 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 323/1000 | Train Loss: 6.8615 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 324/1000 | Train Loss: 7.4406 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 325/1000 | Train Loss: 6.8609 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 326/1000 | Train Loss: 7.0671 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 327/1000 | Train Loss: 7.1756 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 328/1000 | Train Loss: 7.0419 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 329/1000 | Train Loss: 6.9820 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 330/1000 | Train Loss: 7.4591 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 331/1000 | Train Loss: 6.8729 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 332/1000 | Train Loss: 6.9174 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 333/1000 | Train Loss: 6.9053 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 334/1000 | Train Loss: 6.7776 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 335/1000 | Train Loss: 7.1293 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 336/1000 | Train Loss: 6.8437 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 337/1000 | Train Loss: 6.8262 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 338/1000 | Train Loss: 6.8129 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 339/1000 | Train Loss: 6.9218 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 340/1000 | Train Loss: 7.3725 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 341/1000 | Train Loss: 7.1402 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 342/1000 | Train Loss: 6.9481 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 343/1000 | Train Loss: 7.0955 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 344/1000 | Train Loss: 6.7853 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 345/1000 | Train Loss: 6.9244 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 346/1000 | Train Loss: 7.0639 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 347/1000 | Train Loss: 7.1842 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 348/1000 | Train Loss: 6.9781 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 349/1000 | Train Loss: 6.7640 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 350/1000 | Train Loss: 7.1112 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 351/1000 | Train Loss: 6.9213 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 352/1000 | Train Loss: 7.1551 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 353/1000 | Train Loss: 7.2216 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 354/1000 | Train Loss: 6.5955 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 355/1000 | Train Loss: 7.2142 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 356/1000 | Train Loss: 7.0138 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 357/1000 | Train Loss: 6.8982 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 358/1000 | Train Loss: 7.1340 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 359/1000 | Train Loss: 7.1485 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 360/1000 | Train Loss: 6.8817 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 361/1000 | Train Loss: 6.9536 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 362/1000 | Train Loss: 6.9575 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 363/1000 | Train Loss: 6.6808 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 364/1000 | Train Loss: 6.9926 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 365/1000 | Train Loss: 6.8425 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 366/1000 | Train Loss: 7.0096 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 367/1000 | Train Loss: 6.9559 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 368/1000 | Train Loss: 6.9217 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 369/1000 | Train Loss: 7.0472 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 370/1000 | Train Loss: 7.1143 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 371/1000 | Train Loss: 6.7867 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 372/1000 | Train Loss: 7.3338 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 373/1000 | Train Loss: 7.0485 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 374/1000 | Train Loss: 6.9974 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 375/1000 | Train Loss: 7.3644 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 376/1000 | Train Loss: 6.9206 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 377/1000 | Train Loss: 6.9365 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 378/1000 | Train Loss: 7.4464 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 379/1000 | Train Loss: 7.0129 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 380/1000 | Train Loss: 7.1279 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 381/1000 | Train Loss: 6.7825 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 382/1000 | Train Loss: 7.0289 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 383/1000 | Train Loss: 6.9083 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 384/1000 | Train Loss: 6.8421 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 385/1000 | Train Loss: 7.2527 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 386/1000 | Train Loss: 7.0353 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 387/1000 | Train Loss: 6.8472 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 388/1000 | Train Loss: 6.8661 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 389/1000 | Train Loss: 7.2698 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 390/1000 | Train Loss: 6.6961 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 391/1000 | Train Loss: 6.7718 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 392/1000 | Train Loss: 7.0845 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 393/1000 | Train Loss: 7.0370 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 394/1000 | Train Loss: 6.9079 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 395/1000 | Train Loss: 6.9339 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 396/1000 | Train Loss: 7.1618 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 397/1000 | Train Loss: 6.9833 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 398/1000 | Train Loss: 7.0409 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 399/1000 | Train Loss: 7.3153 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 400/1000 | Train Loss: 7.0923 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 401/1000 | Train Loss: 7.2170 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 402/1000 | Train Loss: 7.4181 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 403/1000 | Train Loss: 7.2729 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 404/1000 | Train Loss: 6.7005 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 405/1000 | Train Loss: 7.2392 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 406/1000 | Train Loss: 6.7770 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 407/1000 | Train Loss: 6.9997 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 408/1000 | Train Loss: 7.5166 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 409/1000 | Train Loss: 7.1610 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 410/1000 | Train Loss: 7.5540 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 411/1000 | Train Loss: 6.8827 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 412/1000 | Train Loss: 7.2409 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 413/1000 | Train Loss: 6.7490 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 414/1000 | Train Loss: 7.2993 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 415/1000 | Train Loss: 6.7922 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 416/1000 | Train Loss: 7.2319 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 417/1000 | Train Loss: 7.2227 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 418/1000 | Train Loss: 7.0450 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 419/1000 | Train Loss: 7.0646 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 420/1000 | Train Loss: 7.1612 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 421/1000 | Train Loss: 7.1143 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 422/1000 | Train Loss: 7.0757 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 423/1000 | Train Loss: 7.1465 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 424/1000 | Train Loss: 7.0248 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 425/1000 | Train Loss: 6.9088 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 426/1000 | Train Loss: 6.9472 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 427/1000 | Train Loss: 7.0876 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 428/1000 | Train Loss: 7.2038 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 429/1000 | Train Loss: 6.8981 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 430/1000 | Train Loss: 7.1594 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 431/1000 | Train Loss: 7.0930 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 432/1000 | Train Loss: 7.0263 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 433/1000 | Train Loss: 7.3971 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 434/1000 | Train Loss: 7.1099 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 435/1000 | Train Loss: 7.2753 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 436/1000 | Train Loss: 6.8705 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 437/1000 | Train Loss: 6.9672 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 438/1000 | Train Loss: 7.0716 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 439/1000 | Train Loss: 6.9216 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 440/1000 | Train Loss: 7.1120 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 441/1000 | Train Loss: 7.1117 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 442/1000 | Train Loss: 7.3344 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 443/1000 | Train Loss: 7.0174 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 444/1000 | Train Loss: 6.9980 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 445/1000 | Train Loss: 6.6889 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 446/1000 | Train Loss: 7.0922 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 447/1000 | Train Loss: 7.1234 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 448/1000 | Train Loss: 6.9367 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 449/1000 | Train Loss: 6.8995 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 450/1000 | Train Loss: 6.9549 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 451/1000 | Train Loss: 6.8286 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 452/1000 | Train Loss: 7.1562 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 453/1000 | Train Loss: 7.0091 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 454/1000 | Train Loss: 7.2160 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 455/1000 | Train Loss: 7.0843 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 456/1000 | Train Loss: 6.9649 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 457/1000 | Train Loss: 6.9155 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 458/1000 | Train Loss: 7.1512 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 459/1000 | Train Loss: 6.8673 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 460/1000 | Train Loss: 7.3932 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 461/1000 | Train Loss: 6.6020 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 462/1000 | Train Loss: 7.3484 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 463/1000 | Train Loss: 6.4881 | RUL L1: 0.0006 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 464/1000 | Train Loss: 7.0274 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 465/1000 | Train Loss: 6.8360 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 466/1000 | Train Loss: 6.6650 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 467/1000 | Train Loss: 6.9976 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 468/1000 | Train Loss: 7.0171 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 469/1000 | Train Loss: 6.6210 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 470/1000 | Train Loss: 6.9787 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 471/1000 | Train Loss: 7.2061 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 472/1000 | Train Loss: 6.9766 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 473/1000 | Train Loss: 7.0750 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 474/1000 | Train Loss: 6.8635 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 475/1000 | Train Loss: 7.0179 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 476/1000 | Train Loss: 7.0642 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 477/1000 | Train Loss: 6.6088 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 478/1000 | Train Loss: 7.0168 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 479/1000 | Train Loss: 6.6288 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 480/1000 | Train Loss: 6.8168 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 481/1000 | Train Loss: 7.1273 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 482/1000 | Train Loss: 6.7809 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 483/1000 | Train Loss: 7.2088 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 484/1000 | Train Loss: 7.2406 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 485/1000 | Train Loss: 6.9451 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 486/1000 | Train Loss: 7.2096 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 487/1000 | Train Loss: 7.1645 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 488/1000 | Train Loss: 7.0771 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 489/1000 | Train Loss: 6.9656 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 490/1000 | Train Loss: 7.2877 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 491/1000 | Train Loss: 7.0205 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 492/1000 | Train Loss: 6.8940 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 493/1000 | Train Loss: 6.8686 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 494/1000 | Train Loss: 7.0222 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 495/1000 | Train Loss: 7.1967 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 496/1000 | Train Loss: 6.8388 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 497/1000 | Train Loss: 7.4439 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 498/1000 | Train Loss: 7.0746 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 499/1000 | Train Loss: 7.0601 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 500/1000 | Train Loss: 7.0809 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 501/1000 | Train Loss: 7.0228 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 502/1000 | Train Loss: 6.9742 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 503/1000 | Train Loss: 7.1304 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 504/1000 | Train Loss: 7.0382 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 505/1000 | Train Loss: 7.1601 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 506/1000 | Train Loss: 6.9622 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 507/1000 | Train Loss: 7.2240 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 508/1000 | Train Loss: 7.0633 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 509/1000 | Train Loss: 7.2050 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 510/1000 | Train Loss: 6.6900 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 511/1000 | Train Loss: 7.0544 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 512/1000 | Train Loss: 7.2293 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 513/1000 | Train Loss: 7.3023 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 514/1000 | Train Loss: 6.9472 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 515/1000 | Train Loss: 7.1988 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 516/1000 | Train Loss: 6.8128 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 517/1000 | Train Loss: 7.2211 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 518/1000 | Train Loss: 7.3295 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 519/1000 | Train Loss: 6.9173 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 520/1000 | Train Loss: 7.1834 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 521/1000 | Train Loss: 7.1417 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 522/1000 | Train Loss: 7.2032 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 523/1000 | Train Loss: 7.0410 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 524/1000 | Train Loss: 7.2522 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 525/1000 | Train Loss: 7.0336 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 526/1000 | Train Loss: 7.4538 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 527/1000 | Train Loss: 7.0414 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 528/1000 | Train Loss: 7.0272 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 529/1000 | Train Loss: 7.4621 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 530/1000 | Train Loss: 6.9559 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 531/1000 | Train Loss: 6.7237 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 532/1000 | Train Loss: 6.8644 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 533/1000 | Train Loss: 6.8098 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 534/1000 | Train Loss: 7.2815 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 535/1000 | Train Loss: 6.7344 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 536/1000 | Train Loss: 7.0534 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 537/1000 | Train Loss: 7.0265 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 538/1000 | Train Loss: 6.8052 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 539/1000 | Train Loss: 6.9141 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 540/1000 | Train Loss: 6.8540 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 541/1000 | Train Loss: 7.1598 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 542/1000 | Train Loss: 7.0761 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 543/1000 | Train Loss: 7.2193 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 544/1000 | Train Loss: 6.9043 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 545/1000 | Train Loss: 7.0365 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 546/1000 | Train Loss: 6.9570 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 547/1000 | Train Loss: 6.7993 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 548/1000 | Train Loss: 7.1603 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 549/1000 | Train Loss: 6.8556 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 550/1000 | Train Loss: 7.3528 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 551/1000 | Train Loss: 7.1577 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 552/1000 | Train Loss: 6.9850 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 553/1000 | Train Loss: 6.8114 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 554/1000 | Train Loss: 7.0788 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 555/1000 | Train Loss: 7.3738 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 556/1000 | Train Loss: 6.8135 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 557/1000 | Train Loss: 6.8337 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 558/1000 | Train Loss: 7.0253 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 559/1000 | Train Loss: 6.9455 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 560/1000 | Train Loss: 7.1284 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 561/1000 | Train Loss: 6.8538 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 562/1000 | Train Loss: 6.9970 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 563/1000 | Train Loss: 7.2978 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 564/1000 | Train Loss: 6.9748 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 565/1000 | Train Loss: 7.3364 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 566/1000 | Train Loss: 6.7325 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 567/1000 | Train Loss: 7.1969 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 568/1000 | Train Loss: 6.9210 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 569/1000 | Train Loss: 6.8733 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 570/1000 | Train Loss: 6.9890 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 571/1000 | Train Loss: 7.3892 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 572/1000 | Train Loss: 6.9834 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 573/1000 | Train Loss: 7.1465 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 574/1000 | Train Loss: 6.9495 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 575/1000 | Train Loss: 7.0490 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 576/1000 | Train Loss: 7.0766 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 577/1000 | Train Loss: 7.0251 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 578/1000 | Train Loss: 6.9025 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 579/1000 | Train Loss: 6.9627 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 580/1000 | Train Loss: 7.1668 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 581/1000 | Train Loss: 7.3050 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 582/1000 | Train Loss: 6.7870 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 583/1000 | Train Loss: 7.0987 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 584/1000 | Train Loss: 7.1780 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 585/1000 | Train Loss: 7.0148 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 586/1000 | Train Loss: 7.2233 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 587/1000 | Train Loss: 7.1362 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 588/1000 | Train Loss: 6.9833 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 589/1000 | Train Loss: 7.0531 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 590/1000 | Train Loss: 6.8672 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 591/1000 | Train Loss: 7.1622 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 592/1000 | Train Loss: 7.0815 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 593/1000 | Train Loss: 7.2135 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 594/1000 | Train Loss: 6.9165 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 595/1000 | Train Loss: 7.2288 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 596/1000 | Train Loss: 7.5005 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 597/1000 | Train Loss: 6.8744 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 598/1000 | Train Loss: 6.9553 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 599/1000 | Train Loss: 7.2084 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 600/1000 | Train Loss: 7.3439 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 601/1000 | Train Loss: 7.2606 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 602/1000 | Train Loss: 7.0870 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 603/1000 | Train Loss: 7.1844 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 604/1000 | Train Loss: 6.9319 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 605/1000 | Train Loss: 6.8474 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 606/1000 | Train Loss: 6.6947 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 607/1000 | Train Loss: 7.0444 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 608/1000 | Train Loss: 6.8352 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 609/1000 | Train Loss: 7.7392 | RUL L1: 0.0008 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 610/1000 | Train Loss: 7.0016 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 611/1000 | Train Loss: 7.0143 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 612/1000 | Train Loss: 6.9052 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 613/1000 | Train Loss: 7.2372 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 614/1000 | Train Loss: 7.3246 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 615/1000 | Train Loss: 6.9363 | RUL L1: 0.0007 | Val Loss: 10.5896 | Val RMSE: 0.0459\n",
      "Epoch 616/1000 | Train Loss: 7.0173 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 617/1000 | Train Loss: 7.0654 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 618/1000 | Train Loss: 7.0632 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 619/1000 | Train Loss: 6.7731 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 620/1000 | Train Loss: 7.1627 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 621/1000 | Train Loss: 7.1120 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 622/1000 | Train Loss: 7.0218 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 623/1000 | Train Loss: 7.3866 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 624/1000 | Train Loss: 7.0952 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 625/1000 | Train Loss: 7.1558 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 626/1000 | Train Loss: 6.7675 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 627/1000 | Train Loss: 7.2944 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 628/1000 | Train Loss: 7.3476 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 629/1000 | Train Loss: 6.9911 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 630/1000 | Train Loss: 7.0473 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 631/1000 | Train Loss: 6.9921 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 632/1000 | Train Loss: 7.1262 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 633/1000 | Train Loss: 6.8012 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 634/1000 | Train Loss: 7.1089 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 635/1000 | Train Loss: 7.2280 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 636/1000 | Train Loss: 7.0570 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 637/1000 | Train Loss: 6.7790 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 638/1000 | Train Loss: 7.3146 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 639/1000 | Train Loss: 7.2291 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 640/1000 | Train Loss: 7.1738 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 641/1000 | Train Loss: 6.8602 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 642/1000 | Train Loss: 6.7886 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 643/1000 | Train Loss: 7.0689 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 644/1000 | Train Loss: 6.6685 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 645/1000 | Train Loss: 6.8617 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 646/1000 | Train Loss: 6.8971 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 647/1000 | Train Loss: 7.0804 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 648/1000 | Train Loss: 7.2410 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 649/1000 | Train Loss: 7.2969 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 650/1000 | Train Loss: 7.0650 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 651/1000 | Train Loss: 7.0701 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 652/1000 | Train Loss: 7.0937 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 653/1000 | Train Loss: 6.9557 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 654/1000 | Train Loss: 6.6034 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 655/1000 | Train Loss: 7.0925 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 656/1000 | Train Loss: 7.1533 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 657/1000 | Train Loss: 6.9911 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 658/1000 | Train Loss: 7.0695 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 659/1000 | Train Loss: 6.9026 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 660/1000 | Train Loss: 7.0738 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 661/1000 | Train Loss: 7.3941 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 662/1000 | Train Loss: 7.0213 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 663/1000 | Train Loss: 7.4837 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 664/1000 | Train Loss: 7.1074 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 665/1000 | Train Loss: 7.0201 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 666/1000 | Train Loss: 7.2888 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 667/1000 | Train Loss: 7.1570 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 668/1000 | Train Loss: 7.4575 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 669/1000 | Train Loss: 6.9599 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 670/1000 | Train Loss: 7.1617 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 671/1000 | Train Loss: 7.1147 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 672/1000 | Train Loss: 7.2362 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 673/1000 | Train Loss: 6.8018 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 674/1000 | Train Loss: 6.8813 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 675/1000 | Train Loss: 7.0574 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 676/1000 | Train Loss: 7.0877 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 677/1000 | Train Loss: 7.0518 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 678/1000 | Train Loss: 7.1548 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 679/1000 | Train Loss: 6.8829 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 680/1000 | Train Loss: 6.9089 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 681/1000 | Train Loss: 6.9086 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 682/1000 | Train Loss: 7.3685 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 683/1000 | Train Loss: 6.9037 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 684/1000 | Train Loss: 6.7357 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 685/1000 | Train Loss: 6.7636 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 686/1000 | Train Loss: 7.0990 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 687/1000 | Train Loss: 7.0481 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 688/1000 | Train Loss: 6.8495 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 689/1000 | Train Loss: 7.1618 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 690/1000 | Train Loss: 6.9348 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 691/1000 | Train Loss: 6.7216 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 692/1000 | Train Loss: 6.7672 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 693/1000 | Train Loss: 7.0929 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 694/1000 | Train Loss: 7.3424 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 695/1000 | Train Loss: 7.3128 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 696/1000 | Train Loss: 7.1793 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 697/1000 | Train Loss: 6.8126 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 698/1000 | Train Loss: 7.0738 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 699/1000 | Train Loss: 7.2998 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 700/1000 | Train Loss: 6.6782 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 701/1000 | Train Loss: 6.9770 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 702/1000 | Train Loss: 6.8956 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 703/1000 | Train Loss: 6.8539 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 704/1000 | Train Loss: 7.0979 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 705/1000 | Train Loss: 7.0885 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 706/1000 | Train Loss: 7.2332 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 707/1000 | Train Loss: 6.5868 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 708/1000 | Train Loss: 7.0612 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 709/1000 | Train Loss: 6.8374 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 710/1000 | Train Loss: 6.8186 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 711/1000 | Train Loss: 6.9599 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 712/1000 | Train Loss: 7.2422 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 713/1000 | Train Loss: 6.9963 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 714/1000 | Train Loss: 7.1958 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 715/1000 | Train Loss: 7.1300 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 716/1000 | Train Loss: 7.0240 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 717/1000 | Train Loss: 7.1305 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 718/1000 | Train Loss: 7.1112 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 719/1000 | Train Loss: 6.9303 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 720/1000 | Train Loss: 7.4562 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 721/1000 | Train Loss: 7.1616 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 722/1000 | Train Loss: 6.8895 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 723/1000 | Train Loss: 6.9157 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 724/1000 | Train Loss: 7.2264 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 725/1000 | Train Loss: 7.3976 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 726/1000 | Train Loss: 6.7809 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 727/1000 | Train Loss: 7.1125 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 728/1000 | Train Loss: 7.2500 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 729/1000 | Train Loss: 7.3649 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 730/1000 | Train Loss: 7.3273 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 731/1000 | Train Loss: 7.0434 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 732/1000 | Train Loss: 7.0754 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 733/1000 | Train Loss: 7.1921 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 734/1000 | Train Loss: 6.9173 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 735/1000 | Train Loss: 7.2375 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 736/1000 | Train Loss: 7.1222 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 737/1000 | Train Loss: 6.9541 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 738/1000 | Train Loss: 6.8564 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 739/1000 | Train Loss: 7.1621 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 740/1000 | Train Loss: 6.9782 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 741/1000 | Train Loss: 7.3909 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 742/1000 | Train Loss: 7.4237 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 743/1000 | Train Loss: 6.9320 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 744/1000 | Train Loss: 7.0589 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 745/1000 | Train Loss: 7.5443 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 746/1000 | Train Loss: 7.2525 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 747/1000 | Train Loss: 7.0553 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 748/1000 | Train Loss: 7.1286 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 749/1000 | Train Loss: 6.8272 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 750/1000 | Train Loss: 6.8852 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 751/1000 | Train Loss: 6.8819 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 752/1000 | Train Loss: 7.0963 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 753/1000 | Train Loss: 7.2404 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 754/1000 | Train Loss: 7.1580 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 755/1000 | Train Loss: 7.4190 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 756/1000 | Train Loss: 7.3105 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 757/1000 | Train Loss: 7.1352 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 758/1000 | Train Loss: 7.4526 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 759/1000 | Train Loss: 7.1537 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 760/1000 | Train Loss: 6.9078 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 761/1000 | Train Loss: 6.8412 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 762/1000 | Train Loss: 7.0997 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 763/1000 | Train Loss: 6.8362 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 764/1000 | Train Loss: 7.1196 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 765/1000 | Train Loss: 7.0876 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 766/1000 | Train Loss: 7.0353 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 767/1000 | Train Loss: 6.9734 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 768/1000 | Train Loss: 7.2511 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 769/1000 | Train Loss: 7.2844 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 770/1000 | Train Loss: 6.8517 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 771/1000 | Train Loss: 6.7474 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 772/1000 | Train Loss: 6.6828 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 773/1000 | Train Loss: 7.0805 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 774/1000 | Train Loss: 7.2482 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 775/1000 | Train Loss: 7.1619 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 776/1000 | Train Loss: 7.0357 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 777/1000 | Train Loss: 7.4024 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 778/1000 | Train Loss: 7.0945 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 779/1000 | Train Loss: 7.0046 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 780/1000 | Train Loss: 7.3952 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 781/1000 | Train Loss: 6.9852 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 782/1000 | Train Loss: 6.9123 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 783/1000 | Train Loss: 7.3491 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 784/1000 | Train Loss: 7.0775 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 785/1000 | Train Loss: 7.2212 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 786/1000 | Train Loss: 6.8968 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 787/1000 | Train Loss: 6.9936 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 788/1000 | Train Loss: 7.3045 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 789/1000 | Train Loss: 6.9870 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 790/1000 | Train Loss: 6.7006 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 791/1000 | Train Loss: 7.1233 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 792/1000 | Train Loss: 7.0178 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 793/1000 | Train Loss: 6.4984 | RUL L1: 0.0006 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 794/1000 | Train Loss: 7.2438 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 795/1000 | Train Loss: 7.0196 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 796/1000 | Train Loss: 7.1225 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 797/1000 | Train Loss: 6.8900 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 798/1000 | Train Loss: 7.1999 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 799/1000 | Train Loss: 7.2506 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 800/1000 | Train Loss: 7.2095 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 801/1000 | Train Loss: 7.0491 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 802/1000 | Train Loss: 7.1420 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 803/1000 | Train Loss: 6.8719 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 804/1000 | Train Loss: 7.2708 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 805/1000 | Train Loss: 6.8357 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 806/1000 | Train Loss: 6.8639 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 807/1000 | Train Loss: 7.0189 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 808/1000 | Train Loss: 6.8603 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 809/1000 | Train Loss: 7.1999 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 810/1000 | Train Loss: 7.0366 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 811/1000 | Train Loss: 7.0295 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 812/1000 | Train Loss: 7.4825 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 813/1000 | Train Loss: 6.8402 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 814/1000 | Train Loss: 6.9721 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 815/1000 | Train Loss: 6.9631 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 816/1000 | Train Loss: 6.9086 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 817/1000 | Train Loss: 7.0315 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 818/1000 | Train Loss: 6.9114 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 819/1000 | Train Loss: 7.4319 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 820/1000 | Train Loss: 6.7907 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 821/1000 | Train Loss: 7.1509 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 822/1000 | Train Loss: 7.1254 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 823/1000 | Train Loss: 7.0117 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 824/1000 | Train Loss: 6.5938 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 825/1000 | Train Loss: 7.1983 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 826/1000 | Train Loss: 7.0010 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 827/1000 | Train Loss: 6.5969 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 828/1000 | Train Loss: 7.2939 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 829/1000 | Train Loss: 6.7577 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 830/1000 | Train Loss: 6.9474 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 831/1000 | Train Loss: 6.9460 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 832/1000 | Train Loss: 6.7302 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 833/1000 | Train Loss: 6.7912 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 834/1000 | Train Loss: 6.7130 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 835/1000 | Train Loss: 6.8207 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 836/1000 | Train Loss: 6.8331 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 837/1000 | Train Loss: 7.3590 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 838/1000 | Train Loss: 6.9835 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 839/1000 | Train Loss: 7.0842 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 840/1000 | Train Loss: 6.6692 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 841/1000 | Train Loss: 7.1647 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 842/1000 | Train Loss: 7.2364 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 843/1000 | Train Loss: 7.2040 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 844/1000 | Train Loss: 7.2409 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 845/1000 | Train Loss: 7.1156 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 846/1000 | Train Loss: 7.0496 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 847/1000 | Train Loss: 6.9078 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 848/1000 | Train Loss: 6.9798 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 849/1000 | Train Loss: 6.9457 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 850/1000 | Train Loss: 7.2142 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 851/1000 | Train Loss: 6.8412 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 852/1000 | Train Loss: 6.9646 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 853/1000 | Train Loss: 7.7165 | RUL L1: 0.0008 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 854/1000 | Train Loss: 7.4065 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 855/1000 | Train Loss: 6.9659 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 856/1000 | Train Loss: 6.8312 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 857/1000 | Train Loss: 6.8981 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 858/1000 | Train Loss: 7.1363 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 859/1000 | Train Loss: 7.0112 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 860/1000 | Train Loss: 6.7851 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 861/1000 | Train Loss: 6.9384 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 862/1000 | Train Loss: 7.2886 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 863/1000 | Train Loss: 7.1659 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 864/1000 | Train Loss: 7.3509 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 865/1000 | Train Loss: 6.9590 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 866/1000 | Train Loss: 6.7950 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 867/1000 | Train Loss: 6.9140 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 868/1000 | Train Loss: 7.2439 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 869/1000 | Train Loss: 6.7942 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 870/1000 | Train Loss: 7.3656 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 871/1000 | Train Loss: 7.2273 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 872/1000 | Train Loss: 7.2746 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 873/1000 | Train Loss: 7.1144 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 874/1000 | Train Loss: 7.0116 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 875/1000 | Train Loss: 6.9386 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 876/1000 | Train Loss: 7.0847 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 877/1000 | Train Loss: 7.0329 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 878/1000 | Train Loss: 6.8992 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 879/1000 | Train Loss: 6.7577 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 880/1000 | Train Loss: 7.1482 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 881/1000 | Train Loss: 7.1768 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 882/1000 | Train Loss: 7.1799 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 883/1000 | Train Loss: 7.3739 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 884/1000 | Train Loss: 6.9343 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 885/1000 | Train Loss: 7.1086 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 886/1000 | Train Loss: 7.0005 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 887/1000 | Train Loss: 6.7879 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 888/1000 | Train Loss: 6.8451 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 889/1000 | Train Loss: 6.9888 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 890/1000 | Train Loss: 7.4518 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 891/1000 | Train Loss: 6.7579 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 892/1000 | Train Loss: 6.9060 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 893/1000 | Train Loss: 6.8247 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 894/1000 | Train Loss: 7.2292 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 895/1000 | Train Loss: 7.0963 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 896/1000 | Train Loss: 6.9300 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 897/1000 | Train Loss: 7.0092 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 898/1000 | Train Loss: 7.0156 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 899/1000 | Train Loss: 6.9833 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 900/1000 | Train Loss: 6.8745 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 901/1000 | Train Loss: 6.8762 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 902/1000 | Train Loss: 7.1036 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 903/1000 | Train Loss: 7.1935 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 904/1000 | Train Loss: 7.0405 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 905/1000 | Train Loss: 7.1418 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 906/1000 | Train Loss: 6.8797 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 907/1000 | Train Loss: 7.2941 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 908/1000 | Train Loss: 6.7851 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 909/1000 | Train Loss: 6.6193 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 910/1000 | Train Loss: 6.8794 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 911/1000 | Train Loss: 7.0261 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 912/1000 | Train Loss: 7.1088 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 913/1000 | Train Loss: 7.0696 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 914/1000 | Train Loss: 7.1962 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 915/1000 | Train Loss: 7.0789 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 916/1000 | Train Loss: 6.9199 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 917/1000 | Train Loss: 6.8702 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 918/1000 | Train Loss: 7.0634 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 919/1000 | Train Loss: 6.8033 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 920/1000 | Train Loss: 6.4717 | RUL L1: 0.0006 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 921/1000 | Train Loss: 7.0280 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 922/1000 | Train Loss: 7.1494 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 923/1000 | Train Loss: 6.8058 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 924/1000 | Train Loss: 6.8563 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 925/1000 | Train Loss: 7.0034 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 926/1000 | Train Loss: 7.2727 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 927/1000 | Train Loss: 7.0549 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 928/1000 | Train Loss: 7.1048 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 929/1000 | Train Loss: 7.2549 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 930/1000 | Train Loss: 7.0623 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 931/1000 | Train Loss: 7.0416 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 932/1000 | Train Loss: 7.2051 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 933/1000 | Train Loss: 7.0908 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 934/1000 | Train Loss: 7.1730 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 935/1000 | Train Loss: 6.9980 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 936/1000 | Train Loss: 7.0606 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 937/1000 | Train Loss: 7.0330 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 938/1000 | Train Loss: 7.1690 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 939/1000 | Train Loss: 7.0305 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 940/1000 | Train Loss: 6.9024 | RUL L1: 0.0007 | Val Loss: 10.5897 | Val RMSE: 0.0459\n",
      "Epoch 941/1000 | Train Loss: 6.5784 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 942/1000 | Train Loss: 7.3149 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 943/1000 | Train Loss: 6.6030 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 944/1000 | Train Loss: 6.9959 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 945/1000 | Train Loss: 6.8734 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 946/1000 | Train Loss: 7.1326 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 947/1000 | Train Loss: 7.0663 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 948/1000 | Train Loss: 6.9324 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 949/1000 | Train Loss: 7.3049 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 950/1000 | Train Loss: 7.2615 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 951/1000 | Train Loss: 6.9604 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 952/1000 | Train Loss: 7.1021 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 953/1000 | Train Loss: 6.9700 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 954/1000 | Train Loss: 7.3480 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 955/1000 | Train Loss: 7.1386 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 956/1000 | Train Loss: 6.8259 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 957/1000 | Train Loss: 7.0340 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 958/1000 | Train Loss: 6.9292 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 959/1000 | Train Loss: 6.8419 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 960/1000 | Train Loss: 6.8527 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 961/1000 | Train Loss: 7.1006 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 962/1000 | Train Loss: 7.2426 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 963/1000 | Train Loss: 6.9879 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 964/1000 | Train Loss: 7.3108 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 965/1000 | Train Loss: 6.9499 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 966/1000 | Train Loss: 7.1278 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 967/1000 | Train Loss: 7.2590 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 968/1000 | Train Loss: 6.8844 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 969/1000 | Train Loss: 7.1278 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 970/1000 | Train Loss: 7.2684 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 971/1000 | Train Loss: 6.9732 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 972/1000 | Train Loss: 7.0795 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 973/1000 | Train Loss: 6.5952 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 974/1000 | Train Loss: 6.7635 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 975/1000 | Train Loss: 6.8326 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 976/1000 | Train Loss: 7.1355 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 977/1000 | Train Loss: 6.7992 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 978/1000 | Train Loss: 6.9417 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 979/1000 | Train Loss: 6.8179 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 980/1000 | Train Loss: 7.1473 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 981/1000 | Train Loss: 7.1910 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 982/1000 | Train Loss: 6.7975 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 983/1000 | Train Loss: 6.7417 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 984/1000 | Train Loss: 7.3915 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 985/1000 | Train Loss: 6.8564 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 986/1000 | Train Loss: 7.1768 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 987/1000 | Train Loss: 7.0652 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 988/1000 | Train Loss: 7.0189 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 989/1000 | Train Loss: 7.3218 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 990/1000 | Train Loss: 6.6399 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 991/1000 | Train Loss: 6.9824 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 992/1000 | Train Loss: 7.0265 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 993/1000 | Train Loss: 7.1978 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 994/1000 | Train Loss: 6.8687 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 995/1000 | Train Loss: 7.0122 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 996/1000 | Train Loss: 6.9722 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 997/1000 | Train Loss: 6.9010 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 998/1000 | Train Loss: 7.3316 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 999/1000 | Train Loss: 7.1942 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "Epoch 1000/1000 | Train Loss: 6.9724 | RUL L1: 0.0007 | Val Loss: 10.5898 | Val RMSE: 0.0459\n",
      "\n",
      "--- Running Final Test ---\n",
      "üèÜ Final Test RMSE for Window 100: 0.0377\n"
     ]
    }
   ],
   "source": [
    "data_ranges = ['all','low','mid','high']\n",
    "\n",
    "train_moe_model(WINDOW_SIZES,*datasets['all'],'best_lstm_model-window-100_model_pinn_data_low.pth','best_lstm_model-window-100_model_pinn_data_mid.pth','best_lstm_model-window-100_model_pinn_data_high.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "248363c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:55.463082Z",
     "iopub.status.busy": "2026-02-08T15:20:55.462769Z",
     "iopub.status.idle": "2026-02-08T15:20:55.974069Z",
     "shell.execute_reply": "2026-02-08T15:20:55.973319Z"
    },
    "papermill": {
     "duration": 0.729685,
     "end_time": "2026-02-08T15:20:55.975589",
     "exception": false,
     "start_time": "2026-02-08T15:20:55.245904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Starting Testing Inference...\n",
      "\n",
      "üèÜ Final Test RMSE: 0.0377\n",
      "üìâ Final Test MAE:  0.0323\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Sample Breakdown (Interpretability Check):\n",
      "True RUL   | Pred RUL   | Low W    | Mid W    | High W   | Decision\n",
      "---------------------------------------------------------------------------\n",
      "0.13       | 0.19       | 0.17     | 0.53     | 0.30     | Mid\n",
      "0.19       | 0.19       | 0.17     | 0.53     | 0.30     | Mid\n",
      "0.15       | 0.19       | 0.17     | 0.53     | 0.30     | Mid\n",
      "0.24       | 0.19       | 0.17     | 0.53     | 0.30     | Mid\n",
      "0.17       | 0.19       | 0.17     | 0.53     | 0.30     | Mid\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "def load_base_model(path, input_size):\n",
    "    # Ensure this matches your generic PhysicsInformedLSTM class definition\n",
    "    model = PhysicsInformedLSTM(input_size=input_size) \n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    return model\n",
    "        \n",
    "def test_and_visualize(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Storage for analysis\n",
    "    all_true_rul = []\n",
    "    all_pred_rul = []\n",
    "    all_weights = [] # To store [w_low, w_mid, w_high]\n",
    "    all_soh = []     # To see at what SoH the model switches experts\n",
    "    \n",
    "    print(\"\\nüîç Starting Testing Inference...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, _, y_rul_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_rul_batch = y_rul_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_rul, k, a, b, weights = model(X_batch)\n",
    "            \n",
    "            # Store results (Move to CPU for plotting)\n",
    "            all_true_rul.extend(y_rul_batch.cpu().numpy().flatten())\n",
    "            all_pred_rul.extend(pred_rul.cpu().numpy().flatten())\n",
    "            all_weights.extend(weights.cpu().numpy())\n",
    "            \n",
    "            # Store the current SoH (last step of input window)\n",
    "            # Assuming SoH is feature 0\n",
    "            current_soh = X_batch[:, -1, 0].cpu().numpy()\n",
    "            all_soh.extend(current_soh)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_true_rul = np.array(all_true_rul)\n",
    "    all_pred_rul = np.array(all_pred_rul)\n",
    "    all_weights = np.array(all_weights) # Shape: [N_samples, 3]\n",
    "    all_soh = np.array(all_soh)\n",
    "\n",
    "    # --- 1. Calculate Metrics ---\n",
    "    rmse = np.sqrt(mean_squared_error(all_true_rul, all_pred_rul))\n",
    "    mae = mean_absolute_error(all_true_rul, all_pred_rul)\n",
    "    print(f\"\\nüèÜ Final Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"üìâ Final Test MAE:  {mae:.4f}\")\n",
    "\n",
    "    # --- 2. Visualization: Prediction vs Actual ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(all_true_rul, all_pred_rul, alpha=0.5, color='blue', label='Predictions')\n",
    "    plt.plot([min(all_true_rul), max(all_true_rul)], [min(all_true_rul), max(all_true_rul)], 'r--', lw=2, label='Ideal')\n",
    "    plt.xlabel(\"True RUL\")\n",
    "    plt.ylabel(\"Predicted RUL\")\n",
    "    plt.title(f\"RUL Prediction Accuracy (RMSE: {rmse:.2f})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 3. Visualization: Expert Selection (The \"Brain\" of the MoE) ---\n",
    "    # We sort by RUL to see how expert usage changes as battery degrades\n",
    "    sort_idx = np.argsort(all_true_rul)\n",
    "    sorted_rul = all_true_rul[sort_idx]\n",
    "    sorted_weights = all_weights[sort_idx]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.stackplot(sorted_rul, \n",
    "                  sorted_weights[:, 0], # Low Expert\n",
    "                  sorted_weights[:, 1], # Mid Expert\n",
    "                  sorted_weights[:, 2], # High Expert\n",
    "                  labels=['Low Expert', 'Mid Expert', 'High Expert'],\n",
    "                  colors=['#ff9999', '#66b3ff', '#99ff99'], \n",
    "                  alpha=0.8)\n",
    "    \n",
    "    plt.xlabel(\"Remaining Useful Life (RUL)\")\n",
    "    plt.ylabel(\"Gate Weight (Confidence)\")\n",
    "    plt.title(\"Mixture of Experts: Which Model is Active?\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.gca().invert_xaxis() # High RUL on left (Fresh), Low RUL on right (Dead)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 4. Deep Dive: Inspect 5 Random Samples ---\n",
    "    print(\"\\nüî¨ Sample Breakdown (Interpretability Check):\")\n",
    "    indices = np.random.choice(len(all_true_rul), 5, replace=True)\n",
    "    \n",
    "    print(f\"{'True RUL':<10} | {'Pred RUL':<10} | {'Low W':<8} | {'Mid W':<8} | {'High W':<8} | {'Decision'}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for idx in indices:\n",
    "        t_rul = all_true_rul[idx]\n",
    "        p_rul = all_pred_rul[idx]\n",
    "        w = all_weights[idx]\n",
    "        \n",
    "        # Determine dominant expert\n",
    "        expert_names = ['Low', 'Mid', 'High']\n",
    "        dominant = expert_names[np.argmax(w)]\n",
    "        \n",
    "        print(f\"{t_rul:<10.2f} | {p_rul:<10.2f} | {w[0]:<8.2f} | {w[1]:<8.2f} | {w[2]:<8.2f} | {dominant}\")\n",
    "\n",
    "# --- How to Run It ---\n",
    "# 1. Load your best saved model structure\n",
    "# (Ensure expert_low/mid/high are loaded as done in training)\n",
    "WINDOW_SIZE =100\n",
    "expert_low, expert_mid, expert_high = load_base_model('best_lstm_model-window-100_model_pinn_data_low.pth', INPUT_SIZE),load_base_model('best_lstm_model-window-100_model_pinn_data_mid.pth', INPUT_SIZE),load_base_model('best_lstm_model-window-100_model_pinn_data_high.pth', INPUT_SIZE)\n",
    "model = TrainableGompertzMoE(expert_low, expert_mid, expert_high, input_dim=1, window_size=WINDOW_SIZE).to(device)\n",
    "model.load_state_dict(torch.load(f\"best_moe_model_window_{WINDOW_SIZE}.pth\"))\n",
    "\n",
    "\n",
    "\n",
    "# test_loader  = give_paths_get_loaders(test_paths, WINDOW_SIZE, shuffle=False)\n",
    "# 2. Run the test\n",
    "test_and_visualize(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3178d522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:56.499067Z",
     "iopub.status.busy": "2026-02-08T15:20:56.498762Z",
     "iopub.status.idle": "2026-02-08T15:20:56.663551Z",
     "shell.execute_reply": "2026-02-08T15:20:56.662648Z"
    },
    "papermill": {
     "duration": 0.467846,
     "end_time": "2026-02-08T15:20:56.665332",
     "exception": false,
     "start_time": "2026-02-08T15:20:56.197486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_lstm_model-window-100_model_pinn_data_all.pth\r\n",
      "best_lstm_model-window-100_model_pinn_data_high.pth\r\n",
      "best_lstm_model-window-100_model_pinn_data_low.pth\r\n",
      "best_lstm_model-window-100_model_pinn_data_mid.pth\r\n",
      "best_moe_model_window_100.pth\r\n",
      "history-from-3-window_99_model_pinn.png\r\n",
      "history-full-log-window_99_model_pinn.png\r\n",
      "history-full-window_99_model_pinn.png\r\n",
      "last_model_window_100_model_pinn_data_all.pth\r\n",
      "last_model_window_100_model_pinn_data_high.pth\r\n",
      "last_model_window_100_model_pinn_data_low.pth\r\n",
      "last_model_window_100_model_pinn_data_mid.pth\r\n",
      "__notebook__.ipynb\r\n",
      "training_metrics__window_99_model_pinn.npz\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b472a01b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:57.119915Z",
     "iopub.status.busy": "2026-02-08T15:20:57.119147Z",
     "iopub.status.idle": "2026-02-08T15:20:57.122930Z",
     "shell.execute_reply": "2026-02-08T15:20:57.122315Z"
    },
    "papermill": {
     "duration": 0.232536,
     "end_time": "2026-02-08T15:20:57.124333",
     "exception": false,
     "start_time": "2026-02-08T15:20:56.891797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "098b3c5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T15:20:57.564262Z",
     "iopub.status.busy": "2026-02-08T15:20:57.563936Z",
     "iopub.status.idle": "2026-02-08T15:20:57.567167Z",
     "shell.execute_reply": "2026-02-08T15:20:57.566535Z"
    },
    "papermill": {
     "duration": 0.22552,
     "end_time": "2026-02-08T15:20:57.568541",
     "exception": false,
     "start_time": "2026-02-08T15:20:57.343021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for X_batch, _, y_rul_batch in test_loader:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef12ca",
   "metadata": {
    "papermill": {
     "duration": 0.244789,
     "end_time": "2026-02-08T15:20:58.037133",
     "exception": false,
     "start_time": "2026-02-08T15:20:57.792344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5945289,
     "sourceId": 12908579,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 287936110,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8645.330851,
   "end_time": "2026-02-08T15:21:01.437115",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-08T12:56:56.106264",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
